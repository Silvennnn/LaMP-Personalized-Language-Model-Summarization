410	41065	Very Sparse Stable Random Projections, Estimators and Tail Bounds for Stable Random Projections	Stable random projections are widely used in data streaming, data mining, and machine learning. They provide a unified and efficient method for approximating norms and distances between data streams. This paper focuses on improving stable random projections in three ways. Firstly, by using very sparse projections, the processing and storage costs can be significantly reduced. This is achieved by replacing the α-stable distribution with a mixture of a symmetric α-Pareto distribution and a point mass at the origin. The rate of convergence is analyzed as a function of the parameters β, α, and data regularity. For example, when α = 1 and data has bounded second moments, a small value of β can lead to a fast convergence rate.
411	41187	On cognitive mechanism of the eyes: the sensor vs. the browser of the brain	This paper discusses the role of the eyes in neuropsychology, specifically as a sensory organ and a cognitive browser. While it is known that the eyes contribute to 70% of sensory input to the brain, their function as a browser for memory and cognition has not been fully recognized. The paper argues that the eyes serve as a bi-directional organ, both sensing external stimuli and providing internal perceptual abilities such as consciousness, memory, motivation, and emotion. This sheds light on the important role of the eyes in the thinking process and has practical implications for understanding various cognitive mechanisms. 
412	41268	Joint code-encoder-decoder design for LDPC coding system VLSI implementation	This paper discusses a design approach for implementing low-density parity-check (LDPC) coding systems using both irregular LDPC code construction and VLSI implementations of encoder and decoder. The goal is to construct efficient LDPC codes that can be easily implemented in hardware. To achieve this, a heuristic algorithm is proposed for constructing implementation-aware irregular LDPC codes that can provide strong error correction. The paper also presents the corresponding hardware architectures for the encoder and decoder. This approach offers a promising solution for implementing LDPC coding systems in hardware with high performance and low complexity.
413	41392	Efficient Algorithms and Implementations for Optimizing the Sum of Linear Fractional Functions, with Applications	The paper introduces a new algorithm for solving the sum of linear fractional functions (SOLF) problem in 1-D and 2-D. The algorithm focuses on solving the off-line ratio query (OLRQ) problem, which involves finding optimal values for a sequence of linear fractional functions subject to linear constraints. By utilizing geometric properties and parametric linear programming, the algorithm can solve the OLRQ problem in O((m+n)log (m+n)) time. This can significantly speed up iterations in a known iterative SOLF algorithm, improving the overall efficiency in solving the SOLF problem. The algorithm has been tested and shown to outperform other commonly-used approaches in most cases. It has also been successfully applied to problems in computational geometry and other fields, improving upon previous results.
414	41467	Network Coding-Based Broadcast In Mobile Ad Hoc Networks	In multi-hop wireless networks, broadcast operation is crucial for disseminating information to all nodes. Previous methods for broadcast support have been either probabilistic (with nodes randomly rebroadcasting packets) or deterministic (with nodes pre-selecting specific neighbors for rebroadcasting). This paper introduces the use of network-coding in deterministic approaches, resulting in a significant reduction in the number of transmissions. Two algorithms are proposed, using local topology information and opportunistic listening: 1) a simple XOR-based coding algorithm with up to 45% gains compared to non-coding, and 2) a Reed-Solomon based algorithm with up to 61% gains in simulations. This coding-based deterministic approach outperforms a previously proposed coding-based probabilistic approach.
415	41518	The role of the velocity-slowness mapping in fan filtering of image sequences	A new method for designing fan filters for moving objects has been introduced, using a mapping from velocity sets to slowness sets. The main challenge in using fan filters for image sequences is due to the lack of one-to-one mapping between velocity and slowness, making it difficult to specify filter responses at all velocities. However, the fan filter shows potential for being a useful tool in analyzing image sequences. The strengths and limitations of this approach are discussed.
416	41678	Beamsteering on Mobile Devices: Network Capacity and Client Efficiency	In this study, the focus is on the use of beamsteering technology to improve the efficiency and capacity of wireless networks for mobile devices. The current omni directional communication limits energy efficiency and causes interference, so the researchers propose using beamsteering to make mobile devices directional. This approach is feasible for devices such as Netbooks and eBook readers and can lead to a balance between device efficiency and network capacity. The team developed a distributed algorithm, called BeamAdapt, to help mobile clients reach their optimal operating point without central coordination. Simulations show that using BeamAdapt can significantly reduce power consumption while maintaining network throughput in large-scale networks.
417	41728	IQ estimation for accurate time-series classification	Time-series classification is an important research topic in data mining and computational intelligence due to its numerous applications. The simple k-NN classifier using dynamic time warping (DTW) distance has been found to be competitive among other state-of-the-art time-series classifiers. However, in our research, we observed that using a fixed number of nearest neighbors (k) may not always result in optimal performance. This is because the complexity and characteristics of time-series data can vary from region to region. To address this issue, we propose a new method called individual quality (IQ) estimation, which estimates the expected accuracy for each time series and k value. We then combine the results of multiple k-NN classifiers using the IQ estimations to make a final prediction. Our experiments on 35 benchmark data sets demonstrate the effectiveness of our proposed IQ-MAX and IQ-WV algorithms, which outperform two baseline methods.
418	41839	Clothes Co-Parsing Via Joint Image Segmentation and Labeling With Application to Clothing Retrieval.	This paper presents a new system for clothing co-parsing (CCP) that aims to jointly parse unsegmented clothing images into semantic configurations. The system consists of two phases: image cosegmentation, which extracts consistent regions using exemplar-SVM, and region colabeling, which constructs a graphical model to assign labels based on context. The system was evaluated on the Fashionista dataset and a new dataset called SYSU-Clothes, achieving high segmentation accuracy and recognition rates. The system was also applied to a challenging task of cross-domain clothing retrieval, where it outperformed previous methods in accurately identifying clothing items from online stores based on fine-grained parsing results. 
419	41954	Cache-Oblivious R-Trees	A cache-oblivious data structure has been developed for efficiently storing and retrieving a set of axis-aligned rectangles in the plane. The structure uses an axis-aligned bounding-box hierarchy and has provable performance guarantees. It can efficiently find all rectangles in the set that intersect with a given query rectangle or point. If the set does not contain a large number of points within each rectangle, the structure can answer rectangle and point queries with a limited number of memory transfers. A variant of the structure can achieve the same performance even with overlapping rectangles. The performance of the rectangle query matches that of the best-known linear-space cache-aware structure. 
4110	41106	Multi-marker tracking for large-scale X-ray stereo video data.	Analyzing large amounts of video data is a major challenge in the era of big data. In medical research, for instance, tracking infected cardiac movements in animals using X-ray sequences is time-consuming and difficult. To address this, a two-stage graph-based data association method is proposed. This approach considers challenges such as occlusions, low contrast, and inaccurate detections, and uses a directed acyclic graph to connect 3D observations. Tracklets are then linked into longer tracks using global features. The method is tested on X-ray datasets of beating sheep hearts and outperforms standard tracking approaches in accuracy and efficiency. It can also be applied to other video data.
4111	411155	A Method For Verifying Concurrent Java Components Based On An Analysis Of Concurrency Failures	Java is a programming language that supports concurrent (simultaneous) execution of multiple tasks. However, verifying the correctness of concurrent programs is more challenging than sequential programs due to non-determinism and specific problems like interference and deadlock. The ConAn testing tool was developed to test concurrent Java components, but it has limitations in detecting certain types of failures. Other verification tools have been proposed, but they also have their own strengths and weaknesses. This paper suggests a method that combines ConAn with other static and dynamic verification techniques to effectively verify concurrent Java components. It uses a Petri-net model to analyze common concurrency problems and failures, and then determines suitable tools for detecting them. This method aims to provide a comprehensive approach for verifying concurrent Java components.
4112	411252	A general framework for assembly planning: the motion space approach	Assembly planning involves finding the most efficient way to put together a product using its individual parts. A new method, called motion space, has been developed to help with this task. This framework breaks down assembly motions into smaller, independent movements that can be easily identified and do not interfere with each other. The motion space is then divided into cells, with each cell containing fixed blocking relations to prevent collisions between parts. This approach has been applied to various types of assembly motions and has been found to be efficient and effective in complex assemblies. However, there are still some challenges that need to be addressed.
4113	4113126	A federated approach to distributed network simulation	The article discusses the use of federated simulation techniques to create parallel simulations of computer networks. This approach involves dividing the topology and protocol stack of the network into submodels and creating a simulation process for each one. The runtime infrastructure software provides services for communication and synchronization. The article addresses issues that arise in both homogeneous and heterogeneous federations, including differing packet representations and incomplete implementations of network protocol models. The authors propose a dynamic simulation backplane mechanism to facilitate interoperability among different network simulators. Two methods for using the backplane are discussed: cross-protocol stack and split-protocol stack. Results from experiments on two computing clusters demonstrate the scalability of this approach. The article concludes that federated simulations are an effective method for creating efficient parallel network simulation tools.
4114	4114102	Hazy Image Modeling Using Color Ellipsoids	This paper introduces a new framework for analyzing single image dehazing methods using color ellipsoids. Existing methods lack in explaining why a particular method was chosen. The research uses both synthetic and real world images to demonstrate the properties of color ellipsoids, which can indicate the amount of haze in an image. This framework is then applied to analyze a specific dehazing method, showcasing its usefulness. Overall, the use of color ellipsoids offers a promising approach to understanding and improving single image dehazing methods.
4115	411512	mClock: handling throughput variability for hypervisor IO scheduling	This paper discusses the allocation of resources in virtualized servers, which run a variety of virtual machines with different needs. The hypervisor is responsible for managing these resources and ensuring isolation between VMs. While current methods provide control over CPU and memory allocation, there is limited support for IO resource management. This paper introduces a new algorithm, mClock, for IO resource allocation in hypervisors. It allows for proportional sharing, minimum reservations, and maximum limits on IO allocations for VMs. The algorithm has been implemented in the VMware ESX server hypervisor and shows promising results in terms of VM performance and application latency. An adapted version, dmClock, is also presented for distributed storage environments.
4116	4116144	Inter-domain stealthy port scan detection through complex event processing	Large enterprises today are comprised of complex interconnected software systems that span multiple domains. This presents a challenge for implementing effective security defenses. A paper proposes an Intrusion Detection System (IDS) architecture that uses an open source Complex Event Processing engine called Esper to detect inter-domain stealthy port scans. The architecture includes software sensors deployed in different domains that send events to Esper for correlation. The IDS uses a Rank-based SYN (R-SYN) port scan detection algorithm, which combines and adapts three detection techniques to identify malicious host behavior. Evaluation results show that the algorithm is accurate in detecting port scan activities with low false positive rates. This approach offers a low cost of ownership and high flexibility for enterprises.
4117	411747	Spliced Video and Buffering Considerations for Tune-In Time Minimization in DVB-H for Mobile TV	 ther encapsulated into RTP packets.A new video splicing method has been proposed to reduce the tune-in time of mobile TV in DVB-H. Tune-in time is the time between receiving a broadcast signal and starting to play the media. This method uses a random access point picture to minimize the time between decoding and outputting the media. In IPDC over DVB-H, an additional stream of IDR pictures is sent to the IP encapsulator to replace pictures in the bitstream, ensuring compliance with HRD requirements. A video rate control system is also proposed to satisfy HRD requirements while achieving good video quality. This method aims to optimize the broadcast experience for handheld devices with limited battery life. 
4118	411842	Caching-aided coded multicasting with multiple random requests	Caching networks, where a source communicates with multiple users through a shared link, have recently gained attention. One specific type, the shared link caching network, involves a source with access to a file library and users who can store segments of these files. The source sends a coded multicast message to satisfy all users' requests at once. The objective is to find the smallest possible average codeword length to fulfill these requests. This paper focuses on the case where each user makes independent requests according to a common distribution. An achievable scheme using random vector caching placement and multiple groupcast index coding is proposed, which is shown to be order-optimal in certain scenarios. The impact of the number of requests per user on the performance of these schemes is also analyzed. 
4119	411981	Distributed Multicell and Multiantenna Precoding: Characterization and Performance Evaluation	This paper discusses downlink multiantenna communication using cooperative precoding done by base stations in a distributed manner. Previous research has assumed that transmitters have complete knowledge of data symbols and channel state information (CSI). However, in this study, each base station only has local CSI. For instantaneous CSI, a method to achieve the outer boundary of the achievable rate region is proposed for two multi-antenna transmitters and two single-antenna receivers. The paper also presents distributed versions of traditional beamforming techniques and shows how the design can be improved using the virtual SINR framework. Similar results are derived for the case of local statistical CSI. The paper provides heuristics for power allocation and demonstrates the performance through numerical simulations.
4120	412036	Robust computations with dynamical systems	In this paper, the computational power of Lipschitz dynamical systems that are robust to infinitesimal perturbations is discussed. The previous study on this topic was limited to systems that were not considered natural from a classical mathematical perspective. However, the authors prove that their previous results can be extended to Lipschitz and computable dynamical systems. They also show that if robustness to infinitesimal perturbations is required, the reachability problem for these systems becomes decidable. This means that undecidability of verification does not apply to Lipschitz, computable, and robust systems. The authors also demonstrate that the perturbed reachability problem is co-recursively enumerable and complete, even for C∞-systems.
4121	412191	Connected graph searching	The graph searching game involves searchers trying to capture a fugitive on a graph by moving along edges, while the fugitive tries to avoid capture by moving along unguarded paths. The minimum number of searchers needed to guarantee the capture of the fugitive is known as the search number. This paper focuses on the game under the restriction of connectivity, where the locations of the graph that are clean must remain connected during the search. Many traditional mathematical tools do not work under this restriction, and the paper also explores the "price of connectivity," or the additional number of searchers needed for a connected search. The study provides estimations and tight bounds for the price of connectivity on general graphs and trees, and also presents a complete characterization for connected graph searching on trees. It is shown that the connected search number can be computed efficiently on trees and has connections to other tree invariants such as the Horton-Strahler number. 
4122	412259	Exploiting selective placement for low-cost memory protection	Embedded processing applications, such as those used in the automotive and medical fields, require low-cost and reliable hardware designs. Traditionally, reliable memory systems have used coded storage techniques like ECC, which can detect and correct memory faults but come with a high cost overhead. In this article, the authors propose a new partial memory protection scheme that offers high-coverage fault protection for program code and data at a lower cost compared to traditional methods. Their approach involves profiling program code and data usage to identify critical elements, which are then placed into limited protected storage resources. Through fault-injection experiments, the authors confirm that their approach provides high levels of fault protection (99&percnt; coverage) with limited memory protection resources (36&percnt; protected area).
4123	412359	Small-time scaling behavior of Internet backbone traffic	In this study, the authors conduct a wavelet analysis of Internet backbone traffic traces to investigate the causes of small-time scaling phenomena. They find that the second-order scaling exponents at small time scales are mostly close to 0.5, indicating uncorrelated traffic fluctuations. However, some traces exhibit larger exponents, suggesting correlation. The traces also display mostly monofractal behaviors at small time scales. Through analyzing flow composition, the authors identify dense flows as the main factor in causing correlation at small time scales. They also find that the proportion of dense vs. sparse flows greatly influences small-time scalings. These findings have important implications for networking modeling, service provisioning, and traffic engineering.
4124	412463	Packet audio playout delay adjustment: performance bounds and algorithms	The paper discusses adaptive playout delay adjustment in packet audio applications, where packets are buffered and delayed to compensate for variable network delays. The goal is to minimize the playout delay while avoiding excessive loss due to late packet arrivals. The paper presents efficient algorithms for computing upper and lower bounds on the optimum average playout delay, as well as a new adaptive delay adjustment algorithm that tracks network delays and uses delay percentile information to dynamically adjust playout delay. This algorithm outperforms existing ones and performs close to the theoretical optimum in measured audio delay traces.
4125	4125389	MSPlayer: Multi-Source and multi-Path LeverAged YoutubER.	The popularity of online video streaming through mobile devices has increased significantly in recent years. YouTube has reported a significant rise in the percentage of its traffic streaming to mobile devices. However, users often face limited bandwidth which affects their streaming experience. The deployment of content delivery networks (CDNs) has helped to improve this by replicating popular videos at different sites, allowing users to stream from nearby locations with low latency. With the availability of multiple wireless interfaces on mobile devices, a new solution called MSPlayer has been proposed. It takes advantage of multiple video sources and network paths to provide high quality and robust video streaming. The solution has been tested on a testbed and through YouTube with positive results.
4126	412687	Truth, justice, and cake cutting.	Cake cutting is often used as a metaphor for dividing a diverse and divisible resource. There have been many studies on how to fairly divide a cake, but only a few have considered the self-interest of individuals and the strategic implications. However, these studies have a limited concept of truthfulness. This paper aims to address the problem of dividing a cake in a way that is both truthful and fair, while also being Pareto-efficient. The authors propose a notion of dominant strategy truthfulness, which is commonly used in social choice and computer science. They present both deterministic and randomized mechanisms for cake cutting that are truthful and fair, depending on the agents' valuation functions. 
4127	412764	Key management for restricted multicast using broadcast encryption	This content discusses the problem of securely communicating with a group of users over an insecure broadcast channel, specifically in the contexts of satellite/cable pay TV and the Internet MBone. The main concerns are the number of key transmissions and the number of keys held by each receiver. Previous schemes have limitations, such as high setup costs or the need for receivers to keep a large number of keys. The proposed approach addresses these issues by using a single key structure and allowing a controlled number of non-target users to receive the broadcast. This is achieved through -redundant establishment key allocations, which limit the total number of recipients and prevent unauthorized access. The performance of these schemes is measured by the number of key transmissions, redundancy, and the likelihood of free-riders being able to decrypt the broadcast. New lower bounds and establishment key allocations are presented and evaluated through simulation. 
4128	412870	Advanced Classification And Rules-Based Evaluation Of Motion, Visual And Biosignal Data For Patient Fall Incident Detection	This paper discusses the importance of monitoring human physiological data in emergency situations, particularly for elderly individuals living alone. Various techniques have been proposed for identifying distress using motion, audio, and video data. The paper presents an integrated patient fall detection system that combines visual data from overhead cameras with motion and physiological data from the subject's body. Trajectory tracking and acceleration data are utilized to detect falls, while biosignals like ECG and SPO2 can indicate the severity of the incident. The paper also evaluates different classifiers and meta-classifiers for their accuracy in detecting falls, and includes a user-based evaluation. 
4129	4129148	ForeGraph: Exploring Large-scale Graph Processing on Multi-FPGA Architecture.	Large-scale graph processing faces challenges such as poor locality, scalability, random access pattern, and data conflicts. FPGAs have characteristics that make them a promising solution for accelerating these applications, such as high throughput for random data access. However, the limited on-chip memory and off-chip bandwidth on a single FPGA chip can hinder large-scale processing. A multi-FPGA architecture can improve this, but data partitioning and communication schemes must be carefully considered to ensure locality and reduce data conflicts. In this paper, the authors propose ForeGraph, a framework for large-scale graph processing using a multi-FPGA architecture. Each FPGA board only stores a partition of the graph, reducing communication. Under their scheduling scheme, each FPGA chip can process the graph in parallel without conflicts. Experimental results show that ForeGraph outperforms previous FPGA-based systems by 4.54x when executing PageRank on a 1.4 billion edge Twitter graph. The average throughput is over 900 MTEPS, which is 2.03x higher than previous work.
4130	413044	Primal Cutting Plane Algorithms Revisited	Dual fractional cutting plane algorithms are commonly used to tighten a linear relaxation of an integer program in order to solve it more efficiently. However, there are also primal cutting plane algorithms that were developed in the 1960s, but have received little attention. These algorithms aim to improve the feasible solution of the original problem through the use of cutting planes. In this paper, the authors propose a new primal algorithm for 0-1 problems, utilizing strong valid inequalities, and provide promising computational results. They also discuss the potential for extending this approach to general mixed-integer programs.
4131	413165	On probabilistic models for uncertain sequential pattern mining	This article discusses the use of uncertainty models in sequential pattern mining. It explores situations where there is uncertainty about a source or an event, and proposes the use of probabilistic databases to model these uncertainties. Two notions of frequentness are considered, namely expected support and probabilistic frequentness, and their interestingness criteria are described. The article also examines the complexity of evaluating these criteria, showing that in cases of source-level uncertainty, evaluating probabilistic frequentness is #P-complete and cannot be solved in polynomial time, while the remaining cases can be evaluated in polynomial time. 
4132	4132125	Multi-user multiple-input single-output downlink transmission systems exploiting statistical channel state information	Multi-user multiple antenna systems are popular due to their high spectral efficiency. However, they require receiver and transmitter channel state information (CSI), especially for the downlink channel. In this study, the authors explore the ergodic sum rate and propose a low complexity adaptive transmission scheme for multiple-input single-output downlink multi-user systems with statistical CSI at the base station (BS). They develop an approximation for the ergodic sum rate and determine the optimal constraint for each user's statistical CSI to maximize this approximation. Based on this, they derive a statistical beamforming space division multiple access transmission scheme and propose new adaptive transmission algorithms. These algorithms only require the BS to have knowledge of the statistical CSI of each user, and are shown to perform well.
4133	413356	Native Double Precision LINPACK Implementation on a Hybrid Reconfigurable CPU	The use of double precision (DP) arithmetic on embedded CPUs without native support can significantly decrease performance and power efficiency. However, hybrid reconfigurable CPUs, which can change their instruction set at runtime, offer a promising solution for applications requiring unsupported instructions. Experiments on a Stretch S6 platform have shown that limited reconfigurable resources are enough to provide native support for DP arithmetic. By implementing a DP fused multiply-accumulate (FMA) extension instruction, the S6 achieved a peak performance of 200 MFlop/s and a sustained performance of 22.7 MFlop/s at a clock frequency of 100 MHz, outperforming LINPACK's software-emulated DP arithmetic by a factor of 5.7 with higher numerical accuracy. Additionally, the S6 can also implement multiple floating-point operators in parallel for single precision operations.
4134	413454	Semi-Supervised Clustering via Matrix Factorization	In recent years, there has been a growing interest in semi-supervised clustering methods, which use supervisory information to group data. This information is typically in the form of constraints that indicate the similarity or dissimilarity between data points. This paper introduces a new approach to semi-supervised clustering using matrix factorization. The algorithm can also be extended to co-cluster data sets of different types with constraints. Experiments on UCI data sets and real world BBS data sets demonstrate the effectiveness of this approach.
4135	4135143	A general graph-based semi-supervised learning with novel class discovery	The paper proposes a general graph-based semi-supervised learning algorithm that not only aims to achieve semi-supervised learning but also discovers latent novel classes in the data. The algorithm uses normalized weights on a data graph to output probabilities of data points belonging to labeled classes or the novel class. It is theoretically interpreted through three viewpoints on the graph: regularization framework, label propagation, and Markov random walks. Experiments on toy examples and benchmark datasets show the effectiveness of the algorithm. 
4136	4136111	On the (dis)similarity of transactional memory workloads	The use of transactional memory as an alternative to traditional concurrent programming in multicore systems has posed challenges for computer and software engineers. Previous research has used microbenchmarks or manually converted multithreaded workloads to evaluate transactional memory designs, but there is no consensus on how to describe their runtime characteristics. This has led to an unknown level of redundancy in the workloads used for evaluation. To address this, the authors propose a set of architecture-independent transaction-oriented workload characteristics and use clustering algorithms to analyze a set of transactional memory programs. The results show that using these characteristics can reduce the number of simulations needed and help identify specific feature subsets for evaluation in future transactional memory designs. 
4137	413719	A Bandwidth-Efficient Coded Cooperative Communications System	The proposed coding scheme aims to increase spectral efficiency and improve diversity in repetitive information transmission. It uses a parallel concatenation of two bit-interleaved coded M-ary modulators (BICM) with scalable repetition of information symbols. This allows for a tradeoff between bandwidth and performance by adjusting the fraction of repeated information and the spectral efficiency of M-ary modulation. Simulation results for different M-PSK and M-QAM systems in block fading scenarios show that this scheme can significantly improve performance for a given bandwidth efficiency. 
4138	413896	Nonsmooth Optimization for Beamforming in Cognitive Multicast Transmission	The optimal beamforming problems for cognitive multicast transmission are typically nonconvex and difficult to solve. The conventional method involves reformulating the problems as convex semi-definite programs with nonconvex and discontinuous rank-one constraints. However, this approach often fails to find satisfactory solutions and requires the use of randomization techniques. In contrast, this paper presents a new approach that formulates the problems as SDPs with continuous reverse convex constraints. A nonsmooth optimization algorithm is then used to find the optimal solution, which has been shown to be almost globally optimal in simulation results. This approach is more efficient and requires less computational load than the conventional method. 
4139	413930	Mobile text entry using three keys	This content describes six techniques for three-key text entry, which use Left- and Right-arrow keys and a Select key to maneuver a cursor over characters. The keystrokes per character (KSPC) range from 10.66 to 4.23. Two techniques, Method #2 and Method #6, were chosen for formal evaluation. Method #2 arranges characters alphabetically while Method #6 uses linguistic enhancement to minimize cursor distance after each entry. Both methods place SPACE on the left and utilize a snap-to-home cursor mode. In an experiment with ten participants, both techniques had entry rates of 9-10 words per minute. The challenges and opportunities for using linguistic knowledge and typamatic keying strategies are also discussed. 
4140	414044	Pointing at 3d target projections with one-eyed and stereo cursors	This study compares the use of stereo and mono-rendered cursors for selecting 2D-projected 3D targets. Two mouse-based and two remote pointing techniques were tested in a 3D Fitts' law pointing experiment, with the first experiment using fixed depth targets. The results showed that one-eyed cursors only improved screen-plane pointing techniques, and target depth did not affect pointing throughput. A second experiment with varying depth targets and only screen-plane pointing techniques found that screen-space projections of Fitts' law parameters resulted in consistent throughput, regardless of target depth. This suggests that in the absence of stereo cue conflicts, projecting Fitts' law parameters onto the screen can accurately predict performance and is not influenced by target depth differences.
4141	41418	A Boosting-Based Framework for Self-Similar and Non-linear Internet Traffic Prediction	Internet traffic prediction is crucial for network management and optimization, but the complex nature of network traffic makes accurate prediction challenging. This paper proposes a boosting-based framework for predicting self-similar and non-linear traffic by treating it as a regression problem. The framework uses Ada-Boost and Principle Component Analysis to take advantage of self-similarity while avoiding its drawbacks. A feed-forward neural network is used to capture the non-linear relationship in the traffic. Real network traffic experiments show that this framework is effective in predicting traffic.
4142	414278	Robust Multi-Network Clustering via Joint Cross-Domain Cluster Alignment	Network clustering is a widely researched problem that has gained significant attention in recent years. While most existing methods focus on clustering nodes within a single network, there are many applications where multiple related networks exist. These networks may be constructed from different domains and have instances that are related to each other. In this paper, a robust algorithm called MCA is proposed for multi-network clustering. MCA considers cross-domain relationships between instances and has several advantages over single network clustering methods. It can detect associations between clusters from different domains and achieves more consistent results by leveraging the duality between clustering individual networks and inferring cross-network cluster alignment. MCA also provides a more robust solution to noise and errors. Extensive experiments on real and synthetic networks show the effectiveness and efficiency of MCA. 
4143	414329	Using FCA to suggest refactorings to correct design defects	Design defects are mistakes in the design of software that make it difficult to maintain. Detecting and correcting these defects is important for creating high-quality software. Modern techniques can accurately identify design defects, but correcting them is still a manual process that is prone to errors. To address this issue, formal concept analysis (FCA) has been used as a framework for refactoring, specifically redistributing class members. A new approach is proposed that combines metrics and FCA to remove defects in object-oriented programs. A case study of a specific defect, called the Blob, in the Azureus project is used to demonstrate the effectiveness of this approach. 
4144	41440	Dealing with Disappearance of an Actor Set in Social Networks	Social networks are complex structures made up of entities and connections. When a node or group of nodes disappears from the network, it can disrupt the flow of information and cause the network to disconnect. The aim of this paper is to build upon previous research on managing node disappearances by addressing the disappearance of a group of nodes. The proposed approach focuses on the role of the group in maintaining network connectivity and restoring information flow. The group is categorized into three classes based on its relationship to the network, and the approach involves adding new links and finding substitutes to maintain the network's quality. Unlike other methods, this approach considers information flow as a key factor in identifying potential links and substitutes. The solution is validated through experiments and shows improvements in response time and the number of added links.
4145	414521	An efficient implementation for the 0-1 multi-objective Knapsack problem	This paper introduces a new approach for solving 0-1 multi-objective knapsack problems using dynamic programming. The approach utilizes dominance relations to eliminate partial solutions that cannot lead to new nondominated criterion vectors. This results in a more efficient method compared to existing methods, as shown through numerical experiments on different types of instances. The approach also outperforms other exact methods and has been tested for the first time in the three-objective case. 
4146	41463	Hereditary Domination in Graphs: Characterization with Forbidden Induced Subgraphs	The leaf graph of a connected graph is created by adding a new vertex with degree one to each noncutting vertex. It has been proven that if a connected graph is not dominated by any of its induced paths, then it is dominated by a connected induced subgraph whose leaf graph is also an induced subgraph of the original graph. This means that the minimal graphs not dominated by any induced subgraph isomorphic to a certain class of graphs will be cycles or leaf graphs of graphs not in that class. If the class of graphs is closed under the operation of taking connected induced subgraphs, then the hereditarily dominated graphs can be identified by the following forbidden induced subgraphs: leaf graphs of connected graphs that are not in the class, but all of their connected induced subgraphs are, and the cycle of length equal to the shortest path not in the class. This solves a problem that has been open since the 1980s. A different method has been used by Bacsó to solve the case of induced-hereditary classes.
4147	414748	Approximation complexity of min-max (regret) versions of shortest path, spanning tree, and knapsack	This paper explores the approximation of min-max (regret) versions of common problems such as shortest path, minimum spanning tree, and knapsack. The authors establish fully polynomial-time approximation schemes for these problems when the number of scenarios is limited, using relationships between multi-objective and min-max optimization. They also propose a scheme for min-max regret shortest path using dynamic programming and trimming techniques. However, they prove that min-max regret knapsack is not possible to approximate. For an unbounded number of scenarios, the problems become strongly NP-hard and the authors provide non-approximability results for min-max (regret) versions of shortest path and spanning tree. 
4148	414823	A size-sensitive inequality for cross-intersecting families.	The content discusses the concept of cross-intersecting families A and B, where all subsets of A and B have at least one common element. Pyber's theorem states that for n2k, the cardinality of A and B is at most (n1k12). The present paper strengthens this theorem by proving that for n1k1+niki+1, the cardinality of A and B is at most (n1k1+niki+1)(n1k1nik1). This inequality is optimal. Additionally, the paper presents a shorter proof of Pyber's theorem and another proof of an inequality by Frankl and Tokushige (1992) without the need for computation.
4149	41496	Constructions of almost difference families	This paper discusses almost difference families (ADFs), which are a generalized version of almost difference sets (ADSs). The paper presents various methods for constructing ADFs and identifies several infinite classes of ADFs. These results expand upon previously known difference families and demonstrate the usefulness of ADFs in creating partially balanced incomplete block designs. These designs have practical applications in combinatorial and statistical problems. 
4150	415012	Adaptive Mctf Based On Correlation Noise Model For Snr Scalable Video Coding	This paper presents a new method for scalable video coding called subband adaptive motion compensated temporal filtering (MCTF). It also introduces a revised model for quantization in this technique. Hierarchical MCTF is commonly used in scalable video coding to utilize temporal correlation between frames. However, the strength of this correlation varies with the level of temporal transform and different spatial frequencies in a frame. The proposed method adjusts the strength of motion compensation based on the correlation and noise characteristics of subbands, while also adjusting the quantization step according to the MCTF structure. This adaptive approach improves the coding performance of scalable video coding by maximizing the use of temporal correlation while limiting the propagation of reconstruction noise.
4151	415158	Directional filtering transform for image/intra-frame compression.	Traditional transforms have been adapted to have a directional component, resulting in different outcomes depending on the order of the transformations. This paper examines the effects of transform orders on coding gain in an anisotropic image model. It is found that the transform orders have minimal impact on coding gain when fully decomposed, but in practical compression schemes where high-pass bands are not fully decomposed, different orders can affect coding performance. To address this, an adaptive transform order is proposed, which evenly distributes prediction modes and takes into account interblock and intrablock correlations. Experimental results in H.264 intraframe coding show the effectiveness of this approach.
4152	415224	Swift: A Hybrid Digital-Analog Scheme for Low-Delay Transmission of Mobile Stereo Video	Efficient and robust wireless stereo video delivery is crucial for mobile 3D applications. Existing solutions either have high coding efficiency but are not robust to channel variations, or have the opposite characteristics. In this paper, a hybrid digital-analog (HDA) solution is proposed to combine the advantages of both approaches. One frame in each stereo pair is digitally encoded for basic quality, while the other is analogly processed to take advantage of good channels. A zigzag coding structure is designed to explore intra-view and inter-view correlations, and a reference selection mechanism is used to improve coding efficiency. The system also addresses optimal power and bandwidth allocation between digital and analog streams. A trace-driven evaluation on a software-defined radio platform shows that the proposed system, named Swift, outperforms an omniscient digital scheme or can achieve comparable performance with 2x power savings. Subjective quality assessments also demonstrate that Swift provides better visual quality than a straightforward HDA extension of SoftCast.
4153	41538	Channel-adaptive resource allocation for scalable video transmission over 3G wireless network	The paper discusses the challenges of transmitting scalable video over 3G wireless networks and proposes resource allocation strategies to optimize video quality and minimize power consumption. The proposed schemes take into account the varying wireless channel and network conditions, and use a combination of unequal error protection and automatic repeat request to adapt to these conditions. The authors also propose a power-minimized bit allocation scheme for mobile devices. Simulation results using a progressive fine granularity scalability video codec demonstrate the effectiveness of the proposed schemes in improving video quality under different network conditions. 
4154	415421	Log-domain wave filters	The article presents a method for designing log-domain wave filters, which are used to simulate passive LC ladder prototype filters with low sensitivity. The technique involves transposing the signal flow graph of the linear domain to the log-domain using complementary operators to maintain linear operation. The effectiveness of this approach is demonstrated through simulations of a fifth-order low-pass and fourth-order bandpass log-domain wave filter using HSPICE. These filters are suitable for low-voltage and high-frequency applications.
4155	415524	Model checking system software with CMC	Complex systems often have errors that are difficult to detect, especially when they involve specific situations and complicated sequences of events. Traditional testing methods are not always effective in finding these errors. In recent years, formal verification techniques have become more popular as they can check for errors in all possible system behaviors. However, these techniques require creating an abstract model of the system, which can be unreliable and miss implementation errors. CMC is a framework that directly runs software code without the need for an abstract model, making it more reliable and effective. When used to model check the AODV routing protocol and the IP Fragmentation module in the Linux TCP/IPv4 stack, CMC was able to find numerous bugs and verify correctness.
4156	415649	On-the-fly auditing of business processes	Information systems that support business processes can be complex, making it difficult to ensure that business rules are being followed. To simplify this task, a separate system called a monitor can be designed. The monitor collects events from the business processes and checks if the rules are being followed. A business rule language (BRL) is needed to verify the rules over finite histories. This BRL can express many common business rules and has two important properties: future stability and past stability. To handle the increasing history, the monitor uses abstractions of the history and a labeled transition system. This system can be transformed into a colored Petri net for easier verification. 
4157	415775	A Measurement-Based Simulation Study of Processor Co-allocation in Multicluster Systems	The article discusses the challenges and potential benefits of using multiple clusters in systems such as the Distributed ASCI Supercomputer. While the availability of processors in multiple clusters can be advantageous for applications, the slow network connections between clusters can lead to degraded performance for single-application multicluster execution. Scheduling policies for these systems also need to consider the additional restrictions of fitting each component of a job into separate clusters. The paper presents a measurement study of the runtime and communication time for two applications on single clusters and multicluster systems, as well as simulations of different multicluster scheduling policies. It concludes that restricted forms of co-allocation in multiclusters can improve performance compared to not allowing co-allocation at all.
4158	415876	GRENCHMARK: A Framework for Analyzing, Testing, and Comparing Grids	Grid computing is becoming increasingly popular for aggregating and sharing diverse resources. However, in order for grid development to be successful, it is important to demonstrate that grids can reliably support real applications and to create benchmarks to measure this support. Traditional benchmarks are not always suitable for grid environments, so a middle-ground approach is needed. This is where GRENCHMARK comes in - a framework for generating and submitting synthetic workloads that accurately represent the applications used in modern grids. With over 35 synthetic and real applications, GRENCHMARK is versatile and can be used for grid system analysis, functionality testing, and comparison of different grid settings. The results obtained with GRENCHMARK in the DAS multi-cluster grid show its effectiveness.
4159	41594	A Graph-based model for context-aware recommendation using implicit feedback data	Recommender systems have been successful in dealing with information overload, but they are mostly designed for scenarios where explicit feedback is available. This can be limiting in situations where only implicit feedback is present. Additionally, most existing methods only consider user and item dimensions, ignoring other important contextual information such as time and location. To address these limitations, the authors propose a graph-based recommendation framework that incorporates contextual information into the recommendation process. This framework, called Multi-Layer Context Graph (MLCG), models the interactions between users and items and includes a variety of contextual information. Two novel ranking methods, Context-aware Personalized Random Walk (CPRW) and Semantic Path-based Random Walk (SPRW), are developed based on MLCG. The authors demonstrate the effectiveness of their approach through experiments on two real-world datasets. 
4160	416021	Performance evaluation of a DySER FPGA prototype system spanning the compiler, microarchitecture, and hardware implementation	Specialization and accelerators are being suggested as a solution to the slowdown of Dennard scaling. One such accelerator, called DySER, uses a co-designed compiler and microarchitecture to dynamically synthesize large functional units that match program regions. This paper presents a full prototype implementation of DySER integrated into the OpenSPARC processor, a co-designed compiler in LLVM, and a performance evaluation on an FPGA system running Ubuntu Linux and full applications. The results show that DySER provides energy efficient speedups with a 6X performance improvement and minimal overhead. The DySER compiler is effective at extracting computationally intensive regular and irregular code, but struggles with non-computationally intense irregular code. The authors also highlight the need for open-source baseline processors and declarative tools for designing ISA-exposed accelerators more efficiently.
4161	416127	A Cross-Domain Recommendation Model for Cyber-Physical Systems	Cyber-physical systems (CPS) are intelligent systems that interact with other systems through information and physical interfaces. With an increased reliance on CPS, there has been a significant collection of human-centric data, leading to information overload in various domains. Recommender systems in CPS, which provide information recommendations based on historical ratings from a single domain, face the challenge of data sparsity. To address this issue, researchers have proposed recommendation models that transfer knowledge across multiple domains, assuming a common rating pattern. However, in real-world scenarios, domains may not necessarily share the same rating pattern, and the diversity among domains may outweigh the benefits of a common pattern, resulting in decreased performance. To overcome this, a new cross-domain recommendation model is proposed in this paper, which learns both a common rating pattern and domain-specific patterns to improve performance. Experiment results on real-world datasets demonstrate the superiority of this model compared to existing methods for cross-domain recommendation in CPS.
4162	416247	The Use of Lexical Context in Question Answering for Spanish	This paper discusses a prototype created by the Language Technologies Laboratory at INAOE for evaluating Spanish monolingual question-answering (QA) at CLEF 2004. The prototype uses context and lexicons to identify potential answers to factoid questions, and employs pattern recognition for definition questions. The paper describes the methods used at various stages of the system and the prototype's architecture. The results of this approach are also presented and analyzed.
4163	4163114	NLEL-MAAT at CLEF-ResPubliQA.	The report discusses the participation of NLE Lab in the QA@CLEF-2009 competition. The lab used the JIRS passage retrieval system, which is focused on redundancy, to find answers to questions in a large document collection. The passages were ranked based on the number, length, and position of question n-grams. The system performed well in monolingual English, but had lower results in French. This is attributed to the varying styles of questions in different languages. Overall, the report highlights the effectiveness of the JIRS system in retrieving relevant passages for question answering tasks.
4164	416415	The Use of Lexical Context in Question Answering for Spanish	The Language Technologies Laboratory at INAOE created a prototype for evaluating Spanish monolingual question-answering (QA) at the CLEF 2004 competition. Their approach focuses on using context at a lexical level to find potential answers to factoid questions, and also utilizes pattern recognition to identify potential answers to definition questions. This paper details the methods used at various stages of the system and the prototype's architecture for QA. The results of this approach are presented and analyzed.
4165	4165121	A Layered Lattice Coding Scheme for a Class of Three User Gaussian Interference Channels	The paper focuses on a specific type of communication system called three user Gaussian interference channels. It proposes a new method called layered lattice coding, which uses lattice codes to align interference at each receiver. This approach is found to have multiple degrees of freedom and achieve higher rates compared to the traditional Han-Kobayashi coding scheme. The paper suggests that layered lattice coding can improve the performance of communication systems using Gaussian interference channels.
4166	416610	Using the Representation in a Neural Network''s Hidden Layer for Task-Specific Focus of Attention	In this paper, the concept of focusing attention on important features in real-world tasks is discussed. A mechanism called saliency map is proposed, which uses a computed expectation of input contents to identify important regions in the input. This map can then be used to emphasize crucial features and downplay less relevant ones. The effectiveness of this method is demonstrated in a robotics task and its applicability is also shown in a non-visual domain. The paper provides details about the architecture and algorithm used, as well as empirical results. Overall, the saliency map is shown to be a useful tool for achieving task-specific focus of attention. 
4167	416773	Proofs as computations in linear logic	Uniform proof and resolution are fundamental concepts in the proof-theoretic characterization of logic programming. The class of Abstract Logic Programming Languages effectively captures these ideas for a wide range of logical systems. In logic programming, the structure of formulas, such as Horn clauses and hereditary Harrop formulas, is crucial in differentiating between programming and theorem proving. This paper presents an extension of hereditary Harrop formulas and a corresponding logical system, which serve as the foundations for a logic programming language. The study is based on Forum, a presentation of higher-order linear logic using uniform proofs. A subset of its formulas has been identified as suitable for representing various programming paradigms.
4168	416846	Recovery Of Correlated Sparse Signals Using Adaptive Backtracking Matching Pursuit	Distributed compressive sensing is a technique used to reconstruct multiple signals from a small number of measurements. By using a modified Compressive Sensing (CS) algorithm called Modified Basis Pursuit (Mod-BP), which utilizes Partially Known Support (PKS), the remaining signals in a joint-sparse signal ensemble can be reconstructed if one signal is known apriori. However, Mod-BP is slow in converging, making it unsuitable for practical applications such as video frame reconstruction. To improve the recovery performance at a shorter time, Carrillo et al proposed using PKS in iterative greedy algorithms. However, these algorithms are blind to incorrect atoms present in the PKS, which is common in video frames. To address this, the authors introduce Adaptive Backtracking Matching Pursuit (AdBMP), which effectively utilizes the PKS to reconstruct sparse signals. Experimental results show that AdBMP outperforms existing PKS based iterative greedy algorithms in terms of reconstruction accuracy.
4169	416939	Passive Source Localization Using Time Differences of Arrival and Gain Ratios of Arrival	This paper investigates the use of gain ratios of arrival (GROAs) and time differences of arrival (TDOAs) to improve the accuracy of source localization. The GROAs are defined as the ratio of the received signal amplitudes at a reference sensor to other sensors, and are dependent on the distance between the source and receivers. The paper utilizes a Gaussian random signal model to derive the Cramer-Rao lower bound (CRLB) for source location estimation using GROAs and TDOAs. The results show that the improvement from GROAs increases with the factor c/omegao, where c is the signal propagation speed and omegao is the signal bandwidth. An algebraic closed-form solution for source location using GROAs and TDOAs is also developed and shown to reach the CRLB accuracy under the Gaussian data model. Numerical simulations support the theoretical findings.
4170	417065	Depth map enhancement using adaptive steering Kernel regression based on distance transform	This paper introduces a new method for improving noisy depth maps using adaptive steering kernel regression based on distance transform. This approach considers spatial and photometric properties of pixel data to filter out noise more effectively. By refining the steering kernel according to local region structures, flat and textured areas, the method is able to reduce noise in depth maps. The process involves creating distance transform maps from the depth map and its corresponding color image, and modifying the steering kernel using a newly-designed weighing function. This function expands the kernel in flat areas and shrinks it in textured areas towards local edges in the depth map. Experimental results show that this method performs better than other techniques for enhancing depth maps.
4171	417169	Camera handoff with adaptive resource management for multi-camera multi-object tracking	Camera handoff is a critical aspect of multi-camera surveillance systems, as it enables the continuous tracking and consistent labeling of objects of interest. While existing algorithms focus on data association, there are still many unanswered questions in developing an efficient camera handoff method. This paper proposes a trackability measure that evaluates tracking effectiveness, allowing for timely and optimal camera handoff selection. The measure considers resolution, distance to the edge of the camera's field of view, and occlusion. The algorithm also addresses the decrease in frame rate in real-time tracking systems by using an adaptive resource management mechanism to allocate resources to multiple objects with different priorities. Experimental results show a 20% increase in overall tracking rate compared to previous methods.
4172	417270	A Polynomial Approach To Nonlinear State Feedback Stabilization Of Saturated Linear Systems	This paper discusses the problem of stabilizing saturated linear systems using nonlinear control laws. The proposed approach uses a rational formulation of the control action, with a parameter sigma that is determined by solving an implicit equation based on the system state. By utilizing a sum-of-squares formulation and imposing a polynomial dependence on sigma, constructive solutions for local exponential and global asymptotic stabilization are presented. This approach offers a more efficient and effective method for controlling these systems compared to previous approaches.
4173	417366	An Augmented Reality System for Epidural Anesthesia (AREA): Prepuncture Identification of Vertebrae.	Our proposed system uses augmented reality technology to assist in spinal needle insertion for epidural anesthesia. It relies on a trinocular camera to track an ultrasound transducer and generate a panoramic image of the lumbar spine. This image is then used to automatically identify the lumbar levels and overlay them onto a live camera view of the patient's back. We conducted validation tests to ensure the accuracy of panorama generation, lumbar level identification, overall system accuracy, and its ability to handle changes in the spine's curvature. Results from 17 subjects showed that our system can achieve an error within the acceptable range for epidural anesthesia, making it a feasible and effective tool for this procedure.
4174	41746	Language-independent multi-document text summarization with document-specific word associations.	Automatic text summarization is the process of creating a condensed version of a document or set of documents. The paper proposes a new method for generating summaries in multiple languages, using word associations specific to the given document(s). This statistical approach is effective in selecting sentences for the summary. The method has been tested in various languages and has shown to outperform other methods.
4175	41757	Reducing VM Startup Time and Storage Costs by VM Image Content Consolidation.	Elastic cloud applications require fast virtual machine (VM) startup in order to handle increased workload. While previous studies have looked into VM startup time in clouds, the impact of VM image (VMI) disk size and its contents is not well understood. To address this gap, a detailed study was conducted on Amazon EC2. Based on the findings, a new method was developed for optimizing the size and contents of VMIs. This approach was tested with an open-source Platform-as-a-Service runtime, resulting in a significant reduction in disk size, faster startup time, and lower storage costs compared to unmodified VMIs.
4176	417619	Modeling Language Evolution	The article presents a model for how languages evolve within a society. It explains that under certain conditions, the languages used by individuals within a society will eventually converge on a common language. The article also delves into a few specific scenarios that demonstrate this convergence.
4177	417722	Quantitative Driven Optimization of a Time Warp Kernel.	The pending event set in a Parallel Discrete Event Simulation (PDES) is a collection of events available for execution. In a Time Warp synchronized simulation engine, these events are aggressively scheduled without strictly enforcing causal relations. This approach aims to process events in a way that implicitly maintains their causal order without incurring the overhead of strict enforcement. However, on a shared memory platform, scheduling events in their Least TimeStamp First (LTSF) order can lead to contention and negatively impact performance. To address this, profile data from Discrete-Event Simulation (DES) models can be used to optimize the simulation kernel. By grouping events based on this profile data, performance can be improved. However, the size of event groups must be carefully considered as smaller groups can improve performance while larger groups may lead to more frequent causal violations and slower simulation. 
4178	417870	Generalized Posynomial Performance Modeling	The paper introduces a new method for automatically generating posynomial symbolic expressions to describe the performance of analog integrated circuits. These expressions are based on SPICE simulation data and have high accuracy at the device level. The method solves a non-convex optimization problem without local minima to determine the coefficient and exponent sets for the posynomial expression. This approach can be applied to both linear and nonlinear circuits and allows for the automatic generation of a sizing model that accurately describes the analog circuit. This eliminates the need for hand-crafted analytic models, saving time and effort. Experimental results demonstrate the effectiveness of this approach.
4179	417913	Summarization of scientific documents by detecting common facts in citations	Scientific articles require more effort to read due to the need to search and go through many citations. To address this issue, a citation-guided method is proposed for summarizing multiple scientific papers. It is observed that citations in one paragraph or section often discuss a common fact, represented by a set of noun phrases. A multi-document summarization system is designed based on detecting these common facts. A challenge is the use of different terms for the same fact in citations, which is addressed by using a term association discovering algorithm. This allows for clustering of citations based on common facts, which are then used to retrieve relevant sentences and create a summary. Experiments show that this method outperforms three baseline methods in terms of ROUGE metric. The system also expands citations through the identification of common facts in the scientific literature.
4180	41808	On effective flip-chip routing via pseudo single redistribution layer	The use of flip-chip design in power distribution has led to the use of redistribution layer (RDL) for interconnection. However, RDL can become congested, making it difficult to complete routing on a single layer. A previous algorithm using two layers of RDLs was proposed, but it often required more than one layer of routing area. To address this issue, a pseudo single-layer concept was adopted, along with heuristics and observations to reduce constraints. This approach resulted in 100% routability and was compared to manual and commercial methods, showing its effectiveness in a real industrial case.
4181	418118	Sidelines: An Algorithm for Increasing Diversity in News and Opinion Aggregators	Aggregators use votes and links to select and display a smaller selection of news and opinion pieces from a larger pool of content. However, relying solely on the most popular items may not accurately represent the diversity of opinions and topics present. To address this, three diversity metrics - inclusion, non-alienation, and proportional representation - are defined. The Sidelines algorithm is then introduced as a way to increase diversity in result sets by suppressing a voter's preferences after a preferred item is selected. In comparison to popular collections, Sidelines showed increased inclusion and decreased alienation. In an online experiment, readers were more likely to encounter challenging views with Sidelines. This can help create more diverse news and opinion aggregators for users.
4182	418217	A graph-search framework for associating gene identifiers with documents.	The process of curating a model organism database involves finding the identifiers for each gene mentioned in an article. A semi-automated approach is used, where the article is associated with a list of potential gene identifiers. Different methods for solving this geneId ranking problem are compared, including a graph-based method and a learning method for re-ranking the output. The results show that the performance of named entity recognition (NER) systems can vary when used with a "soft dictionary" of gene synonyms. The graph-based approach outperforms individual NER systems and can be further improved through learning. It is also noted that the effectiveness of a NER system for geneId finding cannot be accurately measured by its entity-level performance. The best approach for geneId ranking is to combine multiple NER systems, using easily-available resources.
4183	418377	Apprenticeship learning via inverse reinforcement learning	This article discusses learning in a Markov decision process using an expert's demonstration instead of a given reward function. This approach is useful in complex tasks like driving, where it is difficult to specify a reward function. The expert's goal is to maximize a reward function expressed as a linear combination of known features. The proposed algorithm uses "inverse reinforcement learning" to recover the unknown reward function and learns the task demonstrated by the expert. This algorithm is shown to converge in a few iterations and produce a policy that performs similarly to the expert, based on the expert's unknown reward function.
4184	418415	Deterministic automata simulation, universality and minimality	Finite automata have become a popular model in theoretical physics, particularly in studying the distinction between endophysical and exophysical perception. These studies often involve Moore experiments, which have shown that it is impossible to determine the initial state of an automaton. This has led to the suggestion of a discrete model for Heisenberg uncertainty. However, the classical theory of finite automata, which includes initial states, is not suitable for this purpose. This paper focuses on studying finite deterministic automata without initial states, using an extension of the Myhill-Nerode technique. The goal is to define and analyze the complexity of different types of simulations between automata. The paper also presents the construction of minimal automata, which are proven to be unique up to an isomorphism. All of these findings are based on using "automata responses" to simple experiments and do not consider any information about the internal workings of the automata.
4185	418567	Framework for Active Clustering With Ensembles	Clustering methods are useful for organizing large collections of video and images, particularly when it comes to identifying and grouping faces. A new semisupervised approach combines ensemble clustering and active learning to improve accuracy. The user is asked to provide soft link constraints for pairs of faces that are difficult to match. This approach was evaluated on various types of data, including blurry videos, images of women with makeup, and photographs of twins. The results show that this method is more robust to noise and more accurate than other approaches. This new clustering algorithm is also more effective and efficient than current methods.
4186	4186125	Quantifying and Verifying Reachability for Access Controlled Networks	This paper discusses the importance of quantifying and querying network reachability for security monitoring, auditing, and network management purposes. While previous attempts have been made to model network reachability, there have not been feasible solutions for computing it. The paper proposes a suite of algorithms for quantifying reachability based on network configurations, such as Access Control Lists (ACLs), and also presents solutions for querying network reachability. The proposed network reachability model takes into account various aspects of network protocols and features, and the algorithms have been implemented in a tool called Quarnet. Experiments on a university network have shown that the offline computation of reachability matrices takes a few hours, and online processing of a reachability query takes an average of 0.075 seconds.
4187	418748	Memetic cooperative models for the tool switching problem.	This work focuses on using cooperative memetic algorithms, which combine different optimization strategies, to solve the tool switching problem (ToSP). The ToSP is a difficult optimization problem in the field of flexible manufacturing and has been previously tackled using various algorithms. The authors present a proof-of-concept of their cooperative memetic model and conduct experiments to compare its performance with individual memetic algorithms and local search approaches. They find that the cooperative model consistently outperforms the individual components and also conduct a sensitivity analysis to better understand the relationship between the different elements of the model. 
4188	418818	Short Paper: Limitations of Key Escrow in Identity-Based Schemes in Ad Hoc Networks	Identity-based cryptography (IBC) has gained attention for its potential to secure ad hoc networks. This study focuses on the role of the Trust Authority (TA) as a key escrow, which is a characteristic of all IBC schemes. The unique role of key escrow in ad hoc networks is explored and compared to other networks. Adversary models for dishonest TAs in ad hoc networks, including a new model where TAs use spy nodes to record and report communications, are introduced. Analytical results demonstrate that in most ad hoc network scenarios, the TA can be prevented from functioning as a key escrow.
4189	418934	W-privacy: understanding , and  inference channels in multi-camera surveillance video	Surveillance systems record massive amounts of video every day, containing valuable information that can be used for various purposes. However, the potential privacy loss caused by public access to this video is a concern. Privacy solutions must consider the different ways in which personal information can be inferred from the video, such as location, time, and activities, in addition to direct identifiers like faces. This paper presents a privacy loss model that takes into account identity leakage through multiple inference channels in video data. The model combines identity leakage and sensitive information to calculate the overall privacy loss, and experimental results show its effectiveness.
4190	419084	Scalable 3D video of dynamic scenes	This paper introduces a scalable 3D video framework that captures and renders dynamic scenes. The system uses multiple 3D video bricks, each consisting of a projector, two grayscale cameras, and a color camera. Structured light with complementary patterns is used to acquire texture images and pattern-augmented views simultaneously. Depth maps are extracted from the pattern images using space-time stereo, and a view-independent, point-based 3D data structure is created. This allows for effective enforcement of photo consistency and removal of outliers, resulting in high-quality rendering using EWA volume splatting. The framework also allows for easy editing of 3D video, as demonstrated through compositing techniques and spatio-temporal effects.
4191	419151	Hole filling of a 3D model by flipping signs of a signed distance field in adaptive resolution.	The use of range finders to observe objects often results in holes and gaps in the model, making it unsuitable for certain applications. A new method is proposed to fill these gaps using a Signed Distance Field (SDF) that stores distances from a voxel to the nearest point on the mesh model. By minimizing the area of the interpolating surface, a smooth and continuous surface can be generated. Unlike other methods, this approach does not require boundary conditions and can handle high curvature holes. It has been successfully tested on both synthetic and real objects.
4192	419219	Aggregating E-commerce Search Results from Heterogeneous Sources via Hierarchical Reinforcement Learning	The paper discusses the challenge of aggregating search results from different sources in an E-commerce environment. Unlike traditional web search, this task requires dynamically deciding which source should be presented on each page. It also addresses the difficulty of ranking items from different sources due to differences in relevance scores. The authors propose a hierarchical approach to address these issues, with a high-level task for source selection and a low-level task for item presentation. Both tasks are formulated as sequential decision problems and learned through reinforcement learning. The proposed model shows significant improvements in search performance and user satisfaction.
4193	41930	Extract interaction detection methods from the biological literature.	Efforts have been made to extract protein-protein interactions from the literature, but not much has been done on extracting their detection methods. This is important because different methods may affect the reliability of the reported interactions. However, the variety of method mentions makes automatic extraction difficult. To address this, a Correlated Method-Word (CMW) model was developed to extract detection methods by considering the correlation between methods and related words. The model was tested on a corpus of 5319 documents and showed promising results, outperforming previous methods. This demonstrates the model's ability to overcome challenges posed by diverse method mentions and capture the correlation between methods and related words.
4194	419442	Breaking the Synaptic Dogma: Evolving a Neuro-inspired Developmental Network	The majority of artificial neural networks do not change within a learning environment, in contrast to biological networks which have time-dependent morphology and can change through interactions. To address this, a new type of neural network has been designed that models different components of biological neurons and allows for changes in response to the environment. This network begins as a small randomly generated network and evolves through the use of seven programs that simulate various aspects of biological neurons. The goal is to demonstrate that this type of network can learn through experience and the authors discuss their ongoing research in using this network to learn how to play checkers.
4195	419559	Chosen Ciphertext Security via Point Obfuscation.	This paper presents two new constructions for chosen ciphertext secure public key encryption (PKE) from general assumptions. The constructions make use of an obfuscator for point functions with multi-bit output (MBPF obfuscators) that satisfies a security property called AIND in the presence of hard-to-invert auxiliary input. The first construction is based on a chosen plaintext secure PKE scheme and an MBPF obfuscator satisfying AIND with computationally hard-to-invert auxiliary input, while the second construction is based on a lossy encryption scheme and an MBPF obfuscator satisfying AIND with statistically hard-to-invert auxiliary input. These constructions demonstrate the strength of AIND security in relation to other security notions for MBPF obfuscators. Additionally, the paper shows that a lossy encryption scheme can be constructed from a point obfuscator that satisfies certain properties, leading to a CCA secure PKE scheme constructed solely from a re-randomizable and composable point obfuscator. The authors believe that these results provide a connection between the seemingly isolated cryptographic primitives of CCA secure PKE and program obfuscators.
4196	419637	Analyzing and Modeling Router---Level Internet Topology	The Internet topology has been studied and it shows that the connectivity of nodes follows a power-law distribution. However, this does not fully determine the network structure, especially when studying routing control. This paper examines the structures of router-level topologies in actual ISP networks, revealing a high level of clustering. This is not accounted for in existing modeling approaches. To address this, a new realistic modeling method is proposed, where new nodes are likely to connect to the nearest nodes and new links are added based on node utilization. This method produces similar metrics to actual ISP topologies while maintaining a power-law degree distribution. 
4197	419729	Botnet spam campaigns can be long lasting: evidence, implications, and analysis	The accurate identification of spam campaigns launched by large numbers of bots in a botnet is crucial in order to generate effective spam campaign signatures and defeat spamming botnets. The traditional approach of clustering spam with the same label, such as a URL, can be easily bypassed by techniques like obfuscating URLs. In this paper, the authors conduct a comprehensive study of content-agnostic characteristics of spam campaigns, such as duration and source-network distribution of spammers, to determine their effectiveness in assisting label-based clustering methods. They analyze a five-month trace of seven URL-based botnet spam campaigns and find that these campaigns have longer durations than previously reported. This has implications for spam campaign signature generation and the authors also uncover new findings about workload distribution, sending patterns, and coordination among spamming machines. 
4198	4198141	DaVinci: dynamically adaptive virtual networks for a customized internet	This paper discusses the benefits of running multiple virtual networks with different performance objectives on a shared substrate. The traditional approach of statically dividing resources between virtual networks is often inefficient, while dynamic resource allocation can be unstable. The paper proposes a solution using optimization theory, where each substrate link periodically reallocates bandwidth shares between its virtual links and each virtual network independently maximizes its own performance objective. Numerical experiments with various types of traffic show that this approach is efficient and adaptable. This allows for more efficient and flexible management compared to a single, compromise design.
4199	419931	Space-code bloom filter for efficient traffic flow measurement	Per-flow traffic measurement is crucial for various purposes such as usage tracking, traffic management, and detecting anomalies. However, existing methods either rely on inaccurate random sampling or only account for large flows. This paper introduces a new technique, using a data structure called Space Code Bloom Filter (SCBF), for measuring per-flow traffic accurately and efficiently at high speeds. SCBF utilizes a Maximum Likelihood Estimation (MLE) approach to estimate the size of each flow in a multiset. By adjusting parameters, SCBF allows for a balance between accuracy and complexity. It also introduces a new concept called blind streaming, contributing to data streaming. The performance of SCBF was evaluated on real-world data and showed promising results with low storage and computational requirements.
41100	4110068	Maximally permissive controlled system synthesis for non-determinism and modal logic.	In this paper, the authors propose a new technique for controlled system synthesis on non-deterministic automata. This technique involves restricting the behavior of an uncontrolled system to satisfy a given logical expression, while adhering to the rules of supervisory control. The authors use a requirement formalism that extends Hennessy-Milner logic with modalities from Gödel-Löb logic, allowing for a broad range of control requirements to be expressed. The paper contributes to the field of control synthesis by achieving maximal permissiveness in a non-deterministic context and addressing controllability through partial bisimulation. The authors provide a well-defined and complete derivation of the synthesis result, supported by computer-verified proofs. An algorithmic form of the synthesis method is also presented, with an analysis of its computational complexity. The paper demonstrates the effectiveness of the proposed technique in two industrial case studies and discusses its scalability.
41101	4110153	Characteristic matrices of compound operations of coverings and their relationships with rough sets.	This paper discusses the use of matrices in representing generalized rough sets, specifically focusing on coverings. While matrices have been used in other types of rough sets, they have not been extensively studied in connection with coverings. The paper defines three composition operations of coverings and studies their characteristic matrices, as well as their relationships with covering approximation operators. The paper also introduces a new matrix representation for neighborhoods called the type-2 characteristic matrix. The importance of knowledge fusion and decomposition is highlighted, and the representable properties of covering approximation operators are explored. The paper concludes that matrices have potential for studying covering-based rough sets in data mining and machine learning.
41102	4110264	Characterization of the electromagnetic side channel in frequency domain	This article proposes a new method for identifying and analyzing the electromagnetic (EM) leakage of electronic devices. The approach focuses on identifying the frequencies that leak the most information and uses cryptanalysis techniques to estimate security risks. Two approaches are introduced: an empirical one and one based on information theory. These techniques are low cost, automatic, and fast, and can be performed with an oscilloscope and software. The article also mentions TEMPEST evaluations, which require specialized equipment and are time-consuming. The proposed approach does not replace regulatory TEMPEST evaluations, but can identify leakage with high confidence. An example is provided to demonstrate the effectiveness of this approach in recovering a key from a keyboard at a distance of 5 meters.
41103	411032	Aligning codebooks for near duplicate image detection	Detecting near duplicate images in large databases, such as those found on social networks, digital investigation archives, and surveillance systems, is crucial for various image forensics applications. To address this, hashing techniques are commonly used to index large quantities of images and detect copies from different archives. Recently, the Bags of Visual Features paradigm has been enhanced by incorporating multiple descriptors, such as Bags of Visual Phrases, to take advantage of the coherence between different feature spaces. This paper proposes further improvements to this approach by considering coherence not only during image representation, but also during codebook generation. A new image database with over 3,300 images and 500 scenes with real near duplicates is also introduced for benchmarking purposes. Additionally, a method for compressing the image representation for storage is suggested. Experimental results show that the proposed near duplicate retrieval technique surpasses the original Bags of Visual Phrases approach.
41104	41104110	Using General Impressions to Analyze Discovered Classification Rules	Data mining is a crucial aspect of information retrieval, but one of its main challenges is evaluating the subjective interestingness of discovered rules. Previous research has shown that in real-life applications, a large number of rules are generated but only a few are useful or interesting to the user. This is due to the difficulty of manually analyzing a large number of rules to identify the interesting ones. To address this issue, a technique has been proposed that compares the discovered rules against a type of existing knowledge called general impressions. These general impressions are specified using a representation language, and algorithms are used to analyze the rules. The results of this analysis help identify which rules conform to the general impressions and which ones are unexpected, and therefore considered interesting. 
41105	4110565	Coordination guided reinforcement learning	In this paper, the authors propose a new approach to reinforcement learning (RL) for multi-agent problems. The idea is to use expert coordination knowledge, in the form of constraints, to guide the learning process. These constraints are used to restrict the joint action space and direct exploration towards more promising states, resulting in a faster learning rate. The authors introduce a two-level RL system that incorporates these coordination constraints for online applications. This approach allows for knowledge sharing between constraints and features, and has been tested on a soccer game and a real-time strategy game, showing improved learning rates compared to a single-level approach.
41106	41106118	Biasing mutations in cooperative coevolution	Coevolution is a process in which species evolve in response to each other, with their genetic changes being reciprocal. One type of coevolution is cooperative coevolution, where species work together to solve problems. In this type of coevolution, an individual's fitness is determined by how well it collaborates with individuals from another species. Cooperative revolutionary algorithms (CCEAs) are an extension of evolutionary algorithms, but with a focus on cooperation. In this paper, the authors explore the use of biased mutations in CCEAs and compare them to conventional CCEAs with binary representations. Their experiments show that biased mutations can improve the performance of CCEAs, especially when using high orders of binary representations.
41107	41107135	Joint Server and Network Energy Saving in Data Centers for Latency-Sensitive Applications	The paper discusses the challenges of achieving energy proportionality in data centers that support latency-sensitive applications. Previous studies have focused on either making servers energy proportional or reducing network power consumption for applications with lower latency requirements. The proposed solution, called EPRONS, aims to minimize overall data center power consumption by trading off network slack for additional computation time. A linear programming model is used to consolidate search queries and background flows to a minimal subnet, turning off unused switches and links without violating application deadlines. Servers use a power saving technique based on Dynamic Voltage and Frequency Scaling (DVFS) and can request additional network resources if needed. Experimental results show significant power savings of up to 31.25% in the data center.
41108	4110849	Designing participation in agile ridesharing with mobile social software	In order for sustainability initiatives to be effective, they often require participation at the local community level. This paper examines the use of technology, specifically mobile social software and interface design, to facilitate carpooling as a means of increasing participation. Convenience, ease of use, and adaptability to individual circumstances were found to be crucial in designing a successful carpooling system. While global technology platforms may have limited reach if they do not cater to local needs, a localized approach focusing on designing technology to support and grow mobile social ridesharing networks is proposed. This paper contributes to the understanding of Human-Computer Interaction (HCI) in the context of designing participation.
41109	4110943	Representing Guard Dependencies in Dataflow Execution Traces	The use of heterogeneous parallel systems is becoming increasingly popular in computing. However, one of the main challenges is how to fully utilize the available computational power when porting existing programs or developing new ones. Various design space exploration methods have been developed, but determining the feasible design space for dynamic dataflow programs is still a challenge. This paper proposes a new methodology that uses homotopy theoretic methods to define the design space through a serial execution. It also introduces the concept of dependencies graph with the addition of two new types of dependencies and a 3-tuple notation to represent them. 
41110	4111031	Why is it so difficult for a robot to pass through a doorway using ultrasonic sensors?	The article discusses the challenges of controlling a robot to perform complex tasks, such as navigating through a narrow doorway. The authors argue that while high-level goals are typically used to describe these tasks, details must be provided in order to effectively control the robot. They demonstrate this difficulty using real data from a robot, and propose an architecture based on reconfigurable objects that contain domain knowledge and information about the robot's sensors and actuators. This approach allows for the transformation of high-level goals into specific commands at the time of execution. The authors provide an example of how their approach can be used to successfully complete the task of navigating through a narrow doorway.
41111	4111167	Discovery and Evaluation of Aggregate Usage Profiles for Web Personalization	Web usage mining is a technique that can be combined with other personalization approaches, such as collaborative filtering, to overcome limitations such as subjective user ratings, scalability, and poor performance with high-dimensional and sparse data. However, simply discovering patterns from usage data is not enough for effective personalization. The key is to derive actionable “aggregate usage profiles” from these patterns. This paper presents and evaluates two clustering techniques for discovering overlapping aggregate profiles that can be used by recommender systems for real-time personalization. The results show that these techniques can achieve effective personalization early on in a user's visit to a website, using only anonymous clickstream data and without explicit user input or detailed knowledge about them.
41112	4111254	Analysis of time-delay estimation in reverberant environments.	Room reverberation poses a significant challenge for creating reliable microphone-based source localization systems. To address this issue, researchers have turned to statistical room acoustics and analyzed the effectiveness of GCC-based methods for time-delay estimation. One notable finding is that the PHAT time-delay estimator is proven to be the most optimal among a group of cross-correlation based time-delay estimators. This analysis sheds light on the potential for improving the accuracy and robustness of microphone-based source localization systems.
41113	4111311	A fast handoff scheme for IP over Bluetooth	The use of multiple electronic devices such as cell phones, PDAs, and laptops has become common in daily life. The popularity of Bluetooth, a wireless technology that allows communication between different devices, has been growing due to its ability to connect incompatible devices. One application of Bluetooth, called BLUEPAC, allows users to access public networks like the Internet. However, frequent handoffs between base stations can cause connection losses and hurt network performance. To address this issue, a new scheme has been proposed that involves a base station controller assisting a mobile device in establishing a connection with a new base station. Simulation results have shown that this scheme reduces handoff delay and has a minimal impact on the base station.
41114	4111457	Assessing the computational benefits of AREA-oriented DAG-scheduling	The AREAOriented scheduling (AO-scheduling) paradigm was developed to improve the performance of modern computing platforms, which are "task hungry" and benefit from having many tasks available for allocation to processors. It is a weaker but more robust successor to IC-scheduling, which is not achievable for many DAGs. AO-scheduling coincides with IC-scheduling for optimal DAGs and can be achieved for all DAGs, but the computational complexity of optimal AO-scheduling is not yet known. This paper uses simulation experiments to compare AO-scheduling with other heuristics and shows that it can enhance the efficiency of task-hungry platforms, depending on processor availability and DAG structure.
41115	4111532	Sparse Representations For Classification Of High Dimensional Multi-Sensor Geospatial Data	Modern geospatial sensing techniques capture a large amount of information, resulting in high dimensional feature spaces. This includes hyper-spectral imagery, which records the reflectance/radiance of hundreds of contiguous spectral bands, and full-waveform LiDAR, which records the entire waveform of the return laser pulse. However, despite the high dimensionality, the underlying information can often be represented in a lower dimensional subspace, making it sparsely represented in an appropriate dictionary. Sparse representation based classification has been successfully used for face recognition tasks, but its effectiveness for geospatial image analysis tasks has been limited. To improve performance, researchers have developed a subspace learning preprocessing method that effectively reduces the dimensionality of geospatial data, making it more suitable for sparse representation based classification.
41116	4111618	Cooperative mobile guards in grids	The article discusses the concept of weakly cooperative mobile guards on grids. These guards are allowed to move along vertical and horizontal segments and can see points on the same segment or on crossing segments. The authors propose a quadratic time algorithm for finding the minimum weakly cooperative guard set in polygon-bounded and simple grids. They also present a faster algorithm for horizontally or vertically unobstructed grids. The article then explores the use of weakly cooperative mobile guards in complete rectangular grids with obstacles, showing that a minimum of k+2 guards can cover the grid as long as both dimensions are larger than the number of obstacles. The authors also prove that the problem of finding the minimum weakly cooperative guard set is NP-hard for grids with at most three crossing segments, making the minimum k-periscope guard problem for 2D grids also NP-hard.
41117	41117159	High-Level Estimation Techniques For Usage In Hardware/Software Co-Design	High-level estimation techniques play a crucial role in design decisions such as hardware/software partitioning and design space explorations. Finding the right balance between accuracy and computation time is essential for the feasibility of these techniques. The paper introduces high-level estimation techniques for hardware effort and hardware/software communication time, which provide quick and sufficiently accurate results. It also demonstrates how these techniques can be used to address conflicting design goals, such as performance and hardware effort constraints. The proposed solution is a cost function for hardware/software partitioning that allows for dynamic weighting of its components. Experiments show that combining these estimation techniques leads to more effective hardware/software implementations compared to approaches that only consider single constraints.
41118	4111853	A scheduling algorithm for optimization and early planning in high-level synthesis	The increasing complexity of embedded and programmable systems has made manual mapping of applications onto these systems a tedious task. This has led to the use of high-level synthesis in design flows. In this study, the focus is on data flow graph (DFG) scheduling, which is a crucial part of high-level synthesis. The authors propose an algorithm that schedules a chain of operations with data dependencies, using a technique from computational geometry. This allows for flexibility in optimization objectives and has shown promising results in terms of finding optimal solutions and generating latencies comparable to the optimal solution. The algorithm also allows for the incorporation of different optimization goals, as demonstrated in various examples. 
41119	4111934	Using Sample Selection To Improve Accuracy And Simplicity Of Rules Extracted From Neural Networks For Credit Scoring Applications	This paper presents a method for selecting samples in credit scoring using an ensemble of neural networks. The ensemble identifies outliers by checking the classification accuracy of the networks on the original training data. These outliers are then removed from the dataset, resulting in a simpler and more accurate neural network. The remaining data is used to train and prune another network, and a rule extraction algorithm is applied to generate concise and comprehensible rules. Experimental results show that this approach results in higher predictive accuracy and simpler neural networks compared to using the original dataset. 
41120	411208	Feature-aligned T-meshes	High-order and regularly sampled surface representations are preferred over general meshes as they are more efficient and simplify geometric modeling and processing algorithms. Recent algorithms convert general meshes to regularly sampled form, but feature alignment makes it difficult to get coarse regular patch partitions. This paper suggests using quadrilateral T-meshes, where the intersection of faces may not be the whole edge or vertex, but a part of an edge. This allows for a smaller number of patches and vertices in a base domain while still maintaining feature alignment. T-meshes also have the advantages of quadrangulations, such as high-order representations and easy packing of geometric data into textures, while supporting different types of discretizations for physical simulation.
41121	4112110	Interactive Editing of Large Point Clouds	This paper introduces a new data structure for real-time visualization and interactive editing of large point clouds. This structure allows for efficient rendering and handling of very large data sets through out-of-core storage. Unlike previous approaches, it also allows for dynamic operations such as insertion, deletion, and modification of points with minimal impact on performance. This enables real-time local editing of large models while maintaining a multi-resolution representation for visualization. The structure is demonstrated through a prototypical editing system that provides both real-time local editing tools and a two-resolution scripting mode for larger changes. The system is evaluated on synthetic and real-world examples up to 63GB in size.
41122	4112235	Segment-based tetrahedral meshing and rendering	This article discusses a framework for handling large unstructured tetrahedral meshes in a more efficient manner. The framework is based on a multi-resolution model made up of segments, each containing thousands of tetrahedra. These segments can be adapted in real time to different viewing and classification parameters. Dependencies between segments are stored in a hierarchical graph to ensure consistency. The concept of multi-triangulations is extended to tetrahedral meshes and the segments are stored in a compressed format for efficiency. This framework allows for more interactive visualization of large meshes on standard computers.
41123	4112364	On the Methodology for Calculating SFN Gain in Digital Broadcast Systems	The Single-Frequency Network (SFN) mode is an alternative to the traditional Multi-Frequency Network (MFN) for broadcast networks. SFN uses the same frequency for all transmitters, resulting in better frequency reuse and improved quality of service. However, not all areas within the service area will benefit from SFN, as some may experience degraded quality due to SFN echoes. This paper presents a methodology for calculating the SFN gain, which is a measure of potential gain or interference in SFN. The gain is investigated for a DVB-H network and can be used for coverage planning in future broadcast networks.
41124	4112411	Throughput optimization of wireless LANs by surrogate model based cognitive decision making	The growth of wireless networks and limited availability of the electromagnetic spectrum have led to increased interference and decreased quality of service for users. In response, a new cognitive decision engine has been proposed to optimize the throughput of IEEE 802.11 links in the presence of interference. This engine uses a model to predict throughput based on the current state of the network, allowing it to find the best configuration for controllable parameters. In testing, the cognitive decision engine outperformed a scenario without its use, improving performance by over 100%. This solution shows promise for improving wireless network performance in the face of interference.
41125	4112530	Transmitter Design for Uplink MIMO Systems With Antenna Correlation	The article discusses the study of uplink transmission in multiple-input multiple-output (MIMO) systems with antenna correlation. The focus is on schemes that require only channel covariance information at the transmitter (CCIT), which is cheaper than full channel state information at the transmitter (CSIT). The study starts with mutual information analysis and finds that a simple CCIT-based scheme, known as statistical water-filling (SWF), can perform similarly to the optimal full CSIT-based scheme in MIMO systems with more receive antennas than transmit antennas. The implementation of SWF in coded systems is then explored, with the use of an iterative linear minimum mean squared error (LMMSE) receiver and an extrinsic information transfer (EXIT) chart type curve matching technique. Simulation results show that the proposed scheme outperforms the conventional equal power transmission. It is also found to be efficient in multi-user uplink MIMO systems with distributed channel information.
41126	4112658	Distances in benzenoid systems: further developments	This note discusses new findings on distances in benzenoids, including an algorithm that can compute the Wiener index of a benzenoid system in O(n) time. It also introduces a dismantling scheme for benzenoid systems, derived through breadth-first search on their dual graphs. The note also addresses the clustering problem for sets of atoms in benzenoid systems, proposing an efficient implementation of the k-means clustering algorithm. 
41127	4112791	Local Color Vector Binary Patterns From Multichannel Face Images for Face Recognition.	This paper introduces a new face descriptor called local color vector binary patterns (LCVBPs), which utilizes color information for face recognition. LCVBP is composed of two patterns, color norm and color angular, which capture texture patterns and spatial interactions among spectral-band images. By combining these features, LCVBP achieves high performance on five public databases and outperforms other existing face descriptors. The proposed method is also compared to state-of-the-art techniques, further demonstrating its effectiveness for challenging face images.
41128	4112879	Granular Computing: A Rough Set Approach	Information granule calculi form the basis of granular computing, defined by constructs such as information granules, inclusion and closeness relations, and operations on these granules. As different information sources may have varying interpretations of granule languages, rough inclusion and closeness are used instead of equality. Examples of basic constructs are provided, and the creation of more complex information granules is described through terms. The synthesis problem of robust terms, which satisfy a given specification to a satisfactory degree, is discussed. A method for synthesizing information granules using decomposition of specifications is also presented. These issues are particularly relevant for applications involving spatial reasoning, knowledge discovery, and data mining.
41129	4112920	Extended cubes: enhancing the cube attack by extracting low-degree non-linear equations	This paper introduces an improved version of the original cube attack, which can extract simple low-degree equations in addition to linear ones. This extended cube attack can be used in cryptosystems where the original attack may fail due to a lack of sufficient linear equations. The paper also presents a side channel cube attack on the PRESENT block cipher using the Hamming weight leakage model, which is a more relaxed assumption compared to previous work. The extended cube attack reduces the time and data complexity compared to previous attacks, making it applicable to both the 80-bit and 128-bit key variants of PRESENT.
41130	41130101	Automatic Preview Frame Selection for Online Videos	In this paper, the authors discuss the importance of the preview frame in online videos and the challenges of automatically selecting the best frame. They propose a classification approach that considers three types of features - informativeness, attention, and aesthetics - in order to characterize each frame. Due to the imbalanced training data, they utilize random forests and identify visually similar frames to increase the number of positive training samples. The proposed method is evaluated on a set of news videos and shows promising results. The authors also analyze the contribution of each visual feature for future studies.
41131	4113132	Longitudinal study of a building-scale RFID ecosystem	RFID deployments are increasing in popularity in both industrial and consumer settings. However, there are challenges in managing RFID data streams and dealing with limitations in reader accuracy and coverage. Pervasive computing also presents additional issues related to user acceptance and system utility. A four-week study of a building-scale RFID deployment with 47 readers and 300 tags was conducted to better understand these challenges. The study found that the deployment produced manageable amounts of data, but with significant differences among participants and objects. Tag detection rates were low and varied among different types of tags, participants, and objects. Users need guidance for effective tag placement and are more likely to wear tags with compelling applications. Probabilistic modeling and inference techniques can help overcome data gaps and errors, but may add computational and storage overhead.41132
41132	41132133	Random sampling in residual graphs	This article discusses a new algorithm for finding the maximum flow value in an undirected graph with n vertices and m edges. The algorithm takes Õ(m+nv) time and is based on randomly sampling edges from the residual graph and finding augmenting paths. Special sampling probabilities are assigned to the edges in Õ(m) time, making the algorithm simple to implement. This approach allows for a more efficient way of finding the maximum flow in a graph compared to traditional methods. 
41133	41133164	Co-Utility: Self-Enforcing protocols for the mutual benefit of participants.	Protocols are rules that govern how agents interact with each other in society, and those based on mutually beneficial cooperation are particularly beneficial as they improve societal welfare without the need for a central authority. In this study, the concept of co-utility is introduced as a framework for cooperation between rational agents, where the best strategy for each agent is to help another agent achieve their best outcome. The study focuses on self-enforcing protocols in game-theoretic terms and develops the concept of co-utile protocols, exploring when co-utility can arise. The case of anonymous query submission to a web search engine is used to illustrate the design of co-utile protocols, with theoretical analysis and empirical results showing how co-utility can make cooperation self-enforcing and improve agents' welfare.
41134	4113460	Meta-level patterns for interactive knowledge capture	The current tools for acquiring knowledge have limited understanding of how users input knowledge and how it is used, resulting in a lack of assistance in organizing knowledge authoring tasks. Users must compensate for this by keeping track of their past mistakes, current status, potential new problems, and possible courses of action on their own. This paper introduces a new extension to existing knowledge acquisition tools that organizes past interactions using declarative meta-level patterns and improves suggestions based on these interactions. The focus is on assessing confidence in suggestions, suggesting how to perform knowledge authoring actions based on successful past actions, and monitoring changes in the environment to suggest modifications to the knowledge base. A preliminary study shows that this approach can reduce incorrect suggestions and prevent user mistakes, leading to improved problem solving results.
41135	4113538	RMS-TM: a comprehensive benchmark suite for transactional memory systems	Transactional Memory (TM) is a proposed solution for easier parallel programming on Chip Multiprocessors (CMPs) by replacing traditional lock synchronization constructs. However, the evaluation of TM designs is still lacking realistic applications and comparison against locks. The paper introduces RMS-TM, a benchmark suite consisting of seven real-world applications from the RMS domain, addressing current TM research issues and providing a mix of short and long transactions with different characteristics. This makes it a useful tool for evaluating TM designs, especially on high core counts. The evaluation with STM and HTM systems shows that RMS-TM is also scalable, making it a valuable addition to current TM benchmarks.
41136	4113682	A simple P-complete problem and its language-theoretic representations	The article introduces a new version of the Circuit Value Problem where each gate follows the NOR function and has one of its inputs connected to the previous gate. This problem is still considered P-complete and can be represented as a simple language over a two-letter alphabet using language equations. Three different grammars, including a conjunctive grammar, a Boolean grammar, and an LL(1) Boolean grammar, are created to represent this problem. Additionally, the problem can also be represented by a trellis automaton with 11 states and a linear conjunctive grammar with 20 rules. 
41137	4113769	Combining linear-time temporal logic with constructiveness and paraconsistency	This paper introduces two new versions of linear-time temporal logic, called IB[l] and PB[l], which are extensions of intuitionistic logic and Nelson's paraconsistent logic. These logics are designed to handle not only temporal reasoning, but also constructive and paraconsistent reasoning. The time domain in these logics is bounded by a fixed positive integer, but they are still able to derive most of the typical temporal axioms of LTL. The paper also proves completeness, cut-elimination, normalization, and decidability theorems for these logics, as well as providing sound and complete display calculi. This work serves as a theoretical basis for representing various forms of reasoning in computer science. 
41138	4113822	Protection of Data and Delegated Keys in Digital Distribution	Cryptography is an effective way to protect digital information from unauthorized access. However, in open networks, if an encrypted message is withdrawn by an unauthorized user, an additional mechanism is needed to convert it into a form that can only be accessed by an authorized user. This can be achieved through a proxy cryptosystem, using techniques from the ElGamal or RSA cryptosystems. A blind decryption protocol can also be used to protect privacy and ensure secure information delivery. However, there is a risk of an "oracle problem" where a decrypting person can freely compute exponentiation for any message selected by a requesting person. To address this, a transformable signature can be used, and this paper presents another solution to prevent the abuse of blind decryption protocols in information provider systems.
41139	4113960	Icis 2007 Panel Report: Bridging Service Computing And Service Management: How Mis Contributes To Service Orientation	Service computing is a rapidly growing field in enterprise computing, as organizations strive for agility and to meet changing business needs. Major corporations are implementing initiatives to restructure their IT systems through service computing, creating new challenges and research questions. A framework is needed to align technology and management in this area. The International Conference on Information Systems 2007 held a panel on Bridging Service Computing and Service Management, where panelists discussed the importance of MIS taking a leadership role in this research. The paper highlights the viewpoints of each panelist and presents a joint perspective on bridging service computing and service management.
41140	4114015	Weighted tree automata and weighted logics	This article introduces a weighted monadic second order logic for trees using a commutative semiring as weights. It is shown that a restricted version of this logic can characterize the class of formal tree series accepted by weighted bottom-up finite state tree automata. The restriction can be lifted if the semiring is locally finite. This extends previous results for tree languages and also expands recent findings on formal power series on words to formal tree series. 
41141	4114146	Analysing GeoPath diversity and improving routing performance in optical networks	This paper discusses the increasing vulnerability of telecommunication networks to cascading and regional-correlated challenges, such as natural disasters and intentional attacks. The authors propose a network vulnerability identification mechanism and a new graph resilience metric called cTGGD, which incorporates geographical diversity to measure the resiliency of optical fibre networks. They also present two heuristics for solving the path geodiverse problem and propose the GeoDivRP routing protocol, which outperforms OSPF when faced with area-based challenges. The paper also analyses potential attacker strategies and demonstrates the effectiveness of restoration plans. 
41142	4114234	Summarizing transactional databases with overlapped hyperrectangles	Transactional data, which refers to data that records individual transactions, are found everywhere. To analyze these databases, various methods have been proposed, such as frequent itemset mining and co-clustering. This research presents a new problem of succinctly summarizing transactional databases by linking the database's structure to numerous frequent itemsets. The problem is formulated as a set covering problem using overlapped hyperrectangles, which has been proven to be NP-hard and has a connection to the compact representation of a directed bipartite graph. An approximation algorithm, Hyper, is developed to solve this problem in polynomial time with a logarithmic approximation ratio. A pruning strategy is also proposed to speed up the algorithm's processing, and an efficient algorithm, Hyper+, is introduced to further summarize the hyperrectangles by allowing false positive conditions. The generated hyperrectangles can be properly visualized and have been proven effective and efficient in summarizing both real and synthetic datasets. 
41143	4114373	Design and implementation of a new 3-DOF electromagnetic micropositioner utilizing flexure mechanism	The paper introduces a compact micropositioner with three degrees-of-freedom (DOFs) and large travel ranges. It utilizes a monolithic parallel flexure mechanism with electromagnetic actuators and optical sensors for precise 3-DOF motion. The system is controlled by an adaptive sliding-mode controller that consists of a sliding mode controller, adaptive law, and force allocation. Experimental results show that the system has satisfactory stiffness and precision.
41144	4114431	Prediction-as-a-Service for Meme Popularity	Memes are ideas, behaviors, or styles that spread among people in a community. Recent research has shown that memes follow epidemic patterns on microblogs and their popularity can be predicted. Some models consider the detailed structure of community graphs to accurately predict meme spread, while others sacrifice accuracy for efficiency by assuming equal connectivity within a community. This paper proposes a prediction service that combines the benefits of both approaches. It considers community graph properties without the complexity of their structural details, resulting in more efficient online consumption. The service identifies memes that have the potential for widespread coverage by comparing their virality to a community threshold. It also takes into account sampling biases to make reliable thresholds and accurate predictions, as demonstrated in experiments.
41145	411455	Mobile data gathering with Wireless Energy Replenishment in rechargeable sensor networks	The advancement of wireless energy transfer technology has allowed for the charging of sensor batteries in a wireless sensor network (WSN), enabling perpetual operation of the network. This has opened up new possibilities for designing sensor network protocols. However, the variability of energy recharging rates in wireless rechargeable sensor networks poses a challenge in achieving optimal data gathering strategies. To address this, a joint framework of wireless energy replenishment and mobile data gathering is proposed in this paper. The framework considers different sources of energy consumption and the time-varying nature of energy replenishment. The proposed distributed algorithm includes subalgorithms for each sensor node and the mobile collector, and is evaluated through numerical simulations. The results demonstrate the convergence of the algorithm and the impact of utility weight on network performance.
41146	4114631	Simple dynamics for plurality consensus	The Plurality Consensus process is studied in a communication network where n anonymous agents support an initial opinion and can revise it based on a random sample of neighbors. The goal is for all agents to converge to the stable configuration where the majority color is supported. The initial color configuration has a large bias, with the plurality color being supported by a higher number of nodes. The process is based on a basic model with a clique network and a majority rule update (3-majority dynamics) where each agent looks at the colors of three random neighbors. The convergence time is proven to be Θklog n for various values of k and n, which is a linear relationship. It is shown that looking at more than three random neighbors does not significantly speed up the process, with samples of polylogarithmic size only having a polylogarithmic impact on the speed. This is in contrast to the median process, which has a much faster convergence time. 
41147	4114743	Greedily Improving Our Own Centrality in A Network	The closeness and betweenness centralities are commonly used to measure the importance of a vertex in a complex network. High values of these centralities can have positive effects on the vertex. This paper focuses on the problem of increasing a vertex's centrality by creating a limited number of new edges connected to it. It is proven that this problem is difficult to solve efficiently unless the P=NP conjecture is true. A greedy approximation algorithm with a nearly optimal ratio is proposed and tested on simulated and real networks.
41148	4114814	Packing cycles in undirected graphs	The paper discusses the problem of finding the largest collection of edge-disjoint cycles in an undirected graph, known as CYCLE PACKING, which has practical applications in computational biology. The complexity and approximability of this problem is studied, and it is shown to be APX-hard but can be approximated within O(log n) by using a simple greedy approach. The authors also provide a non-trivial example where the greedy approach achieves a ratio of Ω(√logn/(loglogn)). For graphs with a certain level of sparsity, an approximation ratio of 2t/3 can be achieved in polynomial time. A linear programming relaxation for the problem is also discussed.
41149	4114946	Texture analysis using feature-based pairwise interaction maps	Pairwise pixel interactions have been shown to be effective in both feature-based and model-based texture analysis. The feature-based interaction map (FBIM) approach has been successfully applied in various applications and has been discussed in different aspects, but there is no complete description of the method available. This paper offers a comprehensive overview of the FBIM method, including all major algorithms and systematic experimental studies to showcase its capabilities. This serves as an up-to-date survey of the approach and its potential in texture analysis.
41150	4115045	Parallel scheduling for cyber-physical systems: Analysis and case study on a self-driving car	The increasing complexity of software for Cyber-Physical Systems (CPS) has led developers to turn to multi-core processors and parallel programming models like OpenNP to ensure timely execution. In this paper, the authors propose a real-time scheduling method for parallel tasks on multi-core processors, specifically in CPS like self-driving cars. They extend the fork-join parallel task model and develop a task stretch* transform to efficiently schedule tasks with varying numbers of parallel threads. The proposed algorithm, implemented on Linux/RK and Boss (winner of the 2007 DARPA Urban Challenge), shows promising results with a resource augmentation bound of 3.73, meaning tasks can be completed faster on fewer processors. The scheme is evaluated on Boss through its driving quality, including curvature and velocity profiles.
41151	411517	Deadline-Based Differentiation in P2P Streaming	The paper discusses the benefits of splitting a P2P video distribution into multiple media flows with varying priorities. This approach allows for the development of flexible and adaptive streaming systems, suitable for both VoD and TV. It also minimizes network resource usage by discarding low-priority flows when receivers do not have enough resources. The paper focuses on chunk-based video distribution in unstructured meshes, using a push strategy and a deadline-based scheduling algorithm to prioritize flows and avoid sending duplicate chunks. The results of experiments demonstrate improved streaming performance and PSNR measures compared to strict priority and single stream distribution.
41152	41152224	Feature deduction and ensemble design of intrusion detection systems	This study aims to identify important input features for building a computationally efficient and effective intrusion detection system (IDS). Two feature selection algorithms, Bayesian networks (BN) and Classification and Regression Trees (CART), were evaluated, as well as an ensemble of the two. Results showed that significant input feature selection is crucial for designing a lightweight and effective IDS for real-world systems. The study also proposes a hybrid architecture for combining different feature selection algorithms in IDS. Overall, the study highlights the importance of feature selection in creating a successful IDS for detecting intrusion and misuse patterns.
41153	4115349	Evolutionary Design of Intrusion Detection Programs	Intrusion detection is the process of monitoring a computer system or network for signs of unauthorized access or attacks. This paper proposes the development of an Intrusion Detection Program (IDP) that uses genetic programming techniques to detect known attack patterns. The IDP acts as a last line of defense, working alongside other preventive measures. Three genetic programming techniques - Linear Genetic Programming (LGP), Multi-Expression Programming (MEP), and Gene Expression Programming (GEP) - were evaluated for designing the IDP. Results showed that genetic programming techniques can be effective in creating lightweight and accurate IDPs, making them a viable alternative to traditional intrusion detection systems based on machine learning. 
41154	4115456	CompChall: Addressing Password Guessing Attacks	Passwords are a common way to authenticate users, but they also make them vulnerable to dictionary attacks. These attacks come in two forms: offline, which requires eavesdropping on communication, and online, which can be done by anyone. While public key cryptography can protect against offline attacks, there is no effective solution for online attacks. To address this issue, a new authentication protocol called CompChall is proposed. It uses one-way hash functions and a challenge-response system to prevent online dictionary attacks. The system is designed to be easy for legitimate users, but difficult for adversaries attempting to launch multiple login requests. The protocol is also stateless, making it less susceptible to DoS attacks.
41155	4115533	Toward a distributed benchmarking tool for biometrics	Computer science research is constantly evolving, making it necessary to efficiently test new algorithms on a significant benchmark. This requires a powerful computation capability, which can be achieved through distributed computing in research laboratories. In the field of biometric systems, numerous computations on various data are needed, typically images. To simplify this task, the paper proposes a software-based solution that uses a server to distribute computation tasks among available clients. The results of experiments using this software demonstrate its effectiveness.
41156	4115660	A signal-flow-graph approach to on-line gradient calculation.	The article discusses how signal flow graphs (SFGs) can effectively represent nonlinear dynamic adaptive systems, such as dynamic recurrent neural networks. These systems are described as a general connection of simple components, similar to an electrical circuit. While SFGs are commonly used for qualitative descriptions in the neural network community, this article demonstrates their potential for rigorous representation and computational purposes. The article presents a method for computing the system's output or cost function with respect to its parameters, using the SFG representation theory and its properties. This method can be applied to various types of dynamic systems, including feedforward, time-delay, and recurrent neural networks. The article also highlights the importance of on-line learning for real applications, such as digital signal processing and system identification and control.
41157	4115725	Improving manual reviews in function-centered engineering of embedded systems using a dedicated review model	Model-based engineering of embedded systems requires manual validation activities, such as reviews and inspections, to ensure that the system meets the intended goals of stakeholders. Changes in stakeholder intentions often require revisions of previously developed and documented engineering artifacts, which are not always properly documented or consistently incorporated. Manual reviews are typically used in industry to ensure that stakeholder intentions are adequately considered in these artifacts. This article introduces a review model specifically for behavioral requirements and functional design specifications, which has been shown to significantly increase the effectiveness and efficiency of manual reviews in controlled experiments. The use of this review model also leads to more confident decisions and is perceived as more supportive by reviewers. 
41158	4115891	Analysis And Optimization Of Message Acceptance Filter Configurations For Controller Area Network (Can)	Due to cost pressures, processors used in automotive ECUs have limited resources such as low clock speeds and limited memory. CAN is used to connect these ECUs, but the broadcast nature of CAN can cause unnecessary processing load on receiving nodes. To reduce this load, hardware filters can be used to filter out irrelevant messages based on message IDs. However, selecting the best filter configuration to minimize load is a complex problem and is NP-complete. This paper proposes using Simulated Annealing to find near-optimal filter configurations, specifically for cases where there are more desired messages than available filters.
41159	4115958	Design of a novel finger haptic interface for contact and orientation display.	The article discusses a new portable device that allows for haptic interaction with virtual environments. It is a lightweight fingertip interface that provides cutaneous feedback and displays contact and non-contact transitions in immersive virtual environments. The device is mounted on a kinesthetic haptic interface and has a described kinematics, mechanical design, and control system. Tests have shown that the device performs well in tasks involving shape exploration and is comparable to a kinesthetic haptic interface. This new device has the potential to enhance the experience of virtual reality by providing both cutaneous and kinesthetic feedback.
41160	411609	GyroPen: Gyroscopes for Pen-Input With Mobile Phones	GyroPen is a method that uses standard built-in sensors in smartphones to reconstruct the motion path of a pen-like interaction. It focuses on the angular trajectory of the phone's corner touching a surface, using data from the gyroscopes and accelerometers. This eliminates the need for accurate 3-D position estimation, which can be challenging with low-cost accelerometers. The method is connected to a handwriting recognition system and has been proven effective in two experiments, with novice participants achieving an average time of 37 seconds to write their first word and experienced users writing at a speed of 3-4 seconds per English word with an 18% character error rate. This suggests that GyroPen has potential for text entry.
41161	4116165	We Don’t Need No Bounding-Boxes: Training Object Class Detectors Using Only Human Verification	Training object class detectors usually requires a large set of images with manually drawn bounding-box annotations, which is time consuming. A new approach is proposed where annotators only need to verify automatically generated bounding-boxes. The process involves re-training the detector, re-localizing objects, and human verification. The verification signal is used to improve re-training and reduce the search space for re-localization. This method is different from traditional weakly supervised methods. Experiments on PASCAL VOC 2007 show that this approach leads to rapid production of high-quality annotations, comparable to fully supervised training. Additionally, it reduces total annotation time by 6-9 times due to the quick verification task.
41162	411626	Software Development Effort Estimation Using Fuzzy Logic - A Survey	Effort estimation is a crucial aspect of project management, especially in software development. It involves predicting the amount of time and resources required to complete a project. This estimation is typically done at the beginning of the project, when the problem to be solved is still unknown. However, accurately estimating effort is a challenging task and can greatly impact the success of a project. To address this issue, fuzzy logic, which mimics the imprecise decision-making processes of the human brain, has been used in some software estimation models. This survey aims to analyze the use of fuzzy logic in existing models and provide a comprehensive review of software estimation techniques used in both industry and literature, including their strengths and weaknesses.
41163	41163279	Capture, Management, and Utilization of Lifecycle Information for Learning Resources	The lifecycle of learning resources involves various processes such as creation, usage, provision, and reuse. To ensure reusability, learning resources often need to be adapted for different contexts, resulting in multiple re-authoring processes. These processes generate different types of information that can be useful for future retrieval, authoring, and use of learning resources. This paper examines the lifecycle of learning resources and the information that is generated, and proposes a distributed architecture to capture, process, manage, and utilize this information in a generic manner. The framework has been implemented in three steps and initial evaluations have shown promising results. 
41164	4116416	Predicting The Existence Of Design Patterns Based On Semantics And Metrics	The identification of design patterns is an important aspect of the reengineering process, as it provides valuable information for designers. However, existing pattern detection methods often have limitations, such as only detecting exact pattern instances or lacking guidelines on which pattern to search for first. To address these issues, a new approach is proposed which involves a preliminary "sniffing" step to detect potential patterns and rank them based on their similarity to design elements. This approach uses design metrics to evaluate the structure and meaning of different patterns. By optimizing the pattern detection process, this approach aims to improve the understanding of existing designs and facilitate code and design improvements. 
41165	411655	On being cool: exploring interaction design for teenagers	This paper discusses a series of studies that examined the concept of 'cool' in relation to designing interactive products for teenagers. The studies confirmed a three-dimensional model of cool, which includes having, doing, and being cool. The methods used in the studies were effective in exploring this model and providing insights for product design. The paper also presents key aspects for designing cool products and provides an example of their application. It highlights the complexity of cool as a design space and offers insight on how to avoid designing products that are considered uncool. Overall, the studies demonstrate the relevance and importance of understanding cool in designing interactive products for teenagers.
41166	4116611	Exploiting Local Knowledge to Enhance Energy-Efficient Geographic Routing	Geographic routing is a popular method for routing information in large wireless sensor networks. It uses a greedy forwarding strategy where a sensor node chooses the most promising neighbor closer to the destination. Energy efficiency is crucial in these networks due to limited battery life. In this paper, a new algorithm called Locally-Optimal Source Routing (LOSR) is introduced. Unlike existing algorithms, LOSR considers all nodes in the neighborhood to find a locally optimal path towards the next hop. Source routing is then used to guide data packets along this path. Simulation results demonstrate that LOSR outperforms existing solutions in terms of energy efficiency in various network scenarios.
41167	411679	Robust stability of interval bidirectional associative memory neural network with time delays.	The paper introduces a new model, called the interval dynamic BAM (IDBAM) model, which combines the conventional bidirectional associative memory (BAM) neural network with signal transmission delay. The IDBAM model is able to study the effects of deviations in network parameters and external perturbations on the network's behavior. The paper presents several different Lyapunov functionals and uses the Razumikhin technique to derive sufficient conditions for unique equilibrium and robust stability of the IDBAM model. The paper also extends its investigation to the time-varying delay case and derives robust stability criteria for BAM with perturbations of time-varying delays. The analysis method used in the paper can consider various types of activation functions, making it applicable to a wide range of BAM neural networks. The authors believe that the results obtained in this paper have significant implications for the design and application of BAM neural networks.
41168	4116838	Visualizing lifelog data for different interaction platforms	The use of interactive platforms and devices in people's daily lives is rapidly increasing, with new applications and services constantly emerging. This has led to the need for considering affordances and contexts for these devices when designing visualization and interaction strategies for lifelogging. As the activity becomes more ubiquitous, we will be interacting with a diverse set of devices. In this paper, the authors describe their project where they created interactive visualizations and usage scenarios for lifelogging on three different devices: smartphones, tablets, and desktops. Each device was utilized to maximize its unique interaction capabilities, resulting in three distinct lifelog data usage scenarios.
41169	41169147	Extraction of events and temporal expressions from clinical narratives.	This paper discusses the task of event and timex extraction from clinical narratives, specifically in the context of the i2b2 2012 challenge. While previous approaches have used multi-class classifiers to identify event types, this paper introduces a sentence-level inference strategy that considers the relationships between events. The authors also propose novel features, such as clinical descriptors from medical ontologies, to improve the accuracy of the extraction process. For timex extraction, the authors adapt a state-of-the-art system and develop rules to complement it. Overall, the system achieved a high F1 score for both event and timex extraction. The paper also includes a detailed error analysis and suggests ways to further improve accuracy.
41170	4117027	Towards situated speech understanding: visual context priming of language models	Fuse is a situated spoken language understanding system that combines visual context and speech interpretation to accurately identify objects in a scene. By fusing knowledge of visual semantics and the specific contents of a scene, the system can anticipate various ways a person might describe an object and use this information to improve speech recognition. A dynamic visual attention mechanism is also used to focus processing on likely objects in the scene as spoken utterances are processed. This integration of visual context in speech recognition has been shown to significantly improve accuracy in evaluations and has potential applications in mobile and assistive technologies. 
41171	4117161	Model-driven multidimensional modeling of secure data warehouses	Data warehouses, multidimensional databases, and online analytical processing applications are crucial for companies to make informed decisions based on historical data. It is important for these systems to have strong security and confidentiality measures in place from the beginning of a project. However, current proposals for conceptual multidimensional modeling do not consider security as a key element and do not allow for specifying confidentiality constraints. To address this issue, the authors propose an Access Control and Audit (ACA) model for conceptual multidimensional modeling and extend the Unified Modeling Language (UML) with this model. The use of the Object Security Constraint Language (OSCL) ensures that these constraints are not arbitrarily applied. This approach is also aligned with the Model-Driven Architecture, Model-Driven Security, and Model-Driven Data Warehouse, making it compatible with recent technologies.
41172	41172229	CQA-ENV: An Integrated Environment for the Continuous Quality Assessment of Software Artifacts	The quality of software artefacts is a growing concern for software development organizations. The quality of the final software product is heavily influenced by the quality of the models produced throughout the development process. With the emergence of Model Driven Development, there is a need for a more generic and flexible methodology for assessing the quality of software artefacts. In this paper, the authors propose an integrated environment called "CQA-ENV" which includes a methodology for continuous quality assessment and a set of tools to support it. This environment can be used by companies offering quality assessment services, as well as software development organizations looking to perform their own evaluations. 
41173	4117339	Improving contextual advertising matching by using Wikipedia thesaurus knowledge	Contextual advertising is a common form of online advertising that involves placing relevant ads within the content of a web page to improve user experience and increase ad clicks. However, traditional keyword matching techniques often struggle with this type of advertising due to issues such as multiple meanings for keywords and a lack of relevant semantics. To address these problems, a new approach that utilizes Wikipedia thesaurus knowledge has been introduced. This approach involves mapping each page to a keyword vector and using two additional feature vectors based on Wikipedia concepts and categories to determine relevant ads. Results from experiments show that this approach outperforms traditional methods and can significantly improve ad selection.
41174	4117467	On-line scheduling with precedence constraints	 for this problem (see [2] ). Hence, to prove lower bounds on the competitive ratio of randomized algorithms, it is enough to find a probability distribution on the input sequences such that the expected competitive ratio of any deterministic algorithm is at least c . To prove the lower bounds for the related machines case, we use the same probability distribution as in [4] . In the restricted assignment case, we use a new probability distribution. Consider the following probability distribution: for some constant p > 0, each job j will arrive with probability p / m and have running time w j = m / p . We define the distribution such that jobs with running time m / p can be processed on any machine, while jobs with running time less than m / p can be processed on a subset of machines. We show that the expected competitive ratio of any deterministic algorithm for this distribution is Ω ( log m) .
41175	4117543	WHOIS Lost in Translation: (Mis)Understanding Domain Name Expiration and Re-Registration.	Internet domain names have an expiration date and can be claimed by a new owner if not renewed. However, there is limited understanding of how often and how quickly these domain names are re-registered after expiration, leading to potential bias in studies. While registration data is available in Whois databases, scalability issues and data ambiguities make studying re-registrations difficult. By focusing on domains that are about to be deleted, researchers were able to track 7.4 million domains over 10 months and found that re-registrations can happen soon after expiration, particularly for older domains. The complexity of Whois data can also pose challenges for both operational and research communities.
41176	4117623	AccessMiner: using system-centric models for malware protection	System call models are commonly used to analyze the behavior of programs. They are used by intrusion detection systems and for access control. With the rise of malware, system calls have been proposed as a way to distinguish between benign and malicious processes. However, most models are based on specific behaviors of individual applications and may not generalize well when exposed to diverse, real-world applications. Previous studies have also been limited in their data collection and only evaluated a small set of programs. To address these limitations, a new system-centric approach is proposed, which models the general interactions between programs and the operating system. This approach shows promising results in detecting malware while minimizing false positives.
41177	41177101	Static analysis for detecting taint-style vulnerabilities in web applications	The number of web applications has increased significantly in recent years, leading to an increase in security vulnerabilities. Manual code reviews are time-consuming and costly, making the need for automated solutions clear. This paper proposes using static source code analysis, specifically flow-sensitive, interprocedural, and context-sensitive data flow analysis, to identify vulnerable points in a program. In addition, they use a precise alias analysis to target the unique reference semantics found in scripting languages. The paper also discusses an iterative two-phase algorithm for resolving file inclusions, enhancing the accuracy and number of vulnerability reports. The techniques presented can be applied to various types of vulnerabilities, and have been implemented in Pixy, a tool for detecting cross-site scripting and SQL injection vulnerabilities in PHP programs. Testing on popular open-source web applications revealed hundreds of previously unknown vulnerabilities, demonstrating the effectiveness of the techniques for security audits.
41178	4117812	Analysis of Minimum Distances in High-Dimensional Musical Spaces	The proposed method aims to improve music search engines and recommended systems by automatically measuring content-based music similarity. It uses features extracted from unlabeled audio data and a distance threshold to efficiently solve retrieval tasks. The method is compatible with locality-sensitive hashing, resulting in significantly faster retrieval times. The method's performance was evaluated on three different music similarity tasks, achieving near-perfect results in two tasks and 75% precision at 70% recall in the third task. The approach is efficient for large music collections and can be implemented with different databases.
41179	4117943	A Perceptual Subspace Method For Sinusoidal Speech And Audio Modeling	The problem of representing a signal segment as a sum of damped sinusoidal components is important in various fields like speech and audio processing. Typically, subspace-based methods are used to estimate the model parameters, taking advantage of the shift-invariance property. However, these techniques do not consider the relevance of the model components in terms of perception. This paper proposes a combination of subspace-based methods with a perceptual distortion measure to extract perceptually relevant components. Experiments using audio signals show that this algorithm significantly improves signal quality compared to traditional subspace-based methods, based on both objective and subjective evaluations.
41180	4118056	Three novel opportunistic scheduling algorithms in CoMP-CSB scenario	Coordinated scheduling/beamforming (CSB) is a technique within the coordinated multi-point (CoMP) transmission that has gained attention for its potential to reduce inter-cell interference and increase cell-edge throughput. However, current scheduling algorithms for CSB are complex and have high overhead. In this paper, three new opportunistic scheduling algorithms are proposed that consider both the intended channel of the scheduled user and the orthogonality between the intended channel and interference channels from nearby cells. These algorithms aim to exploit multi-user diversity and mitigate interference. Simulation results show that all three algorithms improve signal to interference plus noise ratio (SINR) and achieve higher throughputs and utilities compared to existing algorithms. Algorithm 2 has the best performance, but requires more complexity and overhead than Algorithms 1 and 2. Optimal parameters for all three algorithms are also provided to balance performance and complexity. 
41181	4118145	Efficient algorithms for computing a class of subsethood and similarity measures for interval type-2 fuzzy sets	Subsethood and similarity measures are important concepts in fuzzy set theory, specifically for type-1 and interval type-2 fuzzy sets. Rickard et al. have defined a measure for IT2 FS subsethood, based on Kosko's T1 FS subsethood measure and the Representation Theorem. Nguyen and Kreinovich have also extended the Jaccard similarity measure for T1 FSs to create a measure for IT2 FS similarity. The authors also propose efficient algorithms for computing these measures and simulations show that they outperform existing algorithms. These measures and algorithms have practical applications in fields such as pattern recognition and decision making.
41182	4118295	Advanced computing with words using syllogistic reasoning and arithmetic operations on linguistic belief structures	This paper presents solutions to an Advanced Computing with Words problem, which is equivalent to one of Zadeh's challenge problems on linguistic probabilities. The problem is interpreted using a syllogism based on the entailment principle, resulting in two linguistic belief structures. These structures are then combined using addition to obtain a belief structure for the variable of interest. Pessimistic and optimistic probabilities can be inferred from this belief structure using Linguistic Weighted Averages and compatibility measures. Vocabularies for linguistic attributes and probabilities are chosen and modeled using interval type-2 fuzzy sets. The resulting probabilities are translated into words from the vocabulary for easier understanding by humans.
41183	4118360	Performance Evaluation of WMN-GA System for Node Placement in WMNs Considering Exponential and Weibull Distribution of Mesh Clients and Different Selection and Mutation Operators	This paper evaluates the performance of the WMN-GA system for the node placement problem in wireless mesh networks (WMNs). The study considers two types of distributions for mesh clients and various selection and mutation operators. A population size of 64 and 200 generations are used for evaluation. The performance is measured using the giant component and number of covered users metrics. Results indicate that the WMN-GA system performs better for Exponential distribution of mesh clients. 
41184	4118459	A Fuzzy-Based Approach for Classifying Students' Emotional States in Online Collaborative Work	Emotion awareness has become an important aspect in collaborative work in academia, enterprises, and organizations that use group work. With the widespread use of ICT's, collaboration can mostly be done through communication channels like forums and social networks. The emotional state of users during collaborative activities, such as learning or project work, can greatly impact their performance and outcomes. Therefore, it is crucial to monitor and use this emotional information to provide feedback and support. In this paper, a fuzzy approach is proposed to process data collected from communication channels and classify the emotional states of users. This is achieved by using a fuzzy-based emotive classification system, which is fed with data from the ANEW dictionary. The proposed system has been evaluated using real data from collaborative learning courses in academia.
41185	41185138	A Fuzzy-Based Cluster-Head Selection System forWSNs Considering Different Parameters	Cluster formation and cluster head selection are crucial factors in sensor network applications, as they greatly impact the communication energy consumption within the network. However, selecting an appropriate cluster head can be challenging in different environments with varying characteristics. To address this issue, a power reduction algorithm using fuzzy logic and node movement has been proposed. Unlike previous systems, this algorithm takes into account 4 linguistic parameters: Remaining Power of Sensor, Degree of Number of Neighbor Nodes, Distance from Cluster Centroid, and Sensor Speed. By considering the node's speed, the algorithm can predict if it will leave the cluster, aiding in cluster-head decision making. Simulation results demonstrate the effectiveness of this approach in selecting optimal cluster heads.
41186	41186173	Model driven middleware: A new paradigm for developing distributed real-time and embedded systems	Distributed real-time and embedded (DRE) systems are crucial in various domains, including avionics, telecommunications, tele-medicine, and defense applications. These systems are increasingly interconnected through networks, creating systems of systems. One of the main challenges for DRE systems is meeting a diverse set of quality of service (QoS) requirements, such as predictable latency, scalability, and security, in real-time. Managing long lifecycles is also a significant challenge, as DRE systems often rely on commercial-off-the-shelf (COTS) technology that may change over time. To address these challenges, a new software paradigm called Model Driven Middleware (MDM) combines model-based development techniques with QoS-enabled component middleware. The CoSMIC toolsuite, which uses MDM, helps developers and integrators throughout the lifecycle of DRE systems by facilitating component partitioning, software configuration validation, QoS assurance, and adaptation to changing technology. 
41187	4118740	SAT solving for argument filterings	This paper discusses the use of a propositional encoding for lexicographic path orders in dependency pairs, making it possible to use SAT solvers for analyzing the termination of term rewrite systems. The paper focuses on two key issues: finding a lexicographic path order and an argument filtering process to orient inequalities, and how the choice of argument filtering affects the set of inequalities that need to be oriented. The proposed method has been implemented in the termination prover AProVE, and experiments have shown significant improvements in both speed and termination proving capabilities.
41188	4118862	Single-Carrier Incremental Relaying with Joint Tx/Rx FDE	The proposed scheme is an incremental relaying method that utilizes joint Tx/Rx frequency-domain equalization (FDE) for single-carrier transmission. If a source node successfully sends a packet to a relay node but not to the destination node, the source and relay nodes cooperate for retransmission. This is possible because they share channel state information (CSI) with the destination node. The joint Tx/Rx FDE is performed by optimizing the weights among the three nodes based on the minimum mean square error (MMSE) criterion and a total transmit power constraint. Computer simulations show that this scheme is effective.
41189	4118912	Exploring Affiliation Network Models As A Collaborative Filtering Mechanism In E-Learning	This article discusses the use of data from online interactions between learners and tutors to aid in decision making for future learning activities. Specifically, the article explores the use of an affiliation network model, blockmodeling, and m-slices to determine the configuration of topics and learner groups. The results of a case study showed that these techniques can be used to filter participants, rearrange topics, and dynamically change the structure of a course. This approach can be seen as a type of collaborative filtering based on social network structure. 
41190	4119018	The ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics	The ACL Anthology is an online database containing conference and journal papers related to natural language processing and computational linguistics. While its main purpose is to provide a reference for research, it can also be utilized as a platform for further study. A new reference corpus, the ACL Anthology Reference Corpus (ACL ARC), has been created from the ACL Anthology and is available for use by researchers. This corpus was developed through collaboration with several research groups and is intended to be a standard testbed for experiments in scholarly document processing. The ultimate goal is to make the corpus widely accessible and encourage its use in bibliographic and bibliometric research.
41191	41191110	Analysis and control of power grasping	The article discusses the issue of grasping objects with a robotic hand and the limitations of controlling interactions due to potential defects in the fingers. It highlights the importance of coordinating such defective systems to still perform effectively, as it is common in many manipulation operations. The paper then focuses on optimizing contact forces in power grasps through establishing a basis of grasp forces and developing a control algorithm that minimizes the risk of slippage and maintains bounded contact forces. This allows for the asymptotic convergence to an optimal grasp force configuration. The strategies and analysis presented in the paper aim to improve the effectiveness of robotic hand grasping in various manipulation tasks.
41192	4119211	Probabilistic signature based generalized framework for differential fault analysis of stream ciphers.	Differential Fault Attack (DFA) involves injecting faults at random locations and times in order to identify the exact location and timing of the fault. This paper introduces a new approach to DFA using probabilistic signatures and Maximum Likelihood methods. This technique is able to identify faults in a variety of encryption algorithms, including the Grain family, MICKEY 2.0, and Trivium. It also provides improved fault attacks for all versions of the Grain family and MICKEY 2.0, and is able to handle cases where certain bits of the keystream are missing. The paper also presents a solution to the previously unsolved problem of identifying faults in random time for Grain 128a, and reduces the required number of faults for MICKEY 2.0 by 60%.
41193	4119365	HAUCA Curves for the Evaluation of Biomarker Pilot Studies with Small Sample Sizes and Large Numbers of Features.	Biomarker studies aim to find a combination of measured attributes to help diagnose a specific disease. These attributes are often obtained through high-throughput technologies like next generation sequencing, resulting in a large number of potential biomarkers. However, with a small sample size of only 24 patients, it is difficult to accurately identify specific biomarkers using purely statistical methods. Despite this limitation, researchers can still determine if there are more biomarkers strongly correlated with the disease compared to what would be expected by chance. This can be done using a method based on the area under the ROC curve (AUC). The authors also provide estimations for sample sizes in follow-up studies that can successfully identify concrete biomarkers and build disease classifiers. The method can also be adapted to other performance measures besides AUC.
41194	4119448	Neuro-fuzzy control based on the NEFCON-model: recent developments	 optimizationFuzzy systems are widely used in various industries and scientific fields, but designing and optimizing them can be time-consuming. To address this issue, algorithms have been developed to automatically construct and optimize fuzzy systems. One popular approach is combining fuzzy systems with learning techniques from neural networks, known as neuro-fuzzy systems. This paper discusses the NEFCON-Model, which uses reinforcement learning to optimize a Mamdani-style fuzzy controller rule base in real-time. The paper also describes methods for determining a fuzzy error measure for a dynamic system and presents implementations and an application example. These implementations are available for non-commercial use. The paper highlights the benefits of using hybrid neuro-fuzzy systems for system optimization.
41195	4119566	Supporting Knowledge Sharing In Heterogeneous Social Network Thematic Groups	Thematic groups are becoming increasingly popular in online social networks (OSNs), as users collaborate and share ideas within these groups. Personal software agents can assist users who belong to multiple groups within different OSNs by facilitating relationships and cooperation. These software agents are able to gather detailed information about users and their interests, which can be used to match them with relevant groups. This article presents a multi-agent framework that focuses on the role of software agents and the use of a shared dictionary for each group. Personal agents are associated with individual users to share knowledge related to a specific theme, while group agents support their respective groups by interacting with personal agents and managing group membership. The common dictionary of each group is a key element in enabling knowledge sharing and interoperability between personal and group agents. Each user agent can personalize their own dictionary and contribute to the group's dictionary by selecting relevant categories.
41196	411965	A fault-tolerant self-organizing flocking approach for UAV aerial survey.	In this paper, the authors focus on the issue of self-organization for a group of multirotor UAVs during a monitoring mission. This type of mission involves collecting pertinent data from a specific area and sending it back to a Base Station. The paper delves into the challenges of coordinating the movements and tasks of multiple UAVs to efficiently complete the mission. The authors propose a solution using decentralized control algorithms that allow the UAVs to communicate and coordinate with each other without relying on a central controller. The effectiveness of this approach is demonstrated through simulation results.
41197	4119765	Augmenting a Web Server with QoS by Means of an Aspect-Oriented Architecture	This paper discusses the use of aspect-orientation to handle Quality of Service (QoS) on a web server, specifically Jigsaw. By using this approach, QoS concerns can be separated from communication-related concerns and improvements can be made to the web server's processing of incoming requests. This is achieved by associating requests with priorities and implementing checks on resource usage and tasks. The suggested aspects are connected at compile-time to existing classes, keeping the QoS-enforcing code separate from the web server modules. Overall, this approach allows for more effective handling of QoS parameters on a web server.
41198	411987	Reconstructive explanation: explanation as complex problem solving	Existing explanation facilities are better suited for knowledge engineers conducting system maintenance rather than end-users. This is because the current methods simply provide a trace of the problem-solving steps. However, a more effective approach involves reorganizing the reasoning process and incorporating additional information to support the result. This highlights the complexity of explanation as a problem-solving process that relies not only on the reasoning but also on domain knowledge. A new computational model of explanation is introduced, which is argued to be more effective than traditional methods. This model takes into account the need for additional knowledge and results in significant improvements in the explanation process.
41199	4119963	Global Linear Complexity Analysis Of Filter Keystream Generators	The article introduces a new algorithm for determining lower bounds on the global linear complexity of PN-sequences that have been filtered nonlinearly. The algorithm is designed to be efficient and can be implemented in both software and hardware using bitwise logic operations. It can be applied to any type of filter generator, making it versatile. The focus of the algorithm is to achieve large lower bounds, which demonstrate the exponential growth of the global linear complexity for these types of sequences. 
41200	412007	Foundations for Knowledge-Based Programs using ES	Reiter introduced a way to handle knowledge-based Golog programs with sensing, where program execution can be based on tests that reference the agent's knowledge. This approach allows reasoning about knowledge to be reduced to classical reasoning from an initial first-order theory. However, this method has limitations, such as only considering knowledge about the current state and not allowing quantifying-in formulas. This is due to the use of Reiter's version of the situation calculus. A new situation calculus developed by Lakemeyer and Levesque offers a solution to these limitations by allowing for an expansion of Reiter's foundations and removing the mentioned restrictions.
41201	412015	Firing Fuzzy Rules with Measure Type Inputs	This article discusses the issue of determining the satisfaction level of a fuzzy systems rule base when the antecedent condition is expressed using a normal fuzzy set and the input information is also in the form of a fuzzy set. The authors explore different approaches for calculating this satisfaction level and outline the necessary requirements for any formulation used. They then introduce the concept of a measure and demonstrate how it can be applied to uncertain values associated with a variable. The requirements for determining satisfaction level using a measure are also discussed, with some examples provided. Finally, the authors show how this method can be used for fuzzy rules with probabilistic inputs.
41202	4120259	Visual Protection of HEVC Video by Selective Encryption of CABAC Binstrings	This paper introduces a method for protecting the newly developed video codec HEVC through selective encryption of binstrings in a format-compliant manner. The approach differs from that of H.264/AVC and addresses challenges such as non-dyadic encryption space and adaptive context. An algorithm is proposed for converting non-dyadic spaces to dyadic, ensuring that the encrypted bitstream remains format-compliant and maintains the same bit-rate. The method requires minimal processing power and is suitable for a variety of applications, providing protection for contour, motion, and texture information. Experimental evaluation and security analysis support the effectiveness of this method for content protection. 
41203	4120386	Minimum parent-offspring recombination haplotype inference in pedigrees	The paper discusses the problem of haplotype inference under the Mendelian law of inheritance on pedigree genotype data. The minimum recombination principle is used to determine the most likely haplotype configurations with the least number of recombinations. The authors propose a variation of the Minimum Recombination Haplotype Inference (MRHI) problem, called the k-MRHI problem, which adds the constraint that the number of recombinations in each parent-offspring pair is at most k. They present an efficient algorithm using dynamic programming to solve the k-MRHI problem and also propose an algorithm to find the root of the pedigree tree and reduce the time complexity. Experiments on real and simulated data show that their algorithm outperforms other popular algorithms in terms of both speed and accuracy.
41204	4120412	Computational Argument as a Diagnostic Tool: The role of reliability.	Formal and computational models of argument are useful tools for teaching in complex fields like law, public policy, and science. These areas rely heavily on open-ended arguments, but students may not have been taught a structured approach to constructing and analyzing arguments. Computational models can act as argument tutors, guiding students through the process using an explicit model. However, it is important to ensure that these models can be easily understood and evaluated, which is currently being studied in the LARGO tutoring system. Ongoing research is being conducted to determine the effectiveness of argument diagrams produced by this system.
41205	4120534	Re-evaluating LARGO in the Classroom: Are Diagrams Better Than Text for Teaching Argumentation Skills?	Diagramming is a helpful way to teach argumentation skills in complex subjects, but it is challenging for an intelligent tutoring system (ITS) to provide feedback without a clear evaluation process. LARGO is an ITS designed for legal argumentation that offers advice by identifying patterns in students' diagrams. A study was conducted using LARGO in a first-year law class, but it did not show significant improvement compared to a text-based note-taking tool. This may be due to low use of the graphical tools and different motivations. However, there is potential for LARGO to benefit lower-aptitude students if they engage more with the system. 
41206	4120676	Does supporting multiple student strategies in intelligent tutoring systems lead to better learning?	Intelligent Tutoring Systems (ITS) are challenging to construct due to the concept of strategy freedom, where students have the ability to use multiple approaches to solve a problem. However, a study was conducted in two US middle schools with 57 students in grades 7 and 8 to determine if this freedom leads to more robust learning. Three versions of an ITS for solving linear equations were developed, varying in the amount of strategy freedom. The results showed that students' algebra skills improved, but there was no difference in learning gain and motivation between the different versions. This suggests that a small amount of strategy freedom is beneficial in early algebra learning, but students tend to stick to a standard strategy and its minor variations rather than exploring alternative strategies. This study validates the complexity of ITS architectures.
41207	4120715	Quadratic factorization heuristics for copositive programming	Copositive optimization problems are a type of conic programming where the objective is to optimize linear forms over the copositive cone, subject to linear constraints. This class of problems is NP-hard and can be used to formulate quadratic programs with linear constraints, even when some variables are binary. Most methods for solving these problems aim to approximate the copositive cone from within, but a new method has been proposed that approximates the cone from outside by using the dual problem. This approach involves considering descent directions in the completely positive cone and solving regularized strictly convex subproblems. By replacing the completely positive cone with a nonnegative cone, the problem can be solved using linearization techniques, resulting in promising numerical results on various test problems.
41208	4120890	Fast population game dynamics for dominant sets and other quadratic optimization problems	The authors propose a new population game dynamics method for finding dominant sets in graphs, inspired by infection and immunization processes. This method has linear time and space complexity and is shown to have increasing population payoff along any non-constant trajectory. Dominant sets are proven to be stable points for this dynamics under the assumption of symmetric affinities. This approach is applicable to a wide range of quadratic optimization problems in computer vision. Experimental results show that this method is significantly faster and equally accurate compared to standard algorithms.
41209	4120944	Kernel-level scheduling for the nano-threads programming model	 ses/threads, and then exploit concurrency by managing the parallel execution of these processes.Multiprocessor systems are popular for low and high-end servers, performing diverse tasks like number crunching, simulations, databases, and web servers. Due to the variety of workloads, system utilization, throughput, and execution time are crucial performance metrics. This paper presents efficient kernel scheduling policies and a new kernel-user interface to support parallel execution in different workloads. It includes user level threads to exploit parallelism and a two-level scheduling policy to coordinate resources with the number of threads generated by applications. The performance of the proposed policies is compared to the gang scheduling policy of IRIX 6.4 on a Silicon Graphics Origin2000, showing significant improvements in workload and application execution times, as well as cache performance. However, parallel applications on multiprogramming systems often suffer from performance degradation due to interferences between programs contending for resources. This is because these applications are designed assuming a dedicated system, and manage parallel execution by setting up the desired number of processes/threads.
41210	4121058	On the longest common rigid subsequence problem	The longest common subsequence problem (LCS) and closest substring problem (CSP) are both used to identify common patterns in strings and have been extensively studied. LCS has been proven to be not solvable in polynomial time within a certain ratio, while CSP is known to be NP-hard but has a possible solution through a PTAS. The paper focuses on the longest common rigid subsequence problem (LCRS), which is similar to LCS and CSP and is often used in finding motifs in biological sequences. The paper proves LCRS to be Max-SNP hard and provides an exact algorithm with a relatively fast average running time.
41211	4121113	Augmenting a Conceptual Model with Geospatiotemporal Annotations	Many real-world applications require organizing data based on space and time, but existing conceptual models do not explicitly capture these spatial and temporal semantics. As a result, database designers must ad hoc discover and implement these concepts. To address this issue, the authors propose an annotation-based approach that allows designers to first focus on non-temporal and non-geospatial aspects before adding geospatiotemporal annotations. This approach provides a higher level of abstraction and extends the semantics of conventional conceptual models. The authors argue that this approach is both expressive and easy to understand and implement, balancing expressiveness and simplicity in conceptual modeling. 
41212	41212109	Discovering Architectures from Running Systems	A common challenge for software developers is ensuring that a system matches its intended architectural design. To address this issue, a technique called DiscoTect uses runtime observations to create an architectural view of the system. This involves creating mappings that translate low-level system events into higher-level architectural operations, using a formal language called DiscoSTEP. Two case studies demonstrate the effectiveness of DiscoTect in verifying conformance to an existing architectural specification. This approach is useful for legacy systems and provides a practical solution for dynamically checking system consistency. 
41213	412138	Data intensive production systems: the DIPS approach	The paper discusses the design of the DIPS system and its main contributions. These include the use of special data structures and a matching algorithm to efficiently identify when rule conditions are satisfied for execution. The paper also proposes a concurrent execution strategy that outperforms traditional sequential algorithms. A correct and serializable execution based on locking is described. The matching technique used in DIPS is also fully parallelizable, making it suitable for parallel computing environments.
41214	4121432	On the L(h, k)-labeling of co-comparability graphs and circular-arc graphs	The article discusses the L(h, k)-labeling problem, which involves assigning integer labels to vertices in a graph, with the constraint that adjacent vertices must be at least h units apart and vertices at a distance of 2 or less must be at least k units apart. This problem is NP-complete, so the focus is on finding efficient solutions for special classes of graphs, specifically co-comparability, interval, and circular-arc graphs. The article presents the first algorithm for labeling these types of graphs with a bounded number of colors, and also shows an improvement for the special case of interval graphs with k = 1. 
41215	4121536	The IS risk analysis based on a business model	As technology becomes increasingly essential for organizations, the risk of disruptions to operations due to IS failure has become a major concern. However, traditional methods of IS risk analysis do not accurately assess the impact of disruption on the organization's objectives. Quantitative methods do not consider the loss of operations, while qualitative methods are subjective. This study proposes a business model-based approach to IS risk analysis that takes into account the importance of different business functions and the necessity level of various assets. This method adds an organizational investigation stage and utilizes methodologies such as paired comparison and asset dependency diagrams. 
41216	41216125	Soft-Decision Decoding of Linear Block Codes Using Preprocessing and Diversification	Order-w reprocessing is a soft-decision decoding method for binary block codes where up to w bits are systematically flipped on the most reliable basis. This method is improved by incorporating preprocessing rules and using a second basis for practical signal-to-noise ratios (SNRs). The resulting multibasis order-w reprocessing scheme has comparable complexity to order-w reprocessing but achieves better performance, particularly at high SNRs. This approach also accurately characterizes the performance of the Chase and generalized minimum distance decoding algorithms. The proposed algorithm successfully decodes the (192,96) Reed-Solomon concatenated code and the (256,147) extended BCH code with a block-error rate of 10-5 and a computational cost that is affordable.
41217	4121763	A novel Gaussianized vector representation for natural scene categorization	This paper introduces a new method for representing scene images using a Gaussianized vector approach. The process involves encoding images as an ensemble of orderless bag of features and then using a global Gaussian Mixture Model to randomly distribute the features into Gaussian components. The parameters of the distribution are determined by the posteriors of the features on the Gaussian components. The resulting supervectors, which are a compact representation of each scene image, are shown to follow a standard normal distribution. Experiments using this representation demonstrate better performance in scene categorization compared to the traditional bag-of-features approach.
41218	4121819	A Fast and Flexible Clustering Algorithm Using Binary Discretization	The paper introduces a new clustering algorithm, BOOL, for multivariate data. It can detect arbitrarily shaped clusters and is noise tolerant. The algorithm works in two steps - first, the data is discretized and represented as binary words, and then clusters are constructed by agglomerating smaller clusters using this representation. This step is done efficiently by sorting the binary representations. Experiments show that BOOL is faster than K-means and two other state-of-the-art algorithms for detecting non-convex clusters. The algorithm is also robust to changes in parameters, thanks to the hierarchical structure of clusters introduced by improving the accuracy of the discretization. 
41219	4121948	A game-theoretic intrusion detection model for mobile ad hoc networks	This paper addresses the issue of improving the effectiveness of intrusion detection systems (IDS) in ad hoc networks by proposing a unified framework. The framework aims to balance resource consumption among nodes, catch and punish misbehaving leaders, and maximize the probability of detection for elected leaders. This is achieved by electing a cost-efficient leader, using a cooperative game-theoretic model to analyze checker interactions, and formulating a game between the leader and intruder. The proposed framework is shown to increase the overall lifetime of the cluster, reduce false positives, and effectively execute the detection service. Empirical results are provided to support the solutions.
41220	4122089	The failure and recovery problem for replicated databases	A replicated database is a distributed database that stores data redundantly at multiple sites to improve system reliability. However, managing replicated data can be difficult as sites fail and recover. A replicated data algorithm has two parts: a read-write discipline and a concurrency control algorithm. The read-write discipline ensures logical conflicts are addressed, while the concurrency control algorithm manages physical conflicts. This paper presents a theory, based on serializability, for proving the correctness of replicated data algorithms. Three specific algorithms are discussed: Gifford's “quorum consensus” algorithm, Eager and Sevcik's “missing writes” algorithm, and Computer Corporation of America's “available copies” algorithm. 
41221	4122146	Huddle: automatically generating interfaces for systems of multiple connected appliances	Huddle is a new system designed to make using connected appliances easier in both homes and workplaces. These systems, such as home theaters and presentation rooms, can be challenging to use because users must figure out how to split their tasks into smaller ones for each appliance and find the specific functions of each appliance to complete their tasks. Huddle simplifies this process by automatically generating interfaces for multiple appliances based on the content flow within the system. This allows users to easily complete their tasks without having to navigate through multiple interfaces. Huddle aims to make using connected appliances a more seamless and user-friendly experience.
41222	4122282	Similarity searching in medical image databases	The proposed method is designed to handle approximate searching in medical image databases. The method uses attributed relational graphs to represent image content, including both expected and unexpected objects. The method is based on the assumption that a fixed number of labeled objects are common in all images, while there may also be a variable number of unlabeled objects. Queries can be answered by example, such as finding images similar to a specific X-ray. The images are indexed using R-trees and the method has desirable properties such as approximate search, no false dismissals, and fast search speeds. This makes it a suitable option for handling large databases.
41223	412231	Testing software in age of data privacy: a balancing act	Database-centric applications (DCAs) are commonly used in enterprise computing and require complex databases. Testing of these applications is often outsourced to reduce costs and improve quality. However, when proprietary DCAs are released, their databases cannot be shared with test centers due to data privacy laws. This results in testing being performed with anonymized data, leading to lower test coverage and fewer discovered faults. To address this issue, a new approach is proposed that combines program analysis with a data privacy framework designed specifically for software testing. This allows organizations to balance privacy concerns with the needs of testing. A tool has been developed and applied to Java DCAs, showing that by anonymizing data based on their impact on the application, test coverage can be maintained at a higher level.
41224	4122431	Sixth international workshop on traceability in emerging forms of software engineering: (TEFSE 2011)	TEFSE 2011 is a workshop that will gather researchers and practitioners to address the difficulties of maintaining traceability for various forms of software engineering artifacts. The main goal of this 6th edition is to expand on the previous work done by the traceability research community and tackle the ongoing traceability challenges. The workshop will be a collaborative event where participants will discuss key problems related to software artifact traceability and propose potential solutions. Furthermore, it aims to highlight the significance of preserving traceability information during software development, improve collaboration between academia and industry, and facilitate the transfer of technology. 
41225	41225108	Efficient Online Learning For Optimizing Value Of Information: Theory And Application To Interactive Troubleshooting	The optimal value of information problem involves selecting a set of tests at minimal cost to make the best decision based on observed outcomes. Existing algorithms are either unreliable or have high run times and assume a known distribution of test outcomes, which isn't always the case. To address these issues, a sampling-based online learning framework is proposed. This includes a dynamic hypothesis enumeration strategy for efficient information gathering and a progressive learning algorithm for unknown parameters. The approach has been proven to identify near-optimal decisions with high probability and has been successfully applied in a real-world troubleshooting application, resulting in high-quality decisions at low cost.
41226	4122632	Trains of thought: generating information maps	In a world where information is abundant, it can be challenging to make sense of all the different pieces of knowledge. This is where metro maps come in - a methodology for creating structured summaries of information. These maps serve as a guide to navigate and explore complex stories and relationships among different pieces of information. The algorithm used to generate these maps ensures that important information is included while also showing the development of the story. The process of creating these maps is efficient and can be adjusted by users to align with their interests. Studies have shown that metro maps are effective in helping users acquire knowledge in an efficient manner.
41227	4122754	The low power energy aware processing (LEAP)embedded networked sensor system	In order to meet the demands of critical environmental monitoring applications, embedded networked sensor systems now require complex and high-power sensor devices, as well as high-performance computing and communication capabilities. To address these needs while maintaining low energy consumption, a new multiprocessor node architecture, called Low Power Energy Aware Processing (LEAP), has been developed. This architecture includes energy monitoring and power control scheduling for all subsystems, including sensors. A distributed node testbed has also been created to demonstrate the effectiveness of the LEAP architecture in meeting both performance and energy efficiency objectives for a wide range of applications. 
41228	4122847	A metric conceptual space algebra	The use of cognitive modeling is important for designing spatial information systems that can effectively work with human users. Building concept representations using geometric and topological structures allows for semantic similarity and combination operations. Spatial concepts, which are closely connected to the physical world, are especially well-suited for this approach. Despite its advantages, the use of conceptual spaces is limited due to a lack of a comprehensive algebra. To address this, a metric conceptual space algebra is proposed, using convex polytopes to represent conceptual regions and incorporating context as a key element. This algebra is applied to a proof-of-concept spatial information system.
41229	412297	Facing Fault Management as It Is, Aiming for What You Would Like It to Be	Telecommunication systems are designed with redundancy and complexity to ensure reliability and high-quality service. This requires advanced tools for fault identification and management. To reduce the number of alarm events and accurately identify underlying faults, monitoring, filtering, and masking techniques are used. Fault management is a challenging task due to uncertainty in presenting symptoms. The paper discusses two approaches for fault management: rule discovery and Bayesian Belief Networks (BBNs). The former aims to reduce symptoms and provide better diagnostic assistance, while the latter uses intelligent techniques. The paper suggests that the research and development of both approaches can complement each other in efficient fault management.
41230	4123020	Improving Relative Transfer Function Estimates Using Second-Order Cone Programming.	This paper discusses how to estimate the Relative Transfer Function (RTF) between microphones using incomplete initial measurements. The initial measurement is only known for a few frequency bins and is completed by finding its sparsest representation in the time domain. This is done by solving a Second-Order Cone Program (SOCP), with free parameters representing the distance of the completed RTF from the initial estimate. These parameters are chosen based on the theoretical performance of the initial estimate. Experiments with real-world data show that this method greatly improves the accuracy of the RTF, particularly in situations with low signal-to-noise ratios.
41231	41231117	MuiCSer: A Process Framework for Multi-disciplinary User-Centred Software Engineering Processes	The paper introduces MuiCSer, a conceptual process framework for Multi-disciplinary User-centred Software Engineering (UCSE) processes that combines principles from software engineering and user-centred design to improve user experience. It aims to provide a common understanding of important components and activities in UCSE processes and serves as a reference for future research. The framework is demonstrated through the (re)design of a system and has been useful in studying the role of multi-disciplinary team members and process coverage of existing artefact transformation tools. 
41232	4123288	Investigating The Influence Of Communication And Input Devices On Collaboration In Virtual Environments	Collaborative virtual environments (CVEs) are gaining popularity for both recreational and professional purposes, making it essential to understand the factors that affect collaboration between users. A study was conducted using a puzzle solving task in a basic interactive virtual environment to investigate the impact of voice communication and the use of different input/output devices. The results showed that voice communication plays a crucial role in dividing tasks and improving collaboration in CVEs. However, the use of different devices did not significantly affect collaboration, indicating that the availability of different options does not hinder the acceptance of CVEs. This suggests that there is no need to mandate a specific device for CVEs, particularly for home users.
41233	4123315	Constructing Authorization Systems Using Assurance Management Framework	The model-driven approach has gained attention in developing secure software and systems, and developers are now utilizing it in the early stages of the software development life cycle (SDLC). However, security concerns are often overlooked due to the lack of proper mechanisms and tools. To address this issue, the authors propose a multilayered SDLC based on an assurance management framework (AMF) that focuses on developing authorization systems. The AMF enables the creation of a formal security model, specification and verification of security policies, generation of security codes, and rigorous testing. The authors also share their experience in implementing a role-based authorization system using this approach.
41234	4123415	State-aware Network Access Management for Software-Defined Networks.	OpenFlow is a popular technique used in Software-Defined Networks (SDNs) that allows for greater programmability, control, and flexibility in managing network flows. However, its focus on simplicity and efficiency means that it lacks the ability for stateful forwarding in the data plane and limited access to connection-level information. To address these limitations, a new framework called STATEMON has been proposed, which adds global state-awareness to OpenFlow. This allows for better access control in SDNs and enables the implementation of stateful network applications such as firewalls and port knocking. Evaluations of STATEMON show minimal message exchanges and manageable overhead, making it a practical and feasible solution for SDNs.
41235	41235328	Adaptive thermal management for portable system batteries by forced convection cooling	The lifespan of a battery is greatly affected by its operating conditions, especially the temperature. Higher temperatures can significantly accelerate the aging process of batteries. To address this issue, a forced convection cooling technique is introduced for batteries used in portable systems. However, there is a tradeoff between the battery's service time and cycle life when using a cooling fan, as the fan's power consumption can decrease the battery's charge capacity. This paper presents the first solution for the adaptive thermal management problem in portable systems, using a hierarchical algorithm that combines reinforcement learning and dynamic programming. This approach aims to find the optimal balance between fan speed and battery temperature to prolong the battery's lifespan.
41236	4123621	Non-Negative Group Sparsity with Subspace Note Modelling for Polyphonic Transcription.	Automatic music transcription (AMT) involves breaking down a piece of music into pitch and time components. This is typically done by using non-negative matrix factorisation (NMF) methods to decompose the spectrogram of the music. However, using only one atom to represent each note may not accurately capture the changing spectrum of a note over time. Previous studies have tried to address this by using different atoms for different parts of a note or by using large dictionaries. In this paper, the use of subspace modelling and group sparsity is explored to better capture the variability in note spectra. This approach leads to improved AMT results when applied to a generic harmonic subspace dictionary.
41237	412376	Building detection from mobile imagery using informative SIFT descriptors	This article proposes a new method for reliable outdoor object detection on mobile phone imagery using off-the-shelf devices. The method, called the 'Informative Descriptor Approach', uses SIFT features (i-SIFT descriptors) and an attentive matching approach to improve upon traditional SIFT descriptor based keypoint matching. In the off-line learning stage, standard SIFT responses are evaluated using an information theoretic quality criterion and a decision tree is used to map SIFT descriptors to entropy values. The authors argue that the proposed i-SIFT method has advantages over standard SIFT encoding in terms of performance complexity and demonstrate its effectiveness in a mobile vision experiment on the MPG-20 reference database. 
41238	4123856	Validating Adequacy and Suitability of Business-IT Alignment Criteria in an Inter-Enterprise Maturity Model 202	The alignment of business requirements with information technology is a crucial issue in enterprise computing. While there are existing criteria to measure alignment within a single enterprise, there is a lack of research on identifying or rethinking these criteria in inter-enterprise settings. In these settings, alignment is influenced by economic processes rather than centralized decision-making. To address this, researchers have developed a maturity model for business-IT alignment in inter-enterprise settings. This paper discusses the multi-method approach used to validate the criteria included in the model, including a focus group session and case study. The results and implications for the model are also presented.
41239	4123916	Relevance and problem choice in design science	The idea of rigor versus relevance is based on the misconception that rigor involves linear technology transfer and positivistic science. However, historical insights from the history of science and technology show that this is not the case and that technology is not transferred linearly from research to practice. Both technical and social sciences share similarities in terms of practical problem solving and research focus. Relevance is also context-dependent and depends on the goals of stakeholders. This paper highlights the importance of applicability in design science, and discusses the need to consider the context and practical conditions in research. It also discusses the relevance of these insights for information systems design science and its implications for research methods, strategy, and knowledge transfer.
41240	4124041	Scalable hardware monitors to protect network processors from data plane attacks	Modern router hardware in computer networks relies on programmable network processors for various packet forwarding operations, making them vulnerable to attacks launched through the data plane without access to the control interface. Previous research has shown that a single malformed UDP packet can take over a vulnerable network processor and cause a devastating denial-of-service attack. To defend against these attacks, hardware monitoring systems can be used to track processor core operations and detect any deviations from expected behavior, triggering reset and recovery actions. This paper introduces a Scalable Hardware Monitoring Grid that allows for the dynamic sharing of monitoring resources among multiple processor cores. The system has been tested on large network processors with numerous cores and a multicore prototype has been implemented on an FPGA platform.
41241	4124118	Reactive scheduling in a job shop where jobs arrive over time	The paper discusses the dynamic job shop scheduling problem (DJSSP) with job release dates, which is common in production systems. Unlike the classical job shop scheduling problem (CJSSP), where all jobs are known at the beginning, DJSSP deals with continuously arriving jobs with unknown attributes. The proposed heuristic approach uses reactive scheduling to solve this problem by breaking it down into sub problems. These sub problems are solved using priority scheduling and scheduling rules (SRs) are used to prioritize the system elements. The paper also proposes an approach using gene expression programming (GEP) to construct efficient SRs for DJSSP and compares them with rules constructed by other methods on various measures of performance.
41242	412425	Effective Information Spreading in Social Networks	Traditional data management techniques are no longer sufficient in the face of the emerging Big Data paradigm. The increasing amount of data related to social interactions between users requires advanced analysis strategies. The heterogeneity and speed of this data also necessitate the creation of new data storage and management tools. This paper presents a framework specifically designed to analyze user searches while connected to a social network, with the goal of identifying influential users who can spread their influence throughout the network. Understanding user preferences is crucial in various fields such as viral marketing, tourism promotion, and food education.
41243	4124352	Detecting broad phonemic class boundaries from greek speech in noise environments	This study evaluates the performance of an automatic segmentation method for Greek speech signals into phonemic classes. The method was tested on clear speech and speech with various types of noise. The results showed high accuracy of 76.1% for clear speech without over-segmentation. However, there was a slight decrease in accuracy (4%) when the speech was distorted with wideband noise. This suggests that the method is effective for segmenting speech signals, but may be affected by certain types of noise. Overall, the results were promising and demonstrate the potential of this approach for automatic speech segmentation.
41244	4124459	Phone duration modeling: overview of techniques and performance optimization via feature selection in the context of emotional speech	The accurate modeling of prosody, including phone duration, is crucial for creating high-quality synthetic speech that sounds natural. This study evaluates different modeling techniques, such as decision trees and linear regression, that have been successfully used for various tasks. Feature selection techniques, such as RReliefF and Correlation-based Feature Selection, are also applied to improve performance. The study uses a Modern Greek emotional speech database and found that feature selection significantly improves the accuracy of phone duration prediction, with the M5p trees model achieving the best results. This highlights the importance of feature selection in phone duration modeling for emotional speech.
41245	4124582	Learning verb complements for modern greek: Balancing the noisy dataset	The article discusses the challenges of automatically learning to identify verb complements from natural language corpora without the use of complex linguistic resources. This often leads to noise in the data and an imbalanced set of class-labelled feature-value vectors, with one class heavily underrepresented. To address this, the authors propose balancing the learning data by applying one-sided sampling to the training corpus. They also introduce the use of the value difference metric, which is better suited for nominal attributes, in this process. The effectiveness of this approach is tested on various learning algorithms, including Bayesian learners and decision trees, resulting in a 22% improvement in performance. The authors also present a new variation of Bayesian belief networks, called COr-BBN, which further improves the results. Overall, their approach achieves a 73.7% f-measure for the complement class using just a phrase chunker and basic morphological information for preprocessing.
41246	4124635	Ontology fusion in HLA-based collaborative product development	This paper introduces a new method for combining ontologies in HLA-based collaborative product development systems, with the goal of promoting mutual understanding among different systems. The approach involves three steps: mapping, alignment, and merging of ontologies. It utilizes an axiom-based deduction strategy and takes into account heavily weighted ontologies. This approach is able to identify all explicit and derived inter-ontology relationships, and can even find implicit equivalent relationships. By improving the efficiency of preparation and reducing the workload for platform adjustments, this approach has the potential to enhance the applicability and flexibility of collaborative development systems.
41247	4124770	Coordination of Free/Libre Open Source Software Development	This paper examines the coordination mechanisms used in free/libre open source software (FLOSS) development and compares them to those used in mainstream software development. Using coordination theory, the study analyzed data from three successful FLOSS projects and found similarities in the management of task-task dependencies, but differences in how task-actor dependencies are handled. While proprietary software development typically involves a designated developer for each piece of code, FLOSS projects often rely on "self-assignment" by volunteers. The paper discusses the potential transferability of these practices to mainstream software development and suggests areas for further research.
41248	4124818	Decentralized, connectivity-preserving, and cost-effective structured overlay maintenance	This paper discusses the maintenance of structured overlays in decentralized peer-to-peer (P2P) systems, taking into account potential system and network failures. The authors propose a specific set of requirements for overlay maintenance protocols, including decentralization, maintaining connectivity, and convergence to the desired structure. They also present a complete protocol with proof that meets these requirements. This protocol addresses challenges related to decentralization and concurrency in the system and overcomes limitations of existing maintenance protocols, such as the need for a centralized bootstrap system, known stabilization time, and large local membership lists. 
41249	4124955	Matroid Online Bipartite Matching and Vertex Cover.	The Adwords and Online Bipartite Matching problems have gained attention in recent years due to their relevance to Internet advertising. New models and extensions have been developed to address practical issues, and this paper proposes a new generalization based on matroids. This new setting is potentially useful in both theory and practice due to the rich structures and expressive power of matroids. The paper presents algorithms for Matroid Online Bipartite Matching that achieve the same performance as the classical version of the problems. These algorithms are $1-1/e$-competitive and are applicable to both the small bid assumption and the random arrival model. A key aspect of these algorithms is a primal-dual waterfilling procedure that accommodates for matroid constraints. This paper also introduces techniques for handling submodularity in online problems, which may be of interest for future research.
41250	4125045	A wireless mesh network testbed in rural mountain areas	A wireless mesh network testbed has been created in two rural mountain areas to address the digital divide. The testbed consists of 22 nodes installed on power poles and is connected to a control server at Niigata University through a gateway node at the local government office. Each node has two wireless LAN interfaces and an access point. Throughput and packet transmission success rates are measured in the testbed, and technical challenges in managing and operating the network in rural areas are being investigated. This testbed aims to provide disaster-tolerant and reliable broadband access to bridge the digital gap in rural areas.
41251	4125149	PhotoMap: using spontaneously taken images of public maps for pedestrian navigation tasks on mobile devices	Public maps are commonly found in mid- to large-sized cities, as well as in parks and near hiking trails. These maps serve as a useful tool for both tourists and locals to orient themselves and find information about unfamiliar places. They offer advantages over mobile maps from services like Google Maps Mobile or Nokia Maps, as they often include local landmarks and sights that are not shown on digital maps. These maps, known as 'You are here' (YAH) maps, are designed for specific purposes such as zoo or hiking maps, and are aesthetically pleasing. A new technique called PhotoMap uses images of these YAH maps, taken with a GPS-enhanced mobile camera phone, as background maps for on-the-fly navigation tasks. The challenge is to accurately georeference the image to support pedestrian navigation, and a study has been conducted to determine the suitability of various public maps for this task. The feasibility of using these georeferenced photos for navigation on GPS-enabled devices is also evaluated.
41252	4125245	Automatic classification of reading disorders in a single word reading test	The traditional method for evaluating reading disorders in clinical practice relies on perceptual assessment. To address this issue, the use of automatic speech processing techniques is proposed to classify reading disorders. A study was conducted with 38 children suspected of having a reading disorder, using a German standard test. Each child's reading errors and duration were recorded and compared to age-dependent limits for diagnosis. Results showed that 30 of the 38 children were confirmed to have a reading disorder. This paper presents the findings on using automatic evaluation for a single word reading test, achieving a recognition rate of 78.9% for detecting reading error limits and 97.4% for classifying reading disorders. 
41253	4125323	Efficient shared memory with minimal hardware support	Shared memory is a popular approach for developing parallel programs, as it is seen as more user-friendly compared to message passing. This model can be implemented through hardware, software, or a combination of both. However, it faces the challenge of maintaining cache coherence, which is crucial for efficient performance. Hardware-coherent multiprocessors have proven to be more efficient than distributed shared-memory (DSM) emulations on message-passing hardware. This paper proposes an alternative option, a network or multiprocessor system that offers a global physical address space and allows for non-coherent memory accesses without interrupting remote processors. This option, called NCC-NUMA, has been tested through simulation and shown to have comparable performance to fully hardware-coherent designs, and outperforms traditional DSM systems. 
41254	4125428	Lifted cover facets of the 0-1 knapsack polytope with GUB constraints	This paper discusses the use of facet-defining inequalities from minimal covers as effective cutting planes in 0-1 integer programming algorithms. The authors build upon previous work by Balas and Zemel by providing a set of inequalities that can determine the lifting coefficients for facet-defining inequalities in the 0-1 knapsack polytope, regardless of the order in which variables are lifted. They also extend this result to address the 0-1 knapsack problem with generalized upper bound constraints. These findings can improve the efficiency and accuracy of algorithms used to solve 0-1 integer programming problems.
41255	4125580	Decomposition-Based Recursive Least Squares Identification Methods For Multivariate Pseudo-Linear Systems Using The Multi-Innovation	This paper explores the estimation of parameters in multivariate pseudo-linear autoregressive systems. A decomposition-based recursive generalised least squares algorithm is proposed to estimate the parameters by dividing the system into two subsystems. To enhance the accuracy of parameter estimation, a decomposition-based multi-innovation recursive generalised least squares algorithm is also developed using the multi-innovation theory. Simulation results demonstrate the effectiveness of these algorithms.
41256	4125628	CLIFT: a Cross-Layer InFormation Tool to perform cross-layer analysis based on real physical traces	Network simulations often rely on approximated models rather than real physical traces, which can make it difficult to accurately analyze the impact of link layer reliability schemes on transport layer performance. In order to address this issue, the authors propose a software called CLIFT that translates real physical events from a given trace into a format that can be used in a network simulator like $ns$-2. This allows for more realistic cross-layer analysis, particularly in the transport layer. The software is specifically designed for 4G satellite communications scenarios and provides metrics for consistent cross-layer analysis. The paper discusses the internal mechanisms and benefits of using CLIFT in simulations.
41257	4125759	Task scheduling for heterogeneous reconfigurable computers	This article discusses the problem of executing a changing set of tasks on a reconfigurable system, which includes a processor and a reconfigurable device. The scheduling of tasks is managed by a scheduler, which can allocate them to either the processor or the device. A placer is responsible for placing tasks on the device at runtime, using a database of equivalent implementations. However, if there is not enough space on the device, the scheduler may have to preempt a running task or run it on the software. This article presents an implementation of a placer module and explores task preemption, both of which are part of an operating system for a reconfigurable system in development.
41258	4125848	Robust blind watermarking of point-sampled geometry	This article discusses the use of digital watermarking to protect the copyright of 3-D models. However, traditional methods cannot be applied to point clouds, as the connectivity information may change due to attacks. To address this issue, the article proposes a new blind watermarking mechanism that uses a cluster tree to embed and extract watermarks from 3-D point-sampled geometry. This technique is robust against various attacks, such as affine transformations, reordering, cropping, simplification, and noise addition. It can also be applied to 3-D meshes and achieves high hiding capacity with low distortion. The time complexity is estimated to be O(n log n), where n is the number of 3-D points.
41259	4125918	Evaluation of Three Parametric Models for Estimating Directional Thermal Radiation from Simulation, Airborne, and Satellite Data.	This research evaluated the performance of three published directional thermal radiation models (RL, BRDF, Vinnikov) for correcting thermal radiation anisotropy for land surface temperature (LST) measurements. Results showed that the Vinnikov model performed the best at canopy scale, particularly for erectophile canopy and low leaf area index (LAI). At pixel scale, all three models had a stable effect, but the Vinnikov model was most effective for correcting LST in savannas, grasslands, and areas with low LAI. The Vinnikov model was then calibrated for different land cover types and was found to significantly improve the accuracy of LST measurements, reducing the RMSE by 0.89 K. This study highlights the importance of an appropriate model for correcting thermal radiation anisotropy in order to improve the accuracy of LST measurements.
41260	4126013	Transform-space view: performing spatial join in the transform space using original-space indexes	Spatial joins are a common technique used in databases to find pairs of objects that have a specific spatial relationship. To improve the efficiency of these joins, indexes are often used. Original-space indexes, like the R-tree, are complex to optimize because they deal with the extents of objects. Transform-space indexes, on the other hand, only deal with points and are simpler to optimize. However, they cannot be applied to original-space indexes. To address this issue, a new concept called the transform-space view is proposed, which allows an original-space index to be interpreted as a transform-space index without any modifications. This leads to the development of the transform-space view join algorithm, which outperforms existing algorithms in terms of buffer size, disk accesses, and wall clock time. This algorithm has the potential to be applied in other spatial query processing algorithms in the transform space. 
41261	4126196	Extracting and mining protein-protein interaction network from biomedical literature	SPIE-DM is a biomedical data mining system designed to extract and analyze protein-protein interactions from biomedical literature, specifically from MedLine. The system consists of two phases: phase 1 focuses on extracting the interactions using a scalable and portable method called SPIE, resulting in a scale-free network graph. Phase 2 uses a clustering algorithm called SFCluster to mine the network and identify potential protein complexes, which are important for studying protein functionality. This algorithm considers the characteristics of scale-free networks and utilizes local density and neighborhood functions to identify meaningful clusters at different density levels. Experiments on 1600 chromatin proteins demonstrate the effectiveness of SPIE-DM for extracting and mining data from biomedical literature databases. 
41262	4126244	Speaker Diarization for Conference Room: The UPC RT07s Evaluation System	In this paper, the authors discuss the UPC speaker diarization system used in the NIST Rich Transcription Evaluation. The system is based on the ICSI RT06s system and utilizes agglomerative clustering with a modified Bayesian Criterion measure. This is the first time UPC has participated in the evaluation and their goal was to create a baseline system for future research in diarization. Prior to diarization, the system includes a Speech/Non-Speech detection module and a Wiener Filtering module. The speech parameterization also includes a Frequency Filtering method. The authors also made some changes to the complexity selection algorithm and introduced a new post-processing technique for the shortest clusters. 
41263	4126356	Person Tracking in Smart Rooms using Dynamic Programming and Adaptive Subspace Learning	The article discusses a new vision system for tracking a single person in a smart room using synchronized, calibrated stationary cameras. The system has three components: initialization, tracking, and drift detection. The main innovation is an adaptive tracking mechanism that uses subspace learning to track the person's appearance in two-dimensional views. This approach includes a "forgetting" mechanism to reduce drifting and replaces the previous mean-shift tracking method. The system also includes a robust initialization component using face detection and spatio-temporal dynamic programming. Overall, this system outperforms previous systems in tracking a presenter in data from the CHIL project.
41264	4126414	Intelligent virtual agents in collaborative scenarios	The success of interactive games and virtual communities depends on the ability of intelligent virtual agents (IVAs) to meet user expectations and display believable behaviors. In scenarios where users and IVAs interact as a group, it is important to have believable group dynamics. To address this, a model was developed based on theories of group dynamics from social psychology. The model was implemented in a computer game where users work with a group of IVAs to complete tasks. An evaluation experiment showed that the model had a positive impact on users' social engagement, specifically in terms of trust and identification with the group.
41265	4126558	A logic-based axiomatic model of bargaining	This paper presents a mathematical model for analyzing bargaining situations. It uses propositional logic to describe the situation and total pre-orders to represent the preferences of the bargainers. The authors propose a solution to n-person bargaining problems based on the concept of minimal simultaneous concessions. This solution is unique and can be characterized by five logical axioms. This framework offers a basic solution to multi-person, multi-issue bargaining problems in discrete domains. It can also be applied to continuous bargaining problems by discretizing them. In this case, the solution is equivalent to the well-known Kalai-Smorodinsky solution.
41266	41266105	Per-survivor processing Viterbi decoder for Bluetooth applications	.A new noncoherent sequence detection receiver has been developed for Bluetooth systems, based on Rimoldi's decomposition of the transmit signal. Software simulations have demonstrated its power efficiency. In this paper, a hardware implementation of the decoder for the receiver is presented. A low-complexity Viterbi decoder is described, which uses per-survivor processing in a two-state trellis. The prototype decoder is implemented in an FPGA and its bit-error-rate performance is compared to that of the software simulations. This shows the effectiveness of the hardware implementation and its potential for use in Bluetooth systems.
41267	412670	The MMCPP/GE/c Queue	The article discusses the probability distribution of queue length in a multi-server, single queue system with a generalized exponential service time distribution and a Markov modulated compound Poisson arrival process. This type of arrival process, which is commonly used in ATM networks, involves bulk arrivals with geometrically distributed batch sizes that are modulated by a Markovian arrival phase process. The result, obtained using the method of spectral expansion, is exact and applies to both infinite and finite capacity queues. The Laplace transform of the interdeparture time probability density function is also derived. This analysis could be useful for modeling networks of switching nodes with correlated and bursty internal arrival processes.
41268	4126884	A knowledge-based framework for multimedia adaptation	The delivery of personalized multimedia content over the internet has the potential to revolutionize future multimedia applications and is a key focus in ongoing MPEG-7 and MPEG-21 standardization efforts. This technology will allow for automatic preparation of digital content according to the user's device capabilities, network conditions, and content preferences. However, this will require dealing with a variety of end user devices, media formats, and metadata. To address these challenges, a knowledge-based approach is proposed, utilizing standardized Semantic Web Services technology to express tool capabilities and execution semantics. This approach has been successfully implemented and evaluated in an ISO/IEC MPEG Core Experiment and is currently being evaluated by the standardization body. 
41269	4126930	Cost-Efficient Development of Virtual Sales Assistants	The concept of a virtual sales assistant represents the idea of an interactive online tool that mimics the role of a human salesperson in physical stores. These assistants aim to enhance the shopping experience for users and add a social and emotional aspect to e-commerce platforms. While many promising pilot applications have been researched, not many have been implemented commercially. This highlights the gap between advanced technology and practical profitability for e-commerce sites. Drawing from industry experience, the authors propose key factors for developing efficient and persuasive virtual sales assistants, and offer a framework for creating conversational sales recommender systems. The paper also shares insights from real-world applications.
41270	4127012	Towards next generation coordination infrastructures.	Coordination infrastructures are essential in designing multiagent systems. With the rise of agent technology, many different coordination infrastructures have been developed. This paper aims to review the current state of coordination infrastructures and identify areas for future research. The authors suggest that the next generation of coordination infrastructures should focus on becoming more socially aware, providing decision support for agents, and increasing openness for decentralized design and execution. Additionally, the paper highlights promising approaches and research areas to address these challenges. Overall, the authors stress the importance of developing advanced coordination infrastructures to improve the effectiveness and efficiency of multiagent systems. 
41271	4127136	Ddrp: An Efficient Data-Driven Routing Protocol For Wireless Sensor Networks With Mobile Sinks	The addition of mobile sinks to a wireless sensor network can greatly improve its performance. However, this can also lead to unexpected changes in network topology, resulting in excessive protocol overhead and offsetting the benefits of using mobile sinks. To address this issue, a data-driven routing protocol (DDRP) is proposed. The objective of DDRP is to reduce protocol overhead for data gathering in wireless sensor networks with mobile sinks. This is achieved by using the broadcast feature of the wireless medium for route learning, where data packets carry additional information about the distance to the target mobile sink. Simulation results show that DDRP can significantly reduce protocol overhead and increase network lifetime while maintaining a high packet delivery ratio.
41272	4127261	Iteration-free PDL with storing, recovering and parallel composition: a complete axiomatization.	This article discusses the axiomatization and completeness of PRSPDL0, which is a variant of PDL that does not use iterations and allows for parallel composition. The program operation of parallel composition is not originally definable in the language of PDL, but it can be defined by introducing propositional quantifiers. Instead of using axioms, the article proposes using a non-traditional rule of proof to define the program operation in the expanded language. The proof of the Truth Lemma is supported by large programs.
41273	4127357	Adaptive Rsu Re-Routing Under Delay Constraint In Multi-Hop Vehicular Networks	Vehicular wireless networks (VWNs) use multi-hop communication between vehicles and roadside units (RSUs). However, this can result in longer packet delivery delays. To address this issue, this paper suggests an adaptive RSU re-routing strategy that changes the associated RSU based on delay or throughput constraints. This allows for better selection of packet size, balancing the trade-off between delay and throughput to meet the needs of various applications in multi-hop VWNs. Performance results demonstrate the effectiveness of this approach in meeting the diverse requirements of applications in VWNs.
41274	4127487	On the expected number of failures detected by subdomain testing and random testing	The effectiveness of subdomain testing and random testing strategies can be evaluated using the E-measure, which measures the expected number of failures detected. This provides valuable information about the fault detecting capabilities of these strategies. Our analysis considers the general case of overlapping subdomains, rather than just disjoint subdomains, and introduces new conditions for determining the efficacy of subdomain testing compared to random testing. Additionally, we establish connections between the E-measure and the previously used P-measure, and use these connections to characterize subdomain testing based on the P-measure. 
41275	4127522	An ordering of secondary task display attributes	This paper discusses a study that challenges the established display design guidelines for focal images when they are used as a secondary task in a dual-task situation. The study proposes a new ordering guideline for secondary task image attributes based on human cognitive ability to extract information. This is necessary because the effectiveness of an image in conveying meaning decreases when it is shifted from a focal to a secondary task. The ordering of secondary task attributes also varies depending on the level of degradation in the primary task. These findings highlight the need for alternate guidelines for displaying images in a dual-task setting.
41276	412763	STOC: extending oracle to support spatiotemporal data management	This paper discusses the development of STOC (Spatio-Temporal Object Cartridge), an extension of Oracle that allows for practical management of spatiotemporal data. Previous research has primarily focused on modeling, indexing, and query processing, with little attention given to implementation. STOC is developed as a PL/SQL package and can be integrated into Oracle, providing users with access to spatiotemporal data types and functions using standard SQL. The paper provides an overview of STOC's features, discusses its architecture and implementation, and presents a case study demonstrating its capabilities. This extension offers a valuable tool for managing spatiotemporal data in various applications.
41277	4127747	Generating semantic-based trajectories for indoor moving objects	This paper introduces a new method for creating trajectories of indoor moving objects based on semantic information. Traditional methods for collecting this data require expensive positioning equipment and complex environment setups. To address these issues, the paper proposes using virtual positioning equipment to simulate the indoor environment. The semantic-based approach takes into account the type of moving objects, their relationship with locations, and the distribution of trajectories. This method is more realistic and provides useful data for indoor data management analysis. The paper also presents a tool for generating these trajectories and conducts a case study to demonstrate its effectiveness. The results show that the tool can generate semantic-based trajectories for different indoor scenarios.
41278	4127854	Tighter low-rank approximation via sampling the leveraged element	The authors propose a new randomized algorithm for computing a low-rank approximation to a given matrix. Their method involves biased sampling based on leverage scores, then weighted alternating minimization to minimize error. This approach can leverage input sparsity and produce approximations in spectral norm. It requires O(nnz(M)+nκ2r5/ε2) computations, with a better dependence on error ε compared to existing methods. The algorithm is also parallelizable and has two interesting extensions: a method for computing a low-rank approximation to the product of two matrices, and an improved algorithm for distributed PCA with smaller communication complexity. 
41279	4127926	Sums of squares of polynomials and their applications	A multivariate polynomial can be expressed as a sum of squares (SOS) if it can be written as a sum of squares of other polynomials. This condition can be used to test for polynomial nonnegativity, as a polynomial that is SOS will always be greater than or equal to 0 for all values of x. The decomposition of a polynomial into a sum of squares has been extensively studied in pure and applied mathematics, with recent developments in computational implications. Semidefinite programming (SDP) has emerged as an efficient approach for computing SOS decompositions, with the potential for practical applications in areas such as real algebraic geometry. This talk will provide an overview of this research area, discussing the basics of SDP and how it can be used to compute SOS decompositions, certify results, and obtain structured infeasibility certificates for real solutions of polynomial equations and inequalities. Examples from various domains will be used to illustrate the methods.
41280	41280184	Reconstructing binary polygonal objects from projections: a statistical view	Tomography is a common technique used to create images of objects by measuring the transmission of waves or particles through them. In many cases, the most important aspects of these images are their geometric features. However, traditional methods of reconstructing these images, such as filtered back-projection, can produce inaccurate results when the data is sparse and noisy. In this paper, a new framework is presented that uses statistical methods to extract specific geometric features directly from the noisy data. This approach involves formulating the reconstruction problem as a parameter estimation problem, with the vertices of binary polygons being the defining parameters. By using maximum likelihood or minimum description length criteria, the optimal solution can be found, although it may be challenging due to the nonlinearity of the problem. To address this issue, a novel method for initializing the solution is proposed, using estimated moments of the object obtained from the noisy data.
41281	4128110	Lower Bounds on the Minus Domination and k-Subdomination Numbers	A minus dominating function is a three-valued function defined on the vertices of a graph. The function's values must be -1, 0, or 1, and the sum of its values in any closed neighborhood must be at least one. This means that for every vertex, the sum of its value and the values of its adjacent vertices must be at least one. The weight of a minus function is the sum of its values over all vertices. The minus domination number of a graph is the minimum weight of a minus dominating function. This paper establishes lower bounds for the minus domination of bipartite graphs, proving a conjecture and providing a lower bound for a related graph property.
41282	4128233	Capacity Bounds for a Class of Interference Relay Channels	The article investigates the capacity of a specific type of Interference Relay Channels (IRC), called the Injective Semideterministic IRC. This type of IRC has a relay that can only observe one of the sources. The authors develop a new outer bound and two inner bounds for this type of IRC, using various cooperative strategies and interference decoding techniques. The outer bound is an extension of previous work, while the inner bounds contain existing results. The main result is the determination of the capacity region of the Gaussian class of IRCs, with a fixed number of bits per dimension and a constant gap. The proof involves using different cooperative strategies in different signal-to-noise ratio regimes, highlighting the complexity of the Gaussian IRC. This also shows that a single coding scheme for the Gaussian relay and interference channel may not be optimal, even for achieving capacity within a constant gap.
41283	4128321	Scalable visibility color map construction in spatial databases	Advances in 3D modeling allow us to answer queries about the best location for a new billboard or the hotel room with the best view despite obstacles. This requires measuring the visibility of the target object from various viewpoints in a dataspace. To address this problem, a visibility color map (VCM) is proposed as a surface color map where each viewpoint is assigned a color value representing the visibility of the target. However, computing the visibility for every viewpoint is expensive, so an efficient approach is proposed that takes advantage of human vision and also offers approximations to reduce computational overhead. The effectiveness and efficiency of this approach are demonstrated through experiments on real 2D and 3D datasets.
41284	4128433	Practical, Real-time Centralized Control for CDN-based Live Video Delivery	This year, live video delivery is expected to reach its peak at 50 Tbps. This growing popularity is changing the landscape of Internet video delivery. Content delivery networks (CDNs) must meet users' expectations for fast join times, high quality, and minimal buffering, while also keeping their own costs low and addressing issues in real-time. However, challenges such as latency, loss, and varied workloads make this task difficult. A study found that a centralized controller could improve user experience, but CDNs have avoided this approach due to the challenge of handling failures quickly. To address this, a new approach called VDN has been introduced that uses a centralized algorithm for live video optimization. VDN allows CDN operators to have real-time control over video delivery, despite challenges from the wider network. This approach has shown to increase average bitrate by 1.7 times and decrease costs by 2 times in different scenarios.
41285	41285275	2-layered metadata service model in grid environment	Data intensive tasks have become increasingly important in the grid environment, with data sets reaching hundreds of terabytes and soon petabytes. However, organizing the storage devices in a geographically distributed manner to support collaborative operations on this data presents a challenge. Performance is crucial for such applications, but the varying network conditions make it difficult to ensure consistent service quality. To address this issue, a 2-layered metadata service model has been proposed and implemented in the ChinaGrid Supporting Platform (CGSP), a grid middleware for the ChinaGrid project. This model takes into account the distribution of users and aims to provide a platform for efficient grid data management.
41286	4128659	Mammoth: Gearing Hadoop Towards Memory-Intensive MapReduce Applications	The MapReduce platform is commonly used for processing and analyzing large amounts of data, but it requires well-configured hardware in order to work effectively. However, our survey has shown that many small and medium-sized enterprises do not have suitable hardware configurations for this task, especially in terms of memory constraints. In response to this issue, we have developed Mammoth, a new MapReduce system that utilizes global memory management to improve performance. This includes a rule-based heuristic for memory allocation and revocation, a multi-threaded execution engine, and techniques such as sequential disk accessing and multi-cache. Our experiments have shown that Mammoth can significantly reduce job execution time and improve performance even in memory-constrained systems, making it a promising solution for large-scale data processing and analysis.
41287	4128774	Energy-Efficient Data Gathering for Road-Side Sensor Networks Ensuring Reliability and Fault-Tolerance	The paper discusses a novel tree-based data gathering scheme for road side sensor networks. This application is popular for traffic monitoring, but is challenging due to the required delay sensitivity and reliability, as well as limited sensor resources. The proposed scheme utilizes the strip-like structure of the road network and creates virtual blocks with one active node in each. Efficient scheduling ensures both coverage and power savings, while a tree maintenance module handles sensor node joining and leaving events to maintain network connectivity. Simulation results show minimal overhead for tree maintenance in terms of delay and control message communication.
41288	4128815	Shape of an arbitrary finite point set in IR2	The article discusses the study of the shape of a finite point set in two-dimensional space, where the points are not arranged in a regular grid. The shape of a connected point set is represented by a directed graph that links the boundary points. The authors argue that a proper boundary definition should have certain characteristics, such as regulating scale and being consistent with the definition of connected objects. They propose the use of the a-graph, which is based on a generalization of the convex hull, as a shape descriptor and boundary definition. The computational aspects of the a-graph have been studied, but not its potential use in these contexts. The authors prove that the a-graph meets the desired criteria and also establish a relationship between the a-graph and the opening scale space in mathematical morphology.
41289	4128978	User-oriented healthcare support system based on symbiotic computing	The article describes a proposed healthcare support system that utilizes multi-agent technology and ubiquitous computing in order to provide personalized and efficient healthcare assistance. The system collects and analyzes various data, such as vital signs and physical location, to provide useful information to the user about their health. It is designed to be user-oriented and based on the concept of symbiotic computing. The article details the design and initial prototype implementation of the system. 
41290	4129097	Equality sets of prefix morphisms and regular star languages	We explore the concept of equality sets of prefix morphisms, defined as sets of words that have the same output when applied to two different prefix morphisms. These sets are proven to have a surprising connection to the families of regular star languages and languages formed by applying a projection function to the output of two prefix morphisms. The prefix morphisms must fulfill certain criteria, including not having the output of one letter as a prefix of another. This result sheds light on the relationship between these two families of languages.
41291	4129116	MSO definable string transductions and two-way finite-state transducers	The study of string-to-string functions, also known as string transductions, has been extended to include those that can be defined by monadic second-order (MSO) logic. It has been shown that these transductions can be realized by deterministic two-way finite-state transducers. This means that the equivalence of two MSO definable string transductions can be decided. However, in the case of nondeterministic transductions, it has been found that they are incomparable to those realized by nondeterministic two-way finite-state transducers. This has led to the search for another machine model, and it has been discovered that both classes of MSO definable string transductions can be characterized by Hennie machines, which are two-way finite-state transducers with certain restrictions.
41292	412929	RIM: Robust Intersection Management for Connected Autonomous Vehicles	The use of intelligent transportation systems can greatly improve the flow of intersections for Connected Autonomous Vehicles (CAVs). However, the current Intersection Manager (IM) is vulnerable to model mismatches and external disturbances, resulting in a need for a large safety buffer and lower speeds for CAVs, ultimately reducing the intersection's throughput. To address this issue, a new space and time-aware technique called RIM is proposed, where the IM assigns a safe Time of Arrival (TOA) and Velocity of Arrival (VOA) to approaching CAVs, allowing them to determine and track an optimal trajectory and compensate for model mismatches and disturbances. Results from experiments show that RIM can significantly reduce position error and achieve 2.7X better throughput compared to other techniques.
41293	412936	Adaptive Scratch Pad Memory Management for Dynamic Behavior of Multimedia Applications	Runtime memory access traces can be used in addition to compiler optimizations to reduce energy consumption in memory hierarchy. This is especially useful for multimedia applications that have dynamic and irregular memory access patterns. These types of applications are difficult to optimize with static compiler techniques because their behavior is unknown until runtime and can change during computation. To address this issue, a combined approach of software (compiler and operating system) and hardware (data access record table) techniques is proposed. This approach utilizes a runtime scratch pad memory manager in the operating system to adapt the scratch pad data layout to the input data pattern, resulting in more efficient scratch pad utilization and a reduction in the amount of main memory accesses. Experimental results show that this approach is effective in reducing energy consumption compared to existing compiler and hardware techniques.
41294	412941	PEICS: towards HCI patterns into engineering of interactive systems	Despite significant efforts in research, there is still a lack of standardized description and organization for HCI patterns. This creates challenges for developers in identifying and applying the relevant patterns to solve problems in specific contexts. In order to effectively utilize HCI patterns in the development of interactive computer systems, they need to be integrated into a model-based UI development process. The Pattern-driven Engineering of Interactive Computer Systems (PEICS) workshop aims to bring together different research approaches to advance the field. This includes contributions in areas such as semantics, formalization, languages, support, research directions, and tools. 
41295	4129531	Software extension and integration with type classes	In software engineering and programming-language design, being able to extend and integrate software modules without altering existing source code is a crucial challenge. This article explores this issue at the language level, specifically with the use of type classes in the functional programming language Haskell. By comparing it with other approaches, the authors demonstrate that type classes offer a strong framework for addressing software extension and integration problems. However, they also note some limitations of type classes in this context.
41296	4129626	Encouraging knowledge exchange in discussion forums by market-oriented mechanisms	Discussion forums are an effective way to share knowledge on the internet, but they often struggle with getting users to participate. In this paper, a market-based approach is proposed to incentivize users to engage in knowledge exchange. Unlike other market-based solutions, this approach takes into consideration the unique characteristics of the limited resource – the effort to create information. The paper also introduces a method to continuously improve the system's performance by using user behavior statistics to reduce response time for satisfactory answers.
41297	4129735	Present and Future Challenges Concerning DoS-attacks against PSAPs in VoIP Networks	The integration of voice over IP (VoIP) telephony networks and traditional public switched telephony networks (PSTNs) has made it possible for emergency calls from VoIP peers to reach public service answering points (PSAPs). However, this connection also brings potential vulnerabilities to PSAPs, making them targets for more advanced denial of service (DoS) attacks. This paper discusses the current and future architecture of PSAPs and presents the results of measurements taken at a PSAP regarding the detection of attacks. It also offers potential solutions and evaluates their effectiveness.
41298	4129867	Triangle algebras: A formal logic approach to interval-valued residuated lattices	The paper introduces triangle algebras, a type of residuated lattices with approximation operators and a third point u. These algebras are shown to represent interval-valued residuated lattices (IVRLs). A system of many-valued logic called triangle logic (TL) is also presented, which captures the tautologies of IVRLs. The use of closed intervals as truth values in L is expressed through a set of logical axioms using triangle algebras. This work is an important step towards formally representing residuated t-norm based logics on the lattice of closed intervals of [0,1], similar to how formal fuzzy logics are represented on the unit interval.
41299	41299154	Swarm intelligence supported e-remanufacturing	E-Remanufacturing, a process for recovering and refurbishing used products, has become a popular approach for product recovery management. To increase efficiency in this process, various techniques have been utilized. One of these techniques is swarm intelligence (SI), which takes inspiration from the behavior of swarms in nature. SI has shown success in other fields and is now gaining attention from researchers in the remanufacturing industry. This paper provides an overview of SI methods that have been applied in e-remanufacturing. 
41300	4130014	Adaptive Neuro Fuzzy Inference System, Neural Network and Support Vector Machine for Caller Behavior Classification	This paper discusses the development and implementation of a classification system for caller behavior in Interactive Voice Response systems, specifically for a pay beneficiary application. The system uses Adaptive Neuro-Fuzzy Inference System, Feed forward Artificial Neural Network, and Support Vector Machine classifiers, achieving exceptional results with the ANN classifiers being the preferred models. The ANN classifiers achieved 100% classification accuracy for 'Say account', 'Say amount', and 'Select beneficiary' unseen data, and 95.42% accuracy for 'Say confirmation' unseen data. This classification system has the potential to improve the effectiveness of automated self-service applications. 
41301	4130142	Scaling up Link Prediction with Ensembles.	A network with $n$ nodes can have O(n2) possible links, making it difficult to evaluate all pairwise possibilities in a meaningful way, especially for large networks. Most link prediction methods are designed for a subset of links rather than the entire network due to computational complexity. However, it is important to perform an exhaustive search over the entire network in practice. This paper proposes an ensemble-based approach to scale up link prediction, breaking it down into smaller subproblems that can be solved using latent factor models. This approach has been shown to be effective and scalable in experiments on large networks.
41302	4130240	Spray and wait: an efficient routing scheme for intermittently connected mobile networks	Intermittently connected mobile networks are sparse wireless networks where there is not always a complete path from the source to the destination. These networks are used in various real-life situations such as wildlife tracking, military communication, and inter-planetary communication. Traditional routing schemes do not work well in these networks, so researchers have suggested using flooding-based routing schemes. However, these schemes have high energy consumption and contention issues, leading to delays. To address this, a new routing scheme called Spray and Wait has been introduced, which sends multiple copies of a message and waits for one to reach the destination. This scheme outperforms existing ones in terms of delivery delay and transmissions per message. It is also scalable and easy to implement and optimize for specific performance goals.
41303	4130327	Contextual grammars as generative models of natural languages	The paper discusses the use of contextual grammars, specifically those with maximal use of selectors, as a potential model for natural language syntax. These grammars involve a selection procedure where contexts are added to words based on a set of associated selectors. The paper argues that these grammars are able to accurately describe the syntax of natural languages, including complex structures such as reduplication, crossed dependencies, and multiple agreements. However, there are some center-embedded constructions that cannot be covered by these grammars. The paper also explores the possibility of associating a tree or dependence structure to the generated words, similar to that in descriptive linguistics and link grammars. 
41304	4130486	Changes as First-Class Citizens: A Research Perspective on Modern Software Tooling.	Software is constantly evolving to adapt to the ever-changing real world. A new trend in software evolution research focuses on the concept of change as the driving force behind evolution. By treating change as a first-class entity and analyzing it in depth, researchers are able to gain a better understanding of the complex process of software evolution. A study of 86 articles provides an overview of the current state of this research and presents a roadmap for future directions and areas of study. This approach offers a more accurate and thorough understanding of software evolution and its impact on the constantly evolving context of the real world.
41305	4130559	PAD-Net: Multi-tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing	The paper discusses the important tasks of depth estimation and scene parsing in visual scene understanding. It introduces a new approach, called the multi-task guided prediction-and-distillation network (PAD-Net), which utilizes a joint CNN to simultaneously predict intermediate auxiliary tasks and use them as input for the final tasks. This method aims to improve the robustness of deep representations and provide multi-modal information for better performance. The approach is tested on two datasets and results show its effectiveness in both depth estimation and scene parsing tasks.
41306	4130626	Binary-Code-Allocation Scheme in DS-CDMA Systems	The paper proposes a binary-code-allocation scheme for reducing multiple-access interference in direct-sequence code-division multiple-access systems. The scheme involves giving users an orthogonal binary sequence set and allowing them to choose a better sequence to maximize their signal to interference ratio. The scheme is analyzed through both exact analysis and approximation, and it is found that it can significantly reduce MAI with only a few bits of feedback information. The scheme can also be modified for multiuser adaptation in asynchronous channels, leading to improved performance and increased system throughput. Overall, the proposed scheme is simple, widely applicable, and robust for binary-sequence-adaptation in direct-sequence code-division multiple-access systems. 
41307	4130759	Real-Time Learning Analytics System For Improvement Of On-Site Lectures	The purpose of this study is to introduce a real-time lecture support system for on-site classrooms. The system uses e-learning and e-book systems to collect data from teachers and students during lectures, which is then analyzed and used to provide immediate feedback to the teacher. This allows the teacher to adjust their lecture based on student engagement and understanding. Through a case study and large-scale experiment, the authors confirmed the effectiveness of the system in improving teaching and learning in on-site classrooms. The use of real-time learning analytics is a new and valuable strategy for teachers to improve their lectures and should be considered for immediate implementation.
41308	413087	Dataflow-Based Lenient Implementation of a Functional Language, Valid, on Conventional Multi-processors	This paper presents a lenient implementation of a functional language, Valid, on traditional multi-processors using a data-flow execution scheme. This approach allows for highly concurrent execution of fine-grain function instances, which are created during the execution of a functional program. The lenient execution and split-phase operation help to minimize idle time caused by remote memory access and calls. However, to handle fine-grain parallelism on conventional multi-processors without specialized hardware, it is necessary to reduce overhead. The paper discusses compilation issues and runtime systems for supporting fine-grain parallel execution on two types of multi-processors: shared-memory and distributed-memory. Preliminary evaluations of the implementation on a Sequent Symmetry S2000 and Fujitsu AP1000 are also presented.
41309	413090	Coupling Transparency and Visibility: a Translucent Middleware Approach for Positioning System Integration and Management (PoSIM)	The advancement of wireless technology and the availability of various positioning techniques have created new opportunities for Location Based Services (LBSs). However, this also adds complexity to the development of LBSs. To address this issue, middleware such as PoSIM can be used to access and control different positioning systems on wireless devices. PoSIM offers two levels of visibility for positioning management: a simple, context-based approach and a more advanced access to configurations. This allows LBSs to easily and efficiently utilize multiple positioning sources. The paper focuses on describing the architecture and API of PoSIM, highlighting its design and implementation decisions.
41310	4131028	Learning image similarity from Flickr groups using fast kernel machines.	This paper proposes a method for measuring image similarity by learning from online Flickr image groups. The process involves choosing 103 Flickr groups and building a one-versus-all multiclass classifier to classify test images into a group. The responses of the classifiers are used as features and the distance between feature vectors is calculated to measure image similarity. Experimental results on two datasets show that this approach outperforms using conventional visual features for image matching, retrieval, and classification. To train the classifiers quickly and accurately, the paper introduces a novel algorithm called Stochastic Intersection Kernel MAchine (SIKMA). This algorithm can produce a more accurate kernel classifier than a linear one in just a few minutes on large datasets. 
41311	4131143	View Synthesis By Appearance Flow	The problem of novel view synthesis involves creating new images of an object or scene from different viewpoints. Instead of starting from scratch, this task can be approached by copying pixels from the original image. This is possible due to the high correlation between the visual appearance of different views of the same object. This can be learned by training a convolutional neural network to predict appearance flows, which are 2-D coordinate vectors that determine which pixels from the original image can be used to construct the new view. This approach can be extended to multiple input views, resulting in higher quality synthesized views compared to previous techniques. 
41312	4131226	ReliableWeb Services: Methodology, Experiment and Modeling	This article discusses the impact of various parameters on the dependability of Web services and the methods of improving their availability through redundancy in space and time. The authors conducted experiments to compare the availability of replicated Web services with a single service. They used a replication manager to coordinate the services and employed a round robin algorithm for workload scheduling. A replication algorithm and system configuration are also described. The experiments showed that the proposed model, constructed with Petri-net and verified through different applications, accurately represents the characteristics of Web services. The obtained parameters can be used to demonstrate the dependability of Web services.
41313	41313106	CLUE: Achieving Fast Update over Compressed Table for Parallel Lookup with Reduced Dynamic Redundancy	The routing table size in backbone routers is growing rapidly, with some routers now reaching 400K entries. To address this issue, routing table compression is an effective solution. However, there is also a growing need for fast routing updates due to changes in network topology and new Internet features, as well as the increasing internet speed. To achieve high performance, backbone routers must handle three issues simultaneously: routing table compression, fast routing lookup, and fast incremental update. This is addressed by a complete solution called CLUE, which includes a compression algorithm, improved parallel lookup mechanism, and a new incremental update mechanism. CLUE has been proven to have a speedup factor proportional to the hit rate of redundant prefixes and has shown superior performance in large-scale experiments compared to previous solutions.
41314	413147	A High-Level Signal Integrity Fault Model and Test Methodology for Long On-Chip Interconnections	The paper presents a new method for generating test patterns to detect signal integrity problems on interconnects, taking into account the interconnection topology information. Previous methods using SPICE were found to be complex and time-consuming. To improve accuracy and save time, the paper proposes a new high-level signal integrity fault model that considers process variation and interconnect signal transition. The experimental results show that this model is more accurate for long interconnects compared to previous methods. Additionally, the proposed method is much faster than SPICE-based methods. Overall, this paper presents a more efficient and accurate approach to identifying and addressing signal integrity issues on on-chip interconnects.
41315	41315133	Joint tracking of clean speech and noise using HMMs and particle filters for robust speech recognition.	The proposed method aims to improve the performance of speech recognition in noisy environments by simultaneously tracking both the clean speech signal and the noise. This is achieved by using a combination of particle filters and hidden Markov models. The information gathered from speech tracking is then used to estimate the parameters of the noise. This approach provides more robustness against sudden changes in noise levels. Experimental results using the Aurora-2 connected digit recognition task showed a 12.15% improvement in performance compared to existing methods when the noise mean was updated every 300 milliseconds. This highlights the effectiveness of the proposed dynamic joint tracking framework. 
41316	41316116	Acoustic echo cancellation based on independent component analysis and integrated residual echo enhancement	This paper explores the use of a memoryless noise-suppressing nonlinearity in an acoustic echo canceler (AEC) with normalized least-mean square (NLMS) when there is ambient noise present. The addition of this nonlinearity is justified from an information-theoretic perspective and has a connection to independent component analysis (ICA). This approach allows for new algorithmic possibilities beyond traditional LMS techniques. By combining error enhancement and proper regularization, the AEC can be performed continuously and recursively in the frequency domain, even without the need for double-talk or voice activity detection. This technique has potential for better performance in the presence of both ambient noise and double-talk. 
41317	4131742	Efficient evaluation of XQuery over streaming data	This paper discusses the importance of processing queries over streaming XML data, and presents a new framework and techniques for doing so. The authors propose optimizations to transform XQuery queries for single-pass execution, a methodology for determining when this is possible, and a code generation approach that can handle recursive functions and user-defined aggregates. Their implementation is evaluated using various benchmarks and real-world queries, and is shown to be significantly faster and more efficient than existing systems. The results demonstrate the effectiveness of their approach in handling large datasets and avoiding memory overflows.
41318	4131821	External hashing with limited internal storage	This paper explores the use of external hashing and the potential for improving retrieval speed by storing extra information in internal storage. The first part of the paper focuses on a restricted class of algorithms and derives a lower bound on the required extra storage. An algorithm that achieves this bound is also presented. In the second part, more practical algorithms are developed and analyzed, including one that allows for retrieval in one access using only a small amount of extra storage per bucket. The cost of inserting a record is also considered and found to be low. Overall, this algorithm is highly competitive for applications with a need for fast retrieval.
41319	4131987	Face verification of age separated images under the influence of internal and external factors	This paper focuses on the task of verifying the age of faces in images, taking into account various internal and external factors. The authors propose a hierarchical local binary pattern (HLBP) feature descriptor, combined with an AdaBoost classification framework, to effectively represent and classify faces. The results of experiments on aging datasets show that the proposed framework is robust for both adults and children. The study also looks at the effects of factors such as age gap, gender, ethnicity, pose, expressions, facial hair, and glasses on verification performance. It is found that age gap has a significant impact on accuracy and that cues can aid both humans and machines, but can also mislead. Pose and expression variations have a negative impact, while facial hair and glasses can be useful cues. The ethnicity of the faces also has some effect on performance, but overall, non-linear algorithms do not significantly improve results compared to linear ones.
41320	413204	Bridging the Gap between Enterprise Content Management and Creativity: A Research Framework	ECM is a growing area of research in Information Systems, with a focus on improving consistency, availability, and traceability of content. However, one important aspect that has been overlooked is the role of creativity. While measures such as time and budget are important for organizations, it is also crucial to consider the impact of ECM on employee creativity and innovation. This paper advocates for a research approach that considers both traditional control factors and the influence of ECM on creativity. A research framework is proposed, drawing from various literatures on ECM and creativity. This framework aims to further explore the relationship between these two concepts.
41321	4132183	Inferring gene regulatory networks by integrating static and dynamic data.	The paper proposes a methodology for learning gene regulatory networks from DNA microarray data by integrating different data and knowledge sources. The method is applied to experiments on the yeast cell cycle, using data from deletion mutant experiments, gene expression time series, and the Gene Ontology. The method involves four phases: deriving an initial gene network from static data, selecting cell cycle-related genes from the Gene Ontology, using the network structure to initialize a dynamic model, and updating the network using a genetic algorithm. This approach is compared to a fully data-driven approach and is shown to be more effective in producing robust models. The learned networks can then be validated through wet-lab experiments.
41322	4132249	A function-decomposition method for development of hierarchical multi-attribute decision models	Function decomposition is a machine learning technique that creates a hierarchical structure from labeled data by identifying new aggregate attributes and their descriptions. These new attributes are simpler than the original set and are used to develop a multi-attribute decision model. This method, implemented in a system called HINT, was evaluated on a housing loans problem and three existing decision models. The results showed that function decomposition can produce accurate and transparent decision models. Additionally, human interaction, such as providing assistance or background knowledge, can improve the comprehensibility and accuracy of the models. 
41323	4132329	Research on modeling of complicate traffic simulation system	Traffic simulation has become a crucial tool in dealing with the growing complexity of traffic problems. System modeling has also become increasingly important in these simulations. Cellular automata, which are simple mathematical models, have been proven to be effective in describing complex traffic environments. A simulation model that combines agent technology, HLA/RTI technology, and expanded cellular automata is proposed in this paper. This model allows for flexibility and scalability, while also providing powerful computing resources to tackle complex traffic issues. The model utilizes expanded cellular automata and agent technology to represent the behaviors of passengers, vehicles, and traffic signals. It can be used to determine optimal solutions for traffic disasters, supervise large scale activities, and design traffic environments.
41324	4132415	Lazy home-based protocol: combining homeless and home-based distributed shared memory protocols	This paper introduces a new protocol for a page-based distributed shared memory system that combines the benefits of both homeless and home-based protocols. The protocol uses a homeless diff-based memory update during lock synchronization to reduce critical section time and a home-based page-based update using the invalidation coherence protocol, which is delayed until the next barrier time. This "lazy home-based" update has advantages such as less interruption in home nodes and reduced data traffic and message count. The paper also includes a detailed analysis of the protocol's impact on distributed shared memory applications.
41325	4132544	An agent-based simulation for restricting exploitation in electronic societies through social mechanisms	The problem of non-cooperation in artificial agent societies is a major issue that can lead to free-riding, where some agents consume resources without contributing to the society. To address this problem, new mechanisms for self-organization and management in multi-agent societies have been developed. These mechanisms involve modeling nodes of a P2P system as interacting agents in different groups and using social mechanisms such as tags, gossip, and ostracism to separate cooperative and uncooperative agents. This system has shown promising results in reducing exploitation and encouraging cooperative behavior without centralized control. These mechanisms have potential for use in regulating distributed societies and offer new insights into policy mechanisms for such societies.
41326	4132626	Interfacing a cognitive agent platform with a virtual world: a case study using Second Life	Online virtual worlds are becoming popular for human interaction and simulating multi-agent systems. It would be useful to integrate high-level agent development tools, specifically BDI programming frameworks, with virtual worlds. However, this is a complex task as it requires mapping sensor readings from virtual environments to a logical model. In this paper, the authors explore this problem in the context of agent interactions in Second Life. They propose a framework that allows any multi-agent platform to connect with Second Life and demonstrate it using the Jason BDI interpreter. This framework can bridge the gap between virtual worlds and BDI programming, making it easier for agents to interact with humans in these environments. 
41327	4132745	Refined Coding Bounds and Code Constructions for Coherent Network Error Correction	Coherent network error correction refers to the problem of controlling errors in network coding when the source and sink nodes have knowledge of the network codes being used. This problem is addressed by obtaining improved versions of the Hamming bound, Singleton bound, and Gilbert-Varshamov bound for linear network codes. These bounds are shown to be tight, meaning they are achievable by certain construction algorithms. These algorithms use different approaches, one utilizing existing network coding algorithms and the other using classical error-correcting codes. The significance of these tight bounds is that sink nodes with higher maximum flow values can have better error correction capabilities. 
41328	4132820	Multicast switching fabric based on network coding and algebraic switching theory	Scheduling algorithms play a critical role in improving the throughput of switches. However, they do not guarantee a consistent delay in the switching fabric. This paper presents a new load-balanced wire-speed multicast switching fabric that incorporates network coding. The fabric uses a two-phase self-routing structure based on Boolean-multiplexer to achieve efficient multicast communication. The proposed fabric promises to balance the load and improve the performance of multicast switches. Its effectiveness is demonstrated through simulation results.
41329	4132997	Multi-modal human-machine communication for instructing robot grasping tasks	The challenge of creating intelligent robots is to give them cognitive abilities that allow users to easily and intuitively program them. One way to achieve this is through interactive demonstration, where the machine can learn work tasks from the user. To make this process efficient and user-friendly, the robot must be able to focus its attention, understand spoken instructions, and interpret non-verbal cues such as gestures. Researchers have made progress in developing a hybrid architecture that combines statistical methods, neural networks, and finite state machines to create a system for teaching grasping tasks through man-machine interaction. This system combines visual attention and gestural instruction with speech recognition and linguistic interpretation, and allows for multi-modal communication for dextrous manipulation of objects.
41330	4133012	Scalable target detection for large robot teams	The paper introduces a new asynchronous display method called "image queue" for operators to search through large amounts of data collected by autonomous robot teams. The focus of the method is on urban search and rescue tasks, where it helps to identify targets of interest, such as injured victims, by presenting a comprehensive view of the environment. Compared to a traditional synchronous display with live video feeds, the image queue reduces errors and the operator's workload. It also allows for a call center approach to target detection and enables scalability for large multi-robot systems. The paper concludes that the image queue is a more efficient and effective approach for foraging tasks.
41331	413316	Coalition formation with spatial and temporal constraints	The coordination of emergency responders and robots in disaster scenarios is a big challenge for multi-agent systems. The main issue is how to create the best teams (coalitions) of responders to handle the tasks in the affected area. These teams may have to form, disband, and reform in different parts of the disaster area, as there are typically more tasks than agents available. The tasks themselves are complex and have different deadlines and durations, making the problem even more difficult. This is known as the Coalition Formation with Spatial and Temporal constraints problem (CFSTP), which has been proven to be NP-hard. To solve this problem, a Mixed Integer Program has been designed for small-scale instances, and new anytime heuristics have been developed that can complete 97% of the tasks in large problems. These solutions are the first of their kind for the CFSTP.
41332	4133214	Optimal secure partial-repair in distributed storage systems	In a distributed storage system, certain storage nodes may lose fragments of the source file. These nodes are labeled as "faulty," while those that have not lost any fragments are called "complete." In partial repair, a combination of faulty and complete nodes work together to transmit fragments and recover the lost ones. The goal is to minimize the required bandwidth for this process. However, there is a risk of an eavesdropper intercepting the repairing fragments and obtaining information about the source file. To address this, optimal secure codes are proposed to ensure minimal partial-repair bandwidth while preventing the eavesdropper from gaining any information. A special scenario is also considered for exact partial-repair.
41333	4133331	Sebar: Social Energy Based Routing Scheme For Mobile Social Delay Tolerant Networks	Delay Tolerant Networks (DTNs) are networks that are not always continuously connected, such as mobile social networks. Routing in such networks is challenging due to network partitioning, delays, and changing topology. Social-based approaches have gained interest for DTN routing, using social behaviors of nodes to improve routing decisions. This paper proposes a new social-based routing approach for mobile social DTNs, using a metric called social energy. This metric quantifies a node's ability to forward packets and is generated through node encounters and shared among communities. The proposed SEBAR protocol considers social energy of encountering nodes and favors nodes with higher social energy. Simulations show the effectiveness of SEBAR compared to other existing DTN routing schemes.
41334	4133477	Towards Truthful Mechanisms for Binary Demand Games: A General Framework	The Vickrey-Clarke-Groves (VCG) mechanisms are highly regarded in the world of truthful mechanism design, but they are limited in their applicability. They only work for optimization problems with a utilitarian objective function and require the output to optimize the objective function, which can be computationally challenging. If we use VCG mechanisms for polynomial-time algorithms that approximate the optimal solution, they may no longer be truthful. To address these limitations, researchers are exploring the possibility of designing a truthful non-VCG payment scheme that is computationally tractable for a given output method. This paper focuses on binary demand games and shows that a truthful mechanism exists if the output method satisfies a certain monotone property. It also presents efficient algorithms for computing truthful payments through various combinations of subgames.
41335	4133596	Flow-level traffic matrix generation for various data center networks	With the rise of cloud computing technologies, the number of data centers deployed by governments, enterprises, and universities has increased. To reduce costs, various architectures and topologies have been proposed for data center networks. However, constructing a data center can be expensive and there are many technologies that can affect its structure. Before building a data center, its characteristics must be confirmed to meet requirements. This can be done through simulation and testing using a traffic generation method. The proposed method in this study uses flow-level traffic matrix and Python and iPerf programming languages to generate network traffic. The results showed that the generated traffic closely resembles real network traffic collected from a data center network. 
41336	4133618	Sentence Correction Incorporating Relative Position and Parse Template Language Models	Sentence correction is a growing concern in computer-assisted language learning. Despite efforts using grammar rules and statistical machine translation, current techniques are not strong enough to handle the common errors made by non-native speakers. To address this issue, a relative position language model and a parse template language model have been proposed to supplement traditional language modeling methods. A corpus of incorrect English-Chinese sentences and their corrections has been manually assessed by human evaluators. The results show that the proposed approach outperforms a top phrase-based statistical machine translation system in terms of error correction, as determined by human evaluation.
41337	413371	A benchmark library and a comparison of heuristic methods for the linear ordering problem	The linear ordering problem is a challenging combinatorial optimization problem that involves finding the best possible ordering of a set of items represented by a weighted digraph. This paper explores various heuristics and metaheuristic algorithms for solving this problem and presents the results of extensive computational experiments. Additionally, a new benchmark library called LOLIB is introduced, which contains both previously used instances and new ones for this problem. This research provides valuable insights and tools for tackling the NP-hard linear ordering problem.
41338	4133816	Adaptive tracking control for discrete-time switched nonlinear systems with dead-zone inputs.	 This paper examines the problem of adaptive tracking control for a discrete-time switched nonlinear system with dead-zone input, using a class of switching signals with average dwell time. The complex structure of these systems and the presence of dead-zones pose a significant challenge for control. To address this, fuzzy logic systems are used to approximate the unknown terms of each subsystem. Both stable and unstable subsystems are considered, and the controllers, update laws, and switching signals are designed to ensure stability and bounded signals. A simulation example is presented to demonstrate the effectiveness of the proposed approach.
41339	413392	Exploring geospatial cognition based on location-based social network sites	Geospatial cognition is an important skill for navigating sophisticated urban spaces and making informed decisions in our daily lives. However, developing urban cognition can be a time-consuming and challenging task, especially in unfamiliar or rapidly changing cities. Gathering information from experienced locals can also be a tedious process. To address this, researchers are exploring the use of location-based social networks to collect and utilize the urban cognition of the crowd. They have developed a method to extract crowd movement data from these networks, which can provide valuable insights into common urban cognition and measure the socio-cognitive distances between different urban clusters. This data is then used to create a simplified two-dimensional map of cognitive relationships, allowing for a more intuitive understanding of urban space. In an experiment using Twitter data, this socio-cognitive map was found to accurately represent the cognitive proximity between different urban clusters. This map can be used for practical purposes, such as finding the nearest neighbor areas in terms of cognitive similarity.
41340	4134032	Towards random field modeling of wavelet statistics	This paper explores the statistical properties of signals and images in the wavelet domain. The authors discover that the correlation between wavelet scales can be significant, despite commonly used decorrelated-coefficient models. They investigate different statistical-interaction models and propose an efficient and quick method for describing wavelet-based correlations. The paper also demonstrates how this method can be used for hierarchical Markov random field modeling of within-scale and across-scale dependencies. Overall, this paper highlights the importance of considering correlations between wavelet scales in statistical analysis and provides a useful tool for efficient modeling.
41341	41341149	Enabling advanced environmental conditioning with a building application stack	Building-focused applications have the potential to greatly improve the operation and sustainability of buildings. By addressing issues with current architectures and providing a framework for reliable and fault-tolerant operation, a building application stack allows for the development of advanced applications. These applications integrate sensors and actuators from both the building infrastructure and external networks, making use of the architecture's design pattern. Two examples of such applications, a demand-controlled ventilation system and a demand-controlled filtration system, were successfully implemented in a large commercial building. These applications were able to greatly reduce energy consumption while maintaining air quality, showcasing the potential of the architecture for widespread and rapid development in various buildings.
41342	4134265	Hdr Image Multi-Bit Watermarking Using Bilateral-Filtering-Based Masking	The paper introduces a new method for embedding multi-bit watermarks in High Dynamic Range (HDR) images, ensuring the watermark is imperceptible in both the HDR and Low Dynamic Range (LDR) versions. The method uses the wavelet transform of the Just Noticeable Difference (JND)-scaled space of the original image and a visual mask based on the Human Visual System (HVS) to improve image quality. Bilateral filtering is used to locate the detail portion of the image, where the watermark is embedded, and a contrast sensitivity function is applied to modulate the watermark intensity in each wavelet decomposition subband. Experimental results demonstrate the effectiveness of the proposed method in preserving image quality and being robust against tone-mapping operators (TMOs).
41343	4134357	Secure Annotation For Medical Images Based On Reversible Watermarking In The Integer Fibonacci-Haar Transform Domain	The increased use of digital image-based applications has led to large databases that are difficult to use and can pose privacy concerns, especially in medical applications. The common solution is to encrypt both the image and patient data separately, but this can be inefficient as it requires decrypting both files to access information. A new approach for secure medical image annotation is proposed, using a key-dependent wavelet transform, a secure cryptographic scheme, and a reversible watermarking scheme. This allows for the insertion of patient data into encrypted images without knowledge of the original image, encryption without losing information, and reversible recovery of the original image. Experiments show the effectiveness of this system.
41344	4134440	Information retrieval system for human-robot communication: asking for directions	The ACE project aims to develop a robot that can navigate through unfamiliar city environments without relying on GPS or maps. Instead, the robot will interact with humans to gather direction information. A human-robot communication system has been created to allow the robot to ask for directions and store the information as internal knowledge. This system integrates linguistic theories and uses a topological route graph to give feedback to the human and navigate through unknown areas. This innovative approach to navigation could have significant implications for robots and their ability to operate in real-world settings.
41345	4134561	An Automatic Segmentation Combining Mixture Analysis and Adaptive Region Information: A Level Set Approach	This paper introduces a new automatic framework for color image segmentation that combines adaptive region information and mixture modeling. The region information is represented by a mixture of General Gaussian (GG) probability density functions (pdfs) and the segmentation is achieved by minimizing an energy functional based on region contours and mixture parameters. The proposed method allows for easy extension to any number of regions and is fully automatic. Additionally, it produces an accurate and representative mixture of pdfs. The approach utilizes both boundary and region information to guide the segmentation and is validated on real world color images.
41346	4134647	Evaluating Domain Design Approaches Using Systematic Review	Software Product Lines (SPL) are becoming increasingly popular among software companies as a way to achieve reuse. It involves three processes: domain engineering, application engineering, and management. Domain engineering focuses on developing assets that can be reused by different products, forming the core assets. The Domain Specific Software Architecture (DSSA) is a crucial part of this process and plays a key role in the success of the products developed from it. To design a good DSSA, a process must be followed to manage the variability and commonality within the domain. Companies transitioning from single system development to SPLs need systematic activities to utilize existing assets for developing a DSSA. This paper provides a systematic review of domain design approaches, helping companies understand the current landscape and choose or adapt an approach that fits their needs.
41347	4134773	MAHRU-M: A mobile humanoid robot platform based on a dual-network control system and coordinated task execution	This paper presents a mobile humanoid robot platform that can perform various services for humans in their daily environments. To ensure efficient operation in complex environments, a dual-network control system was developed using high-speed IEEE 1394 and Controller Area Network (CAN). A service framework was also introduced to coordinate tasks for the robot. Performance evaluations of the framework and control system were conducted, showing efficient execution of tasks in human workspaces. The platform, MAHRU-M, is able to recognize designated objects and calculate their pose using a particle filter with back projection-based sampling. A unique approach for human-like arm inverse kinematics and a mean-shift algorithm for real-time and robust object tracking were also implemented. Overall, the results show successful execution of services in office or home environments.
41348	4134840	A nonlinear iterative learning method for robot path control.	The proposed method is designed for non-linear dynamic systems with uncertain parameters. It uses a non-linear system model and the model algorithmic control concept in an iterative sequence. A condition for convergency is given, and the method is demonstrated to be effective for continuous-path control of a robot manipulator.
41349	4134932	Performance evaluation of network mobility handover over future aeronautical data link	The aviation industry is working towards standardizing data communication systems for air traffic management in the future. The International Civil Aviation Organization (ICAO) is focusing on standardizing an IPv6-based aeronautical telecommunications network and future radio access technologies. This paper integrates L-DACS 1, a possible future radio access technology, with a realistic IPv6-based network and analyzes the impact of handover delay on TCP performance. The simulation experiments use realistic Frame Error Rate (FER) values and show that by reducing the handover latency and introducing a Home Agent (HA)-buffering method, the transmission completion time can be reduced by at least 10%. These improvements are especially significant for larger data transmissions over a wireless link with a data rate of 31.5kbits^-^1.
41350	4135075	Socially augmenting employee profiles with people-tagging	Employee directories are important for connecting people within an organization for collaboration, problem-solving, and accessing expertise. However, maintaining accurate and updated user profiles is often neglected. This paper presents a system that allows users to tag each other with keywords, which are displayed on their profiles. This "people-tagging" acts as a form of social bookmarking, allowing users to organize their contacts and search for individuals based on specific topics. It also has the added benefit of collectively maintaining each other's profiles. User studies show that people-tagging is effective for contact management and the tags accurately reflect interests and expertise. There were no reports of offensive or inappropriate tags, leading to the belief that people-tagging will become an important relationship management tool in organizations.
41351	413512	Explorations in an activity-centric collaboration environment	The demonstration showcases a new hybrid collaboration technology that combines features of informal and formal collaboration tools. It supports lightweight and flexible activities with dynamic membership and a mix of real-time and delayed collaboration. The system will be introduced and audience members will have the opportunity to use it in various exercises. 
41352	4135245	Return On Contribution (ROC): A Metric for Enterprise Social Software.	The value of enterprise social media applications and their impact on components and users is not easily measurable in traditional economic terms. Instead, a new approach called Return On Contribution (ROC) is proposed, which focuses on the human aspect of service to others. This metric is designed to help manage social software systems and track collaboration among employees through the creation and consumption of information and knowledge. By using ROC, the performance of various social media applications can be monitored, along with understanding usage patterns and the performance of employees. This has implications for organizational knowledge exchange, the role of "lurkers," and the types of measurements that are valuable to employees, managers, and system administrators.
41353	41353165	Overview of VideoCLEF 2008: Automatic Generation of Topic-based Feeds for Dual Language Audio-Visual Content.	VideoCLEF was created in 2008 to develop and evaluate tasks related to analyzing and accessing multilingual multimedia content. The first task, Vid2RSS, focused on classifying dual language videos from Dutch television. Participants were given Dutch archival metadata, speech transcripts in both Dutch and English, and ten thematic category labels to assign to the videos. Five groups participated, using various classifiers and sources of training data. The Dutch speech transcripts and archival metadata were found to be effective as indexing features, but no group was able to significantly improve performance by combining feature sources. The translation task was evaluated and deemed valuable for non-Dutch speaking English speakers, while the keyframe extraction task showed promising results in automatically selecting representative shots. Future years of VideoCLEF will expand the corpus and tasks.
41354	4135453	Pic-A-Topic: efficient viewing of informative TV contents on travel, cooking, food and more	Pic-A-Topic is a prototype system that allows users to selectively view topical segments of recorded TV shows. It uses closed captions and electronic program guide texts to perform topic segmentation and topic sentence selection, and presents a clickable table of contents to the user. Previous work has shown that Pic-A-Topic's accuracy in segmenting travel shows is comparable to manual segmentation. This paper demonstrates that the latest version of Pic-A-Topic is also effective in segmenting other genres such as travel, cooking, food, and talk/variety shows, using genre-specific strategies. An experiment with 26.5 hours of Japanese TV shows shows that Pic-A-Topic's segmentations for non-travel genres are just as accurate as those for travel. The system's accuracy is around 82% of manual performance on average, and the combination of cue phrase and vocabulary shift detection is successful for all targeted genres.
41355	413555	Energy optimal coding for wireless nanosensor networks	Wireless nanosensor networks (WNSNs) are made up of tiny nanosensors that can detect and sense events at the nanoscale. This technology has many potential applications, such as drug delivery and air pollution monitoring. However, one challenge is the limited energy of the nanosensors, making energy-efficient protocols crucial for these networks. This paper focuses on a WNSN with on-off keying (OOK) modulation and proposes a minimum transmission energy (MTE) coding scheme. This scheme minimizes the energy consumption per symbol by mapping m-bit symbols into n-bit codewords with the least number of high-bits. The optimal setting for m and n is determined to achieve the lowest energy consumption per data bit, serving as the lower bound for energy consumption in WNSNs. Numerical results show the effectiveness of the MTE coding scheme.
41356	41356101	Improving discriminative sequential learning by discovering important association of statistics	Discriminative sequential learning models like CRFs have been successful in natural language processing and information extraction due to their ability to capture nonindependent and overlapping features of inputs. However, imbalanced classes, irregular phenomena, and ambiguity in training data can negatively affect their performance. To address this, a data-driven approach has been proposed that discovers and emphasizes important associations in the training data and incorporates them into these models. Experiments on phrase-chunking and named entity recognition tasks using CRFs have shown improved accuracy. This approach also highlights a potential connection between association mining and statistical learning, offering a strategy to enhance learning performance with patterns discovered from large datasets. 
41357	4135778	I Can Already Guess Your Answer: Predicting Respondent Reactions during Dyadic Negotiation	Negotiation is an important part of our daily lives, but it can be difficult to predict how someone will respond to an offer. This study looked at acoustic and visual cues to see if they could predict a person's immediate reaction in a negotiation. The researchers used a dataset of 42 simulated negotiations and found that other sources of information, such as the nonverbal behavior of the proposer, could also predict reactions. Mutual behavioral cues, like symmetry and asymmetry, were also effective predictors, as they captured the nature of the interaction. Finally, the study identified specific audio-visual cues that were most predictive of a respondent's immediate reactions. 
41358	4135828	Convergent Lagrangian and Contour Cut Method for Nonlinear Integer Programming with a Quadratic Objective Function	This paper describes a new and efficient method for solving nonlinear separable integer programming problems with a quadratic objective function. The method combines the Lagrangian dual method with a duality reduction scheme using contour cut. It determines lower and upper bounds for the problem at each iteration using the Lagrangian dual search. To eliminate the duality gap, a new cut-and-partition scheme is used, taking advantage of the quadratic contour structure. The algorithm is proven to find an exact solution in a finite number of iterations and has been tested on problems with up to 2000 integer variables. The results are compared to other methods, showing the effectiveness of the proposed approach. 
41359	4135982	Pinball loss minimization for one-bit compressive sensing: Convex models and algorithms.	The 1bit-CS method, which uses a single comparator to quantize signals, is gaining popularity due to its low power and high speed. However, when noise is present during signal acquisition and transmission, the decoding performance of 1bit-CS is affected. To address this issue, the pinball loss function is introduced, which combines the one-sided ℓ1 loss and the linear loss. Two convex models, the elastic-net pinball model and its modified version with an ℓ1-norm constraint, are proposed to minimize the pinball loss. Dual coordinate ascent algorithms are designed to efficiently solve these models, with their convergence being proven. Numerical experiments demonstrate the effectiveness of the proposed algorithms and the superior performance of the pinball loss function for 1bit-CS.
41360	4136070	A hierarchy-based fault-local stabilizing algorithm for tracking in sensor networks	This paper presents a new approach, called hierarchy-based fault-local stabilization, for handling faults in the Stalk algorithm for tracking in sensor networks. This technique allows for efficient self-healing and fault containment, ensuring that Stalk can still maintain its data structure and perform its functions even when parts of the network are corrupted. The local stabilization is achieved by slowing down the propagation of information as the hierarchy levels increase, preventing misinformation from spreading too far. This approach maintains efficiency and availability even in the absence of faults, with operations taking O(d) time and updates taking O(d*log network diameter) amortized time. Additionally, the tracked object can move freely while Stalk is working, making it more adaptable to changes in the network.
41361	4136129	Automatic Localization and Boundary Detection of Retina in Images Using Basic Image Processing Filters.	This paper proposes a system for automatically localizing and detecting boundaries in retina images to assist ophthalmologists in accurately diagnosing eye diseases such as glaucoma and diabetic retinopathy. The system has three main phases: preprocessing, segmentation, and detection. The preprocessing phase enhances the image and removes noise. The segmentation phase extracts features from main parts of the image, including the optic disc, blood vessels, and fovea. The detection phase identifies and classifies the input image as either left or right eye, helping ophthalmologists to periodically check for diseases. Basic image processing filters are used in all phases, and a simple approach is used to detect left and right images. The system was tested on a subset of images from the DRIVE database and showed promising results.
41362	4136229	Nonpositive Curvature and Pareto Optimal Coordination of Robots	This article discusses the coordination of multiple robots in a shared environment with the goal of optimizing travel time while avoiding collisions. Each robot has a graph representing its possible configurations, and the optimal coordination is defined as a set of points that satisfy both the goal and avoiding obstacles. The article proves a finite bound on the number of optimal coordinations, based on the fact that the coordination space has no positive curvature. This bound also applies to systems with moving obstacles. The proofs use concepts from geometric group theory and CAT(0) geometry.
41363	4136359	Hierarchical control for self-assembling mobile trusses with passive and active links	This paper discusses the concept of active modular trusses, which are self-reconfiguring robots comprised of both active and passive modules. These trusses have many potential applications, such as space exploration and construction tasks. The paper presents a hardware design for truss climbing and hierarchical algorithms for controlling these hyper-redundant modular trusses. The authors also introduce the concept of "Shady," a module that can grip and move along a window frame to provide personalized shading. They discuss the challenges of control and planning for these systems, as well as potential benefits of using active and passive modules in self-reconfiguring robots. 
41364	4136421	Robust MIMO cognitive radio systems under temperature interference constraints	Cognitive radio (CR) systems protect primary users (PU) by imposing temperature interference constraints on secondary users (SU). However, SUs may violate these limitations if perfect SU-to-PU channel state information (CSI) is not available. This paper introduces a novel and distributed design for MIMO CR networks that is resilient to imperfect SU-to-PU CSI. The system design is formulated as a noncooperative game and robust global interference constraints are enforced through pricing. The prices are additional variables to optimize. Using finite-dimensional variational inequalities (VI) in the complex domain, the proposed NE problem is analyzed and alternative distributed algorithms are developed with their convergence properties.
41365	4136556	FlashLight: A Lightweight Flash File System for Embedded Systems	NAND flash memory has become a popular choice for storage due to its high performance. However, to design a flash file system that can fully utilize its capabilities, two key aspects need to be considered. Firstly, an efficient index structure is needed to manage file and data locations. For large storage capacities, the index structure must be stored in the flash memory to reduce memory consumption, but this can impact system performance. Secondly, a garbage collection (GC) scheme is necessary to reclaim obsolete pages. This can lead to additional read and write operations, so a novel approach is required. The article introduces FlashLight, a new flash file system that addresses these issues through a lightweight index structure and an efficient GC scheme. Experiments show that FlashLight outperforms UBIFS by up to 27.4&percnt; by reducing index management and GC overheads by up to 33.8&percnt;.
41366	413664	Transparently bridging semantic gap in CPU management for virtualized environments	Consolidated environments are becoming more common as they combine different workloads using virtual desktop infrastructure and cloud computing. However, this leads to a mismatch between the virtual machine monitor and guest operating systems, resulting in inefficient resource management. In particular, CPU management for virtual machines can greatly impact I/O performance. To address this issue, this paper proposes virtual machine scheduling techniques that bridge the semantic gap by making the virtual machine monitor aware of task-level I/O demands. This improves I/O performance without affecting CPU fairness. Additionally, performance anomalies from indirect use of I/O devices are addressed through scheduling techniques. These techniques were implemented in the Xen virtual machine monitor and evaluated with various benchmarks and real workloads on Linux and Windows guest operating systems. 
41367	413672	Dense Subset Sum May Be the Hardest.	The SUBSET SUM problem involves finding whether a given set of positive integers can be combined to reach a specific target sum. It is still unknown whether there is a faster algorithm than the O*(2(n)(/2)) one proposed by Horowitz and Sahni in 1974. Recent research has focused on the problem's complexity when considering the maximum bin size, which is the largest number of subsets that can yield the same sum. This research has led to the discovery of a truly faster algorithm for instances with small bin sizes, as well as a connection to additive combinatorics and information theory. These findings challenge the current belief that instances with larger bin sizes are more difficult, and provide progress towards solving the open question. 
41368	4136851	Elliptic Curve Scalar Multiplier Design Using FPGAs	A new elliptic curve scalar multiplier with variable key size has been implemented as a coprocessor using a Xilinx FPGA. This design uses the internal memory of the FPGA and is contained within a single chip, resulting in a compact and efficient design. By minimizing data transfer between the FPGA and host, the design achieves high performance. Experimental results demonstrate that the carefully designed hardware architecture is highly regular and makes efficient use of the FPGA's resources. 
41369	413695	Decentralized dynamic task planning for heterogeneous robotic networks.	This paper introduces a decentralized model and control framework for task planning in a network of different types of robots. The framework allows for the design of missions, or sets of tasks, to achieve overall objectives without being limited by the capabilities of the robots. The concept of skills, defined by the mission designer, is used to distribute tasks among the robots. A decentralized control algorithm is also developed, based on the concept of skills, to coordinate task assignment and execution. The paper discusses the conditions under which the decentralized model is equivalent to a centralized one and provides experimental results to demonstrate its effectiveness in a real-world scenario.
41370	413703	Impact of blog design features on blogging satisfaction: an impression management perspective	According to global trends, many bloggers are motivated by self-presentation and use third party tools and hosting services to enhance their blogs. By providing user-friendly tools for self-presentation, blogging satisfaction can be increased. This is supported by the theory of impression management, where bloggers use content, functional, and aesthetic design features to manage their online identity and seek confirmation from readers. This perceived identity verification can lead to a heightened sense of satisfaction in blogging. Results from a survey and focus group discussion support this idea, and have both theoretical and practical implications.
41371	413710	Mapping the Cerebral Sulci: Application to Morphological Analysis of the Cortex and to Non-rigid Registration	The authors propose a method for extracting parametric representations of the cerebral sulci from magnetic resonance images and discuss its application in quantitative morphological analysis and spatial normalization and registration of brain images. The method uses deformable models based on characteristics of the cortical shape, with an active contour being initialized along the outer boundary of the brain and deforming along the medial surface of a sulcus under the influence of an external force field. Results of this methodology and its applications are presented in the paper. 
41372	4137235	A specialisation calculus to improve expert systems communications	This work aims to improve the behavior of traditional input/output expert systems, which only provide certainty values for propositions in uncertain reasoning contexts. Instead, this work proposes a more informative approach where the expert system provides a set of formulas, including both propositions and specialized rules with unknown propositions in their left part. This approach is based on a family of propositional rule-based languages founded on multiple-valued logics, and a deductive system using a Specialisation Inference Rule (SIR) is presented. The soundness and literal completeness of this system are proven, and its implementation utilizes techniques of partial evaluation. This approach also offers a way of validating knowledge bases. 
41373	4137320	An OFDM based MAC protocol for underwater acoustic networks	This paper explores the use of OFDM based MAC protocols in underwater acoustic networks. Due to the challenges posed by underwater channels, existing protocols are not suitable. To address this, the authors propose a new protocol called TDM with FDM over OFDM MAC (TFO-MAC) which combines TDM and FDM/OFDM to optimize the use of available bandwidth. This is achieved through dynamic channel assignment, power optimization, and modulation method selection by powerful base stations. The problem is formulated as a mixed integer programming problem and a greedy algorithm is proposed to solve it. Simulation results demonstrate that TFO-MAC can achieve high throughput and fairness in a cellular-like underwater network.
41374	41374217	P2P Content Distribution to Mobile Bluetooth Users	The widespread use of handheld devices, particularly smart phones, for personal entertainment has led to the integration of Bluetooth technology in these devices. This allows for the distribution of entertainment content, such as music and videos, through various means such as opportunistic downloads and peer-to-peer collaboration. However, the design of peer-to-peer content distribution protocols is heavily influenced by the unique characteristics of Bluetooth, which differs from traditional Internet-based content distribution methods. Despite this, there is a lack of understanding about the performance of Bluetooth in dynamic environments with factors like mobility, interference, and different versions and chipsets. Through extensive measurement studies, it has been found that Bluetooth-based content distribution suffers from issues like time-consuming resource discovery and limited bandwidth, even with the latest Bluetooth version. To address these challenges, effective strategies are discussed to improve the performance of resource discovery and downloading phases.
41375	4137513	β-PSR: A Partial Spectrum Reuse scheme for two-tier heterogeneous cellular networks	This paper presents a Partial Spectrum Reuse (PSR) scheme, called "β-PSR", to increase spectrum efficiency in two-tier heterogeneous cellular networks. The scheme assigns a uniform portion β of the spectrum to each micro Base Station (BS) at random. The optimal value of β for minimizing service outage probability is analyzed, but it is not given in an explicit form. However, a closed-form limit of β is derived when the user data rate requirement is close to zero compared to the system bandwidth. This limit is a function of traffic intensity, macro/micro BS density, and transmit power. Numerical results show that the optimal β is well approximated by the derived limit, leading to an adaptive PSR scheme with near-optimal performance based on statistical network information.
41376	4137648	A pair of forbidden subgraphs and perfect matchings	This paper examines the connection between forbidden subgraphs and the existence of a matching in a graph. A graph G is considered H-free if no graph in the set H is an induced subgraph of G. The study focuses on the set H of connected graphs with three or more vertices. The paper provides a complete characterization of H in two scenarios: when all graphs in H are triangle-free, and when H consists of two graphs. It also looks at the case of odd order graphs and defines a near-perfect matching as one where all but one vertex are connected by edges. The paper further characterizes H in cases where every H-free graph of sufficiently large odd order has a near-perfect matching.
41377	41377102	Facility location in sublinear time	This paper presents a randomized approximation algorithm for the metric Minimum Facility Location problem, with uniform costs and demands and the option for every point to open a facility. The algorithm has a running time of O(n log2n), where n is the number of metric space points, making it sublinear with respect to the input size. The authors also show that there is no o(n2)-time algorithm, even a randomized one, that can approximate the optimal solution for this problem. This result extends to other related problems such as minimum-cost matching, bi-chromatic matching, and n/2-median.
41378	4137873	Lifelong learning of human actions with deep neural network self-organization.	Lifelong learning is crucial in autonomous robotics, but current deep neural models for action recognition from videos do not support it. Thus, a self-organizing neural architecture is proposed, which uses growing self-organizing networks with recurrent neurons to process time-varying patterns. The architecture also includes a set of hierarchically arranged recurrent networks for unsupervised learning of action representations. This results in a system that can incrementally process perceptual cues and adapt its responses over time. Experimental results show that the proposed model is competitive with state-of-the-art methods for batch learning, even when sample labels are missing or corrupted. The model also demonstrates the ability to adapt to non-stationary input without catastrophic interference.
41379	4137913	Food Image Recognition Using Pervasive Cloud Computing.	Food image recognition is becoming increasingly important in e-health applications. However, this is a challenging task due to the diverse nature of food and the impact of elements such as color, light, and view angles on food images. In response to this challenge, the use of SIFT and Gabor descriptors as food image features and the KMeans algorithm for feature clustering has been proposed based on empirical and experimental research. To improve the performance of food image recognition, the use of pervasive cloud computing is suggested due to the high computing requirements for a large number of concurrent recognition requests. Evaluations have shown that this approach can achieve acceptable recognition rates, and the use of MapReduce programming has provided significant performance advantages compared to traditional client-server approaches.
41380	413800	Clustering of time-course gene expression profiles using normal mixture models with autoregressive random effects.	Gene expression data, such as yeast cell cycle data, is often expressed periodically over time. However, traditional Fourier series approximations used to cluster this data have been found to be insufficient due to their lack of accounting for the correlation between expression measurements over time and the correlation among gene expression profiles. To address this issue, a new mixture model with autoregressive random effects of the first order has been proposed. This model has been shown to outperform existing models in clustering time-course data, particularly when the gene profiles are highly correlated. The model has been successfully applied to both synthetic and real datasets, producing relevant clusters of co-regulated genes. The developed R package allows for flexibility in specifying the random effects, leading to improved modelling and clustering of time-course data.
41381	4138145	Age-related differences in the initial usability of mobile device icons	Mobile devices have the potential to greatly benefit older adults (age 65+) but they have been slow to adopt them. While research has focused on improving usability for this population, little attention has been given to the difficulty they may have with existing graphical icons. In response to this, a qualitative exploratory study and a follow-up experimental study were conducted to determine which icon characteristics are most helpful for initial icon usability for older adults. The study found that older participants had more difficulty with existing icons, but that icons with semantically close meaning, familiarity, labeling, and concreteness were easier for them to use. These findings can inform designers in creating icons that are better suited for older adults' abilities and technology experience.
41382	4138225	Participatory design with individuals who have amnesia	The article discusses a participatory design process with individuals who have anterograde amnesia, making it difficult for them to form new memories. The design process is described, along with techniques used to support memory during and between design sessions. The authors identify cognitive assumptions of participatory design that do not work when working with individuals with amnesia. They introduce an analytical framework for researchers and practitioners to use when working with people with cognitive impairments. A cognitive deficit unrelated to memory encountered during the process is analyzed using this framework, and an unexpected benefit is highlighted.
41383	4138338	Linear algorithms to recognize interval graphs and test for the consecutive ones property	The paper discusses the consecutive ones property in a matrix, where ones in each column appear consecutively. It presents a data structure to efficiently test for this property and find the desired permutation of rows in linear time. One use of this property is in identifying interval graphs, which are graphs where vertices correspond to intervals on the real line and are adjacent if their intervals intersect. The paper also mentions a previous characterization of interval graphs using the consecutive ones property. To simplify the algorithm, the columns of the matrix are processed in a specific order. This leads to an algorithm for recognizing interval graphs, which can be extended to the general consecutive ones problem. 
41384	4138486	Adaptive maximum margin criterion for image classification	This paper introduces a new method, called the adaptive maximum margin criterion (AMMC), for image classification. While many algorithms have been developed for discriminant analysis, they often treat all training samples equally and overlook the varying impacts of each sample on learning a discriminative feature subspace. The AMMC addresses this issue by assigning different weights to each training sample, taking into account their individual contributions to feature space learning. Additionally, the paper also proposes a semi-supervised version of the AMMC, called the semi-supervised adaptive maximum margin criterion (SAMMC), which incorporates both labeled and unlabeled samples for improved classification performance. The effectiveness of these methods is demonstrated through experimental results. 
41385	4138585	RT-ROS: A real-time ROS architecture on multi-core processors	ROS, an open-source operating system for robots, is popular and rapidly advancing in the robotics community. However, because it runs on Linux, ROS cannot guarantee real-time capabilities, which are necessary for tasks such as robot motion control. This paper introduces a new real-time architecture for ROS called RT-RTOS, designed for multi-core processors. It allows real-time and non-real-time tasks to be executed separately on different cores, providing real-time support for real-time tasks and other functions on Linux. Through experiments with real robot applications on a dual-core processor, it is shown that RT-RTOS effectively enables real-time support for ROS with high performance by utilizing the multi-core architecture.
41386	4138662	Online reranking via ordinal informative concepts for context fusion in concept detection and video search	This paper proposes a new approach, called ordinal reranking, to improve the ranking of search results or detection lists by utilizing co-occurrence patterns of semantic concepts. The approach is based on ranking functions, which are more effective than classification methods in identifying ordinal relationships. It also does not require ad hoc thresholding or offline learning/training data. A concept selection measurement, wc-tf-idf, is also introduced to select relevant concepts for reranking. This approach can be applied to various tasks such as video search and concept detection and has shown significant improvements in performance over existing methods in terms of mean average precision. 
41387	4138753	Quantifying spatio-temporal dependencies in epileptic ECOG	Researchers have found evidence that the mechanisms responsible for epileptic seizures can be better understood by continuously tracking the brain's spatio-temporal mappings. They propose using a SOM-based similarity index measure to quantify these dynamics, which has been shown to be both statistically accurate and computationally faster, making it suitable for real-time analysis. In order to track changes in this measure over time and space, a spectral clustering approach is used. Preliminary analyses on multivariate epileptic ECOG data show that patterns of spatio-temporal dynamics vary from seizure to seizure, suggesting that there may be distinct patterns associated with the transition from inter-ictal to pre-post ictal stages.
41388	4138837	Query Complexity in Expectation.	The article discusses the query complexity of computing a function in expectation, where the algorithm must output a random variable with an expected value equal to the function value using as few input queries as possible. Both the randomized and quantum query complexity are characterized by two polynomial degrees, with the quantum complexity potentially being much smaller for some functions. These complexities are related to the extension complexity of polytopes, which can be upper bounded using quantum query algorithms. An example is given for approximating the slack matrix of the perfect matching polytope. The article concludes by showing that these complexities correspond to the Sherali-Adams and Lasserre hierarchies.
41389	4138963	Algorithmic complexity of protein identification: combinatorics of weighted strings	In this article, we address a problem in computational biology called the ONE-STRING MASS FINDING PROBLEM. This involves finding a substring in a given string with a specific weight, where the weight of a string is the sum of the weights of its individual letters. We aim to find a data structure and query algorithm that can efficiently solve this problem, with a focus on using minimal storage space and query time. We present two algorithms, LOOKUP and INTERVAL, which have subquadratic storage space and sublinear query time. We also discuss other variations of the problem and how our algorithms can be adapted to solve them. Additionally, we explore the combinatorial properties of weighted strings.
41390	4139034	LP-based Approximation Algorithms for Capacitated Facility Location	The capacitated facility location problem with hard capacities involves finding the optimal way to open facilities and assign clients to those facilities in order to minimize costs. This problem is NP-hard and current approximation algorithms are based on local search techniques. However, there is still room for improvement and one approach is to use a linear programming lower bound. The researchers have made progress in this area by presenting a 5-approximation algorithm for a special case where all facility costs are equal. This algorithm involves breaking the problem into smaller single-demand capacitated facility location problems, solving them, and then combining the solutions.
41391	4139124	A Flexible, Extensible Simulation Environment for Testing Real-Time Specifications	MTSim is a versatile simulation platform designed for the Modechart toolset (MT). It allows users to customize and extend their simulations by plugging in their own viewers and injecting events into the execution trace. MTSim also offers features such as monitoring and assertion checking, as well as user-specified handlers for handling assertion violations. The paper also introduces WebSim, a suite of simulation tools for MT, and an application-specific component of MTSim that simulates the cockpit of an F-18 aircraft and responds to user inputs to model a bomb release function. With its flexible and interactive features, MTSim provides users with a comprehensive and customizable simulation experience. 
41392	4139232	A search-based bump-and-refit approach to incremental routing for ECO applications in FPGAs	Incremental physical CAD, also known as engineering change order (ECO) process, is a common approach to making design changes in circuitry. This is typically done late in the design process to correct logical or technological issues. Incremental routing is an important aspect of this methodology, as it allows for changes to be made only to the affected portion of the circuit while minimizing changes to the rest. In this article, a new approach called bump and refit (B&R) is introduced for efficient incremental routing in FPGAs. This method rearranges existing nets within their channels to find valid routes for the new or modified nets without using extra resources or affecting the electrical properties of existing nets. The B&R approach is compared to other incremental routing techniques and found to be significantly faster and more effective, especially for FPGAs with complex switchboxes. However, it may be slower for FPGAs with a different type of switchbox. Overall, B&R offers a promising solution for efficient incremental routing in modern FPGAs.
41393	4139311	CDMA transmission with complex OFDM/OQAM	The article suggests a new method, called OFDM/OQAM, as an alternative to the commonly used MC-CDMA technique for downlink transmission. This new method combines the benefits of both OFDM and CDMA, while also providing a theoretically optimal spectral efficiency. The proposed method uses an orthogonally multiplex quadrature amplitude modulation (OQAM) to achieve perfect reconstruction of complex symbols over a distortion-free channel. The effectiveness of the proposed method is demonstrated through a comparison with conventional MC-CDMA and an OQAM-CDMA combination using realistic channel models. 
41394	4139434	An approach for an AVC to SVC transcoder with temporal scalability	The scalable extension (SVC) of H.264/AVC allows for temporal, spatial, and quality scalability in the encoded bitstream, providing flexibility for various devices and networks. However, this scalability must be implemented at the encoder side, making it difficult for existing H.264/AVC content to benefit from SVC's tools. As the migration from MPEG-2 to H.264/AVC is currently underway, it is unlikely that a switch to SVC will occur soon. Therefore, efficient techniques for converting single-layer content to a scalable format are needed. This paper discusses an approach for temporal scalability transcoding from H.264/AVC to SVC, which reduces coding complexity by 55.75% without compromising coding efficiency.
41395	4139526	RD-Optimization for MPEG-2 to H.264 Transcoding	This paper presents a low-complexity algorithm for inter-frame prediction in an MPEG-2 to H.264 transcoder. The algorithm uses MB information from MPEG-2, including coding modes, coded block patterns, and mean and variance data, to compute an optimal MB coding mode decision with reduced computational complexity. Decision trees are built using data mining algorithms and RD optimized mode decisions, resulting in highly efficient mode decisions. The proposed transcoder is 35% faster than the RD optimized H.264 reference transcoder with minimal PSNR degradation. It also outperforms the SAE cost-based H.264 transcoding by over 3 dB.
41396	4139624	Conceptual Feedback For Semantic Multimedia Indexing	This paper discusses the problem of automatically detecting visual concepts in images or video shots. The current state-of-the-art systems involve feature extraction, classification, and fusion. However, these approaches often compute detection scores independently for each concept. The proposed method, "conceptual feedback," takes into account the relationships between concepts to improve detection performance. The method involves adding a vector of normalized detection scores to the pool of available descriptors, which is then processed and fused with the original detection scores. This feedback can be iterated multiple times and is compatible with using temporal context to improve performance. In an evaluation, the method showed a 15.3% relative improvement in overall performance compared to only using temporal re-scoring.
41397	4139726	Dynamic load balancing without packet reordering	Dynamic load balancing is a technique used by ISPs to prevent network congestion from load spikes or link failures. However, current methods for splitting traffic across multiple paths can cause packet reordering and impact TCP congestion control. The proposed FLARE algorithm splits traffic at the granularity of bursts of packets, avoiding packet reordering and achieving accuracy and responsiveness similar to packet switching. FLARE is easy to implement and requires minimal router state. 
41398	4139860	Energized geocasting model for underwater wireless sensor networks.	In sensor networks, energy is crucial for communication between nodes. The lifespan of the network is affected by the energy available at each node. Researchers have been working on ways to conserve energy, and this paper proposes a new energy efficient technique for underwater sensor networks. It builds upon the RMTG protocol and takes into account the current energy level of nodes to select the best relay node. By preferring shorter paths, the algorithm reduces energy consumption and extends the lifespan of nodes. Simulation results show that this algorithm outperforms the RMTG protocol in terms of network energy, path energy, and the number of dead nodes.
41399	4139949	Combination and Integration in the Perception of Visual-Haptic Compliance Information	Researchers studied how individuals perceive the compliance of materials in a virtual environment through both visual and haptic cues. Two experiments were conducted where subjects had to discriminate between different levels of compliance. The results showed that each modality (vision or haptic) independently produced a compliance estimate, which were then integrated to create an overall value. The integration process was found to be a weighted summation of two random variables, defined by the single modality estimates. This model provided accurate predictions, even if the weights were not optimal. Results also showed that the weights were optimal when vision and haptic inputs were congruent, but not when they were incongruent.
41400	4140064	Familiarity or conceptual priming: event-related potentials in name recognition.	Recent research has focused on the different components of recognition memory, as measured by event-related potentials (ERPs). Recollection is typically accompanied by a late, positive deflection in the parietal region, while an earlier, frontal component has been suggested to reflect familiarity. However, this frontal component, known as the FN400, has also been proposed to be linked to implicit memory, specifically conceptual priming. A study using an episodic memory task with famous and non-famous names found that the FN400 was affected by frequency (how common the names were), while the parietal component was influenced by fame (how well-known the names were). This suggests that the FN400 is associated with familiarity, but not conceptual priming.
41401	4140142	Face Alignment Using Segmentation and a Combined AAM in a PTZ Camera	This paper presents a new approach to face alignment using the Active Appearance Model (AAM) in surveillance systems with PTZ cameras. The AAM typically struggles with face images affected by lighting, background clutter, and camera orientation, so the authors propose a robust fitting method that combines Person-specific and Generic models. Face segmentation is achieved using histogram back-projection and skin color histograms, which are updated with a skin mask from the AAM. This method also improves face recognition by combining Generic and Person-specific models, with minimal impact on processing time. Experimental results show that the proposed method produces accurate face parameters and is not sensitive to initial shapes.
41402	414027	A QoS degradation policy for revenue maximization in fault-tolerant multi-resolution video servers	Video servers, which support multiple clients and multimedia content, are at high risk for disk failures due to their use of large-scale disk arrays. To address this weakness, they often reserve a significant amount of their resources for potential disk failures, resulting in under-utilization during normal operation. This paper proposes a solution to improve resource utilization in these fault-tolerant multi-resolution video servers. By using a method that degrades the quality of service and an admission control algorithm, the system can maximize revenue while guaranteeing a minimum quality level for each client in case of a disk failure. This approach utilizes the multi-resolution property of video streams and achieves graceful degradation at near-optimal levels. Simulation results demonstrate that this technique can significantly increase the number of clients admitted for video service and greatly enhance resource utilization rates.
41403	414030	Measuring technology effects on software change cost	The article presents a methodology for accurately measuring the impact of technology on software change effort. It uses metrics to analyze small changes in software and determine the influence of technology. The methodology is demonstrated through a case study that examines the effect of two specific technologies: a version-sensitive source code editor and a domain-engineered application environment. This approach allows for a more precise understanding of how technology affects software change effort. 
41404	41404126	Convergence and attractivity of memristor-based cellular neural networks with time delays.	This paper discusses the convergence and attractivity of memristor-based cellular neural networks (MCNNs) with time delays. The MCNN is modeled using a differential inclusion with a realistic memristor model. The global solutions of the MCNN are proven to be essentially bounded. The state of the MCNN is shown to converge to a critical-point set in the saturated region of the activation function, if the initial state is also in the saturated region. The convergence time period is finite and can be estimated using given parameters. The paper also proves the positive invariance and attractivity of the state in non-saturated regions. Simulation results of numerical examples are provided to support the theoretical findings.
41405	4140543	A Complete Algorithm For Designing Planar Fixtures Using Modular Components	Modular fixturing systems are commonly used in manufacturing for holding parts in place during machining operations. However, designing the optimal arrangement of the modules can be time-consuming and requires expertise. In this paper, a new algorithm is presented that takes a polygonal description of the part and efficiently generates all possible fixture designs that kinematically constrain the part in the plane. The algorithm is based on part geometry and a graphical force analysis, and is capable of finding the optimal fixture design based on a chosen quality metric. While the algorithm only considers planar forces and motions, it is a crucial step towards solving the larger 3-D fixture design problem. This approach has potential applications for a wide range of manufactured parts.
41406	4140623	Offline recognition of unconstrained handwritten texts using HMMs and statistical language models.	This paper discusses a system for recognizing handwritten text in English without any constraints on the data. The system utilizes Statistical Language Models to enhance its performance. Experiments were conducted using data from single and multiple writers and lexicons of varying sizes (10,000 to 50,000 words). Results showed that using language models can significantly improve the accuracy of the system, with error rates reduced by 50% for single writer data and 25% for multiple writer data. The paper also compares this approach to other methods in the literature and proposes an experimental setup for recognizing unconstrained text. 
41407	4140789	Video indexing and similarity retrieval by largest common subgraph detection using decision trees	The largest common subgraph (LCSG) is a useful measure of similarity between a query and a database of models, but it is time-consuming to calculate. To improve the speed of matching, new algorithms have been developed that use prior knowledge of the database. This paper introduces a new algorithm based on similar principles that greatly reduces the computational complexity of finding the LCSG between a known database and an online query. This makes it more efficient to detect the LCSG in real-time.
41408	4140850	Asymptotically optimal geometric mobile ad-hoc routing	The paper introduces AFR, a new distributed mobile ad-hoc routing algorithm that only requires communication between direct neighbors. The algorithm has a worst-case route cost of &Ogr;(c2), which is a significant improvement compared to previous algorithms. It is the first algorithm with a cost bounded by a function of the optimal route. A lower bound of $Ogr;(c2) is also established, making AFR asymptotically optimal. The paper also presents a non-geometric algorithm with the same lower bound, but it requires some memory at each node. This highlights the trade-off between using geometry and memory for routing.
41409	4140921	Articulated object registration using simulated physical force/moment for 3D human motion tracking	This paper presents a 3D registration algorithm that uses simulated physical force/moment to track human motion. The algorithm works by aligning a 3D model with sparsely reconstructed points from multiple cameras. The displacement between the model and points generates a simulated force/moment that is used in an Iterative Closest Points approach. The algorithm also incorporates human kinematic constraints through a hierarchical scheme for model state updating. Experiments on synthetic and real data show that the method is efficient and robust for tracking unconstrained human motion.
41410	4141035	CI-Graph simultaneous localization and mapping for three-dimensional reconstruction of large and complex environments using a multicamera system	The paper focuses on a new algorithm called CI-Graph SLAM, which uses submapping and graphical methods to efficiently map large environments. This method is faster and more consistent than the classical extended Kalman filter (EKF) solution. The algorithm creates a graph of submaps and a spanning tree with certain properties, allowing for efficient updates and corrections. It was tested in both synthetic and real-world environments, where it outperformed other techniques for loop closure detection and submap estimation. The paper concludes that CI-Graph SLAM is a valuable approach for SLAM, with potential applications in various fields such as robotics and autonomous vehicles. 
41411	4141158	User Guidance Of Resource-Adaptive Systems	This paper introduces a framework for designing resource-adaptive software systems for small mobile devices. The framework allows users to control tradeoffs between different aspects of quality of service. The paper presents a model for capturing user preferences and prototype interfaces for eliciting these preferences. It also describes the integration of the framework into an existing software infrastructure for ubiquitous computing. The main research question addressed is whether it is feasible to coordinate resource allocation and adaptation policies in a way that users can understand and control in real time. The paper includes an evaluation of both systems and usability, including a user study. The contributions of this work include design guidelines, APIs for integrating new applications, and a way to model quality of service tradeoffs using utility theory that can be easily used by users with diverse backgrounds. 
41412	4141253	Evaluating The Effects Of Land-Use Development Policies On Ex-Urban Forest Cover: An Integrated Agent-Based Gis Approach	The DEED model, an agent-based model using spatial data, was used to evaluate the impact of lot-size zoning and municipal land-acquisition strategies on forest-cover outcomes in Scio Township, Michigan. The model incorporated agent characteristics, behavior, and landscape aesthetics data from surveys and spatial analyses. Results showed that large lot-size zoning policies lead to sprawl and increased forest cover, with municipal land acquisition having a stronger effect on forest conservation. The location strategy for land acquisition was found to be more effective at increasing forest levels than zoning policies alone. Using this integrated GIS and ABM approach provided valuable insights into the long-term effects of land-use development policies on forest cover.
41413	4141352	Ambient Assisted Living [Guest editors' introduction]	Ambient assisted living (AAL) is the use of technology in a person's daily living and working environment to help them maintain an active and independent lifestyle as they age. The AAL community focuses on human activity recognition and behavior understanding, which involves detecting and recognizing actions and situations within an environment. AAL systems not only observe but also interact with users through prompts and haptic responses. Context awareness and predictive capability are important elements in providing appropriate responses to user needs. This involves using sensors to gather information about the current situation and anticipating future needs. AAL research plays a crucial role in creating technology that can improve the lives of older adults and keep them connected to society.
41414	4141455	An over-segmentation method for single-touching Chinese handwriting with learning-based filtering	The segmentation of touching characters in Chinese handwriting is a difficult task for offline recognition. To address this issue, a new over-segmentation method is proposed in this paper. This method uses geometric features and a learning-based filter to identify potential cuts in the handwriting strokes. The filter is trained to remove unlikely cuts, improving the accuracy of the segmentation. Additionally, this method can handle strings with more than two characters. Tests on two large databases show that this method is effective for both character segmentation and text line recognition in single-touching Chinese handwriting.
41415	4141512	A system for supporting group learning that enhances interactions	This paper discusses a system that supports group learning and encourages interactions among learners. Previous systems for collaborative learning have shown limitations in promoting interactions and discussions. To address this, the proposed system incorporates a board game and computer simulation to externalize each learner's ideas and facilitate collaboration. The system was tested in a public elementary school and results showed that it effectively enhanced interactions, discussions, and learner engagement. The aim of the system is to promote mutual learning through active participation and sharing of ideas among learners.
41416	4141692	Program Graph Structuring for Execution in Dynamic SMP Clusters Using Moldable Tasks	The paper discusses task scheduling in dynamic SMP clusters using moldable computational tasks. These tasks are used as the atomic elements in scheduling algorithms, with a guarantee of schedule length. The shared memory system architecture utilizes dynamic processor clusters and fast data transfers within these clusters. The clusters are implemented using system on chip (SoC) modules connected by a central global network. The proposed task scheduling algorithm for program macro dataflow graphs involves identifying moldable tasks and scheduling them using a 2-phase algorithm. The algorithm has been implemented and tested with simulated program graphs.
41417	4141748	High-Data-Rate DPC-OF/TDMA Based on Multi-Layer STBC Coded MIMO-OFDM	The authors discuss their work on developing a high-speed DPC-OF/TDMA system that can reach data rates beyond 100 Mbps. They introduce a new multi-layer STBC OFDM scheme that combines spatial diversity and spatial multiplexing to improve performance. Simulations are conducted under different MIMO antenna configurations, with a focus on achieving a packet error rate of 1.0e-2 with an Eb/No of 20.0dB. The results show that the evolved DPC-OF/TDMA can achieve a data rate of 921.6Mbps with an average spectral efficiency of 7.2bit/s/Hz using a bandwidth of 128MHz. This demonstrates the potential of the system for high-speed data transmission.
41418	41418168	On the Context-aware, Personalized Delivery of Process Information: Viewpoints, Problems, and Requirements	Enterprises are facing a growing amount of data, which makes it challenging to provide employees and decision-makers with the necessary information. This is especially difficult when it comes to delivering both structured and unstructured information based on the current context and user. Examples of unstructured information include office documents and emails, while structured information includes business process models and data from enterprise information systems. To address this issue, a paper presents findings from three studies in the automotive and healthcare industries, as well as an online survey. The first step is identifying problems with process-oriented information management, and the second step is determining requirements for effectively handling process information.
41419	414194	Multi-perspective Anomaly Detection in Business Process Execution Events.	Anomaly detection is crucial for preventing fraud and security breaches in process model executions. Current approaches focus on control flow and point anomalies but struggle with false positives for unexpected events. This paper proposes a new approach that considers time and resources to detect contextual anomalies. It also accounts for unexpected events by using likelihood of occurrence to reduce false positives. The approach analyzes multiple events together to detect collective anomalies. A prototype implementation and real-life process execution logs from various domains were used to evaluate the effectiveness and practicality of the approach. This approach shows promise in effectively detecting anomalies in process model executions.
41420	4142041	Policy improvement by a model-free Dyna architecture.	This paper proposes a new approach to accelerate policy improvement in reinforcement learning. The approach combines two learning schemes, one using temporal difference method and the other using relative values, to improve the learning process. Instead of creating a complex world model, a simple predictor of average rewards is introduced to the actor-critic architecture in simulation mode. The proposed method is tested on a pendulum system and compared to other methods in a labyrinth exploration experiment, showing faster convergence and better performance. This demonstrates the adaptability and robustness of the proposed method in handling unknown dynamics and achieving desired outputs.
41421	4142154	Automata Evaluation and Text Search Protocols with Simulation Based Security.	This paper discusses efficient protocols for solving two problems: pattern matching and secure oblivious automata evaluation. The first problem involves two parties, where one holds a pattern and the other holds a text, and the goal is to learn where the pattern appears in the text without revealing it or learning anything else about the text. The paper presents several protocols for different levels of security and also generalizes one solution to solve related problems. The second problem involves one party holding an automaton and the other holding an input string, and they need to determine if the automaton accepts the input without learning anything else. The paper presents a novel protocol for this problem that achieves full security against malicious adversaries.
41422	4142291	Agent interaction in distributed POMDPs and its implications on complexity	Effective coordination is crucial for agents to achieve their goals in a multi-agent system. Researchers have used decision theory to model the coordination problem, but the most general models are very complex to solve optimally. Some more limited models are easier to solve, but it is not clear why this is the case. This study aims to fill this gap by identifying a condition that distinguishes between problems that are in the class NP and those that are harder. This condition relates to the amount of information each agent has about the others and whether it can be represented succinctly. Problems that meet this condition are shown to be NP-complete. This concept is illustrated with two interaction protocols, and for problems that do not meet the condition, an NP approximation can be generated using the theoretical results.
41423	4142316	A novel local surface feature for 3D object recognition under clutter and occlusion.	The paper introduces a novel surface feature called the TriSI feature, designed for recognizing 3D objects in cluttered and occluded environments. The feature is constructed by creating a unique Local Reference Frame (LRF) for each feature point using neighboring triangular faces, and generating three signatures from the LRF's orthogonal axes. These signatures are then combined into a compressed TriSI feature. The paper also proposes a hierarchical feature matching algorithm for 3D object recognition using the TriSI feature. Experimental results on popular datasets demonstrate the effectiveness of the TriSI feature in various challenging conditions, and the proposed algorithm achieves the best overall recognition results on four standard datasets. 
41424	4142428	Automatic ink mismatch detection for forensic document analysis	Handwritten document examination often involves determining if any part of the text has been altered or forged with a different pen. This study introduces the use of hyperspectral imaging to detect ink mismatches in handwritten notes. A new technique for selecting informative bands from hyperspectral images is proposed and implemented in an end-to-end camera-based document imaging system. A database of handwritten notes was collected and made publicly available for this purpose. The proposed method improves accuracy by up to 15% compared to using all bands. This study provides solutions for challenges in camera-based hyperspectral document imaging and demonstrates the effectiveness of the proposed method through extensive experiments.
41425	4142545	Mapping Trigger Conditions onto Trigger Units during Post-silicon Validation and Debugging	On-chip trigger units are used for identifying important events during post-silicon validation and debugging. However, their implementation limits the types of events that can be programmed for detection during runtime. This means that some events that were not considered during design cannot be detected due to hardware constraints. To solve this problem, we propose adding architectural features to the trigger units and using an algorithmic approach to automatically map trigger conditions onto the units. This will allow for more flexibility in detecting events of interest during post-silicon validation and debugging. 
41426	414261	Injective optimal realizations of finite metric spaces	A finite metric space (X,d) can be represented by a weighted graph (G,w) with minimal total edge weight, called an optimal realization. Such realizations have applications in fields like phylogenetics, psychology, and compression software. It was previously believed that any map from the vertex set of G to the "tight span" of d must be injective, but this has been disproven. However, this paper shows that certain optimal realizations always have injective maps from their geometric realization to the tight span. These optimal realizations can be constructed from non-injective ones and may lead to new methods for computing optimal realizations from tight spans.
41427	41427137	Experiment for Using Web Information to do Query and Document Expansion.	The ImageCLEF photo task for this year has made some changes compared to previous years. The caption field in image annotations and the narrative field in text queries have been removed, along with the visual queries in the image collection. This means that there is less information available for matching query words and annotations, making it more challenging. To address this, the researchers have explored the web to expand the queries and documents. This has resulted in a 16.11% improvement in performance, but there is also an increase in noise. The media mapping method used in previous years has been applied to expand queries, and it has shown to be effective in the new task. However, document expansion has caused a 28.24% decrease in performance due to the excess noise. 
41428	4142812	Multi-dimensional regression analysis of time-series data streams	This paper discusses the challenge of performing on-line, multi-dimensional analysis and data mining of stream data in real-time production systems and dynamic environments. Due to the potentially infinite amount of data, traditional methods of storing and scanning are not feasible. The paper proposes a compressed regression approach and a partially materialized data cube model to minimize the memory and storage requirements. An exception-guided drilling approach is also developed for efficient exception-based analysis. Algorithms are proposed and compared for analyzing time-series data streams. The study concludes with the identification of the most efficient algorithm for multi-dimensional stream data analysis.
41429	4142929	Scalable Incremental Test-case Generation from Large Behavior Models	Model-based testing is a widely used method for automating the generation and execution of tests while achieving a desired level of coverage. However, applying this technology to large and complex systems can be challenging due to the state-space explosion caused by the size of specification models. This paper introduces a new approach for test-case generation that addresses this complexity by using a synchronous specification language and supporting incremental test-case generation through compositional modeling. The effectiveness of this approach is demonstrated through two industrial case studies involving an electronic control unit and a railway interlocking system. The scalability of the approach is also evaluated through a series of increasingly complex test models.
41430	4143034	A Formal Semantics of Data Flow Diagrams	This paper presents a formal semantics of data flow diagrams used in Structured Analysis, using an abstract model for data flow transformations. The semantics are comprised of VDM functions that transform a data flow diagram into a VDM specification. This allows for both a graphical view through the diagram and a textual view through the specification. The paper focuses on the reasoning behind the transformation choices and provides annotated VDM functions and examples to describe the transformation process. The ultimate goal is to provide software analysts and designers with a comprehensive understanding of the system being modeled.
41431	4143122	A parallel algorithm for zero skew clock tree routing	This paper discusses clock skew, a major factor in determining system performance in deep sub-micron fabrication technology. Previous methods for zero skew clock tree routing assumed uniform wire sizes and did not produce exact zero skew. The paper proposes an iterative algorithm for exact zero skew wire-sizing, which was shown to significantly reduce source sink delay compared to uniform wire sizes. To improve efficiency, a parallel algorithm using a cluster-based clock tree construction algorithm and the zero skew wire-sizing algorithm was also proposed, resulting in an average speedup of 7.8 on an 8-processor server without sacrificing solution quality.
41432	414329	Design, construction, and application of a generic visual language generation environment	The development of visual programming languages (VPLs) and their supporting environments can be time-consuming and tedious. To simplify this process, researchers have created high-level tools, but none can fully replicate the ease of creating textual languages with lex/yacc tools. This paper introduces VisPro, a generic visual language generation environment, which improves upon the conventional model-view-controller framework by decoupling its functional modules for independent development and integration. VisPro consists of visual programming tools that divide the VPL construction process into lexicon definition and grammar specification. The compiler for the VPL is automatically generated based on the grammar specification, creating a programming environment with a visual editor. The paper showcases VisPro's capabilities by creating a simple visual language and a more complex visual modeling language for distributed programming.
41433	4143379	BinProlog: a Continuation Passing Style Prolog Engine	BinProlog is a Prolog system that is highly efficient, compact, and can be easily used on different platforms. It uses a unique method called source-level transformation to convert clauses into binary continuation passing, making it completely free of side effects. The compiler and engine are written in Prolog and use a simplified version of the WAM, which makes it faster and more optimized for executing binary logic programs. BinProlog outperforms other systems that use the full WAM due to its optimized performance. In this article, the author provides a brief explanation of the system, its features, and presents performance data to support its efficiency.
41434	414344	RHENE: A Case Retrieval System for Hemodialysis Cases with Dynamically Monitored Parameters	The paper introduces RHENE, a case-based retrieval system for patients with nephropatologies undergoing hemodialysis. Retrieval of similar cases involves considering both static and dynamic features, and RHENE uses a multi-step procedure for this. The first step involves grouping and classification based on static features to reduce the search space. The second step involves retrieving locally similar cases based on time-dependent features, and computing a weighted average of their distances for a global similarity measure. The paper focuses on implementing this step efficiently using a dimensionality reduction technique called Discrete Fourier Transform and index structures like k-d trees. This allows for efficient retrieval of similar cases, allowing physicians to assess the quality of hemodialysis services.
41435	4143514	Integrating Rule-Based and Case-Based Decision Making in Diabetic Patient Management	The integration of rule-based and case-based reasoning is valuable in medical applications, as it allows for the utilization of both general rules and specific patient cases. This paper presents a decision support tool for managing Insulin Dependent Diabetes Mellitus that combines these two methods. The system aims to provide physicians with a flexible solution for therapy planning by utilizing the strengths of both approaches. The integration is done without favoring one method over the other, but instead leveraging their complementary nature. Rules are used to make suggestions based on structured knowledge, while case-based reasoning is used to adapt the rules to the specific patient's characteristics and experiences. The system has been tested on simulated diabetic patient cases and will be integrated into the T-IDDM architecture.
41436	4143699	Three-way decisions based on decision-theoretic rough sets with dual hesitant fuzzy information.	Decision-theoretic rough sets (DTRSs) are a popular model for making three-way decisions in the field of risk decision-making. However, estimating the loss function of DTRSs in complex and uncertain environments is a challenge. Dual hesitant fuzzy sets (DHFSs) offer a new way to handle uncertain information and can be used to determine the loss function of DTRSs. In this study, the hesitant format of DHFSs is integrated into DTRSs to create a new three-way decision model. This model takes into account the loss functions of DTRSs with dual hesitant fuzzy elements (DHFEs) and includes two methods for deriving three-way decisions: one based on scores and accuracies of DHFEs, and the other using a ranking method with a stochastic strategy. A case study on emergency blood transshipment is used to demonstrate and compare the effectiveness of these methods.
41437	4143720	A Framework for Adaptive Wavelet Prediction in Self-Sizing Networks	This paper proposes a traffic predictor using multiresolution decomposition for adaptive bandwidth control in locally controlled self-sizing networks. These networks can automatically allocate link and switch capacity based on real-time traffic data, providing packet-level quality of service. The study shows that using a wavelet-based method is more effective than other popular methods, such as the Gaussian predictor, for this type of network. The Haar wavelet is found to be the most suitable for traffic prediction, and the impact of other wavelet parameters is also examined. A novel adaptive wavelet predictor is also introduced, which can adjust to sudden changes in incoming traffic.
41438	4143880	Deep Convolutional Neural Networks for Large-scale Speech Tasks.	This paper discusses the use of Convolutional Neural Networks (CNNs) for large vocabulary continuous speech recognition (LVCSR) tasks. CNNs are a type of neural network that can effectively handle spectral variations and correlations in signals, making them a suitable model for speech. The authors investigate the optimal architecture for CNNs in LVCSR tasks, including the number of convolutional layers, hidden units, and pooling strategy. They also explore ways to incorporate speaker-adapted features into the CNN framework, and propose a strategy for using ReLU+dropout during Hessian-free sequence training. Experiments on 3 LVCSR tasks show that the proposed CNN model with speaker-adapted and ReLU+dropout techniques achieves state-of-the-art results with a 12%–14% relative improvement in word error rate (WER) compared to a strong DNN system.
41439	414394	Handling continuous attributes in Ant Colony Classification algorithms	Real-world classification problems often involve both continuous and nominal attributes. However, many Ant Colony Optimisation (ACO) classification algorithms can only handle nominal attributes directly. This paper proposes two new methods to address this limitation. The first method allows for more flexible representation of continuous attribute intervals, while the second method considers attribute interaction, which is not accounted for in previous approaches. Empirical evaluation on eight datasets shows that these methods lead to more accurate classification models. The paper also provides a brief overview of Ant-Miner and cAnt-Miner, as well as discussing the proposed methods and presenting results. The paper concludes with future research directions.
41440	4144063	Task allocation to actors in wireless sensor actor networks: an energy and time aware technique	Task allocation is an important aspect of cooperative applications in embedded systems, such as wireless sensor and actor networks (WSANs), which have constraints on latency and energy usage. Existing algorithms for task allocation focus mainly on energy savings and do not consider time constraints, leading to longer completion times for tasks and a higher chance of network malfunction. This paper proposes a two-phase task allocation technique that takes into account both energy efficiency and reduction of completion times for tasks in WSANs. The first phase assigns tasks equally to actors to measure their capabilities, and in the second phase, tasks are allocated based on these capabilities to minimize completion times. Simulation results show a 45% improvement in task completion times compared to the commonly used opportunistic load balancing algorithm. This technique provides a better balance between load balancing and completion times in WSANs compared to opportunistic load balancing.
41441	4144119	Autonomic specification of self-protection for distributed MARF with ASSL	The paper discusses the successful implementation of formal specifications and code generation for the Autonomic Distributed Modular Audio Recognition Framework (AD-MARF) system. Using the Autonomic System Specification Language (ASSL), a self-protecting mechanism was designed and specified for DMARF. The ultimate aim is to have an autonomic computing layer for DMARF by incorporating autonomic properties at every stage of pattern recognition. Results are presented that complement previous work on self-healing and self-optimization properties. This achievement marks a significant step towards creating a fully autonomous and self-managing audio recognition system. 
41442	4144251	Privacy streamliner: a two-stage approach to improving algorithm efficiency	The paper discusses the challenges faced by data owners in balancing privacy preservation, utility optimization, and algorithm efficiency when releasing data with sensitive information. The authors propose a new approach, called privacy streamliner, to improve algorithm efficiency by separating the processes of privacy preservation and utility optimization. This is achieved by identifying a set of privacy-preserving solutions that do not reveal information to adversaries and optimizing utility within this set. The proposed approach is applied to micro-data release using publicly known generalization algorithms, and the results show improved efficiency compared to existing solutions. 
41443	4144334	Minimum factorization agreement of spliced ESTs	The paper discusses the importance of producing spliced EST sequences in understanding alternative splicing. Given an EST sequence, there may be multiple spliced EST sequences associated with it due to different alignments against the genome. The paper introduces the Minimum Factorization Agreement (MFA) problem, which aims to extract a subset of spliced EST sequences that agree on a common alignment region with the genome or gene structure. The paper presents algorithms for solving the MFA problem, which have been shown to efficiently find the correct spliced EST associated with an EST even with rough alignment. The MFA method also has potential applications in producing and analyzing spliced EST libraries based on biological criteria. 
41444	4144411	Comparison of tree-child phylogenetic networks.	Phylogenetic networks are a type of representation used to study the evolutionary relationships between species. They are a more generalized version of phylogenetic trees and can account for non-tree-like events such as recombination, hybridization, and lateral gene transfer. While there have been attempts to develop a well-founded distance measure for these networks, it has been largely unsuccessful except for a specific type called regular networks. In this paper, a new class of phylogenetic networks called tree-child phylogenetic networks is introduced and a distance measure is defined using a representation called path multiplicity vectors. Algorithms for reconstructing, computing distance, and aligning these networks have also been developed and can be accessed through online tools. 
41445	4144569	A Named Entity Recognition Method Based on Decomposition and Concatenation of Word Chunks	Our method for named entity (NE) recognition involves decomposing and combining word chunks, rather than using word sequences. This allows us to access features such as the first and last words of a chunk, which are not available in sequence-based methods. However, chunks may contain multiple NEs, so we use operators like SHIFT, POP, JOIN, and REDUCE to properly assign NE labels. We tested our method on a dataset of 200,000 annotations from 8,500 news articles in Japanese, and found that it is faster than other methods while maintaining high accuracy.
41446	4144612	Detecting Curved Symmetric Parts Using a Deformable Disc Model	Symmetry is a valuable tool used by researchers in human and computer vision to identify part structure in images without prior knowledge of the scene. This is achieved by using the concept of a medial axis, which is the center of maximal inscribed discs that form a symmetric part. A framework proposed by LEV09 involves using a multiscale super pixel segmentation to generate a sequence of deformable maximal inscribed disc hypotheses. However, this framework has limitations in capturing a wider range of symmetric parts. To overcome this, the authors introduce a global cost that combines pairwise and higher-level smoothing terms, which is then minimized using dynamic programming. This new framework outperforms the previous one on two datasets. 
41447	4144716	Registration of Range Images Based on Segmented Data	A new method for registering range images has been developed, which utilizes the results of the segmentation process. The first set of points for registration is obtained from the first range image, while the second set is obtained by projecting the first set onto geometric parametric models from the second range image. The transformation between the two sets is then computed, resulting in a more precise registration compared to traditional methods. 
41448	41448120	Slepian-Wolf Coding for Nonuniform Sources Using Turbo Codes	This paper discusses the efficiency and optimality of the recently proposed turbo-binning scheme for the uniform source Slepian-Wolf coding problem. It examines the case of nonuniformly distributed sources and finds that any algebraic binning scheme based on linear codes is only optimal for nonuniform sources in an asymptotic sense. The paper proposes two modifications to enhance the performance of the turbo-binning scheme for nonuniform sources: carefully designing the constituent encoder structures to match the turbo code to the source distribution, and using variable-length syndrome sequences to index the bins. Simulations demonstrate that these strategies can result in a significant improvement in compression rate for highly nonuniform sources.
41449	41449102	Password-Authenticated Key Exchange between Clients with Different Passwords	The paper introduces a new framework called Client-to-Client Password-Authenticated Key Exchange (C2CPAKE) which allows for a secure end-to-end channel between clients. This is different from existing schemes which rely on a pre-shared password between a client and server. The proposed framework uses two different passwords from each client and does not require any pre-shared secret. The paper defines security notions and types of attacks for this new framework and proves that their scheme is secure against all types of attacks. Two secure C2C-PAKE schemes are presented, one for a cross-realm setting and another for a single-server setting. 
41450	4145035	An Improved Method of Multiplication on Certain Elliptic Curves	The Frobenius endomorphism is a tool that can greatly improve the efficiency of multiplication on certain elliptic curves. This note introduces a new method to reduce the length of the Frobenius expansion in order to further optimize its use. This is an improvement on previous work by Solinas and M眉ller. Experimental results are also provided and compared to standard curves, showing the improved time-performance of multiplication on these optimized elliptic curves over small finite fields. 
41451	4145176	Learning a model of facial shape and expression from 4D scans.	The field of 3D face modeling has a gap between high-end and low-end methods. High-end methods offer realistic facial animation but require manual work, while low-end methods use less expressive models. A new model, FLAME, aims to bridge this gap by learning from accurately aligned 3D scans and incorporating articulated jaw, neck, and eyeball movements, pose-dependent blendshapes, and global expression blendshapes. The model is trained from over 33,000 scans and is more accurate and expressive than other existing models. FLAME can be easily fit to data and is available for research purposes. 
41452	4145215	Modeling human locomotion with topologically constrained latent variable models	This paper discusses the use of activity-specific motion models for human pose and motion estimation. While these models simplify monocular tracking, they do not address learning models for multiple activities or stylistic variations, or how to combine them with natural transitions between activities. The paper presents a new approach, called the locally-linear Gaussian process latent variable model (LL-GPLVM), which uses a prior to preserve local structure in the training data. The computational complexity of the Gaussian process latent variable model (GP-LVM) is reduced by incorporating sparsification and dynamics within the LL-GPLVM. This allows for the learning of smooth latent models for different activities within a shared latent space, including specific topologies and transitions between activities.
41453	41453171	Groups, fixed sets, symmetries, and invariants	The paper discusses a method for analyzing symmetries in two-dimensional shapes that can be seen from any viewpoint, including those with skewed symmetries caused by perspective distortions. This approach differs from previous methods that only consider affine skewing. The key is identifying structures that remain unchanged under symmetries, creating subgroups of projective transformations with simpler invariants. These invariants make it easier to detect and verify symmetries in a more specific manner.
41454	4145411	Integrated Feature Selection and Parameter Optimization for Evolving Spiking Neural Networks Using Quantum Inspired Particle Swarm Optimization	This paper introduces a new technique, Quantum-inspired Particle Swarm Optimization (QiPSO), for optimizing features and parameters in the Evolving Spiking Neural Network (ESNN). QiPSO uses binary structures to represent information and simultaneously optimizes both the ESNN parameters and relevant features using a wrapper approach. The effectiveness of this method is demonstrated on a synthetic dataset, showing that it can efficiently find the best combination of ESNN parameters and identify the most relevant features. This study presents a unique approach to improving the performance of ESNN and highlights the potential of incorporating quantum-inspired techniques in neural network optimization.
41455	41455319	Statistical Methods for Estimation of Direct and Differential Kinematics of the Vocal Tract.	This article discusses two statistical methods, Artificial Neural Networks and Locally-Weighted Regression, for estimating the kinematic relationships of the speech production system. These methods are necessary for understanding the motor system involved in speech production and can help in areas such as determining speech production goals and acoustic-to-articulatory inversion. The models are optimized and evaluated using synthetic speech data, showing high accuracy in estimating both direct and differential kinematics. Locally-Weighted Regression performs the best overall and requires only a modest amount of training data. The methods are also applied to real-time MRI data with consistent results. Overall, these methods show promise for accurately estimating kinematics in speech production.
41456	4145645	Computing vocal entrainment: A signal-derived PCA-based quantification scheme with application to affect analysis in married couple interactions	Entrainment is a natural occurrence in human-human interactions where both parties adapt their behaviors to each other. This has been studied in psychology for many years and can be described quantitatively to aid in understanding communication, especially in mental health. However, measuring entrainment is challenging due to its subtle nature. In this paper, a new unsupervised method using vocal characteristics is proposed to quantify vocal entrainment. This method was tested on interactions between distressed married couples and showed promising results in accurately differentiating between positive and negative affect. These findings suggest that entrainment plays a crucial role in affective processes during interpersonal interactions.
41457	4145736	Security constructs for regulatory-compliant storage	The storage community has responded to the increasing number of electronic records legislation by enhancing data storage to include privacy, auditability, and a "chain-of-custody." This has been aided by the development of compliance platforms by storage vendors, such as EMC Centera Compliance Edition,™ NetApp SnapLock,™ and IBM Tivoli Security Compliance Manage.™ These platforms add storage management policies to existing systems, which aid organizations in meeting compliance guidelines. However, these features do not provide strong evidence of compliance and can be easily subverted by the file system owner. To address this issue, the use of cryptographic techniques such as digital audit trails, fine-grained secure deletion, and per-block authenticated encryption can provide irrefutable evidence of compliance with regulations. These techniques have been implemented in the ext3cow file system and have been found to have a minimal performance impact.
41458	4145825	Enumeration and classification of benzenoid systems. 32. Normal perifusenes with two internal vertices	A normal perifusene with two internal vertices belongs to the class PF2, which includes systems made of pyrene and annealated catafusenes. The mathematical solution for the numbers of these systems, based on the number of hexagons, is found using combinatorial summation. A similar class, PF2', includes peribenzenoids with n(i) = 2 such as pyrene and annealated catabenzenoids. Recent computerized enumerations and extensions to higher h values are discussed. Another class, PF2*, is defined as normal perihelicenes with n(i) = 2 and their numbers for given h values are reported. These were obtained by subtracting PF2' from PF2. An analytical method, called combinatorial enumeration, is used to reproduce some of the lowest numbers without computers.
41459	4145939	A novel framework for segmentation of deep brain structures based on Markov dependence tree.	The goal of this study is to create a new method for accurately segmenting multiple deep brain structures in medical brain images. This is a challenging task due to the small size and variable shapes of these structures, as well as blurry boundaries and irrelevant background edges. The proposed method uses a template-based approach that combines edge features, region statistics, and inter-structure constraints to detect and locate the target structures without the need for manual initialization. The multi-object template is organized in a hierarchical Markov dependence tree and optimized using a top-to-down strategy. The final segmentation is refined through non-rigid registration between an exemplar image and the target image. Only one training example is needed for this approach, and it has been successfully tested on a publicly available database with expert-segmented brain structures. The results show promising accuracy, with an average Dice score of 0.80 for the caudate nuclei, 0.81 for the putamina, and 0.84 for the thalami.
41460	4146089	Piecewise planar super-resolution for 3D scene	This paper discusses a method for improving the resolution of 3D scenes using multiple low-resolution images. The main challenge is accurately aligning the images due to differences in depth and uncalibrated cameras. The proposed solution involves estimating the depth map for the images and treating registration as a planar segmentation problem. This allows for accurate transformation of each plane in the images. Additionally, the method addresses occlusion by using depth information. The authors also suggest combining multi-view and single view reconstruction for better results. This is achieved by using dictionary learning to incorporate high frequency information from multi-view reconstruction. Experimental results demonstrate the effectiveness of this approach.
41461	41461114	Space efficient signature schemes from the RSA assumption	The RSA assumption is a crucial aspect of signature schemes, as it provides highly reliable security. However, there are only a few known digital signature schemes based on this assumption. This paper proposes several new signature schemes that aim to improve efficiency. The first scheme offers the shortest signatures and public key length compared to existing schemes, but has heavy signing and verification algorithms. The second scheme has a longer public key but more efficient signing and verification costs. The third scheme has even more efficient signing and verification algorithms, but the signature size is longer. These schemes are all based on a new observation about the relationship between m-time signature schemes and short signature schemes.
41462	4146240	An adaptive file-system-oriented FTL mechanism for flash-memory storage systems	The popularity of flash memory has raised concerns about its performance degradation due to its unique characteristics. To address this issue, a new file-system-oriented flash translation layer is proposed. This layer includes a filter mechanism that separates access requests for file-system metadata and file contents, improving performance. A recovery scheme is also proposed to maintain file system integrity. The layer is implemented as a Linux device driver and evaluated with ext2 and ext3 file systems. Realistic traces are also used to test the layer with NTFS. Results show significant performance improvement with minimal system overhead when compared to traditional file systems. 
41463	4146349	Efficient identification of hot data for flash memory storage systems	This research focuses on the impact of identifying hot data in flash memory storage systems. It proposes a method that utilizes multiple independent hash functions to accurately identify hot data with limited space requirements. This reduces the chance of false identification and improves the performance of flash memory access. The research also includes an efficient implementation and an analysis of the chances of false identification. Experiments were conducted to validate the effectiveness of the proposed method and positive results were obtained.
41464	414640	Inferring persistent interdomain congestion.	There is a lot of interest in the technical and policy communities about persistent congestion between different networks and how it affects consumers. To provide evidence for these discussions, a system and method was developed to measure congestion on thousands of interdomain links without direct access. This system, called Time Series Latency Probes (TSLP), identifies links with recurring congestion and was deployed at 86 locations globally. The results showed a correlation with other metrics of interconnection performance impairment. A study was conducted on interdomain links of eight major U.S. broadband providers and no widespread congestion was found, although some links did show recurring congestion patterns. The limitations and potential for third-party monitoring of internet interconnection are also discussed.
41465	4146563	Small-delay-fault ATPG with waveform accuracy	Small-delay faults in circuits are traditionally detected by sensitizing transitions on a path from input to output that goes through the fault site. This can result in incorrect classifications of undetected or falsely detected faults. To address this issue, an automatic test pattern generation algorithm called WaveSAT has been developed. It considers waveforms and gate delays, filtering out small glitches. The algorithm is based on an optimized encoding of the test generation problem using Boolean satisfiability and has been successfully applied to various circuits. WaveSAT is able to detect testable faults and also generate a formal proof for undetectable faults, making it the first scalable and complete algorithm for this purpose.
41466	4146618	Minimal counterexamples for linear-time probabilistic verification.	Counterexamples for property violations play a crucial role in debugging faulty systems and verifying complex systems through counterexample-guided abstraction refinement. This study suggests the use of minimal critical subsystems of discrete-time Markov chains and Markov decision processes as counterexamples for violated ω-regular properties. The minimality of these subsystems is measured by the number of states or transitions, a task that is known to be NP-complete for Markov decision processes. The authors propose a method for computing such subsystems using mixed integer linear programming and demonstrate its effectiveness through experiments. The results indicate that this approach produces significantly smaller counterexamples compared to existing techniques. 
41467	414677	A new feature extractor invariant to intensity, rotation, and scaling of color images	This paper introduces a new approach for extracting invariant features from images using principal component analysis and a competitive learning algorithm. It can be applied to binary, gray-level, or colored-texture images larger than 256x256 pixels. The method can extract features that are invariant to translation, scaling, rotation, and color intensity. In experiments, this technique successfully differentiated images with the same shape but different colored textures. The results showed high recognition accuracy and efficient computational time compared to traditional methods. 
41468	4146838	Statistical tools flavor side-channel collision attacks	Collision attacks exploit the similarity of side-channel leakages to bypass common side-channel distinguishers like correlation power analysis and mutual information analysis. These attacks compare two selective observations, making them similar to simple power analysis attacks. A multi-query collision attack presented at CHES 2010 can detect multiple collisions by comparing leakage averages, but it only works if the leakages and processed intermediate values are related. To improve upon this, the authors propose using higher-order statistical moments and probability density functions to detect collisions, instead of just evaluating means. This approach is supported by practical evidence from four case studies involving FPGA-based masked hardware and a software implementation using boolean masking. 
41469	414696	IT Security in Lübeck – The design of a modern and future-proof security curriculum	The University of Lubeck has launched a new degree program in IT Security in response to the growing demand for secure and reliable systems. This program was established in 2016 due to the high demand for trained security professionals both nationally and internationally. The university's strong focus on security and dependability research also played a role in the creation of this program. The main goal of the program is to train computer scientists who possess both practical skills and theoretical knowledge to build and protect secure IT systems, following the security by design principle on a large scale. 
41470	4147096	Privacy preserving payments on computational RFID devices with application in intelligent transportation systems	Electronic cash is a method of payment that protects the user's identity, making it suitable for systems like public transportation. One specific scheme, developed by Brands, is efficient during the spending phase and uses public-key cryptography. However, the payment devices used in these systems must be affordable and energy-efficient, which can lead to slower transaction times. In this study, researchers demonstrate that with advanced implementation techniques, it is possible to use full-size e-cash schemes on inexpensive payment devices. They successfully implemented Brands' scheme on a computational RFID-token, with the spending protocol taking only 13 milliseconds to execute. However, reloading the card, which is done offline, can be time-consuming and solutions to this issue are discussed. 
41471	4147141	A scalable and programmable simplicial CNN digital pixel processor architecture	The authors suggest a new architecture for an image processor that is based on the mathematical concept of simplicial cellular neural networks. They introduce instruction primitives for common image processing tasks and demonstrate their effectiveness with binary and gray scale images. The proposed design can be manufactured using advanced CMOS technology and is suitable for pixel-level processing due to its efficient digital circuits and wiring.
41472	41472165	Enhancing web services description and discovery to facilitate composition	Web services are becoming increasingly popular in the business world, but their adoption has been hindered by the manual effort required to use them. In order to increase automation and agility, the authors propose the use of more expressive descriptions for Web services. They introduce METEOR-S front-end tools for annotating source code and generating semantic descriptions, as well as WSDL-S, a language designed to incorporate semantic descriptions into the widely used WSDL 2.0 format. These tools aim to streamline the use of Web services and make them more adaptable to the constantly changing needs of businesses.
41473	414738	A Generalization of the Convex Kakeya Problem	This article discusses the problem of finding the smallest convex region that contains a translate of each given line segment in the plane. This problem is a generalization of Kakeya's problem, which involves finding a region that allows a needle to be rotated 360 degrees within it. The article proves that the optimal solution to this problem is always a triangle, and presents an algorithm that can compute this triangle in ¿(nlogn) time. It also shows that if the goal is to minimize the perimeter instead of the area of the region, then placing the segments with their midpoint at the origin and taking their convex hull results in an optimal solution. Additionally, it is proven that the smallest enclosing disk of any compact convex figure is the smallest-perimeter region that contains a translate of any rotated copy of the figure.
41474	414744	An Experimental Assessment of the 2D Visibility Complex	In this study, the authors investigate the size of the 2D visibility complex for randomly distributed unit discs in the plane. They find that the number of free bitangents is linearly related to the number of discs, and they examine how this relationship is affected by the density of the discs. They present an approximation for the number of free bitangents based on the density and number of discs, and note that this approximation can be used to predict the onset of the linear behavior for low densities. Overall, the study provides insights into the behavior of the 2D visibility complex in different density scenarios.
41475	41475100	Approximate Query Processing For Efficient Content-Based Image Retrieval Based On A Hierarchical Som	This paper introduces a new method for similarity matching in image retrieval using a hierarchical self-organizing map (SOM). The approach involves mapping high dimensional input vectors to a low dimensional grid using a local membership function, maintaining the relationships between the input vectors and their neighboring weight vectors. A hierarchical tree is then used to reduce the computation cost of finding the best match unit. The k nearest neighbors of a query vector are retrieved using an approximate query processing approach. Experimental results demonstrate the effectiveness of this approach on both synthetic datasets and image databases. 
41476	4147646	Hybrid orbiting-to-photos in 3D reconstructed visual reality	Virtual navigation through 3D images has become increasingly popular in various applications. This paper focuses on a specific type of virtual travel maneuver, orbiting to photos that show a point-of-interest. The challenge with this maneuver is providing appropriate feedback to the user about each photo while allowing them to manipulate three degrees-of-freedom for orbiting. The authors propose a hybrid approach that combines features from two existing methods, resulting in a more favorable user experience and preference for outdoor scenes. Experimental results show that this approach is successful in providing useful information and improving the virtual navigation experience.
41477	4147741	Position control of X-Y table at velocity reversal using presliding friction characteristics	This paper discusses a method for precision position control of a CNC machining center's X-Y table during velocity reversal. The authors analyze the effects of presliding friction and propose a compensation method based on these characteristics. They also investigate the transition time between presliding and sliding regimes, and establish a relationship between this time and the acceleration at zero velocity. The paper suggests a way to estimate the transition time without measuring velocity and confirms its validity through experiments. Additionally, the authors address the issue of torsional displacement in the X-Y table system and propose a combined compensation method for both friction and displacement. Experimental results demonstrate the effectiveness of the proposed approach.
41478	4147831	Extension based Limited Lookahead Supervision of Discrete Event Systems	Chung-Lafortune-Lin studied the supervisory control of discrete event systems using limited lookahead, where control is determined by truncating the plant behavior within the lookahead window. This approach has been modified to extend the plant behavior beyond the window, eliminating the need for pending traces. This avoids the issue of choosing a conservative or optimistic attitude towards pending traces, which can lead to violations or restrictive control policies. The proposed method also uses relative closure to ensure non-blocking behavior, even when the desired behavior is not relatively closed. It possesses all the desirable properties of the conservative approach and has been applied to concurrency control in database management systems.
41479	4147978	Multi-scale similarities in stochastic neighbour embedding: Reducing dimensionality while preserving both local and global structure.	Stochastic neighbour embedding (SNE) and its variants are methods used for reducing the dimensionality of data in a nonlinear way. These methods rely on soft Gaussian neighbourhoods to measure similarities between pairs of data points and aim to recreate these neighbourhoods in a lower-dimensional space. Previous research has explored the robustness of these methods to norm concentration and proposed enhanced cost functions, such as sums of Jensen-Shannon divergences. This paper introduces a new refinement called multi-scale similarities, which use exponentially increasing bandwidths to replace traditional single-scale neighbourhoods. The objective of multi-scale similarities is to optimize the embedding quality on all scales, preserving both local and global neighbourhoods without the need for the user to specify a scale. Experiments show that this approach improves the quality of dimensionality reduction and captures the structure of data more accurately. 
41480	4148014	A hybrid method of fuzzy simulation and genetic algorithm to optimize constrained inventory control systems with stochastic replenishments and fuzzy demand	This paper discusses multi-periodic inventory control problems and the assumptions typically used to study them. The two main assumptions are continuous review, where orders can happen at any time, and periodic review, where orders can only be placed at the beginning of each period. The paper proposes a new approach that relaxes these assumptions and considers the times between replenishments as independent random variables. The decision variables in this problem are of integer type and there is a single space constraint. Demands are treated as fuzzy numbers and a combination of back-order and lost-sales is considered for shortages. The paper presents a hybrid method of fuzzy simulation and genetic algorithm to solve this problem, and compares its performance with an existing hybrid simulation and simulated annealing algorithm through numerical examples. The results show that the proposed method performs better. The paper also demonstrates the applicability of the proposed methodology through sensitivity analysis on its parameters.
41481	4148128	From Fixed-Length to Arbitrary-Length RSA Padding Schemes	Signing with RSA involves applying a hash or redundancy function to a message, adding padding, and then exponentiating it with the decryption exponent. This is commonly used in various standards. However, a new paper suggests using a secure padding scheme for signing long messages and a separate one for fixed-size messages. This shifts the focus to finding a secure encoding for RSA signatures, highlighting the challenge of finding a secure redundancy function for short messages. This is still an open problem.
41482	414822	Routing and peering in a competitive Internet	 for locations outside of B.The Internet today is made up of independent network providers who are mainly concerned with maximizing their own profits. This has implications for how these providers choose to interconnect with each other. The placement of these interconnection links is a complex problem, but there are some simple solutions for certain cases. Another phenomenon, known as "hot-potato" routing, where outgoing traffic exits a provider's network as quickly as possible, is also influenced by the economic incentives of the providers. In cases where providers can charge each other for flow on their links, there are bounds on the efficiency of this scheme. This is a significant change from traditional analyses of routing, which assumed a single operator was in charge of the network with the goal of improving overall performance. Understanding the economic incentives driving network providers is necessary to understand how they form interconnections, which can be classified as transit or peer relationships. 
41483	414830	Handling Occlusion and Large Displacement through Improved RGB-D Scene Flow Estimation	The accuracy of scene flow is limited by challenges like occlusion and large displacement motion. These problems are often related, as large displacement motion can lead to occluded regions. To address this, a new method using RGB-D data is proposed. This method models occlusion and estimates scene flow simultaneously, while also using an over-parameterized representation to handle large displacement motion. A two-stage optimization process is used, with a new PatchMatch method applied in the RGB-D image space to reduce complexity. This method outperforms others on the Middlebury dataset and is also successful on real data from a Kinect sensor. 
41484	4148447	Spectral clustering based on iterative optimization for large-scale and high-dimensional data.	Spectral graph theory has been an important aspect of manifold learning and is widely used in data clustering. However, its computational demands make it difficult to handle large and high-dimensional datasets. The development of data on the Web has also posed challenges for traditional single-task clustering, leading to the rise of multi-task clustering for applications like video segmentation. In this paper, the authors introduce a Spectral Clustering based on Iterative Optimization (SCIO) method that efficiently solves the spectral decomposition problem for large and high-dimensional datasets and is effective in multi-task clustering. Experiments on synthetic and real-world datasets prove the effectiveness of this approach. 
41485	4148532	A latent shared-component generative model for real-time disease surveillance using Twitter data	The use of data mining to address social problems, also known as "data science for social good," has gained attention from researchers and institutions. This paper focuses on using data to monitor dengue epidemics in small geographical areas. A simple yet effective model is developed to connect fluctuations in disease cases and disease-related Twitter posts. A hidden Markov process is used to drive both the fluctuations in dengue cases and tweets, with a random source of tweets added to represent posts when no cases are recorded. The model is learned through a Markov chain Monte Carlo algorithm and has shown success in predicting disease counts using data from Brazilian towns.
41486	4148641	Infinite Author Topic Model based on Mixed Gamma-Negative Binomial Process.	The incorporation of side information such as authors, time stamps, and emotional tags into traditional text mining models has become increasingly popular in fields like information retrieval, natural language processing, and machine learning. One approach is the Author Topic Model (ATM), which uses author interests as side information in a topic model. However, the ATM requires a predetermined number of topics, which can be challenging in real-world scenarios. To address this issue, the Infinite Author Topic (IAT) model is proposed, which uses a stochastic process to determine the number of topics from the data itself. This model also includes a multi-author contribution component and an efficient Gibbs sampling inference algorithm. Experiments on real-world datasets show the IAT's ability to learn topics, author interests, and the number of topics simultaneously.
41487	4148776	On Finding Templates on Web Collections	Templates are blocks of code used by website creators to maintain a consistent design and navigation across multiple web pages. They are often generated using tools or programs that publish content from a database. While templates are helpful, they can negatively impact the quality of results from systems that automatically process website information, such as search engines. This is because the information in templates is redundant and storing it multiple times can waste computational resources. This paper discusses methods for detecting templates in a scenario where there are multiple templates present. Previous research has focused on detecting a single template, but this scenario is more realistic and requires adjustments to existing algorithms. The proposed methods involve partitioning the collection of web pages into clusters and using a single-template detection procedure on each cluster. A new algorithm is also proposed, which has a linear complexity and is effective in identifying a template with only a small set of pages. Experimental results show that this approach is efficient and accurate.
41488	41488125	Single image super-resolution via phase congruency analysis	The process of creating high-resolution images, known as single image super-resolution (SR), is a challenging task. While self-example-based methods can produce sharp edges, they struggle with textures. Some methods use higher-level image segmentation and external texture databases, but these require a lot of human involvement. This paper discusses the limitations of example-based techniques and proposes a new method using scale space analysis and a robust pixel classification method. This approach can effectively differentiate between edges, textures, and flat regions in images, and adaptively prioritize high-frequency details and scale invariant fractal properties. Experimental results show that this approach produces high-quality images with minimal artifacts. 
41489	4148926	Model transformations for migrating legacy models: an industrial case study	Many automotive companies, including General Motors, have adopted Model-Driven Development (MDD) in their vehicle control software development. GM has been using a custom-built, domain-specific modeling language for this purpose. However, with the development of AUTOSAR, a standard for integrating components from different suppliers, there is a need to migrate these GM-specific models to AUTOSAR models. This paper discusses the use of model transformations, specifically using MDWorkbench and ATL, to address the challenges in this migration process. The authors share their experience and provide recommendations for further research in this field.
41490	4149076	Debug Aware AXI-based Network Interface	As System on Chips (SoCs) become more complex with multiple cores and interconnections, traditional debugging methods need to be improved. This includes not only validating the computational part of a design, but also monitoring and validating communication and synchronization among cores. This paper proposes a debug aware network interface (NI) compatible with AXI standard, which allows for cross-trigger debugging. This means that transactions from one processing element can be monitored and traced by a cross-trigger unit and routed to another processing element or Shared Debugging Unit (SDU). The benefits of this approach include real-time detection and bypassing of severe faults, no need for large internal trace memory, and easier debugging of applications running on multiple processors.
41491	414911	A family of iterative methods with sixth and seventh order convergence for nonlinear equations	This paper introduces a new family of iterative methods for solving nonlinear equations with sixth and seventh order convergence. These methods combine known methods of third and fourth order with Newton's method and use a precise approximation for the last derivative, resulting in high convergence and reducing the number of functional evaluations required. The methods achieve efficiency indices of 1.5651 and 1.6266, making them competitive with other methods. The paper also introduces a new efficiency index that takes into account both computational effort and functional evaluations per iteration, and uses this index to compare the new methods with others in several numerical tests. 
41492	4149261	King-Type Derivative-Free Iterative Families: Real and Memory Dynamics.	This article introduces a biparametric family of order four derivative-free iterative methods for solving nonlinear equations. By manipulating the error equation, these methods can be modified to achieve higher convergence orders up to six. The stability analysis of the family without memory is conducted on quadratic polynomials, revealing areas in the parameter plane with good performance. To further investigate the stability of the family with memory, a discrete multidimensional dynamical system is associated with it. By studying the fixed and critical points of this system, the most stable methods can be identified.
41493	4149324	Efficient and correct execution of parallel programs that share memory	This paper discusses an optimization problem that arises when parallel programs are executed on shared-memory MIMD computers. These computers consist of multiple processors executing sequential program segments and accessing shared variables asynchronously. The goal is to ensure that the execution is sequentially consistent, meaning it appears as if all the instructions were executed sequentially. To achieve this, the paper proposes using a conflict graph similar to that used in distributed databases, which takes into account the program text to avoid the need for locks. This has implications for the design of multiprocessors and offers new optimization techniques for parallel languages with shared variables.
41494	41494140	Group sparse representation for image categorization and semantic video retrieval.	In this paper, a new approach to image representation called group sparse representation (GSR) is proposed for image classification and video retrieval. The idea is to represent a test image as a combination of all training images, using weight coefficients for each image and class. The method is based on a group nonnegative garrote model, resulting in sparse representations that are suitable for discriminant analysis. Experiments on various image and video datasets demonstrate the efficiency and effectiveness of the proposed approach. This approach shows promise in the field of multimedia content analysis and management.
41495	41495195	Cartoon synthesis using constrained spreading activation network	This paper proposes a method for creating cartoons by controlling the character's path in a background image. Pre-experiments were conducted to compare different features such as edge, motion, and color, and a Cartoon Frame Relationship Network was created. A Constrained Spreading Activation Algorithm was used to select visually similar frames to generate the next frame. The synthesized cartoons were then coordinated with the background image's perspective. The experiment results showed that the proposed features were effective in evaluating similarity and the algorithm selected more similar frames compared to others. This approach can create visually smooth cartoons from an existing library.
41496	4149676	Power-controlled feedback and training for two-way MIMO channels	In this paper, the authors discuss the use of feedback in communication systems and how it relates to channel state information. They point out that while most models assume perfect channel state information and noiseless feedback links, this is not the case in practical systems. The paper focuses on the achievable diversity multiplexing tradeoff using i.i.d. Gaussian codebooks, taking into account errors in channel estimation and feedback links in frequency division duplex systems. The authors present a key result that shows that having only one bit of feedback information is just as effective as having more bits, and that this one-bit performance is equivalent to having perfect channel state information and noiseless feedback. They also introduce the concepts of power controlled feedback and training, which are important in dealing with imperfect channel estimation and noisy feedback links. Additionally, they provide an asymptotic expression for the joint probability of the SNR exponents of eigenvalues of the actual channel and the estimated channel.
41497	4149776	Efficient implementation of 3G-324M protocol stack for multimedia communication	This paper discusses the implementation of 3G-324M, the multimedia transmission protocol stack for 3G communication, in order to support real-time video, audio, and data communication among different 3G handsets. The authors present efficient approaches and experiences in the implementation process, including event-driven communication, single-step message transformation, and serialization of nested multiplex table entries. The implementation has been successfully tested in a heterogeneous 3G communication environment and has shown satisfactory performance in transmitting real-time video, audio, and data. 
41498	414987	Improving charging capacity for wireless sensor networks by deploying one mobile vehicle with multiple removable chargers.	Wireless energy transfer is a promising technology for extending the lifespan of wireless sensor networks (WSNs). However, existing studies on sensor charging assume the deployment of one or multiple charging vehicles, which may not be feasible for a real sensor network. This paper proposes a charging model where a single vehicle carries multiple low-cost removable chargers, each powered by a portable high-volume battery. By placing a charger near each energy-critical sensor, the vehicle can charge multiple sensors simultaneously. The paper also presents scheduling algorithms to minimize the dead duration of sensors and the travel distance of the vehicle. Experimental simulations show promising results for the proposed algorithms.
41499	4149966	Attacks vs. Countermeasures of SSL Protected Trust Model	This paper discusses the weaknesses of current anti-spoofing methods and suggests a new SSL protected trust model. It also outlines attacks on this model and presents a solution in the form of the Automatic Detecting Security Indicator (ADSI) scheme. This scheme randomly generates and embeds images into the browser to detect spoofing attempts. When a mismatch is detected, an alarm is triggered to alert the user. This approach is less burdensome for users and requires minimal changes to the browser, making it easy to implement. It also does not require a Logo Certification Authority or personalization, making it suitable for use in internet cafes.
41500	41500105	Spatio-temporal data evolutionary clustering based on MOEA/D	Evolutionary clustering, a method that evolves with time, is gaining importance in data mining research. It is effective in clustering dynamic data, but faces the challenge of considering two conflicting criteria: snapshot quality and history cost. Existing methods combine these into a single objective and use optimization techniques. This paper introduces a new approach using a multi-objective evolutionary algorithm based on decomposition (MOEA/D) to optimize both criteria in the evolutionary k-means algorithm (EKM). Results show that this algorithm outperforms EKM.
41501	4150178	Assessing the Performance of a Novel Tag-Based Reader-to-Reader Communication Paradigm Under Noisy Channel Conditions.	RFID technology has enabled a wide range of applications beyond simple identification of people and goods. In this paper, a new approach is explored where the remaining data storage capacity of passive RFID tags is utilized as a virtual communication channel. This allows for the transmission of information between the tag and reader, expanding the potential uses of RFID technology. The paper discusses the potential benefits and challenges of this approach and highlights its potential for innovative applications.
41502	4150233	Developing Knowledge-Based Intelligent Multimedia Tutoring Systems Using Semantic Content-Based Modelling	The paper explores the challenges of incorporating multimedia into traditional intelligent tutoring systems (ITS). It proposes a new model, using multimedia frames (m-frames), to integrate multimedia syntax and semantics into the ITS architecture. The model is applied in the development of ARISTOTLE, an intelligent multimedia tutoring system (IMTS) designed to teach young children about zoology. The paper highlights the potential of the model for developing effective IMTSs and discusses its implementation in ARISTOTLE. This research contributes to the development of knowledge-based IMTSs, which can effectively utilize semantic video and audio content.
41503	4150331	A framework for adaptive parameter estimation with finite memory	The article discusses the problem of estimating an unknown parameter using sequential measurements from different statistical experiments. The approach involves adaptively selecting the next experiment based on past observations, using a finite memory framework. This is achieved through finite-state parametric Markov chains, with the asymptotic performance of the estimation scheme being linked to the steady-state distributions of these chains. The optimal selection of experiments is reformulated as designing a family of Markov chains with desired steady-state distributions. The article also suggests a quantitative criterion for optimal selection based on minimax ratio regret. 
41504	4150427	Distributed Memory Partitioning of High-Throughput Sequencing Datasets for Enabling Parallel Genomics Analyses	High-throughput sequencing instruments can now process billions of short genomic fragments in a single run, referred to as 'reads'. These datasets have many applications in genomics, metagenomics, and transcriptomics, but analyzing them can be computationally and memory intensive. In this paper, a parallel algorithm is introduced for partitioning large read datasets for distributed-memory parallel analyses. The algorithm also constructs and partitions the associated de Bruijn graph, allowing for applications like de novo assembly to utilize the partitions. A quality evaluation mechanism is proposed and the algorithm is shown to produce high quality partitions. The implementation is available on github.com/ParBLiSS/read_partitioning.
41505	4150536	Antiweb-wheel inequalities and their separation problems over the stable set polytopes	A stable set in a graph is a group of vertices that are not connected to each other. Finding the maximum weight stable set is a difficult problem. One approach is to optimize a linear function over the convex hull STAB(G) of incidence vectors of stable sets. However, it is difficult to find a concise characterization of STAB(G) using linear inequalities. Instead, the goal is to find large classes of valid inequalities that can be efficiently separated. Some examples of these classes include trivial, edge, cycle, and wheel inequalities. This paper introduces a new class called (t)-antiweb-s-wheel inequalities, which is a combination of (t)-antiweb and wheel inequalities. Efficient separation algorithms are also provided for these new inequalities.
41506	41506118	Auswahl von Kameraaktionen zur wissensbasierten Szenenexploration	The process of scene exploration is based on selecting camera parameters for capturing an image that must be optimal for further processing, as demanded by the strategy of active vision. This article discusses the selection of camera parameters through a selection of camera actions represented as concepts in a knowledge base of a semantic network. The knowledge base also contains information about the scene and objects involved in the task. To select the optimal camera action from multiple executable actions at a given time, a scoring calculation is required. The article introduces a new scoring calculation based on ideas from decision theory. The camera actions are scored based on their utility, which is determined by evaluating the instances of the objects in the scene. The effectiveness of this approach is demonstrated through the exploration of an office scene.
41507	4150714	Relevant values: New metadata to provide insight on attribute values at schema level	Research on data integration has led to the development of languages and systems that can effectively combine different data sources to create a unified representation. However, most existing approaches only consider intensional knowledge, ignoring the valuable information provided by extensional knowledge. To address this limitation, the authors propose a technique that adds "relevant values" to the intension of an attribute, extracted from its values. These relevant values not only enrich the schema with domain knowledge, but also aid users in creating and refining queries. The technique is automatic, independent of the attribute domain, and utilizes data mining clustering techniques and emerging semantics from data values. It can be customized with various similarity metrics and is particularly useful for managing frequently changing sources, such as in the Semantic Web.
41508	415087	Offering A Product Recommendation System in E-commerce	The paper presents a product recommendation system for Business-to-customer e-commerce that uses both explicit and implicit ratings. The system recommends products based on the purchase patterns of previous users who have similar patterns to the new user. It uses a weighted cosine similarity measure to find the closest user profile in the database. The system also incorporates Association rule mining to improve the recommendations. Time of transaction is also taken into account to avoid sequence recognition problems. Experimental results show that the proposed method performs well with implicit ratings and the use of association rule further enhances the system's performance.
41509	4150989	A NOC closed-loop performance monitor and adapter	In a Network-on-Chip (NoC), the number of buffers allocated to each communication channel affects performance and power consumption. A runtime mechanism is needed to automatically adjust buffer size based on communication patterns. This paper proposes a control mechanism to resize buffers in an adaptive router, monitored and controlled for each channel. It also presents a technique to isolate faulty buffers, which are crucial for communication performance. Experimental results show that the proposed architecture can decrease latency by 80% and increase throughput by 45% in the absence of faults. In the presence of faults, the proposed architecture maintains performance while saving up to 25% power compared to a homogeneous router.
41510	415106	Model Checking for Modal Dependence Logic: An Approach Through Post's Lattice	This paper explores the computational complexity of model checking in an extended version of modal dependence logic. This extension allows for the use of arbitrary Boolean connectives, building upon the original version introduced by Jouko V\"a\"an\"anen which included the dependence atom dep(.). The study utilizes a Lattice approach developed by Emil Post to classify all possible Boolean functions, considering various fragments of the logical language that incorporate modalities $\Diamond$ and $\Box$, the dependence atom, and logical symbols for arbitrary Boolean functions. 
41511	4151151	Empowering Evolving Social Network Users with Privacy Rights.	The issue of privacy on social networks is a major concern and there is ongoing debate about how to effectively protect users' privacy rights. Many ideas have been proposed, but no single approach has emerged as the leading solution. This paper introduces a new conceptual model for privacy in social networks, highlighting its uniqueness compared to current research. The authors also discuss a potential query language, called PiQL, that was developed based on this model to support user-driven privacy policy creation and enforcement. The strength of this model is its ability to be extended through the use of linguistic constructs in query languages like SQL, as demonstrated in PiQL.
41512	4151244	Static Evaluation of Software Architectures	The software architecture is a crucial aspect of the software system lifecycle as it directly affects business goals, functional and quality requirements. Architecture evaluations are essential in determining the effectiveness of the architecture for its intended usage. This paper shares practical experience and outlines when and how static architecture evaluations contribute to architecture development. Ten purposes and needs for static architecture evaluations are identified, supported by case studies from industry and academia. The results of architecture evaluations influence subsequent steps in architecture development.
41513	415134	Matching parse thickets for open domain question answering.	Parse thickets are a unified representation of a paragraph of text that combines traditional parse trees with anaphora and rhetoric information. They are useful for answering complex questions and improving search accuracy. The operation of generalization measures the distance between paragraphs of text by finding the maximal common sub-graph of two parse thickets. This technique is evaluated in various search domains and has shown to improve search accuracy compared to other methods. An open source plug-in for SOLR has been developed for easy integration with industrial search engines. The impact of different sources of discourse information on the search accuracy is also analyzed. 
41514	4151433	The mean-field computation in a supermarket model with server multiple vacations	The study of queueing networks with server vacations is limited but interesting and challenging. This paper presents a unified method for analyzing a supermarket model with multiple server vacations. By using functional analysis and an operator semigroup, the paper derives an infinite-dimensional system of differential equations and provides a mean-field limit for the Markov processes. An algorithm is also provided to compute the fixed point of the system, which is used to analyze the performance of the supermarket model, including stationary queue length and expected sojourn time. The paper also includes numerical examples to demonstrate the impact of crucial factors on performance. This approach can be applied to other complex supermarket models, making it useful for practical applications in areas such as computer networks and transportation systems.
41515	4151537	EFFORT: energy-efficient opportunistic routing technology in wireless sensor networks.	 The effective use of energy in wireless sensor networks (WSNs) is crucial and routing mechanisms play a significant role in energy consumption. Existing routing schemes have two common issues: certain sensors may quickly drain their energy and packet retransmissions over unreliable links can consume significant energy. To address these issues, a new energy-efficient routing scheme called EFFORT is proposed. It maximizes data gathering in WSNs by utilizing opportunistic routing and a new metric to determine suitable forwarders and relay priorities for each sensor. EFFORT outperforms other routing protocols in terms of energy efficiency and network lifetime, as shown in simulation results. This paper is copyrighted by John Wiley & Sons, Ltd in 2011.
41516	4151657	The McEliece Cryptosystem Resists Quantum Fourier Sampling Attacks	Quantum computers have the potential to break traditional public-key cryptosystems like RSA and El Gamal by being able to factor large integers and extract discrete logarithms. To prepare for the possibility of quantum computers becoming a reality, researchers have been working on developing \emph{post-quantum} cryptosystems that can be implemented using classical computers now, but will remain secure against quantum attacks. This article focuses on the McEliece cryptosystem, which uses \emph{well-permuted, well-scrambled} linear codes and has been shown to be resistant against attacks based on generating and measuring coset states. The authors show that the natural case of the Hidden Subgroup Problem, to which the McEliece cryptosystem reduces, cannot be solved using strong Fourier sampling or measurements of coset states. By extending this result to subgroups of arbitrary structure, including the automorphism groups of linear codes, the authors provide the first rigorous results on the security of the McEliece cryptosystem against quantum adversaries. This strengthens its potential as a post-quantum cryptosystem.
41517	4151752	Robust network supercomputing without centralized control	Traditional approaches to network supercomputing rely on a central master process and numerous worker processes, but this can lead to performance issues and a single point of failure. To address this, a decentralized algorithm is proposed where workers can determine if tasks have been completed and collect results locally. The algorithm has a failure model where the probability of a worker returning a wrong result is less than 1/2. It is proven that the algorithm, which operates synchronously and randomly, can successfully complete n tasks in Θ(log n) rounds with a high probability of correct results. The message complexity is Θ(n log n) and the bit complexity is O(n2 log3 n).
41518	4151812	Towards automation & augmentation of the design of schedulers for cellular communications networks.	Evolutionary Computation is a technique used to automatically improve small cell schedulers in a realistic 4G-LTE cellular network simulation. These evolved schedulers are then further enhanced by human design to increase their robustness. Extensive analysis shows that evolution has discovered a new scheduling technique that performs well across cells of varying sizes and conforms to accepted scheduling frameworks. The evolved solutions outperform a human-engineered benchmark by up to 50%. This approach is also shown to be flexible, allowing for tailored algorithms to be evolved for specific scenarios and corner cases. This work, published in Evolutionary Computation in 2018, suggests that this method can help network operators create unique algorithms for different deployments and delay costly hardware upgrades.
41519	4151981	Stabilization of nonlinear delay systems using approximate predictors and high-gain observers	The article presents a solution to the problem of stabilizing nonlinear systems with long delays at the input and output, using only output feedback. The solution is global and uses the predictor approach over a period that combines the delays, and can handle nonlinear systems with sampled measurements and control applied using a zero-order hold. The sampling/holding periods should be short but do not need to be constant. The proposed approach is based on a class of Lipschitz strict-feedback systems with disturbances and involves a successive approximation of the predictor map, a high-gain sampled-data observer, and a linear stabilizing feedback. The method is robust to perturbations in the sampling schedule and can handle different sampling and holding periods. The approach is also applicable to linear systems, where the predictor can be explicitly calculated. 
41520	4152010	Evaluation of semi-automatically generated accessible interfaces for educational games.	The use of videogames in educational settings, known as serious games, is becoming increasingly popular. However, this also raises ethical concerns about accessibility for all students, as videogames can be difficult for some to access. To address this issue, researchers have explored the potential of automatic adaptations to game interfaces, specifically for point-and-click adventure games, a popular genre in serious games. The adapted games were tested with users who have disabilities, and while they were found to be usable, there were still some usability issues that impacted the overall experience. The researchers suggest further improvements and a refined interface model to address these limitations. 
41521	4152140	An Analysis of Reliable Delivery Specifications for Web Services	The delivery of messages is crucial for the success of Web Services and there are two main specifications, WS-Reliability and WS-ReliableMessaging, competing in this area. This paper provides an analysis of these specifications, focusing on their similarities and differences. The goal is to identify any gaps and provide recommendations for improvement. The analysis reveals differences in philosophies and proposes ways to address the gaps. This information is important for developers and users of Web Services to understand and utilize these specifications effectively. 
41522	415228	A Method Suitable for Vicarious Calibration of a UAV Hyperspectral Remote Sensor	A sensor on a UAV is prone to vibration and natural elements like wind, making calibration challenging. Vicarious calibration, which is closer to real-world conditions, is used alongside laboratory calibration for remote sensors. However, the existing vicarious calibration for UAVs only uses a reflectance-based method, not taking into account the largest source of uncertainty - aerosol-type assumptions. To address this, an improved irradiance-based method is proposed, which considers the difference in radiative transfer between satellites and UAVs. Simulation and field experiments show that this method has higher accuracy and lower uncertainty, making it more suitable for vicarious calibration of UAV sensors. 
41523	4152330	Adaptive Markov Random Field Approach for Classification of Hyperspectral Imagery	This letter introduces an adaptive Markov random field (MRF) technique for classifying hyperspectral imagery. The method incorporates a relative homogeneity index for each pixel to determine the spatial contribution in the MRF classification, preventing overcorrection in areas with high spatial variation. Support vector machines are also used for improved class modeling and estimation of spectral contribution. Experiments on synthetic and real hyperspectral data show that this approach outperforms traditional methods in accurately classifying both homogeneous regions and class boundaries.
41524	4152417	A Novel MKL Model of Integrating LiDAR Data and MSI for Urban Area Classification	The HF-MKL model is a novel approach for urban classification that combines features from two data sources, spectral images and LiDAR data. This model utilizes Gaussian kernels with different bandwidths to measure the similarity of samples at different scales, and then integrates these multiscale kernels using a linear combination. The weights of the different kernels are determined by finding a projection based on maximum variance, allowing for the discriminative ability of the heterogeneous features to be exploited and integrated. The model is optimized using a support vector machine and experiments on real data sets have shown that it outperforms other state-of-the-art algorithms in terms of classification accuracy.
41525	4152520	Distributed multi-robot coordination in area exploration	The paper suggests a reliable and efficient coordination algorithm for multiple robots with limited communication capabilities. It uses a distributed bidding model and incorporates measures to address the restricted range. The bidding algorithm takes into account the distances between robots, promoting close proximity. The paper also introduces a map synchronization mechanism to reduce data exchange when subnetworks merge. Simulation results validate the effectiveness of these measures in improving coordination and handling the limited communication range. This algorithm makes coordination more realistic in multi-robot applications.
41526	4152684	Searching and Browsing Collections of Structural Information	This paper proposes a new approach to querying structured textual information, such as SGML/XML documents. Utilizing knowledge about the structure of documents can improve retrieval by allowing for more precise information needs. However, traditional probabilistic retrieval models have difficulty handling structural information. To address this, the paper presents a new retrieval function based on the probabilistic model that can handle structural roles assigned to individual terms. Efficient evaluation of queries in this framework requires appropriate index structures, including text and structure indexes. The implementation also includes additional features, such as a table of contents for browsing. Initial evaluations show promising results on collections of unstructured documents.
41527	415270	Dynamic User Demand Driven Online Network Selection.	Network selection is crucial in maximizing the benefits of different wireless networks. In order to improve user experience, researchers have studied how to select the best network in a dynamic environment where user demand and network handoff costs constantly change. Dynamic network selection is a promising solution, but it struggles to balance meeting user demand and minimizing handoff costs. To address this issue, an online network selection algorithm is proposed which learns the optimal policy while considering handoff costs. Two additional algorithms are also developed, taking advantage of inherent dependencies in the problem for faster convergence. Simulations demonstrate that these algorithms outperform existing methods by 10%.
41528	41528109	Data Quality Guided Incentive Mechanism Design for Crowdsensing.	Crowdsensing relies on participants using their physical resources and manual efforts to collect data. However, low quality data can harm the accuracy and availability of crowdsensing services. Existing incentive mechanisms have not addressed the issue of data quality. This paper proposes a quality-based incentive mechanism that pays participants according to their data's effectiveness. The mechanism estimates data quality and rewards participants accordingly, aiming to motivate them to efficiently perform tasks. The mechanism was implemented and evaluated, showing improved service quality and profit for the service provider compared to traditional data collection models and uniform pricing schemes. This highlights the importance of considering data quality in the design of incentive mechanisms for crowdsensing.
41529	415293	Radar waveform design in a spectrally crowded environment via nonconvex quadratic optimization	This paper addresses the challenge of designing radar signals in a crowded spectrum, where there is a high demand for both military and civilian wireless services. The goal is to create optimized radar waveforms that are compatible with other licensed electromagnetic radiators. The authors propose using a radio environmental map to guide the waveform optimization process, which focuses on improving radar performance while adhering to spectral constraints. The feasibility of this approach is thoroughly examined, and a solution technique is proposed that involves transforming the problem into a simpler convex optimization task. The resulting waveforms are evaluated based on their signal-to-interference plus noise ratio, spectral shape, and autocorrelation function. The analysis shows that the proposed method can effectively balance these factors to achieve optimal radar performance in a crowded spectrum.
41530	41530200	Fuzzy OLAP association rules mining based novel approach for multiagent cooperative learning	The paper introduces a new approach for multiagent learning in cooperative learning systems. This approach incorporates fuzziness and online analytical processing (OLAP) based data mining to effectively process information from the agents. Using this method, the actions of other agents can be estimated even if they are not in the visual environment of the agent being considered. This is done by extracting online association rules from a data cube. Additionally, a new action selection model is proposed based on association rule mining. The paper also discusses generalizing states by mining multiple-level association rules from a fuzzy data cube. Results from a pursuit domain demonstrate the effectiveness and robustness of this approach.
41531	4153123	From knowledge based software engineering to knowware based software engineering	The paper discusses the development and evolution of PROMIS, a knowledge-based software engineering system introduced in the 1990s. PROMIS aims to automatically generate applications using both domain knowledge and software knowledge. However, the system lacked a suitable representation for domain knowledge. Recently, the concept of knowware, a commercialized form of domain knowledge, has been introduced and is used in conjunction with PROMIS and J2EE to generate applications. The paper also introduces the definitions of knowware, knowledge middleware, and knowware engineering, and discusses the life cycle models and implementation designs for knowware. The paper concludes by discussing the integration of PROMIS, knowware, and J2EE in the PROMIS/KW** framework for application system generation and domain knowledge modeling.
41532	4153249	Communication-Free Widened Learning of Bayesian Network Classifiers Using Hashed Fiedler Vectors.	Widening is a technique that utilizes parallel resources to improve upon greedy algorithms and find better solutions. Previous uses of Widening have involved communication between parallel workers to maintain their distances in the model space. However, a new approach has been introduced that does not require communication, using Locality Sensitive Hashing on Bayesian networks' Fiedler vectors. This method has been shown to produce superior classifiers compared to standard implementations and those generated with only a greedy heuristic. This communication-free, widened extension to a standard machine learning algorithm has the potential to greatly enhance the efficiency and effectiveness of learning algorithms.
41533	4153337	Bounded diffusion for multiscale edge detection using regularizedcubic B-spline fitting	The paper suggests that the regularization factor α is a more effective scale parameter for edge detection compared to the standard deviation (σ) of the Gaussian pre-filter. This leads to the development of a multiscale edge detector (MRCBS) that uses an adaptive scale based on local noise levels, adjusted thresholds for edge details, and anisotropic diffusion for further noise suppression. The α scale space is found to better capture the evolutionary behavior of edges at different scales. This approach improves the accuracy and robustness of edge detection in various applications. 
41534	4153477	AudioDAQ: turning the mobile phone's ubiquitous headset port into a universal data acquisition interface	AudioDAQ is a new platform for continuous data acquisition using the headphone port of a mobile phone. Unlike other phone peripherals, AudioDAQ uses the microphone bias voltage to draw power and encodes data as analog audio, making it compatible with a wider range of phones. It also utilizes the phone's built-in voice memo application for data collection, allowing for simple analog peripherals without requiring a microcontroller. This design is more efficient, universal, and requires no modifications to the phone. The system has been tested and can continuously capture EKG signals for an extended period of time, sending the data to the cloud for storage, processing, and visualization. 
41535	4153521	An Automated Tensorial Classification Procedure for Left Ventricular Hypertrophic Cardiomyopathy.	Cardiovascular diseases are a major cause of death worldwide and effective classification tools are crucial in prevention and treatment. The use of statistical learning theory in magnetic resonance imaging has allowed for the diagnosis of various cardiomyopathies. A two-stage classification scheme has been proposed to differentiate between different types of hypertrophic cardiomyopathies and healthy patients. This involves using a multimodal processing pipeline to calculate robust tensorial descriptors of myocardial mechanical properties from magnetic resonance tagged images. A homomorphic filtering procedure is then used to align the images and provide 3D tensor information. Results have shown that this approach can effectively classify hypertrophic cardiomyopathies even with limited samples. 
41536	4153668	Sufficient conditions for triangle-free graphs to be optimally restricted edge-connected	In graph theory, a k-restricted edge-cut is a set of edges that, when removed from a connected graph G, disconnects it into components with at least k vertices. A graph that allows for k-restricted edge-cuts is called k-connected. The k-edge-degree of a graph is the minimum number of edges between a connected subgraph of size k and its complement. A graph is considered k-optimal if its k-restricted edge-connectivity is equal to its minimum k-edge-degree, and super-k if every minimum k-restricted edge-cut isolates a connected subgraph of size k. This paper examines the cases of k=2 and k=3 and establishes lower bounds for the size of components left by a minimum k-restricted edge-cut in triangle-free graphs that are not k-optimal. Sufficient conditions for a triangle-free graph to be k-optimal and super-k are also discussed.
41537	4153718	Supporting distributed groups with a Montage of lightweight interactions	The Montage prototype is a tool that allows remote collaborators to communicate through audio and video "glances." In a study of a distributed group, it was found that Montage was used for quick and casual interactions that were similar to face-to-face conversations. However, it did not replace other forms of communication such as email, voice-mail, and scheduled meetings. The prototype also integrated other applications to help coordinate future contact. Data was collected on usage, and both video-tape and user perception were analyzed. It was concluded that Montage was a convenient tool for staying connected with colleagues without leaving the office, but other tools were still necessary for organizing future interactions.
41538	4153828	Coupled Gaussian process regression for pose-invariant facial expression recognition	The authors propose a new approach for recognizing facial expressions at various poses using 2D geometric features. The method involves mapping the facial landmark points from non-frontal poses to the corresponding locations in the frontal pose, and then using a multi-class SVM for expression recognition. They also introduce a novel Coupled Gaussian Process Regression (CGPR) model to learn the mappings for pose normalization. This approach is able to handle expressive faces at a wide range of pan and tilt rotations and accurately predict continuous head poses. It is also the first method to be both face-shape-model-free and perform well with discrete training poses.
41539	4153953	Bus interconnection networks	Bus interconnection networks are used to connect processors and are represented by hypergraphs. A survey of different methods for constructing these networks with specific parameters, such as maximum processor degree and network diameter, is provided. The problem for point-to-point networks has been extensively researched, resulting in proposed families of networks. Two approaches for constructing bus networks using these point-to-point networks are discussed: considering the dual of the network and generalizing known constructions. The tools developed in the theory of hypergraphs and directed hypergraphs are summarized for handling this approach.
41540	415402	On perfect neighborhood sets in graphs	A dominating set of a graph G is a subset of vertices S, where every vertex of G not in S is adjacent to a vertex in S. A vertex upsilon of G is called S-perfect if it is adjacent to exactly one vertex in S. A perfect neighborhood set of G is a set where every vertex is either S-perfect or adjacent to an S-perfect vertex. It is shown that for all graphs G, the maximum cardinality of a minimal dominating set (Gamma(G)) is equal to the maximum cardinality of a perfect neighborhood set (Theta(G)). This was proven in a 1999 study by Elsevier Science B.V. 
41541	41541104	Scheduling Nonlinear Sensors For Stochastic Process Estimation	This paper focuses on the problem of using a limited number of sensors to estimate the state of a stochastic process, which is important in applications such as target tracking and simultaneous localization and mapping (SLAM). The challenge lies in dealing with unknown stochastic systems, nonlinear measurements, and limited resources. The authors propose an algorithm that can be applied to general stochastic processes and nonlinear measurements with a time complexity linear in the planning horizon. It offers significant computational advantages over the existing polynomial-time algorithm, achieving a performance only 1/2 away from the optimal. The algorithm also has the same time complexity as the state-of-the-art algorithms for linear systems and measurements. The authors prove two key properties of the entropy of the batch state vector conditioned on the measurements, which enable the efficient evaluation of the algorithm. 
41542	4154221	Razumikhin-type theorems on stability of stochastic retarded systems	The Razumikhin-type theorems have been developed and are effective for the stability of deterministic retarded dynamic systems. However, the stability is not in a global sense for stochastic retarded functional differential equations (SRFDEs). Recent research by Mao (1997a) has provided useful criteria for exponential stability of SRFDEs, but general asymptotic stability has not been considered. This article aims to establish Razumikhin-type theorems on asymptotic stability of stochastic retarded systems, a generalization of Mao's result. It also considers generalised exponential stability for better estimation of Lyapunov exponent of time-variant systems. By understanding the underlying principles of the Razumikhin-type theorems, it is possible to apply them to stochastic retarded systems.
41543	4154324	Gradient-based boosting for statistical relational learning: The relational dependency network case	Dependency networks are models that approximate the joint probability distribution of multiple random variables by using conditional distributions. Relational Dependency Networks (RDNs) are an extension of these networks to relational domains. However, RDNs have a more complex model-selection problem due to the need to explore an unlimited number of relational abstraction levels. The current learning methods for RDNs only learn a single probability tree per random variable, but a new approach using gradient-based boosting has been proposed. This method allows for the creation of highly complex features over multiple iterations, resulting in a more expressive model. Experimental results on various datasets have shown that this boosting method is more efficient in learning RDNs compared to other statistical relational learning approaches.
41544	4154446	MLPNN adaptive controller based on a reference model to drive an actuated lower limb orthosis	This paper proposes a method for controlling an actuated orthosis using an adaptive controller based on a reference model. The adaptive controller only requires knowledge of the global structure of the dynamic model and uses a Multi-Layer Perceptron Neural Network (MLPNN) to estimate dynamics related to inertia, gravitational and frictional forces, as well as other unmodeled dynamics. The stability of the system is analyzed using the Lyapunov formalism, and a first order Taylor series expansion is used to handle the nonlinearities related to the MLPNN. Experimental results using a real orthosis on a dummy show that the proposed controller is effective and robust, performing well in both assistive and resistive force scenarios. 
41545	41545126	Fourier-domain beamforming: the path to compressed ultrasound imaging	Sonography is a medical imaging technique that uses multiple transducer elements to produce images of tissues. These signals are sampled and processed through digital beamforming, which requires high sampling rates that result in large amounts of data. A new technique called compressed beamforming has been developed, which reduces the number of samples needed to reconstruct an image with strong reflectors. However, it cannot handle speckle, an important aspect of medical imaging. To address this, researchers have extended this technique to use beamforming in frequency, taking advantage of the low bandwidth of ultrasound signals. They have also developed a compressed sensing (CS) technique to further reduce the sampling rate. These methods have been successfully demonstrated on in vivo cardiac data and implemented on an ultrasound machine, showing the potential for smaller, more efficient, and cost-effective ultrasound machines in the future.
41546	4154690	Integrating inductive neural network learning and explanation-based learning	The article discusses the importance of combining inductive and analytical learning in order to improve learning methods. The authors propose a method called explanation-based neural network learning (EBNN), which combines explanation-based learning with inductive learning. This method uses a neural network representation of domain knowledge and constructs explanations by chaining together inferences from multiple neural networks. Unlike traditional symbolic approaches, EBNN focuses on extracting derivatives of the target concept from the explanation, which are then used to bias the inductive learning process. The results of experiments on a simulated robot control task show that EBNN requires fewer training examples and is robust to errors in the domain theory.
41547	4154769	Collecting and Analyzing Data from Distributed Control Programs	This paper discusses a collection of tools designed for developers to track and analyze data in C/C++ programs while they are running. These logging tools offer various options for storing data and allow for synchronized logging in distributed programs. One option is to store data in an SQL database. The authors have also developed analysis tools that use interval temporal logic to retrieve data from the database and answer common developer inquiries. The logging tools have been fully implemented and their performance is presented in the paper. The analysis tools are currently undergoing testing with real data from NASA applications.
41548	41548167	Experiential Sampling on Multiple Data Streams	Multimedia systems often have to handle multiple data streams, each containing a significant amount of redundant and noisy data. To optimize computing resources in real-time applications, it is important to focus on a relevant subset of data streams and use it to build an accurate model of the environment. This issue is addressed by formulating it as an experiential sampling problem and proposing an approach to efficiently utilize computing resources on the most informative data streams. The paper also introduces a generalized version of this framework for multiple data streams and presents an evaluation measure for its effectiveness. The framework has been successfully applied to various tasks such as traffic monitoring, face detection, and monologue detection.
41549	4154934	A unified framework to exploit information in BCI data for continuous prediction	In this paper, the authors propose a unified framework for developing effective learning algorithms for continuous prediction using EEG signals in brain-computer interface (BCI). The framework is based on variational Bayesian method and aims to fully utilize the information contained in BCI data. The method involves dividing trials in the training data set into segments and addressing key issues such as unknown intention and making decisions based on individual time intervals. Two auxiliary distributions are introduced in the lower bound on the log posterior to address these issues. The proposed method was evaluated on three BCI competition data sets and showed high accuracy.
41550	4155044	Counting in Trees for Free	The undecidability of MSO logic with Presburger constraints for ordered unranked trees can be resolved by using a decidable modal fixpoint logic. This can be characterized using deterministic Presburger tree automata, allowing for the expression of numerical document queries. Surprisingly, the complexity of satisfiability for this extended logic is the same as the original logic. Non-emptiness for Presburger tree automata is PSPACE-complete, but a tractable subclass exists. Polynomial time and linear space algorithms exist for deciding whether a tree satisfies a formula, and a linear time construction for a Presburger formula for the Parikh image of a regular language is presented.
41551	4155144	RFTL: improving performance of selective caching-based page-level FTL through replication.	Flash memory technology is greatly affected by the type of workload it is performing, leading to poor performance on random writes. To address this issue, Demand-based Flash Translation Layer (DFTL) was introduced, which selectively caches page-level address mappings. However, DFTL can still experience high cache miss rates, causing significant overhead. A new method called Replication-based DFTL (RFTL) has been proposed to minimize this overhead by replicating the cached mapping table. An analytical model was developed to study RFTL's performance, and EagleTree simulator was extended to implement it. Experimental results with synthetic workloads show that RFTL outperforms DFTL, especially for read-dominant workloads, with a 20% improvement in performance and 10% improvement in I/O throughput.
41552	4155229	Four-Chamber Heart Modeling and Automatic Segmentation for 3D Cardiac CT Volumes	This paper presents an automatic heart chamber segmentation system, which is crucial for accurately measuring cardiac function. Two main tasks involved in developing this system are heart modeling and fitting the model to new volumes. The model must be flexible yet accurate, taking into account the complex and non-rigid nature of the heart and its four chambers. Key landmarks, such as valves and cusp points, are explicitly represented in the model and can be reliably detected to guide the fitting process. The paper also introduces two methods for establishing mesh point correspondence, necessary for building a statistical shape model. The proposed approach uses recent advances in learning discriminative object models and a large database of annotated CT volumes to automatically segment the four heart chambers. Experiments show that this approach is efficient and robust, with results outperforming current methods. The study also reports stable results on a large dataset of 323 cardiac CT volumes, with a speed of less than eight seconds for automatic segmentation.
41553	41553100	Nonparametric Regression between General Riemannian Manifolds	Nonparametric regression between Riemannian manifolds is studied using regularized empirical risk minimization. The regularization functionals used in this approach must take into account the geometry of the input and output manifolds and be independent of the chosen parametrization. Three simple regularization functionals are defined and analyzed, and a general scheme for solving the resulting optimization problem is presented. Examples of applications include interpolation on the sphere, fingerprint processing, and correspondence computations between 3D surfaces. Interesting and sometimes counterintuitive implications and new open problems specific to learning between Riemannian manifolds are discussed, highlighting differences from traditional multivariate regression in Euclidean space.
41554	4155463	Transferring Human Impedance Behavior to Heterogeneous Variable Impedance Actuators	This paper discusses different methods for controlling robots with variable impedance actuators (VIAs) in a way that mimics human behavior. The focus is on transferring impedance modulation strategies from human demonstrators to robots with different levels of complexity. Three main approaches are identified: direct, feature-based, and inverse optimal transfer. The first approach is limited to highly biomorphic systems, while the latter two are more versatile and can be applied to various VIAs regardless of their mechanical design. The paper proposes a constraint-based method and an apprenticeship learning framework as examples of these transfer strategies and evaluates their effectiveness in terms of efficiency, ease of use, and task performance through simulations and real-world experiments. 
41555	415554	Learning coupling terms for obstacle avoidance	Autonomous manipulation in dynamic environments is crucial for robots to perform everyday tasks. There are two main approaches in the literature for this - using a complex planning system or modifying a simple plan with sensory feedback. Dynamic Movement Primitives (DMPs) provide a versatile and robust starting point for a controller that can be modified in real-time with a non-linear coupling term. This allows for fast and reactive obstacle avoidance, similar to human behavior. Researchers propose a method for learning this coupling term from human demonstrations and testing its ability to avoid obstacles in a reactive manner. This research aims to develop more advanced reactive control strategies, reducing the need for computationally expensive planning methods.
41556	4155655	Modularity and directionality in genetic interaction maps.	Genetic interactions between genes can reveal important functional relationships, but the vast amount of information from large-scale genetic interaction assays can be challenging to interpret due to experimental noise. A computational approach has been developed to organize these interactions into modules, providing insights into the function of cellular machineries and global properties of interaction maps. Further analysis showed that a significant portion of observed aggravating interactions are unidirectional, where one gene can buffer the effects of perturbing another gene but not vice versa. A freely accessible web tool has been created to browse these results, and additional data is available online.
41557	4155742	A system for exact and approximate genetic linkage analysis of SNP data in large pedigrees.	The use of SNP data in genetic linkage analysis is challenging due to technical, methodological, and computational obstacles. Superlink-Online SNP is an online system that streamlines this process by providing a flexible workflow and powerful analysis tools. It utilizes thousands of CPUs to perform tasks much faster than a single computer. The system offers services such as parallelized MCMC analysis and a novel algorithm for maximum-likelihood haplotyping. This allows for genetic analyses that were previously not possible. Researchers can access Superlink-Online SNP for free and the source code is available for download. Supplementary data is also available online.
41558	4155843	Incorporating Transaction Semantics to Reduce Reprocessing Overhead in Replicated Mobile Data Applications	Transactional replication, specifically the anywhere-anytime-anyway approach, can have unstable behavior when the workload increases. To address this issue, a two-tier replication algorithm was proposed in a previous study. However, this can lead to heavy reprocessing overhead. This paper suggests a different approach - merging histories instead of reprocessing - to reduce this overhead. This involves merging tentative transactions into the base history when a mobile node connects to the base nodes. However, conflicts between the two histories can result in a set of undesirable transactions that need to be backed out. The paper presents novel rewriting algorithms for backing out these transactions, which take transaction semantics into account and are more effective than traditional approaches. 
41559	4155958	Using Coupling-Based Weights for the Class Integration and Test Order Problem	In software development, integrating and testing classes can be complicated due to relationships such as method calls, inheritance, and aggregation. Classes must be integrated and tested in a specific order, and cyclic dependencies can create difficulties that require expensive and error-prone "stubbing" operations. This is known as the class integration and test order (CITO) problem. This paper introduces new techniques and algorithms to solve the CITO problem, including improved edge weights that better model the cost of stubbing and the use of node weights to incorporate more information. A new algorithm for computing integration and test orders is also presented, which is found to be more efficient and yield better results compared to existing approaches. 
41560	4156033	Near Optimal Placement Of Virtual Network Functions	NFV is a new approach to networking where network functions are run on standard servers located in small cloud nodes spread across the network. This allows for greater cost efficiency and control of network flows. However, the placement of these virtual functions within the physical network is a key challenge as it affects the network's performance, reliability, and operation cost. This paper addresses this challenge by proposing near optimal approximation algorithms that consider the distance cost between clients and virtual functions, as well as the setup costs. These algorithms are evaluated through simulations and shown to perform well in realistic scenarios, achieving constant approximation factors while adhering to capacity constraints. 
41561	4156122	Optimal sleep-state control of energy-aware M/G/1 queues.	This paper discusses the optimal control of sleep states in an energy-aware M/G/1 queue. The model considers a range of policies where the server can enter random sleep states upon becoming idle to save energy. Jobs are served after a setup time and the system is analyzed under different cost metrics and distributions. The paper presents a more general model and shows that the optimal control of idle time and sleep states is deterministic, without any benefit from randomization. Two popular cost metrics, energy and response time, are considered and it is proven that the optimal strategy is to either use only the idle state or go to a fixed sleep state and wait for a set number of jobs before starting the setup.
41562	415623	Csma Networks In A Many-Sources Regime: A Mean-Field Approach	The Internet of Everything (IoE) has led to a significant increase in the number of devices and applications that rely on wireless connectivity. This has resulted in wireless networks becoming larger and more complex, with a diverse range of traffic profiles and performance requirements. While there are established methods for evaluating the throughput of persistent sessions in these networks, they do not provide insight into the delay performance of flows with intermittent packet arrivals. To address this issue, a mean-field approach has been developed to analyze buffer contents and packet delays in wireless networks with many sources. This approach simplifies the analysis and provides accurate approximations for a large-scale network.
41563	4156333	Fixed and adaptive model-based controllers for active queue management	This paper introduces new model-based controllers for active queue management (AQM) that support TCP flows. The controllers are based on linearizing a previously developed TCP model using fluid flow and stochastic differential equations. Comparing a simple PI controller to RED AQM, it is found that the PI controller outperforms RED. However, as the number of flows increases, both AQMs struggle to maintain performance. To address this issue, the paper proposes an externally excited adaptive loop (EEAL) controller, which shows faster response to flow changes and provides a dynamic estimate of the number of flows. Overall, the EEAL AQM is shown to be more effective than the PI AQM in managing TCP flows.
41564	415644	The Complexity Of Concept Languages	Terminological Knowledge Representation Systems are designed to organize and process knowledge using taxonomies, or hierarchical structures. These systems use a specialized reasoning engine to make logical inferences based on the information in the taxonomy. The taxonomy is created using a concept language, which has a specific set-theoretic meaning. These systems are often praised for their efficient reasoning abilities, which is a major reason for their use.
41565	4156549	Interventions and belief change in possibilistic graphical models	Causality and belief change are crucial in various applications, and this paper focuses on these concepts in possibilistic graphical models. The authors demonstrate that interventions, which represent causal relationships between events, can be viewed as a belief change process. This can be handled using a possibilistic version of Jeffrey's rule of conditioning. The paper also addresses new challenges in revising graphical models when interventions are involved. The order of introducing observations and interventions is important and the structure of possibilistic networks must be modified to properly handle sequences of both. An efficient method for revising possibilistic causal trees is also presented.
41566	4156636	Merging Qualitative Constraint Networks in a Piecewise Fashion	We have developed a method for merging qualitative constraint networks (QCNs) that efficiently combines conflicting QCNs into a consistent global representation. This algorithm is not limited to a specific qualitative formalism and works by merging constraints between the same pairs of variables. We have also defined constraint merging operators that ensure the resulting QCNs meet logical expectations. This approach addresses the problem of merging QCNs in a flexible and effective manner.
41567	4156716	Probabilistic object recognition using multidimensional receptive field histograms	This paper discusses a probabilistic object recognition technique that does not require matching of images. It builds upon previous work using multi-dimensional receptive field histograms and extends it to compute the probability of an object's presence in an image. The method is shown to be robust and efficient, with low computational cost and linear complexity. Results from experiments with a database of 100 objects demonstrate the effectiveness of the approach. The technique can be used with a variety of filters and is not limited by changes in viewpoint. The paper also introduces a new technique for determining the probability of an object in a scene based on multidimensional receptive field histograms.
41568	4156826	On the size of computationally complete hybrid networks of evolutionary processors	An HNEP is a hybrid network of evolutionary processors, which is a graph where each node is associated with an evolutionary processor, a set of words, an input filter, and an output filter. The evolutionary processors have a finite set of mutations that can be applied to strings. The HNEP functions by rewriting words and redistributing them according to a communication protocol based on filtering. The filters are defined by random-context conditions. HNEPs can generate and accept languages, and this paper proves that any recursively enumerable language can be determined by a 7-node GHNEP and AHNEP. It also shows that GHNEPs and AHNEPs with only 2 nodes are not computationally complete.
41569	4156930	On competence in CD grammar systems	This paper explores the capabilities of cooperating distributed grammar systems (CDGSs) when the cooperation protocol is based on the level of competence on the underlying sentential form. A component is considered to be competent on a sentential form if it can rewrite a specific number of nonterminals in the string. By using this cooperation strategy, CDGSs can provide new characterizations of language families such as random context languages, context-free languages, and ET0L systems with random context. This research can contribute to solving long-standing problems in the theory of regulated rewriting.
41570	415705	Interactive Out-of-Core Visualisation of Very Large Landscapes on Commodity Graphics Platform	The Batched Dynamic Adaptive Meshes (BDAM) technique is a new, efficient way to render and manage large textured landscapes. It utilizes a paired tree structure, with a tiled quadtree for texture data and a pair of bintrees for geometry. These small triangular patches are created and optimized offline using high quality simplification and tristripping algorithms. At each frame, hierarchical view frustum culling and view-dependent texture/geometry refinement is performed with a stateless traversal algorithm. This allows for continuous adaptive terrain rendering using out-of-core data. The proposed technique is not processor intensive and takes full advantage of current graphics hardware. The results of a virtual fly-through over a textured digital landscape derived from aerial imaging are discussed.
41571	4157136	Easy Access to Huge 3D Models of Works of Art	Automatic shape acquisition technologies have advanced quickly, leading to the easy production of large amounts of 3D data. The high accuracy of range scanning technology makes it ideal for use in the Cultural Heritage field. However, two issues arise in this particular application: how to visualize the complex data on standard computers and how to make the visualization tools user-friendly for non-experts. To address these issues, a new visualization system has been developed for use in museums or expositions, with the ability to be used on the web. The system allows users to interact with a large 3D model and view linked multimedia data through a simple "point and click" approach. This is achieved through a continuous level-of-detail representation and automatic selection of the best-fit level of detail for efficient visualization.
41572	4157225	What Is the Set of Images of an Object Under All Possible Illumination Conditions?	The appearance of an object is influenced by both the perspective from which it is viewed and the lighting conditions. If two objects always have distinct appearances under different poses and lighting, then they can always be recognized. This paper examines the set of images of an object under variable illumination, including multiple light sources and shadows. The authors prove that for a convex object with a Lambertian reflectance function, the set of images under point light sources at infinity forms a convex polyhedral cone in n-dimensional space. This cone can be constructed from just three images and can be extended to objects with different shapes and reflectance functions. These findings have implications for object recognition and are supported by examples throughout the paper.
41573	4157325	Crowdsourcing translation: professional quality from non-professionals	Crowd-sourcing translations from non-professional translators can lead to poor and unpolished results without proper quality control. To improve the translation quality, we employ various methods such as obtaining multiple translations and choosing the best one, and using features to evaluate both the translations and translators. These features include factors like country of residence, language model perplexity, and editing rate. By scoring the translations using these features, we can distinguish between acceptable and unacceptable translations. We conduct a study using Mechanical Turk to recreate the NIST 2009 Urdu-to-English evaluation set and demonstrate that our models can produce translations of similar quality to professional translators at a significantly lower cost.
41574	4157459	Bootstrapping statistical parsers from small datasets	The co-training method presented here allows for the improvement of statistical parsers by using a small amount of manually parsed data and a larger pool of raw sentences. This approach has been proven effective in improving parser performance. The method is also applicable when the manually parsed data is from a different domain than the raw sentences or testing material. This shows that boot-strapping can still be useful even without manually produced parses from the target domain. 
41575	4157521	Cooperative gestures: multi-user gestural interactions for co-located groupware	Multi-user, touch-sensing input devices allow for the use of cooperative gestures, where multiple users' gestures are interpreted as one command. These gestures can enhance teamwork, increase awareness of system events, and add a unique aspect to activities. The paper explores potential uses for cooperative gestures, focusing on CollabDraw, a system for collaborative art and photo manipulation. The authors discuss design considerations and present a preliminary design framework. Future research in this area is also identified.
41576	4157654	Certificateless Ring Signatures.	Ring signature schemes allow a single person to sign on behalf of a group without revealing their identity. This is useful in various security applications, but implementing it with traditional public key infrastructure is complex and inefficient. Identity-based solutions also have drawbacks, making certificateless cryptography a better option. However, creating a certificateless ring signature scheme is challenging, as many require verifying the validity of multiple public keys. This paper presents the first certificateless ring signature scheme that does not require this step, making it more efficient for both the signer and verifier.
41577	4157762	BLAC: Revoking Repeatedly Misbehaving Anonymous Users without Relying on TTPs	Some credential systems allow users to authenticate anonymously, but this can lead to misbehavior. To address this issue, some systems allow selective deanonymization of users upon complaint to a Trusted Third Party (TTP). However, this punishment is too severe and cannot be generalized to subjective forms of misbehavior. A new system, BLAC, allows service providers to revoke credentials of misbehaving users without relying on a TTP. This allows for subjective judgment of misbehavior without fear of arbitrary deanonymization. Additionally, a d-strikes-out revocation policy is implemented, revoking users who have repeatedly misbehaved a certain number of times. This allows for the blocking of anonymous users who have engaged in subjective misbehavior, such as defacing web pages.
41578	4157883	Robust independent component analysis by iterative maximization of the kurtosis contrast with algebraic optimal step size.	Independent component analysis (ICA) is a statistical method used to break down a set of random variables into independent components. This is achieved through deflation-based implementations, such as the popular FastICA algorithm, which extract the components one by one. In this paper, a new method called RobustICA is introduced, which uses exact line search optimization to find the best step size for extracting the independent components. This method is efficient and can handle real and complex-valued mixtures without the need for prewhitening. It also has a high convergence speed and performs well on real-world data, as demonstrated in a comparison with other techniques on synthetic data and in the extraction of atrial activity in electrocardiograms.
41579	4157966	Support vector machine learning from heterogeneous data: an empirical analysis using protein sequence and structure	The use of large and diverse biological datasets requires a theoretical framework that can integrate different types of data, such as DNA and protein sequences, protein structures, and gene expression data. Kernel methods, specifically the support vector machine (SVM) algorithm, have emerged as a powerful tool for combining these datasets. Additionally, recent extensions of the SVM allow for weighting of different datasets based on their utility for a specific classification task. In this study, the performance of the SVM in predicting gene function from protein sequence and structure data was investigated. Results showed that the SVM is robust to noise in the input datasets and that a naive, unweighted combination of datasets can perform as well or better than a more sophisticated approach. However, the weighted approach may be necessary for experiments with multiple noisy datasets.
41580	4158076	Combining Software and Hardware Verification Techniques	The industry requires a practical technology to formally verify software and hardware co-designs. This is achieved by combining successful verification techniques from both domains, such as BDD-based symbolic model checking for hardware and partial order reduction for concurrent software programs. A modified version of partial order reduction is proposed in this paper, which can be used with any BDD-based verification tool. A co-verification methodology is also described, utilizing these techniques together. Experimental results show the effectiveness and potential for industrial use of this combined verification approach for moderate-sized systems. 
41581	4158121	The decision problem for the probabilities of higher-order properties	The probability of a property on the class of finite relational structures is the limit of the fraction of structures satisfying the property as the number of elements approaches infinity. This is known as the 0-1 law, which holds for properties expressible in first-order or fixpoint logic. However, the 0-1 law does not hold for second-order properties, and the decision problem for these probabilities is unsolvable. This article investigates logics that are more expressive than fixpoint logic and still adhere to the 0-1 law. Two logics, iterative logic and strict &Sgr;11, are studied and their associated decision problems are determined to be PSPACE-complete and NEXPTIME-complete, respectively. The proofs use combinatorial techniques such as generalizations of Ramsey's Theorem.
41582	4158237	Behavioral-level synthesis of heterogeneous BISR reconfigurable ASIC's	This paper discusses behavioral-level synthesis techniques for designing reconfigurable hardware, specifically for three applications: fault tolerance, manufacturability, and application specific programmable processors (ASPPs). The focus is on efficient built-in self-repair (BISR) methods, which involve replacing a failed module with a backup. However, the paper introduces new heterogeneous BISR approaches that allow for replacing a failed module with a spare of a different type. These approaches use behavioral-level synthesis to explore the design space and include two methods: assignment and scheduling and transformations. Experimental results demonstrate the effectiveness of these techniques.
41583	41583105	Exposure in wireless Ad-Hoc sensor networks	Wireless ad-hoc sensor networks bridge the gap between the Internet and the physical world. However, a major issue in these networks is coverage calculation, which is directly related to exposure. Exposure measures how well an object can be observed by the sensor network over time. In this study, the concept of exposure is formally defined and an algorithm for its calculation is developed. This algorithm is efficient and works for any distribution of sensors, sensor models, and network characteristics. It provides accurate results and its performance is evaluated through experimental results and scalability analysis. The algorithm can be used to determine the worst case coverage in sensor networks.
41584	4158427	On the computation of McMillan's prefix for contextual nets and graph grammars	Recent research has focused on using unfolding semantics for verification purposes. This began with a paper by McMillan, which presented an algorithm for creating a finite complete prefix of the unfolding of a safe Petri net. This allows for a condensed representation of the reachability graph. Extending this algorithm to contextual nets and graph transformation systems is challenging due to the potential for events to have multiple causal histories. A new abstract algorithm has been proposed that generalizes McMillan's construction for bounded contextual nets without encoding into plain P/T nets. To allow for an inductive definition of concurrency, the algorithm associates histories with both events and places. The proposed algorithm can also be applied to graph transformation systems, overcoming limitations of previous algorithms based on read arc encoding. 
41585	4158559	Hierarchical Design Rewriting with Maude	Architectural Design Rewriting (ADR) is a framework for designing dynamic software architectures using conditional rewrite rules and an algebraic presentation. The key features of ADR, including hierarchical design and inductively-defined reconfigurations, make it an expressive and versatile approach. This paper introduces Hierarchical Design Rewriting (HDR), a flavor of ADR that uses hierarchical graphs to handle system specifications with both symbolic and interpreted components. The authors also present a prototype tool based on Maude to support HDR. This demonstrates that HDR is both a formal approach and a practical framework for designing and analyzing software architectures. The paper also outlines a methodology for applying ADR concepts to other scenarios, using a specific algebra of designs and a particular scenario as an example.
41586	4158632	Ubiquitous sketching for social media	The rise of digital social media has revolutionized how we communicate and manage our relationships, but the use of sketching as a social medium has been largely overlooked. This is a missed opportunity, as sketches have the ability to convey visual ideas quickly and effectively, require minimal detail, and capture the personal touch of handwriting. The integration of sketching into social media has the potential to enhance communication and make information sharing more timely. A study of a system called UbiSketch, which combines digital pens, paper, and mobile phones, found that it enabled users to take advantage of sketching's unique benefits and enhance social interaction. This highlights the potential for ubiquitous sketching to enrich our communication practices.
41587	4158744	Task allocation learning in a multiagent environment: Application to the RoboCupRescue simulation	Coordinating agents in a complex environment is challenging, especially when certain task characteristics are unknown, such as the required number of agents. In these situations, agents not only need to coordinate on tasks, but also learn the necessary number of agents for each task. To address this issue, a selective perception reinforcement learning algorithm has been developed, allowing agents to learn the required number of agents for a given task. Despite the presence of continuous variables in the task description, the algorithm enables agents to learn their expected reward based on the task and number of agents. The algorithm has shown improved performance in the RoboCupRescue simulation environment.
41588	4158842	Haptic teleoperation of a mobile robot: a user study	The article discusses the issue of controlling a mobile robot through shared autonomy. The robot has an onboard controller for obstacle avoidance, while the operator uses a haptic probe to indicate desired speed and turning. The robot's sensors provide obstacle-range data, which is converted into forces and reflected to the operator's hand through the probe. This haptic feedback, along with visual information from a front-facing camera, helps the operator navigate the robot effectively. Experiments with both virtual and real environments have shown that this added haptic feedback improves operator performance and presence, resulting in fewer collisions and increased distance from obstacles, without a significant impact on navigation time.
41589	4158942	Probabilistic Principal Surfaces for Yeast Gene Microarray Data Mining	Recent technological advances have led to the production of large data sets across various scientific research fields. While each field has its own specific methods for analyzing data, there is a need for a more general approach to data analysis. In response, the concept of Probabilistic Principal Surfaces (PPS) has been proposed as a versatile tool for data visualization and clustering. PPS offers flexibility and can be applied to a wide range of data-rich fields. A real-life example of PPS being used to analyze yeast gene expression levels from microarray chips is also discussed to demonstrate its potential in data mining applications.
41590	4159035	Spectral indexes of quality, diversity and stability in fuzzy clustering.	In this article, the authors discuss the issue of quality assessment in clustering and propose new methods to measure the effectiveness of clusterings. They focus on using fuzzy paradigms to provide more flexibility in evaluating clusterings. The first approach involves analyzing the co-association matrix to evaluate individual clusterings. The second approach compares the co-association matrices of different clusterings using established partition comparison indexes. The authors also suggest using indexes from spectral graph theory to assess clustering stability and diversity in ensembles of clusterings. By incorporating these methods, a more comprehensive assessment of clustering quality can be achieved. 
41591	4159122	Vector quantization and fuzzy ranks for image reconstruction	Clustering and vector quantization are both methods used to organize and group data. Clustering uses a Voronoi partition, while vector quantization is a different technical problem. However, they have similarities in their approaches to creating a codebook and incorporating fuzzy concepts in their algorithms. The use of fuzzy concepts can also be applied in physical vector quantization systems, such as Neural Gas with a fuzzy rank function. This method can be used to improve the quality of images in lossy compression and reconstruction with vector quantization. 
41592	4159271	Phase-based features for motor imagery brain-computer interfaces.	Motor imagery (MI) brain-computer interfaces (BCIs) allow individuals to control devices using their thoughts. Most MI BCIs use power features in the mu or beta rhythms, but some also use phase synchrony measures like the phase-locking value (PLV). In this study, researchers compared the performance of different phase-based features, including instantaneous phase difference (IPD) and PLV, in controlling a MI BCI. They found that IPD was a strong control signal, with stable phase relations between channels over time. Offline and online trials showed high accuracy for differentiating between MI classes using IPD, with results ranging from 84% to 99%. This was higher than the accuracy achieved using amplitude features, which ranged from 70% to 100%.
41593	4159338	Extraction of Network Topology From Multi-Electrode Recordings: Is there a Small-World Effect?	The study of large-scale neural activity presents challenges for data analysis. To address this, the researchers propose a method for reconstructing functional networks from recorded spike trains. This involves using generalized linear models to estimate effective interactions between neurons, incorporating self-history, input from other neurons, and external stimuli. The resulting graph is analyzed for small-world and scale-free properties. The study found that cortical networks lack scale-free behavior but exhibit small-world structure. However, this structure is overestimated when only a localized sub-sample of neurons is used. This bias may affect previous experiments using multi-electrode recordings. 
41594	4159421	Definition, detection, and recovery of single-page failures, a fourth class of database failures	The traditional failure classes of system, media, and transaction failures have been expanded to include a fourth class called single-page failures. This class covers failures in reading data pages correctly, even after attempts at correction in lower system levels. To efficiently recover from these failures, a new data structure called the page recovery index is needed, which can be maintained transactionally with the same number of log records as current efficient logging and recovery methods. Detection and recovery of single-page failures can be done quickly enough to only cause a delay in data access, without the need to abort the transaction.
41595	4159514	Dual Generative Models For Human Motion Estimation From An Uncalibrated Monocular Camera	We have developed a new method for estimating human walking movements using images from a single camera. This method involves creating two models that represent the variability of walking patterns in terms of both kinematics and appearance. We also introduce a way to combine these models and a new algorithm for tracking and estimating walking in real-time. We tested our method on two different datasets and found positive results. Our approach has the potential to improve the accuracy of gait analysis and could be useful in various applications such as motion tracking and biometrics.
41596	4159623	The development of community members' roles in partnership research projects: An empirical study.	Over the past 3 years, the authors and their colleagues have worked with 11 nonprofit community groups to help them improve their control over information technology. Through a research project, the authors focused on informal learning methods that could benefit the groups and implemented them with key community representatives. In this article, the authors reflect on the progress of two individuals from different organizations in terms of their information technology skills and abilities. They identify four roles that individuals may take on in relation to technology-consumers, planners, doers, and sustainers-and discuss the methods used to promote informal learning. The authors also offer implications for future research on informal learning in community settings.
41597	415971	Distance realization problem in Network Tomography: A heuristic approach	This paper suggests a heuristic method for solving the distance realization problem in Network Tomography, where the goal is to estimate a network's internal structure and link performance using end-to-end measurements. The distance realization problem involves reconstructing a graph or topology from a matrix of pairwise distances between terminal nodes. While there are efficient algorithms for tree realization, finding the optimal realization for a general graph is proven to be NP-hard. The proposed heuristic approach involves three stages: finding a tree realizable distance matrix, constructing a tree, and adjusting for differences between the original and tree realizable distance matrices. It also aims to increase the network's betweenness-centrality measure while meeting the distance constraints.
41598	4159812	CARNA - alignment of RNA structure ensembles.	New algorithmic advancements have made it easier to compare RNA sequences using the gold standard of simultaneous alignment and folding. However, this approach has limitations as it only compares RNAs with a single consensus structure. To address this issue, a web server has been introduced that allows for multiple alignment of RNAs with multiple conserved structures and pseudoknots. The server uses base pair probability dot plots to represent input and output information, providing flexibility in the input and allowing for visual analysis of results. The server uses advanced algorithms to optimize all structural similarities in the input simultaneously, making it more efficient than conventional approaches. It performs on par with general-purpose RNA alignment tools and is freely available at http://rna.informatik.uni-freiburg.de/CARNA.
41599	4159975	A Concurrent G-Negotiation Mechanism for Grid Resource Co-allocation	To achieve the vision of the Grid, it is important to have a strong co-allocation of resources for computationally intensive applications. This can be challenging as resource providers and consumers may have different needs and goals. This work proposes a novel concurrent mechanism that coordinates multiple negotiations between a consumer and multiple resource providers, and manages commitments during the negotiation process. The mechanism includes a utility-based coordination strategy and three commitment strategies, and simulations show that it outperforms existing models in terms of utility, negotiation speed, and success rate. This work highlights the importance of efficient co-allocation in realizing the potential of the Grid.
41600	4160035	Trace semantics via determinization.	This paper discusses trace semantics in coalgebra theory and compares two different approaches that have been used to study trace semantics in recent years. The first approach uses final coalgebras in Kleisli categories and has some limitations, while the second approach utilizes final coalgebras in Eilenberg-Moore categories. The paper presents a systematic study of these two approaches and shows that they are equivalent in cases where both can be applied. This provides a better understanding of trace semantics and its applications in various types of transition systems.
41601	41601232	Behavioural biometrics: a survey and classification	This study focuses on behavioural biometrics, which involves identifying individuals based on their unique skills, style, preferences, knowledge, and strategies in daily tasks like driving, talking on the phone, or using a computer. The authors review current research and analyze the features used to describe different types of behaviour. They also compare the accuracy of various behavioural biometric methods for user verification and discuss potential privacy concerns that may arise from their use. Overall, this study provides a comprehensive overview of the current state and potential future implications of behavioural biometrics.
41602	4160278	A linear time algorithm to list the minimal separators of chordal graphs	Kumar and Madhavan developed a linear time algorithm for finding minimal separators in chordal graphs. This paper presents a new algorithm that also has a linear time complexity. However, unlike Kumar and Madhavan's algorithm, this one can work with any type of PEO, such as Lex BFS, instead of requiring a specific type (MCS PEO) to be computed first. This makes it more versatile and useful for chordal graphs with different properties.
41603	4160339	A Grammar-Guided Genetic Programming Framework Configured For Data Mining And Software Testing	Genetic Programming (GP) is a powerful technique used to solve various problems, but most researchers create customized GP tools for specific problems. These tools often require significant changes to be adapted to new domains. This paper introduces Grammar-Guided Genetic Programming (GGGP) as a solution to this limitation. The authors present a framework called Chameleon, which can be easily configured to solve different problems. The effectiveness of Chameleon is demonstrated through its use in two domains not commonly addressed by existing literature: mining relational databases and software testing. The results show that using the GGGP approach leads to more versatile GP frameworks and can be beneficial in diverse domains.
41604	4160440	Cost Tradeoffs in Graph Embeddings, with Applications (Preliminary Version)	An embedding of a graph G in a graph H is a one-to-one mapping of the vertices of G to vertices of H. The cost of this embedding can be measured by the dilation-cost, which is the maximum distance between adjacent vertices in G, and the expansion-cost, which is the ratio of the size of H to the size of G. This paper presents three cases where minimizing one of these costs results in a significant increase in the other. For example, there is an embedding of n-node complete ternary trees in complete binary trees with low dilation-cost but high expansion-cost. Similar tradeoffs are shown for generic binary trees, which can efficiently embed all n-node binary trees with minimal dilation-cost, but high expansion-cost. This concept is applied to computational systems with computation trees, such as Turing machines and context-free grammars.
41605	4160528	Online Ensemble Learning for Imbalanced Data Streams.	Cost-sensitive learning and online learning are both extensively studied, but there has been limited research on dealing with both issues simultaneously. To address this challenge, a new learning framework is proposed in this paper. The framework combines online ensemble algorithms with batch mode cost-sensitive bagging/boosting algorithms. This bridges two separate research areas and introduces a batch of theoretically sound online cost-sensitive bagging and boosting algorithms. Unlike other online cost-sensitive learning algorithms, the proposed algorithms have guaranteed convergence under certain conditions and have been validated with benchmark data sets. This demonstrates the effectiveness and efficiency of the proposed methods.
41606	4160674	Sketching freeform meshes using graph rotation functions	The article introduces a new method for creating free-form meshes with consistent topology. The 2D curve is interpreted as the projection of the 3D curve with the least curvature, and a topology-consistent strategy is used to trace simple faces on the interconnected 3D curves. This allows the system to automatically identify 3D surfaces. Delaunay triangulation is then applied to the boundary curves of the faces, and harmonic interpolation is used to compute the shape of the triangle mesh. Real-time algorithms are provided for curve generation and surface optimization. The incorporation of topological manipulation enhances the feasibility and benefits of automatically generated models in geometrical modeling.
41607	4160745	Semantic metadata mediation: XML, RDF and RuleML	This paper discusses the challenges of dealing with different types of information, such as data, metadata, and knowledge, in a decision-making system. The focus is on integrating heterogeneous metadata and proposing a mediation model that can reconcile the differences in structure and meaning. The ultimate goal is to provide users with a transparent view of the metadata from various sources, including XML, RDF, and RuleML models. This is achieved by using ontology to manage both structural and semantic heterogeneity. The proposed mediation architecture allows for the decomposition of complex queries into specific queries for each source, followed by the recombination of the results. 
41608	4160832	Initial Assessment of Architectures for Production Systems	This article discusses the use of production systems in artificial intelligence and expert systems and their limitations in terms of speed. The PSM project aims to address this issue by exploring the use of hardware support for production systems. The project's initial findings suggest that the most effective architecture for production systems is one with a small number of fast processors. The article also discusses the Rete algorithm used in production systems and the need for hardware solutions due to limited potential for speed improvements through software techniques. Different methods for speeding up production systems are also explored.
41609	4160950	Regularity and Conformity: Location Prediction Using Heterogeneous Mobility Data	Mobility prediction is crucial for providing proactive location-based services and intelligence for businesses and governments. Previous studies have shown that human mobility is highly predictable and influenced by social conformity. However, current prediction methods do not effectively combine these factors and struggle with incorporating diverse mobility data. To address these limitations, a new hybrid model is proposed that integrates both regularity and conformity of human mobility, and also incorporates a gravity model to learn location profiles from heterogeneous data. The model is evaluated using various city-scale datasets and outperforms existing approaches in terms of accuracy and percentile rank. Results also indicate that human mobility predictability varies over time, with weekdays being more predictable than holidays and predicting unvisited locations being more challenging on weekdays. 
41610	416109	Inferring common origins from mtDNA	In this article, the authors discuss how human migratory events can be traced through the study of DNA variations. By analyzing non-recombinant mtDNA and Y-chromosome sequences, it is possible to identify migrations that occurred between 30,000-70,000 years ago. Coalescence theory allows researchers to trace these sequences back to a common ancestor and use genetic drift mutations as markers to infer past migration and founder events. However, most mutations seen today are relatively recent and not useful for studying deep ancestry. The authors present results from analyzing 1737 mtDNA sequences and using principal component analysis and unsupervised ensemble clustering to identify substructure within haplogroups. They also introduce a new algorithm to mitigate sample size bias and reveal unbiased population events. The analysis shows that the African clades have the greatest heterogeneity and suggests that the M and N haplogroups originated from separate migrations out of Africa. The authors also identify and label branches of the mtDNA tree with high reliability and provide detailed SNP patterns for each haplogroup.
41611	4161156	A clustering algorithm for radial basis function neural network initialization	The paper proposes a new algorithm, Output-Constricted Clustering (OCC), for initializing Radial Basis Function Neural Networks (RBFNN). OCC first roughly divides the output into partitions and then clusters the data within each partition based on input complexity. The algorithm introduces the concept of separability, which helps determine the appropriate number of sub-clusters within each partition. This results in better initialization for RBFNNs and improved approximation performance compared to existing methods. The effectiveness of OCC is demonstrated through several examples. 
41612	41612103	G-ToPSS: fast filtering of graph-based metadata	RDF (Resource Description Framework) is becoming increasingly popular for representing metadata. One application of RDF on the Web is RSS (RDF Site Summary), which has gained significant popularity. However, the current operation of RSS systems is not efficient for large-scale distribution of content. To address this issue, the authors have developed G-ToPSS, a scalable publish/subscribe system that is suitable for applications dealing with high-volume content distribution from multiple sources. G-ToPSS utilizes ontology to provide additional information about the data and also supports RDFS (RDF Schema) class taxonomies. The authors have implemented and tested G-ToPSS, and the results show its superior scalability compared to other alternatives.
41613	4161343	Multivariate selection of genetic markers in diagnostic classification.	In the field of gene expression data analysis, machine learning models face unique challenges due to the large number of variables compared to the number of cases. Identifying relevant genes or groups of genes that can accurately classify a disease is crucial. While many machine learning algorithms have been developed for classification, gene selection has not been thoroughly explored. This study compares several algorithms for selecting gene markers for classification using the simple and efficient supervised learning algorithm of logistic regression. The results from 10 different datasets show that a conditionally univariate algorithm is a viable option for quickly identifying gene markers for disease. Furthermore, logistic regression shows similar performance to more complex algorithms and has a reasonable gene selection process. The algorithm is also easily accessible online, making it a useful tool for future research and development.
41614	4161441	WASP: Scalable Bayes via barycenters of subset posteriors.	Bayesian methods have been touted for their potential in handling big data, but their full potential has not been achieved due to the lack of scalable computational algorithms. To address this issue, a new approach called the Wasserstein posterior (WASP) has been proposed. This approach involves running a posterior sampling algorithm in parallel on different machines for subsets of a large data set, and then combining these subset posteriors using the Wasserstein barycenter through a highly efficient linear program. This results in an atomic form of the estimate, making it easier to estimate posterior summaries of functionals of interest. The WASP approach allows for easy scaling of smaller data sets to handle massive data, and theoretical justification is provided for its posterior consistency and algorithm efficiency. Examples of its application in complex settings such as Gaussian process regression and nonparametric Bayes mixture models are also presented.
41615	4161539	A generalized maximum entropy approach to bregman co-clustering and matrix approximation	Co-clustering is a data mining technique used in various applications such as text clustering, microarray analysis, and recommender systems. A recent approach has been proposed that applies to empirical joint probability distributions. However, there is a need for a more generalized co-clustering framework that can handle different types of matrices and constraints. In this paper, the authors introduce a framework that allows for the use of any Bregman divergence in the objective function and considers various conditional expectation based constraints. This approach is based on the minimum Bregman information principle, which is a generalization of the maximum entropy principle. The methodology also includes new algorithms and incorporates previously known clustering and co-clustering algorithms.
41616	4161631	Distributed Computing with Adaptive Heuristics.	This article discusses the use of distributed computing to study dynamic environments where computational nodes follow adaptive heuristics. These heuristics are simple rules of behavior, such as "best replying" and minimizing "regret", that have been extensively studied in game theory and economics. The article explores when these simple dynamics can converge to an equilibrium in asynchronous computational environments, and presents a non-termination result for a broad class of heuristics with bounded recall. The implications of this result in various applications, including game theory, circuit design, social networks, routing, and congestion control, are also discussed. Additionally, the article examines the computational and communication complexity of asynchronous dynamics and suggests further research in both distributed computing and game theory.
41617	4161759	Handling Missing Values in Rough Set Analysis of Multi-Attribute and Multi-Criteria Decision Problems	Rough sets are a useful tool for analyzing decision problems involving objects described in a data table with condition and decision attributes. However, in practical applications, the data table often contains missing values. To address this, an extension of the rough set methodology has been proposed. This extension applies to both the classical rough set approach based on indiscernibility relations and the new approach based on dominance relations. These adapted approaches consider missing values as directional statements comparing a subject to a referent object with no missing values. The resulting rules are robust as they are supported by at least one object with complete data in the corresponding attributes or criteria.
41618	4161829	Study on a Location Method for Bio-objects in Virtual Environment Based on Neural Network and Fuzzy Reasoning	The difficulty in accurately determining the position of a bio-object in a complex and uncertain environment is a major challenge in designing a manipulator location system. To address this issue, a research study focused on developing an accurate location method using binocular stereo vision in a virtual environment. An experimental platform was established to capture images of the bio-object and environment using binocular stereo vision. Errors in the handling process and imaging were identified and addressed through the use of a neural network and a training system. Fuzzy rule sets were also extracted and modeled to account for the various factors in a natural environment. A fuzzy reasoning model was created based on the experience of agricultural experts to improve the accuracy and intelligence of the location system. A location simulation system was developed using VC++ and EON SDK to automatically simulate the process of a picking manipulator locating a bio-object.
41619	4161977	STR3: A path-optimal filtering algorithm for table constraints.	Constraint propagation is a crucial aspect of Constraint Programming (CP) and involves using filtering algorithms to enforce Generalized Arc Consistency (GAC) on various types of constraints. Recent advancements in GAC algorithms for extensional constraints have focused on manipulating tables directly during search. One such approach is Simple Tabular Reduction (STR), which maintains tables of constraints and their relevant tuples. In this paper, a new GAC algorithm called STR3 is proposed, which is designed specifically for enforcing GAC during backtrack search. STR3 has the advantage of avoiding unnecessary traversal of tables, making it optimal along any path of the search tree. Experimental results show that STR3 is competitive with other state-of-the-art GAC algorithms. 
41620	4162041	Universal 2-State Asynchronous Cellular Automaton With Inner-Independent Transitions	This paper introduces a new type of cellular automaton, a computational model in which cells can have only two states and are updated asynchronously. The update function for each cell takes into account the states of neighboring cells at different distances. This automaton possesses the property of inner-independence, meaning that a cell's state only depends on its neighborhood and not its previous state. This is significant because previous research on inner-dependence has only been done in synchronous cellular automata. The model's universality is proven by constructing three circuit primitives that are universal for a specific class of circuits. This new automaton has potential applications in classical spin systems.
41621	4162112	Reversible computing and cellular automata—A survey	Reversible computing is a new approach to computing that aims to reflect the reversible nature of physical processes. This paper explores the concept of reversible computing, discussing how computation can be done in a reversible system, how a universal reversible computer can be built using reversible logic elements, and how these elements are linked to reversible physical phenomena. The paper also highlights the differences between reversible and conventional computing systems, and how even simple reversible systems or logic elements can have universal computational capabilities. Different models of reversible computing, including reversible logic elements, reversible Turing machines, and reversible cellular automata, are also discussed in this survey/tutorial paper.
41622	4162253	Proof planning with multiple strategies	Proof planning is a technique used in theorem proving that involves a knowledge-based planning process instead of a blind search. This approach utilizes mathematical knowledge at a human-level of abstraction and employs methods and control rules to create an abstract proof plan that can be expanded using tactics. The paper proposes more flexible refinements to this method by adding a strategic level of control through meta-reasoning. This allows for better cooperation between problem-solving strategies and is implemented in the Multi system. The effectiveness of this approach is demonstrated through several large case studies.
41623	416239	Resource management using multiple feedback loops in soft real-time distributed object systems	The paper presents a Resource Management System for a distributed object system that operates in real-time. The system uses a three-level feedback loop to manage resources, which includes a profiling algorithm to monitor resource usage, a least laxity scheduling algorithm to schedule tasks, and hot spot and cooling algorithms to balance loads by allocating and migrating objects. This system consists of a single Resource Manager for the entire distributed system, as well as a Profiler and a Scheduler on each processor. By utilizing these components, the Resource Management System can effectively manage resources and ensure efficient operation of the distributed object system.
41624	4162458	Maturity Model for Liquid Web Architectures.	Liquid Web applications are designed to seamlessly adapt to different connected devices and flow between them according to the user's attention. Unlike traditional centralized architectures, where the application's data and logic are stored on a single Web server, Liquid software requires decentralized or distributed architectures to achieve this mobility between clients. This is achieved by breaking down the application's architecture into layers, using the Model View Controller design pattern. A maturity model is proposed, which outlines different levels of application deployment and synchronization across multiple devices. The goal of this model is to help developers understand and control the design of Web applications that follow the liquid user experience paradigm, and provide a gradual path for existing applications to evolve.
41625	4162537	Team Knowledge and Coordination in Geographically Distributed Software Development	Coordination plays a crucial role in software development as it leads to various benefits such as cost savings, shorter development cycles, and better integration of products. Previous research has focused on team knowledge as the key factor in coordination, but this has primarily been studied in real-time collocated tasks. This study aims to fill the gap by investigating how team knowledge affects coordination in geographically distributed software work. The findings reveal that software teams have three main types of coordination needs- technical, temporal, and process- which vary depending on the individual's role. The study also shows that geographic distance has a negative impact on coordination, but this can be mitigated by shared team knowledge and presence awareness. Additionally, the results highlight the importance of shared task knowledge in coordination among collocated team members. The study proposes further research in this area to gain a better understanding of the impact of team knowledge on coordination in software development.
41626	41626140	A Bayesian Approach to Joint Feature Selection and Classifier Design	This paper introduces a Bayesian approach for simultaneously learning an optimal nonlinear classifier and identifying relevant predictor variables. Using heavy-tailed priors, the approach promotes sparsity in both the utilization of basis functions and features. These priors act as regularizers for the likelihood function, which rewards good classification on training data. An expectation-maximization (EM) algorithm is derived to efficiently compute a maximum a posteriori (MAP) point estimate of the parameters. The algorithm is an extension of existing sparse Bayesian classifiers, which are similar to support vector machines. Experiments using kernel classifiers show effective feature selection and high classification accuracy on various data sets.
41627	4162726	Dual greedy polyhedra, choice functions, and abstract convex geometries	The concept of dual greedy systems and their associated polyhedra, known as dual greedy polyhedra, have been studied by various researchers for different applications. These systems involve maximizing a linear objective function by finding tight inequalities in a greedy manner. Faigle and Kern, as well as Kruger, have looked at dual greedy systems in the context of antichains and partially ordered sets. Kashiwabara and Okamoto have studied them for extreme points of abstract convex geometries. Frank has proposed a related dual greedy algorithm for lattice polyhedra. This paper explores the relationships between dual greedy systems, substitutable choice functions, and abstract convex geometries. It also investigates the submodularity and facial structures of dual greedy polyhedra and considers an extension of this type of polyhedra. 
41628	4162852	The independent even factor problem	This paper presents a solution to the independent even factor problem, which is a generalization of both the matching problem and the matroid intersection problem. The authors establish a min-max formula for odd-cycle-symmetric digraphs, which includes the Tutte-Berge formula for matchings and the min-max formula of Edmonds (1970) for matroid intersection. They also propose a combinatorial algorithm that efficiently finds the maximum independent even factor in these digraphs, and this algorithm is a combination of two existing algorithms. The running time of the algorithm is O(n4Q), and it also provides a common generalization of two other decomposition methods. This algorithm gives a constructive proof of the min-max formula and introduces a new operation on matroids.
41629	4162969	Voyagers and voyeurs: supporting social data analysis	In recent years, there has been a rise in online services that use interactive visualizations to collect and analyze data. This has led to the emergence of social data analysis, where experts and non-experts can collaborate and interact with data. This approach has various applications, such as promoting political transparency, enhancing business intelligence, and facilitating citizen science. However, in order to fully realize the potential of social data analysis, there is a need for further innovation in the design of collaborative data management systems. The speaker will discuss recent efforts to use the internet as a platform for collective data creation, management, and analysis, and explore how these systems can be designed to foster social collaboration in data management and exploration. 
41630	4163036	Microdiversity on Rician fading channels	In this study, the performance of an L-branch equal gain combiner on both slow and nonselective Rician fading channels is analyzed and compared to matched filter receivers with coherent phase shift keying and noncoherent frequency shift keying modulation. The performance is evaluated based on the probability distribution of the signal-to-noise power ratio (SNR) and the average bit error rate (BER). Results using maximal ratio combining and selection diversity combining are presented for comparison. The study also investigates the effects of gain unbalance between branches of the combiner on the SNR distribution and BER. The paper suggests that equal gain combiners may be a feasible alternative to maximal ratio combiners. In addition, the study presents an efficient method for computing the distribution of sums of Rician random variables, which may be useful for other problems involving Rician fading. The suitability of using a Nakagami model to represent Rician fading environments is also examined and a formula for determining the corresponding parameters is provided. The results may be applicable to both microcellular and mobile satellite fading channels. 
41631	4163135	A Cellular Learning Automata-based Algorithm for Solving the Coverage and Connectivity Problem in Wireless Sensor Networks.	In wireless sensor networks, redundant nodes are common due to the likelihood of failures and the need for a long lifetime. To address this, a distributed algorithm is proposed which uses cellular learning automata to select a minimal subset of active nodes that cover the entire network area while also maintaining connectivity. Each node has a learning automaton that decides whether it should be active based on its remaining energy and its neighbors' situations. The algorithm also determines the transmission range of sensor nodes to ensure connectivity. The time and space costs are compared to existing algorithms and simulation results show its effectiveness, especially in high failure and energy depletion scenarios.
41632	4163236	A Verification Mechanism for Secured Message Processing in Business Collaboration	In order to ensure secure message processing in business collaborations, a verification mechanism based on access policies is crucial. This is because incorrect role assignments or unqualified roles in collaborating organizations can lead to authorization policy conflicts and unreliable business collaboration. To address this issue, a role authorization model called Role-Net has been developed using Hierarchical Colored Petri Nets (HCPNs). This model allows for the specification and management of role authorization in business collaborations. A property called Role Authorization Based Dead Marking Freeness has also been defined using Role-Net to verify the reliability of business collaborations according to partners' authorization policies. Additionally, an algebraic verification method for secure message processing has been introduced. 
41633	4163316	A probabilistic strategy for temporal constraint management in scientific workflow systems	Scientific workflow systems require timely completion of workflows, which is managed through temporal constraints. These constraints are set during workflow creation and updated during execution. However, current methods for setting constraints are user-specified and do not consider system performance, leading to frequent violations. Additionally, constraint updating is not well studied but is crucial for workflow management tasks. To address these issues, a probabilistic strategy utilizing a probability-based temporal consistency model is proposed. This strategy includes a negotiation process for setting coarse-grained constraints and a probability time deficit/redundancy propagation process for updating fine-grained constraints during execution. A case study demonstrates the effectiveness of this strategy. 
41634	4163456	Energy Restrained Data Dissemination In Wireless Sensor Networks	Wireless sensor nodes can move within a designated area and communicate with nearby nodes within protocol limits. Since all communication in sensor networks is wireless, a peer-to-peer protocol is used between nodes. Some key considerations for sensor network design include increasing bandwidth needs, fast data retrieval, and efficient data transfer over wireless networks to meet the diverse needs of users. Traditional routing protocols do not address power management concerns, but an energy-efficient routing scheme is crucial. The proposed energy-saving information dissemination scheme is discussed in this paper, with experimental results showing significant energy savings compared to previous methods. 
41635	4163532	Pattern-Based Query Answering	Many users struggle to access information stored in databases due to a lack of knowledge of schemas and structured query languages. Previous efforts have focused on keyword-based searches, with précis queries generating multi-relation database subsets instead of individual relations. However, current approaches have limitations such as relying on user-provided weights and constraints and difficulty capturing different query semantics and user preferences. To address these issues, the proposed pattern-based approach utilizes a repository of précis patterns, which are enriched with tuples from the database to generate logical database subsets. This allows for more efficient and accurate retrieval of information from databases.
41636	4163620	Modeling and language support for the management of pattern-bases	Information overloading is a growing concern in modern web-based information systems. One solution to this problem is using knowledge extraction methods to produce concise representations of data, known as patterns. However, there has been little focus on creating a unified environment for representing and querying different types of patterns. This paper proposes a Pattern-Base Management System (PBMS) to address this issue. The paper includes a formal definition of the logical foundations for managing patterns, along with a formalism for pattern specification and safety restrictions. Additionally, the paper introduces predicates for comparing patterns and query operators. 
41637	4163778	A Geodesic Active Contour Framework for Finding Glass	This paper discusses the challenge of detecting glass objects in images due to their appearance being influenced by the background. Instead of using a simple "glass or not" classification, the authors propose a more nuanced approach using binary criteria to guide the segmentation process. This entails combining measures of similarity between regions made of the same material and differences between regions made of different materials into an objective function. The geodesic active contour framework is then used to minimize this function and label pixels accordingly. The effectiveness of this approach is demonstrated through qualitative and quantitative experimental results. 
41638	4163860	Illumination Cones for Recognition under Variable Lighting: Faces	This paper discusses the challenge of recognizing objects in images that have varying illumination. To address this issue, the authors propose an appearance-based method that uses a small set of training images to generate a representation called an "illumination cone." This representation can model the complete set of images of an object with Lambertian reflectance map under any combination of point light sources. The method is tested on a dataset of 660 images of 10 faces and outperforms existing methods. This method is an implementation and extension of a previous illumination cone representation proposed in another paper, and it also takes into account cast shadows. 
41639	4163988	Ensembles of Restricted Hoeffding Trees	The success of simple classification methods has shown that complex attribute interactions are not always necessary for accurate predictions. This article proposes building an ensemble of Hoeffding trees, each limited to a small subset of attributes, to exploit this phenomenon in data streams. Exhaustive ensembles are created to consider all possible attribute subsets, and a weighting mechanism using sigmoid perceptrons is used to combine the trees and improve accuracy. The perceptrons' learning rate is set using the change detection method for data streams, and ensemble members are reset when they no longer perform well. Experiments show that this approach outperforms bagging for data streams, but at the expense of runtime and memory usage. Additionally, the proposed stacking method can further improve the performance of a bagged ensemble.
41640	4164012	New Applications of the Incompressibility Method.	The incompressibility method is a simple but effective way of proving mathematical statements. It has been successfully applied in various fields. In order to showcase its strength and beauty, we present new straightforward proofs using this method.
41641	4164146	Efficient Algorithms for the Closest String and Distinguishing String Selection Problems	The paper discusses three problems: the closest string problem, the farthest string problem, and the distinguishing string selection problem. These problems have various real-world applications, such as motif detection and genetic probe design. The closest string problem involves finding a center string that is within a certain Hamming distance from all input strings. The farthest string problem involves finding a center string that is at least a certain Hamming distance away from all input strings. The distinguishing string selection problem involves finding a center string that is close to one group of strings and far from another group. The paper presents improved fixed parameter algorithms for each of these problems, with running times of O(Ln + nd 23.25d ), O(Ln + nd 2^{3.25d_b}), and O(Ln + nd 2^{3.25d_b}) respectively.
41642	4164277	Selective Hardening in Early Design Steps	Hardening a circuit against soft errors should be done early in the design process to minimize costs. A successful approach is to target specific parts of the circuit that are critical and likely to cause system dysfunction if an error occurs. However, determining critical areas can be difficult without complete information. A gate-level selection strategy, without considering electrical or timing details, is proposed to identify these critical spots. The effectiveness of this strategy is confirmed using a new SER estimator. Despite using limited information, the results show a significant reduction in soft error susceptibility, surpassing other known topological strategies in terms of hardware and protection. 
41643	416437	Typing on an Invisible Keyboard.	In order to save screen space, researchers have explored the idea of an invisible keyboard for mobile devices. Despite some challenges with accuracy and key overlaps, users were able to recall key positions and improve their typing speed with practice. By adapting the spatial model, the invisible keyboard was found to be 11.5% faster than simply hiding the keyboard. A 3-day user study showed that with practice, the typing speed on an invisible keyboard could reach a practical level, approaching that of a regular visible keyboard. Overall, the research suggests that an invisible keyboard with an adapted spatial model could be a viable and promising option for text entry on mobile devices.
41644	416446	Integrated Grasp Planning and Visual Object Localization For a Humanoid Robot with Five-Fingered Hands	This paper introduces a framework for grasp planning with a humanoid robot arm and a five-fingered hand, specifically for grasping objects in a kitchen environment. The framework utilizes an object model database that contains descriptions of all possible objects in the robot's workspace. Two modules, an offline grasp analysis system and a real-time stereo vision system, use this database to determine the best grasp for each object. The offline system uses simulation and CAD models, while the stereo vision system locates objects in real-time using appearance-based and model-based methods. These components are integrated in a controller architecture to achieve manipulation tasks for the robot. 
41645	41645106	Segmentation and learning of unknown objects through physical interaction	This paper presents a new approach for a humanoid robot to segment and learn about unknown objects without any prior knowledge of the objects or environment. The only requirements are that the object has a smooth surface with distinctive visual features and moves as a rigid body. The robot uses its visual and manipulative abilities to push hypothetical objects and gather information, allowing for successful segmentation and learning of the object's appearance from multiple angles. This model enables robust object recognition in cluttered environments.
41646	416462	On the Convexity Number of Graphs	Convexity is a property of a set of vertices in a graph, where the set contains all vertices that lie on the shortest paths between any two vertices in the set. The maximum number of vertices in a convex set that does not include all vertices in the graph is known as the convexity number. The problem of determining whether the convexity number of a given bipartite graph is greater than or equal to a given integer is shown to be NP-complete. Additionally, the study of graphs with small convexity numbers reveals necessary extension properties and their relationship with upper bounds on the convexity number.
41647	4164735	Learning spectral graph segmentation.	The authors propose a graph learning algorithm for spectral graph partitioning that enables supervised learning of graph structures using labeled data. The algorithm uses gradient descent in the space of all feasible graph weights and involves computing the derivatives of eigenvec- tors with respect to the graph weight matrix. The authors show that these derivatives can be computed in an exact analytical form using the theory of implicit functions and demonstrate that the gradient converges exponentially fast in a simple case. In the context of image segmentation, the authors also demonstrate how to incorporate high level object prior into a shape detection process.
41648	4164830	Recognition of probe cographs and partitioned probe distance hereditary graphs	A graph G is a probe graph of a given class of graphs if it can be partitioned into two sets, one of which is an independent set, and then embedded into a graph of that class by adding edges. If the partition is part of the input, then G is a partitioned probe graph. This article presents a polynomial-time algorithm for recognizing partitioned probe distance-hereditary graphs, using a new data structure. The running time of the algorithm is ${O}(\mathfrak\it{n}^2)$, where $\mathfrak\it{n}$ is the number of vertices in the input graph. It also shows that both partitioned and unpartitioned probe cographs can be recognized in ${O}(\mathfrak\it{n}^2)$ time.
41649	4164915	Run-Time Support for Distributed Sharing in Typed Languages	DOSA is a new run-time system that efficiently supports shared objects in a typed programming language. It is able to distinguish between pointers and data at run-time, which allows for efficient fine-grained sharing through VM support. This eliminates false sharing and improves performance for fine-grained applications. Unlike previous systems, DOSA's approach allows it to perform equally well on both fine-grained and coarse-grained applications. Its architecture also allows for unique optimizations that are not possible in traditional shared memory systems. 
41650	4165050	Synthesis Of Low-Cost Parity-Based Partially Self-Checking Circuits	The article discusses a method for creating efficient partially self-checking multilevel logic circuits with low-cost concurrent error detection (CED). By selecting a subset of inputs and disabling CED when these inputs meet certain criteria, the CED circuitry can be optimized. This methodology is particularly effective for targeting common faults and has shown to significantly reduce error rates in logic circuits. The approach is ideal for applications where cost-effective CED solutions are desired. Experimental results demonstrate the success of this method in achieving a more reliable and cost-efficient logic circuit design.
41651	4165123	Smart-Card Implementation of Elliptic Curve Cryptography and DPA-type Attacks	This paper examines the effectiveness of smart-card implementations of elliptic curve cryptography in protecting against side-channel attacks, specifically those using differential power analysis (DPA) and its variations. The use of random curve isomorphisms is a promising method for preventing DPA attacks, but its implementation must be carefully considered. The paper presents several generalized DPA attacks against improperly implemented curve isomorphisms, including a second-order attack against an additive variant and a refined attack against a more general variant. Additionally, the paper provides a precise analysis of second-order DPA attacks, which is relevant for future research in this area. 
41652	4165235	Discovery and hot replacement of replicated read-only file systems, with application to mobile computing	The "hot replacement" mechanism described in this content allows for the replacement of files in a read-only file system while it is still mounted, without the user being aware. This can improve fault-tolerance and performance. The mechanism monitors the latency of operations on the file system and automatically seeks a replacement file system if the latency degrades. This is particularly useful for mobile computers that may move over a wide area, leading to variable response times and potential problems with latency, failures, and scalability. If a mobile client moves through regions with partial replicas of common file systems, this mechanism can provide increased fault tolerance and more consistent performance.
41653	4165348	Image Retrieval Using Maximum Frequency of Local Histogram Based Color Correlogram	Color histogram and color Correlogram are two commonly used techniques for image indexing in content-based image retrieval (CBIR). While color histogram provides a global representation of the color distribution in an image, it is not robust to large changes in appearance and may yield similar results for different images with the same color distribution. On the other hand, color Correlogram takes into account both the color distribution and spatial information of pixels, making it a more efficient method for CBIR. The proposed algorithm in this paper uses a combination of histogram value divisions and maxima of frequencies to generate a Correlogram, which is then used to calculate the distance between query and database images. The algorithm is tested on a large database of images and shows promising results. 
41654	4165439	Deriving Production Rules for Incremental View Maintenance	Database systems use production rules to automatically maintain derived data, such as views. However, writing these rules can be a difficult and ad-hoc process. To simplify this task, a facility has been developed where a user can define a view using an SQL select expression. The system then automatically generates set-oriented production rules that maintain a materialization of the view. These rules are triggered by operations on the view's base tables and generally perform incremental maintenance by modifying the materialized view according to changes made to the base tables. However, for some operations, more significant recalculations may be needed. Algorithms have been created to analyze the view definition and determine when efficient maintenance is possible based on key information.
41655	4165562	Shape Based Detection and Top-Down Delineation Using Image Segments	The authors propose a segmentation-based detection and figure-ground delineation algorithm that primarily relies on the shape of objects instead of their appearance. The algorithm takes in an image and its bottom-up hierarchical segmentation and uses shape and color information to find a "coherent whole" or collection of segments that consistently vote for an object. A novel top-down figure-ground segmentation process is then applied to accurately delineate the object's boundaries by letting voting segments compete for interpreting each semantic part. This method can accurately detect and segment objects with complex shapes and handle occlusions, achieving results comparable to existing state-of-the-art methods. 
41656	4165653	Genetic Programming for data classification: partitioning the search space	Genetic Programming is a method used to evolve decision trees for data classification, but it often leads to large search spaces. To address this issue, several techniques from machine learning are used to refine and reduce the search space sizes for decision tree evolvers. These refinement methods have been found to improve the classification performance of the algorithms.
41657	4165718	Reverse engineering distributed algorithms	Distributed systems are complex and difficult for humans to understand, making informal reasoning unreliable. To address this, formal tools are needed for building and maintaining these systems. The authors propose a formal approach called "coarsement" for reverse engineering distributed systems, where an implementation is transformed into a high-level specification through a series of intermediate steps. This method adds structure to the system by breaking it down into layers, making it easier to understand and reason about. The authors demonstrate the effectiveness of this approach by applying it to a real-world distributed algorithm for message passing in a network.
41658	4165828	Automatic Detection of Feature Interactions Using Symbolic Analysis and Evolutionary Computation	Phorcys is a new approach for detecting unwanted failures caused by overlapping, conflicting behavior in independently-developed features, known as feature interactions. It uses both symbolic analysis and evolutionary computation at the requirements level to identify these failures, rather than looking for specific unwanted interactions. This allows for more comprehensive detection and provides guidance for mitigation strategies. Phorcys is the only technique currently available to use this combination of methods for detecting failures caused by feature interactions. The approach is demonstrated through its application to an automotive braking system with multiple subsystems. 
41659	4165933	The Agent Reputation and Trust (ART) Testbed Architecture	The ART Testbed is a new initiative that aims to establish a testbed for technologies related to agent reputation and trust. It serves as a competition platform for researchers to compare their technologies and also provides tools for customizable experiments. The testbed focuses on the art appraisal domain, where agents gather opinions from other agents to accurately value paintings for clients. This paper provides an overview of the domain problem and explains the implementation architecture of the testbed, including its Game Server, Simulation Engine, Database, User Interfaces, and Agent Skeleton. 
41660	4166092	Reconfiguration of Vibro-tactile Feedback Based on Drivers' Sitting Attitude	The use of sensory modalities such as vision and hearing in vehicles has increased due to the development of assistance systems, but this has also led to cognitive overload for drivers. Adding the sense of touch as an interaction channel can help alleviate this overload in a natural and non-distracting way. However, haptic feedback is affected by both the driver and environmental conditions, such as different body sizes and seating positions. To address these issues, a vibro-tactile seat with sensors and actuators can dynamically adjust the haptic output to ensure consistent perception for all drivers. This allows for a more intuitive and effective use of haptic technology in vehicles.
41661	4166159	Neural Animation and Reenactment of Human Actor Videos.	The proposed method for generating realistic animations of real humans involves using a video sequence and a controllable 3D template model of the person. This approach significantly reduces production costs compared to traditional methods that require a high-quality 3D model. A neural network is trained to translate simple synthetic images of the human model into realistic images. The network is trained using a combination of motion tracking and synthetic rendering to generate realistic imagery. This method can be used for reenactment and editing of existing videos. The results outperform other learning-based human image synthesis techniques. 
41662	416627	Improving multi-objective code-smells correction using development history	This paper presents a new approach to automated software refactoring recommendation, using a multi-objective optimization problem. The approach takes into account both the change history of the system and past refactorings applied to similar contexts. It is validated on five medium and large size software systems and compared to three state-of-the-art approaches and two existing metaheuristic algorithms. The proposed approach aims to improve the quality of software systems by minimizing code-smells, utilizing development history, and preserving construct semantics. The non-dominated sorting genetic algorithm (NSGA-II) is used to find the best trade-offs between these objectives. The results show that the approach is effective, with a high percentage of code-smells fixed and suggested refactorings being semantically coherent when the change history is incorporated.
41663	4166346	Vector bin packing with multiple-choice	In multiple-choice vector bin packing, we have n items that can be selected in different D-dimensional forms. There are T bin types, each with a cost and size. The goal is to pack the items in bins with the lowest overall cost. This problem is useful for scheduling in networks with guaranteed QoS, but has other applications too. We present an algorithm that approximates the solution with a cost of about ln D times the optimum. It runs in polynomial time when D=O(1) and T=O(logn). This expands on previous results for regular vector bin packing. We also present a PTAS for the multiple-choice version of multidimensional knapsack, where the goal is to pack a maximum weight set of items in one bin.
41664	4166457	Directional Coherence Interpolation For Three-Dimensional Gray-Level Images	The paper introduces a new 3D gray-level interpolation method, called Directional Coherence Interpolation (DCI). This approach improves the visual quality of 3D rendering compared to traditional interpolation methods by utilizing directional image-space coherence. DCI estimates the maximum coherence directions (MCD) from local image intensity and incorporates a smoothness term. To enhance efficiency and robustness, a pyramidal search strategy is also utilized. This approach can incorporate image shape and structure information without the need for explicit object boundary representation. Experiments on synthetic and real medical images show that the proposed method is accurate and efficient in handling general object interpolation. 
41665	4166513	Modeling and rendering of metallic patinas	This paper discusses the importance of weathering in image synthesis and presents an approach for modeling and rendering metallic patinas. Patinas are a film or incrustation on a surface caused by material removal, addition, or chemical alteration. The approach uses a layered structure and a collection of operators to simulate patina development, taking into account the object's geometry and environmental factors. The Kubelka-Munk model is used to represent the reflectance and transmission of light through the layers, resulting in a realistic simulation of the time-dependent appearance of metals. The approach is demonstrated with copper models. The paper falls under the CR categories of Computer Graphics: Three-Dimensional Graphics and Realism, and Computer Graphics: Methodology and Techniques.
41666	416668	Formalization of resilience for constraint-based dynamic systems	Researchers in various fields are interested in creating systems that can withstand and recover from large-scale unexpected events. Current methods for evaluating resilience either focus on qualitative aspects or specific domains. This paper introduces a comprehensive computational model that can represent a wide range of constraint-based dynamic systems. Inspired by existing literature, the authors propose a parameterized property that captures the key elements of resilience regardless of the application domain. This new resilience property is used to assess the resilience of constraint-based dynamic systems.
41667	4166768	A fixpoint characterization of abductive logic programs	A new approach to understanding abductive logic programs is presented, where the belief models are defined as the fixpoint of a disjunctive program created through a program transformation. This transformation combines both negative and positive hypotheses in a consistent manner. The method is extended to cover abductive extended disjunctive programs and allows for a parallel bottom-up model generation process for computing abductive explanations from various types of programs with integrity constraints. This approach simplifies the process of finding abductive explanations and can be applied to a wide range of programs.
41668	4166821	Dynamic Logic for Plan Revision in Agent Programming	This paper introduces a dynamic logic specifically designed for the agent programming language 3APL, which allows agents to have beliefs and plans that can be revised during execution. This makes it difficult to analyze plans using standard propositional dynamic logic. The proposed dynamic logic is tailored to handle plan revision and has a sound and complete axiomatization. It also discusses how this logic can be extended to non-restricted plans and provides examples of proofs using the logic. The paper also explores the connection between proving properties of 3APL agents and proving properties of procedural programs.
41669	416696	Admission control and scheduling for QoS guarantees for variable-bit-rate applications on wireless channels	Ensuring quality of service (QoS) over unreliable wireless channels is a major challenge for future applications. A model has been proposed to describe QoS requirements based on four criteria: traffic pattern, channel reliability, delay bound, and throughput bound. This model has been extended to handle variable bit rate applications and a sharp characterization of schedulability has been obtained. The results can be applied to various wireless applications, including video streaming, VoIP, and wireless sensor networks. Two main issues in QoS over wireless are admission control and scheduling. A necessary and sufficient condition for feasibility of a set of variable bit-rate clients has been analytically derived based on the model. Two scheduling policies have been proposed and shown to be optimal, and can be easily implemented on the IEEE 802.11 standard. Simulation results support the theoretical findings.
41670	416705	Weak Unit Disk and Interval Representation of Planar Graphs.	The article discusses a problem involving representing vertices of a planar graph using unit balls, where the balls representing adjacent vertices must intersect if their corresponding edge is labeled as "near." The problem is proved to be NP-hard, but it is shown that all series-parallel graphs can be represented in this way. The problem is also shown to be equivalent to a variant of graph coloring when considering the line. Examples of girth-4 planar and girth-3 outerplanar graphs that cannot be represented with unit intervals are given, but it is shown that all triangle-free outerplanar graphs and graphs with maximum average degree less than 26/11 can be represented. This provides a simple proof for the representability of planar graphs with large girth.
41671	41671130	Approximate sequencing for variable length tasks	Czumaj et al. analyzed the Variable Length Sequencing Problem (VLSP) in their paper, which focused on efficient information gathering on the Web. They proved that VLSP is NP-complete and provided a polynomial time algorithm for a limited version and an approximation algorithm for a slightly less restricted version. However, the difficulty of the problem was further clarified by showing it is NP-complete to approximate within a factor nk for any fixed integer k. It was also proven that finding the optimal solution is NP-hard, even when the jobs follow a periodic property. To address the NP-hardness of VLSP, the paper introduces an optimal version that maximizes the number of completed tasks and presents an approximation algorithm with a factor of 2 and a polynomial time algorithm for a special case with restricted types of tasks.
41672	4167242	Coordinating Self-interested Planning Agents	This article discusses planning problems where multiple agents must work together on a set of tasks. Each agent is responsible for a subset of tasks and must create their own plan. However, since the agents are non-cooperative, they do not want to revise their individual plans when combining them into a joint plan. The article presents a formal framework for studying the computational aspects of this coordination problem and identifies factors that contribute to its complexity. An example of applying this framework to multi-modal logistic planning is also provided.
41673	4167376	Effectively closed sets and graphs of computable real functions.	This paper compares the computability and complexity of a continuous real function with its graph. It also analyzes the computability and complexity of functions on subspaces of the real line, such as the Cantor space, the Baire space, and the unit interval. Four types of effectively closed sets are defined based on the recursive enumerability of sets of closed or open intervals with nonempty or empty intersection with the set. The relationships between these four types of effectively closed sets are studied, both in general and for closed sets that are graphs of continuous functions. 
41674	4167457	A Light-Weight Framework for Bridge-Building from Desktop to Cloud.	In recent years, there has been a growing use of computational techniques in scientific research, allowing for virtual experiments to be conducted through computer modeling. This trend has expanded from large-scale projects to smaller ones, and has the potential to foster collaboration between researchers. The development of web service and cloud technology further supports this trend, but there are challenges in terms of accessibility for non-experts. To address these issues, a framework has been developed that allows for the deployment of web services on cloud infrastructure without modifications or technical knowledge. This framework has been tested and proven useful in image processing and multi-disciplinary optimization.
41675	4167517	Propagative hough voting for human activity recognition	Hough-transform based voting has been successfully used for object and activity detection, but it can struggle with limited training data. To address this, a new method called propagative Hough voting is proposed for activity analysis. Instead of individual feature voting, random projection trees (RPT) are used to match feature points in a high-dimensional feature space. These trees can index unlabeled feature points in an unsupervised way and propagate label and spatial-temporal information from training samples to testing data. This method does not rely on human detection and tracking, making it robust to variations in activity patterns. It outperforms state-of-the-art techniques on two benchmarked activity datasets, even with limited training data.
41676	4167643	Constrained clustering with local constraint propagation	This paper discusses the problem of multi-class constrained clustering, where pairwise constraints are used to specify which data points belong to the same or different clusters. The authors propose a new algorithm, called Local Constraint Propagation (LCP), which can effectively incorporate the influence of these constraints onto the unconstrained data. LCP not only reveals the structures of the clusters, but also takes into account the influence of all pairwise constraints on each data point. The effectiveness of LCP is demonstrated through experiments on image segmentations.
41677	4167737	ATL with Strategy Contexts: Expressiveness and Model Checking	This study focuses on the alternating-time temporal logics ATL and ATL(star) with the addition of strategy contexts, which allow agents to commit to strategies during formula evaluation. This is in contrast to plain ATL and ATL(star) where strategies can be reset. The use of strategy contexts is shown to have significant expressive power, as it makes the extended logics ATL(sc) and ATLs(c)(star) equally expressive. Despite this, the study also proves that the model-checking problems for these logics remain decidable. This is achieved through the development of a tree-automata-based algorithm for model-checking ATL(sc)(star) on n-player concurrent game structures.
41678	4167828	Analyzing Program Analyses	A complete static analysis of a program means that no errors or imprecisions arise when querying its behavior in both the concrete and abstract interpretations. This is important for gaining confidence in static analysis alarms. The completeness class of an abstraction refers to the set of programs for which the abstraction is complete. It is not recursively enumerable, and a stratified deductive system is used to prove the completeness of program analyses. The main sources of incompleteness are assignments and Boolean tests, and the proof system has two layers - one generic and one abstraction-specific. The second layer can be instantiated with different abstract domains, such as Intervals and Octagons, to provide necessary and sufficient conditions for completeness. 
41679	4167966	Shallow Circuits with High-Powered Inputs	The paper discusses the importance of a deterministic black-box identity testing algorithm for univariate polynomials, which would have implications for the complexity of the permanent. The current known lower bounds from derandomization of multivariate identity testing are weaker. The paper proposes a new approach by focusing on a specific type of polynomials: sums of products of sparse polynomials with sparse coefficients. This leads to new versions of the Shub-Smale tau-conjecture on integer roots of univariate polynomials. The paper also introduces a new result on reduction to depth 4 for arithmetic circuits, which has implications for the lower bound on the size of depth 4 circuits computing the permanent. A slightly superpolynomial or weaker bound on the number of real roots would suffice to obtain this lower bound.
41680	4168026	The partitioned exponential file for database storage management	The rate of increase in hard disk storage capacity is growing faster than the rate of decrease in hard disk seek time. This means that the value of a seek is increasing exponentially relative to the value of storage. To address this trend, a partitioned exponential file (PE file) has been introduced as a customizable storage manager for various types of data. The PE file is designed for use in environments with high update loads and concurrent analytical queries, such as in long-running scientific applications. The proposed Large Synoptic Survey Telescope will produce massive amounts of data, making the PE file a useful tool. The PE file is organized for sequential I/O, reducing the time required for record insertion or retrieval. Benchmarking experiments have been conducted for T1SM, a PE file customized for multi-attribute data records organized on a single numerical attribute. Various data organizations, including the B+-Tree, LSM-Tree, Buffer Tree, Stepped Merge Method, and Y-Tree, have been tested, with T1SM performing well in many cases. It is particularly effective in handling heavy query workloads and intense insertion streams concurrently, maintaining low query latencies.
41681	4168126	A bayesian mixture model with linear regression mixing proportions	Classic mixture models have a fixed prevalence of components, making it difficult to learn how data distributions change over time. To address this, we propose models and Bayesian learning algorithms that can infer the temporal trends of mixture components. We demonstrate the effectiveness of our methods by applying them to tracking changes in antibiotic resistance rates in two bacteria. Our results show that our models can accurately identify temporal patterns in antibiotic resistance. This approach has potential for other applications where understanding the evolution of complex data distributions is important.
41682	416823	Impact of interconnection-free biomolecular computing	The article discusses a new model for a computing system that does not rely on traditional interconnections. Instead, it utilizes biodevices and molecular information to perform logical operations in parallel. The system works by distributing logical information through different types of molecules and using enzymes for parallel selection. The potential impact of this interconnection-free computing on highly parallel processing architectures is explored through the design of parallel sorting networks. This innovative approach has the potential to greatly improve the efficiency and speed of computing systems.
41683	4168352	Reasoning about the transfer of control	DCL-PC is a logic that allows for reasoning about the abilities of agents and coalitions of agents when control is transferred from one agent to another. It is built upon the foundation of CL-PC, a logic for cooperation that uses Boolean variables to represent the choices available to a coalition. DCL-PC adds dynamic logic modalities to reason about control transfer, and can be semantically interpreted in two ways - a direct semantics and a Kripke semantics. The logic is proven to be equivalent in both semantics and has an axiomatization. The computational complexity of model checking and satisfiability in DCL-PC is shown to be PSPACE-complete. Finally, the distinction between first-order and second-order control is explored, with a logical characterisation provided for second-order control.
41684	4168473	Toward Intelligent Biped-Humanoids Gaits Generation	This chapter discusses experimental studies on analyzing natural human walking and presents a biologically inspired design for a simplified bipedal locomotion system for humanoid robots. The design is based on human walking analysis and mimics the mechanism and control of human muscles. A hybrid algorithm is proposed for generating walking gaits, which is a novel approach compared to traditional methods that use kinematics and dynamic equations. The algorithm utilizes particle swarm optimization and can be applied to small humanoid robots with at least six degrees of freedom in the knee, ankle, and hip joints.
41685	4168559	Deliberation in Equilibrium: Bargaining in Computationally Complex Problems	The authors propose a normative theory of interaction, specifically negotiation, between self-interested and computationally limited agents. They focus on a 2-agent setting where each agent has a complex problem and there is potential for collaboration to solve a joint problem. At any time, agents can compute to improve their own solution, their opponent's solution, or the joint solution. At a deadline, the agents must decide whether to implement the joint solution and how to divide its value. The authors present a model for controlling anytime algorithms and analyze the perfect Bayesian equilibria for various scenarios. They also provide algorithms for finding these equilibria.
41686	4168687	LOOM: optimal aggregation overlays for in-memory big data processing	Aggregation is a crucial aspect of processing big data, as it involves distilling information from large datasets. Common operations such as top-k matching and word count rely on fast aggregation, but existing frameworks like MapReduce do not explicitly consider or optimize it. In recent "online" approaches to big data analysis, where data is stored in main memory across nodes, the impact of aggregation time on overall job completion time becomes even more significant. To address this, LOOM is a system designed for efficient big data aggregation within analysis frameworks. It supports two-phased computations and uses heuristics to construct optimized aggregation overlays, resulting in improved performance in microbenchmarks and real-world scenarios. 
41687	4168718	Efficient Tree Layout in a Multilevel Memory Hierarchy	The problem of organizing a tree or trie in a hierarchical memory to minimize block transfers during a search operation is explored. Previous research has shown optimal but complex algorithms when the block-transfer size is known. A simple greedy algorithm is proposed that is almost optimal, while a more flexible but slightly less efficient algorithm is also presented. The latter is extended to the cache-oblivious setting where the block-transfer size is unknown, providing a solution for a multilevel memory hierarchy. The query performance of this layout is only slightly worse than the optimal layout with known block size.
41688	41688102	Fast Counting with Bounded Treewidth	The treewidth of a structure can greatly impact the difficulty of solving a problem, with a constant treewidth making it more tractable. Courcelle's Theorem states that any property defined by Monadic-Second Order (MSO) sentences can be solved efficiently if the treewidth is bounded. This has been extended to counting problems, but the MSO description is not an algorithm. New approaches, such as monadic datalog, have been developed to create efficient algorithms for decision problems based on Courcelle's Theorem. In this paper, the authors apply this approach to fundamental counting problems in logic and artificial intelligence.
41689	4168927	Chromatic Adaptation Performance Of Different Rgb Sensors	This paper compares the performance of different chromatic adaptation transforms (CAT) and transforms based on RGB primaries in imaging systems. The studied CATs include von Kries, Bradford, Sharp, and CMCCAT2000, while the RGB primaries investigated are ROMM, ITU-R BT.709, and "prime wavelength" RGB. A von Kries model is used to linearly scale post-adaptation cone responses with illuminant dependent coefficients. The evaluation is done using 16 sets of corresponding color data and three error prediction metrics. The results show that traditional CATs, Sharp CAT and CMCCAT2000, perform best, but some transforms based on RGB primaries also exhibit good chromatic adaptation behavior. This suggests that white-point independent RGB spaces can be defined for image encoding, assuming the adequacy of the linear von Kries model.
41690	4169031	On Merging Strategy-Proofness	Merging operators are used to combine the beliefs and goals of a group of agents from each individual member's beliefs and goals. When an agent has a preference for the outcome of the merging process, they may try to manipulate it by lying about their true beliefs and goals. This can lead to a more favorable outcome for that agent, but it also raises questions about the fairness and accuracy of the merged result. To ensure a fair process, it is important to have strategy-proof operators that are not susceptible to manipulation. This paper examines the strategy-proofness of various merging operators, including those based on models and formulas, and considers different scenarios such as the number of agents and integrity constraints.
41691	4169153	Iterated belief change: a transition system approach	The transition system approach is a method used to understand how an agent's beliefs change as they take actions. Some actions lead to belief revision and others lead to belief update, but the interaction between these two processes can be complex. To better understand this interaction, a set of basic rules has been developed. Additionally, a new operator has been introduced that helps explain how revisions and updates can work together in a logical way. This approach provides a plausible interpretation for the evolution of beliefs through alternating sequences of revisions and updates.
41692	4169221	Model Comparison to Synthesize a Model-Driven Software Product Line	The current method for developing software product lines is focused on feature and variability modeling. However, there is potential for automatic assistance in identifying commonalities and variabilities within a set of models in a given domain. This paper presents a generic approach for synthesizing a software product line using model comparison. The approach uses EMF Compare, a generic model comparison tool, to detect differences between existing potential product models and Common Variability Language (CVL) to specify variability. The developer can then use the results to automatically induce a preliminary product line model and make further enhancements. An example of a train control product line is used to illustrate the approach.
41693	416930	Dynamically Optimizing High-Dimensional Index Structures	High-dimensional query processing involves optimizing the logical page-size of index structures, which is crucial for efficient performance. However, the optimal page-size is not only determined by static schema information, but also dynamic factors like the number of objects in the database and the degree of clustering and correlation in the data set. To address this issue, a method called DABS-tree has been proposed, which dynamically adapts the page size during insert and delete operations. This is achieved by consulting a cost model before splitting or merging pages. This approach has been shown to outperform the X-tree and sequential scan by factors of up to 4.6 and 6.6, respectively, through experimental evaluation. The DABS-tree algorithm also ensures that all restructuring operations are locally restricted.
41694	4169424	CoCo: coding cost for parameter-free outlier detection	The question of how to automatically identify outliers in a data set is a common one in various fields such as economics, biology, and medicine. However, current methods for outlier detection have several limitations, including the need for specific parameter settings and assumptions about the data distribution. To address these issues, a new technique called CoCo is proposed. CoCo uses a data compression approach to identify outliers, without relying on specific data distributions. The technique also includes a new algorithm for outlier detection and has been tested on both synthetic and real-world data, showing promising results. The source code and data sets used in the experiments are available for further use.
41695	416950	Strategic Port Graph Rewriting: an Interactive Modelling Framework	Strategic port graph rewriting is a method used to implement visual modelling tools in order to simplify the process of specifying and programming complex systems. It involves representing a system using an initial graph and a set of rules, along with a user-defined strategy for applying these rules. This approach has adapted traditional operators from term rewriting languages to work with graph rewriting, as well as introducing new constructs for graph traversal and managing rewriting positions. The language has a formal semantics and is implemented through the PORGY tool, which allows for graph transformation and visualization. 
41696	41696100	Boosting Saliency in Color Image Features	Salient point detection is a method used to identify distinctive events in images. This is typically done by analyzing the local differential structure of images, focusing on the shape of the local neighborhood. However, most detectors only consider luminance information, ignoring the potential of color information. To address this, a new algorithm called color saliency boosting has been developed, which takes into account both shape and color distinctiveness. This is achieved by analyzing the statistics of color image derivatives and transforming them to have equal impact on saliency. This method can be applied to existing feature detectors and has been shown to significantly improve the information content of images.
41697	4169718	A CP framework for testing CP	The success of constraint-based modeling languages has highlighted the need for better software engineering practices, particularly in the testing phase. This paper presents a testing framework that automates test case generation for constraint programming. The framework follows a general development process that involves starting with a simple declarative constraint model and refining it using various techniques such as constraint reformulation and symmetry-breaking. The proposed process uses the initial model as a reference for identifying errors and generating practical test cases. The framework has been implemented in a tool called CPTEST, which has been used to detect errors in well-known benchmark programs. This approach aims to address the potential introduction of faults during the refinement stage of constraint program development.
41698	4169822	MINLP Based Retrieval of Generalized Cases	The use of generalized cases has been found to be beneficial in finding flexible and configurable products, such as reusable components in electronic design automation. This paper focuses on the issue of similarity assessment and retrieval in case bases that contain both traditional and generalized cases. Previous approaches were limited to continuous domains, but this paper expands the scope to include mixed, continuous and discrete domains. The problem is viewed as a nonlinear optimization problem and is extended to a mixed integer nonlinear optimization problem, which is a current area of research in mathematical optimization. The paper also presents two optimization-based retrieval methods that improve response time by using a pre-existing index structure. This is an important development as many real-world applications require mixed domains for case descriptions.
41699	4169935	SemEval-2015 Task 10: Sentiment Analysis in Twitter	The paper discusses the 2015 SemEval shared task on Sentiment Analysis in Twitter, which was the most popular sentiment analysis task at the time with over 40 teams participating. The task consisted of five subtasks, including two previous tasks (expressed sentiment by a phrase in a tweet and overall tweet sentiment) and three new tasks (sentiment towards a topic in a single tweet, overall sentiment towards a topic in a set of tweets, and degree of prior polarity of a phrase). This competition aimed to predict sentiment in various contexts on Twitter and had a high level of participation. 
41700	4170042	Assessing attractiveness in online dating profiles	Online dating is a popular way for people to meet potential partners, but little research has focused on how users perceive each other through their personal profiles. A study examined how users rate attractiveness in online dating profiles, including factors like extroversion and trustworthiness. It found that the photograph in a profile was the strongest predictor of overall attractiveness, but the free-text section also played a significant role. Other qualities, such as age and education, also affected attractiveness ratings but in different ways for men and women. Surprisingly, the fixed-choice elements of a profile had no impact on perceived attractiveness.
41701	4170142	Digital fountains and their application to informed content delivery over adaptive overlay networks	In this study, we focus on optimizing the transfer of large files across adaptive overlay networks. We propose using a digital fountain approach, which offers reliability and flexibility for connection migration and parallel transfers while also being resilient to packet loss. We discuss recent advancements in coding for digital fountains and how they have been effective in reliable multicast and parallel downloading. In the context of collaborative transfers, we address the issue of overlapping encoded symbols acquired by peers and present algorithms for efficient estimation, summarization, and reconciliation of these symbols while minimizing messaging and computation. Through simulations and experiments, we show the benefits of our approach for content delivery in overlay networks.
41702	4170234	Top-k closest pairs join query: an approximate algorithm for large high dimensional data	This paper introduces a new algorithm for efficiently calculating the top k closest pairs join query of two large and high dimensional data sets. The algorithm has a worst case time complexity of 0(d^2 nk) and a space complexity of 0(nd). It guarantees a solution within a factor of 0(d^{1 + \frac{1}{\tau }}) of the exact one, where \tau\in \{ 1,2, \ldots ,\infty \} represents the Minkowski metrics L_\tauof interest and d is the dimensionality. The algorithm utilizes a space filling curve to establish an order between the points and performs a limited number of sorts and scans of the data sets. Experimental results show that the algorithm performs similarly to an exact algorithm in low dimensional spaces, can significantly reduce the data set size in high dimensions under certain conditions, and still returns a solution close to the exact one.
41703	41703225	Comparison of transform coding techniques for two-dimensional arbitrarily shaped images	This paper discusses the potential for advanced multimedia video services to include irregularly shaped image segments in addition to traditional rectangular images. This is particularly relevant for TV weather reports, where chromo-key techniques and video analysis are used to create these irregularly shaped segments. The paper focuses on efficient ways to code these segments using transform techniques, specifically looking at two approaches: brute force transform coding and shape-adaptive transform coding. The former involves filling in the uncovered areas with redundant data, while the latter adapts the transform basis or coefficient calculation based on the shape of the segment. The paper proposes a new adaptive transform based on the same principle as the DCT, and analyzes the tradeoff between compression performance, computational complexity, and codec complexity for different coding schemes. Simulation results show that more complicated algorithms can improve quality, but at a higher cost, while a simple mirror image extension technique can also improve quality without any additional overhead. The paper's contributions include efficient problem formulations, new transform coding techniques, and analyses of tradeoffs. 
41704	41704100	An EM algorithm for shape classification based on level sets.	The paper suggests an EM approach to classify shapes in a database into different classes and simultaneously determine the best shape contours for each class. The level set function is used as the shape descriptor and it is assumed that for each class, there is an unknown underlying level set function that describes the contour of the shapes within that class. The level set function for each shape in the database is considered as a noisy measurement of the underlying function. The EM algorithm uses this measurement model to determine the class labels and estimate the shape contours. This algorithm is efficient, simple, and accurate and is applied to two medical applications.
41705	4170599	Delineating white matter structure in diffusion tensor MRI with anisotropy creases	Geometric models of white matter architecture are becoming increasingly important in neuroscience studies that use diffusion tensor imaging. The most commonly used method for creating these models is fiber tractography. However, for some types of analysis, it may be more useful to look at the first and second derivatives of diffusion anisotropy. This approach involves identifying ridges and valleys in the tensor field, which correspond to regions of high and low anisotropy. These features, known as anisotropy creases, can be used to extract a skeleton of major white matter pathways. This technique has been applied to real diffusion MRI data, and the results have been visualized alongside fiber tractography to confirm their relevance.
41706	4170658	Simultaneous truth and performance level estimation (STAPLE): an algorithm for the validation of image segmentation.	The challenge of accurately assessing image segmentation techniques has been an ongoing issue. This is important because these algorithms often have limited precision and accuracy. While human raters have been the most accepted method for segmentation, there is variability between raters. Automated algorithms have been sought to reduce this variability, but they must be evaluated to ensure effectiveness. However, obtaining a known true segmentation for clinical data is difficult. Using a collection of segmentations by raters is a useful alternative, but the most appropriate measure to compare them has not been determined. The Expectation-Maximization algorithm for Simultaneous Truth and Performance Level Estimation (STAPLE) can address these challenges by estimating the true segmentation and performance levels using a collection of segmentations, incorporating a prior model and spatial constraints. This makes it a valuable tool for evaluating image segmentation techniques in clinical imaging data.
41707	4170732	Simulation of Corticospinal Tract Displacement in Patients with Brain Tumors	In order to plan a surgical strategy for a brain tumor, it is important to understand the spatial relationship between the tumor and the corticospinal tracts. However, manually outlining these tracts from MRI scans is time consuming and impractical. To address this issue, a method has been developed to automatically retrieve structural information by registering a standardized brain atlas to the individual patient's tumor anatomy. This involves segmenting the skin and brain in the patient's MRI scan and then registering a normal brain atlas to the patient's brain using various techniques. The resulting spatial correspondence is used to map the corticospinal tracts from the atlas onto the patient's brain. This method has been tested on 5 patients with different types and locations of tumors and has shown to be accurate in comparison to manual segmentation. This approach allows for the visualization of complex anatomical information with minimal user interaction, making it a useful tool for surgical planning.
41708	41708108	3D Image Matching Using a Finite Element Based Elastic Deformation Model	The authors propose a new method for calculating the deformation field between 3D images. The approach minimizes the squared differences between the images while taking into account the physical properties of the objects represented. The objects are modeled as elastic bodies and the method differs from optical flow methods in three ways: it considers the physical properties of the objects, provides information about the deformed objects, and calculates a global solution instead of local solutions. This is achieved through a finite-element approach, which requires the objects to be meshed. A specialized tetrahedral mesh generator has been developed for this purpose. The method has been successfully tested on simulated and medical data, including muscle exercise imaging and ventricular deformation in multiple sclerosis. 
41709	4170917	Registering UMM Business Collaboration Models in an ebXML Registry	The UN/CEFACT's modeling methodology (UMM) is a framework for creating global choreographies of business processes between organizations. It is important for UMM models to be publicly accessible so they can be used repeatedly and referenced in agreements with trading partners. This paper outlines a mapping of UMM models to the ebXML registry information model (RIM), providing a standardized way for organizations to store and access UMM models. This allows for easier collaboration and implementation of UMM models in inter-organizational business processes. 
41710	417108	On the Complexity of Decidable Cases of Commutation Problem for Languages	The article discusses the complexity of testing the commutation of two languages, specifically when one of the languages is a finite language. This is a problem that has recently gained interest due to progress made in solving it. The complexity of the problem ranges from co-NEXPTIME-complete to P-SPACE complete and co-NP complete, with different types of representations such as nondeterministic automata, regular expressions, and grammars being considered. Interestingly, the complexity does not change when considering a general language instead of a finite one. For the case of two finite sets, polynomial time algorithms are provided that are more efficient than a naive algorithm. However, for deterministic automata, the complexity is more complicated due to the asymmetry of concatenation.
41711	417117	Optimal Rebuilding of Multiple Erasures in MDS Codes.	MDS array codes are commonly used in storage systems because of their efficient encoding and decoding procedures. These codes can correct any number of node failures by accessing the remaining information. However, in practical scenarios, it is more likely to have a specific number of node failures rather than all nodes failing. This raises the question of how much information needs to be accessed to rebuild a certain number of failed nodes. A previous study introduced zigzag codes, which achieve the optimal rebuilding ratio of 1/r for systematic node failures, but require all information to be accessed for parity node failures. This paper presents three results on rebuilding codes, including a fundamental bound on storage size and repair bandwidth, construction of optimal systematic codes for both systematic and parity node failures, and error correction algorithms for zigzag codes. These codes can also be corrected beyond their minimum Hamming distances.
41712	4171216	Permanent deformation analysis of asphalt mixtures using soft computing techniques	This study compares the effectiveness of two soft computing techniques, multi expression programming (MEP) and multilayer perceptron (MLP), for predicting the rutting potential of dense asphalt-aggregate mixtures. The researchers developed correlations between the flow number of Marshall specimens and various influencing factors, such as aggregate content, bitumen percentage, and voids in mineral aggregate. The correlations were based on a comprehensive experimental database, and the relative importance of each variable was determined. A multiple least squares regression (MLSR) analysis was used to compare the MEP and MLP models. The results were validated through a parametric study and showed good agreement with previous studies. The proposed correlations are considered efficient for evaluating the rutting potential of asphalt mixtures, with MEP-based formulas being more practical for engineering applications.
41713	4171352	Firefly Algorithm: Recent Advances and Applications.	Nature-inspired metaheuristic algorithms, particularly those based on swarm intelligence, have been gaining popularity in the past decade. The firefly algorithm, which was introduced about five years ago, has seen a significant increase in literature and application diversity. This paper provides a brief overview of the firefly algorithm and its recent publications. It also delves into the concept of optimality in balancing exploration and exploitation, a crucial aspect for all metaheuristic algorithms. Through a comparison with intermittent search strategy, the paper concludes that metaheuristics like the firefly algorithm perform better than the optimal intermittent search strategy. The article also examines the implications of these algorithms for higher-dimensional optimization problems.
41714	4171431	Papiercraft: A gesture-based command system for interactive paper	Paper is still widely used in active reading and other knowledge-worker tasks due to its convenience and flexibility. However, it becomes difficult to store and access as annotations and cross references accumulate. To address this issue, systems like XLibris simulate paper using tablet PCs, but this approach has limitations. PapierCraft is a gesture-based command system that allows users to manipulate digital documents using paper printouts as proxies. With a digital pen, users can draw gestures on paper to tag, email, copy, and create links to documents. PapierCraft also supports real-time interactions between paper and digital documents. User feedback has been positive, and the article describes the design and implementation of the system.
41715	4171542	A receiver-centric transport protocol for mobile hosts with heterogeneous wireless interfaces	Numerous transport protocols have been developed for use by mobile hosts in wireless environments. These protocols take into consideration the unique characteristics of the last-hop wireless link, such as random errors, varying round-trip times, blackouts, and handoffs. This paper argues that placing the intelligence of the transport protocol at the mobile host adjacent to the wireless link can result in better performance. The proposed protocol, RCP (Reception Control Protocol), is similar to TCP but has improved congestion control, loss recovery, and power management mechanisms. It is also suitable for mobile hosts with multiple interfaces accessing different wireless networks, as it provides scalable congestion control, seamless server migration during handoffs, and efficient bandwidth aggregation. Both simulations and real-world experiments were used to evaluate RCP.
41716	417161	binpac: a yacc for writing application protocol parsers	The process of semantic analysis in network traffic involves parsing the traffic stream to extract high-level protocols. This helps in transforming raw data into structured and meaningful data fields. However, manually constructing protocol parsers is a complex and error-prone task. To simplify this process, a declarative language called binpac has been developed, along with a compiler. This language allows for the efficient and robust creation of protocol parsers. It has been successfully used to build parsers for the Bro network intrusion detection system, replacing some existing analyzers and adding new ones. This allows for easier and more expressive analysis of network traffic using Bro's scripting language. binpac is now available as part of the open-source Bro distribution.
41717	4171741	K-automatic discovery in large image databases	This paper presents an unsupervised grouping method for organizing data items, specifically images, in large image databases. The focus is on a partition clustering approach that addresses the challenge of automatically determining the optimal number of clusters (k). While most partition clustering methods require manual input for k, this method offers an experimental solution. While manual input may be useful in certain domains, it is not realistic for general applications and can be time-consuming and unreliable. The paper also includes keywords such as clustering, partition, and data bases.
41718	4171856	Secure Multicast in a WAN	A secure reliable multicast protocol allows a process to send a message to a group of recipients, ensuring that all correct recipients receive the same message even if a minority of processes are malicious. This protocol is useful for building secure distributed services, but its cost typically increases with the size of the system. To reduce this cost, two approaches are presented: a protocol with a cost proportional to the number of tolerated failures, and a method of relaxing the consistency requirement to a probabilistic guarantee, resulting in a constant cost for very large networks. 
41719	4171949	SlickFeel: sliding and clicking haptic feedback on a touchscreen	SlickFeel is a haptic display setup that can provide two types of feedback to a finger on a touchscreen. Sliding feedback allows the user to feel interactive objects on the screen through changes in friction, while clicking feedback replicates the sensation of pressing a button. This technology has been demonstrated through two scenarios - a simple button-click scenario where the user can feel the position of buttons and a simulated key-click when pressing on them, and a haptically-enhanced thumb-typing scenario where two thumbs can type without looking at the screen. SlickFeel has been integrated with a Kindle Fire tablet, making it compatible with existing mobile touchscreen devices.
41720	41720149	Browsing around a digital library	In the future, working in a digital library will feel qualitatively different than it does today. Instead of browsing through collections, readers and writers will engage in context-directed browsing. This will be made possible by structures created through automatic analysis of the library's contents, including full-text books and journals. Text mining techniques will be utilized to enhance the browsing experience. While present digital libraries are similar to traditional libraries, the digital library of the future will offer a more dynamic and personalized experience for users.
41721	4172159	Information Based Distributed Control for Biochemical Source Detection and Localization.	The paper presents improvements to the Direction of Gradient (DOG) algorithm for detecting a biochemical source with moving sensors. The DOG algorithm is modified to be used as a distributed control scheme for a mobile sensing network, with more efficient numerical procedures replacing the original maximum likelihood estimation. Simulations show that these modifications simplify the algorithm and improve its performance. The algorithm uses the Cram´er-Rao bound (CRB) to minimize the expected location estimation error, and two methods are provided to approximate the ML estimates in a computationally cheaper manner. The motion control algorithm also allows for variable step size, unlike the original paper. The proposed modifications allow the DOG algorithm to be implemented on a mobile sensor network in a distributed manner, improving performance compared to a single sensor. 
41722	4172253	Multiframe temporal estimation of cardiac nonrigid motion	The system described is designed for accurately tracking the motion of the left ventricular endocardial wall in image sequences. It uses an adaptive transversal filter and contour-based description of the object's boundaries to model motion trajectories across multiple frames. The system takes in a set of correspondences between contours and quality measures as input and derives relationships and models between frames. The system's output is validated by comparing computed trajectory estimates with the trajectories of physical markers implanted in the LV wall. Results show that a multiframe temporal model without spatial periodicity constraints provides the best performance with the least computational cost, while a multiframe spatiotemporal model offers the best performance at a higher computational expense.
41723	4172354	On the Scope of the Universal-Algebraic Approach to Constraint Satisfaction	The universal-algebraic approach is a useful method for studying the complexity of constraint satisfaction problems (CSPs). It has previously been applied to CSPs with finite or infinite omega-categorical templates. This paper presents a characterization of CSPs that can be formulated with a finite or omega-categorical template. The approach relies on the fact that in these structures, a relation is primitive positive definable if and only if it is preserved by the polymorphisms of the structure. The paper also introduces results that can be used to study the computational complexity of CSPs with arbitrary infinite templates, showing that every CSP can be formulated with a template where a relation is primitive positive definable if and only if it is first-order definable and preserved by the infinitary polymorphisms. This has applications in describing and analyzing the computational complexity of CSPs, including a description of CSPs that can be solved in polynomial-time and general hardness criteria based on the absence of certain polymorphisms. 
41724	417245	Experiments with a robotic computer: body, affect and cognition interactions	RoCo is a robotic computer that moves its monitor in response to the user's postural movements. A study was conducted to investigate whether the computer's posture can influence the user's posture and affect, and if this affects their performance on a task. The study found that RoCo's posture was associated with the user's posture and affect, leading to increased persistence in problem solving and a higher level of comfort. This supports new theories linking physical posture with affect and cognition. The results suggest that a computer's posture can influence the user's body state and improve task measures.
41725	4172538	Une description probabiliste de la communication parlée entre homme et machine	This paper introduces a formal structure for describing human-computer spoken communication within spoken dialogue systems. This structure utilizes probability to describe the interaction and information processing within each module of a dialogue system. The paper also suggests potential uses for this framework in the design of dialogue systems.
41726	4172649	Automated Design of Cryptographic Hash Schemes by Evolving Highly-Nonlinear Functions	In recent years, several vulnerabilities have been discovered in commonly used cryptographic hash functions like MD4, MD5, and SHA-1. This has led to the recommendation of switching to more secure options like SHA-256 and Whirlpool. However, there is concern that these schemes may also be vulnerable to similar attacks due to their similarities with MD4. To address this, a new approach using Genetic Programming for automated design of cryptographic block ciphers and hash functions is presented. One such scheme, named Wheedham, is proposed and compared with other options like SHA-512 and Whirlpool in terms of security and speed. The results show that this automated approach can produce competitive and secure schemes.
41727	4172731	DroidNative: Automating and optimizing detection of Android native code malware variants.	Mobile malware development has primarily targeted the Android platform, with a focus on creating variants that can evade detection by traditional signature-based detectors. These variants often use obfuscation techniques and are embedded in native code, making them difficult to detect with static analysis tools. In response to this threat, a new system called DroidNative has been proposed, which uses specific control flow patterns to identify and detect malware in both bytecode and native code. When tested on a dataset of 5490 samples, DroidNative achieved a high detection rate of 93.57% and a low false positive rate of 2.7%. Compared to other academic and commercial tools, DroidNative showed a significantly higher detection rate of 99.48% when tested with traditional malware variants.
41728	4172831	Rewrite method for theorem proving in first order theory with equality	This paper details an extension of the term rewriting approach to first order theorem proving, specifically in the theory of first order predicate calculus with equality. This allows the term rewriting method to be as powerful as the combination of paramodulation and resolution. The approach was originally described in Hsiang & Dershowitz 1983.
41729	4172912	Generalized Minimum Noise Subspace For Array Processing.	The paper introduces the generalized minimum noise subspace (GMNS) method, which is an extension of the minimum noise subspace (MNS) method used for blind channel identification. GMNS allows for faster and parallel computation of signal and noise subspaces, even with a fixed number of parallel computing units. The paper presents different batch and adaptive algorithms for implementing GMNS and compares its computational complexity and accuracy with other standard subspace methods through simulated and real-life experiments in radio astronomy. The results show that GMNS strikes a good balance between computational efficiency and subspace estimation accuracy. 
41730	4173034	Performance analysis of MUSIC and Pencil-MUSIC algorithms for diversely-polarized array.	The paper discusses the MUSIC and Pencil-MUSIC methods, proposed by Hua in 1993, for estimating 2D angles and polarizations using crossed dipoles. It provides an asymptotical analysis of these methods and derives first order perturbation expressions for their variances. These expressions are then used to compare the performances of the two methods. The paper also presents a comparison between the Pencil-MUSIC method and the angle-only Pencil-MUSIC method proposed by Cheng and Hua in 1993. Simulation results show good agreement with theoretical results. 
41731	417311	Randomized Shellsort: a simple oblivious sorting algorithm	The paper introduces a randomized version of the Shellsort algorithm that guarantees a running time of O(n log n) and a high probability of successfully sorting any input permutation. This algorithm has potential applications in designing efficient privacy-preserving computations using secure multi-party computation. By converting it to a Las Vegas algorithm, it becomes the first version of Shellsort with a proven running time of O(n log n) and a high probability of success.
41732	4173280	An eigenspace projection clustering method for inexact graph matching	The paper discusses a method for solving inexact graph matching by using a technique called eigenspace renormalization projection clustering (EPC). This involves projecting the vertices of a pair of graphs into a joint eigenspace and then using a form of relational clustering to determine their correspondence. One advantage of this method is its ability to match graphs with different numbers of vertices. The authors demonstrate the effectiveness of this approach through the use of shock graph-based shape matching and also explore its application to random graphs. Initial results show promise for this method as a more objective way to evaluate graph matching.
41733	417333	Towards Training-Free Refinement for Semantic Indexing of Visual Media.	The current method of indexing visual media based on content analysis involves combining individual concept detectors and post-processing their outputs. However, due to limited and imprecise training data, training-based methods struggle to accurately capture relationships between concepts. This paper proposes a training-free refinement algorithm, TFR, that uses concept detection results and incorporates global and temporal neighbourhood information to enhance semantic indexing. Additionally, ontological concept relationships can be integrated into the model for further improvement. Experiments on two datasets show the effectiveness of this approach.
41734	4173471	Mining diversity on networks	Many large-scale networks have recently emerged in various fields, but a crucial measure of a participant's diversity in the network has been overlooked in previous studies. This measure, known as diversity, measures the diversity of connections a node has with its peers. This paper presents a thorough analysis of the concept of diversity, outlining two criteria that capture its meaning and proposing a simple definition that can be easily incorporated. The paper also introduces an efficient algorithm for ranking top-k diversity in dynamic networks. Experiments on synthetic and real datasets yield significant results, with highly diverse nodes being identified as intuitive.
41735	4173518	A transformational approach to compiling Sisal for distributed memory architectures	This paper discusses ways to efficiently execute array computation on Distributed Memory Architectures using compiler-directed program and data transformations. By translating a subset of Sisal, a single-assignment language, into a linear algebraic framework, the authors show that it is possible to reduce load imbalance and non-local memory access. They propose a new test to identify load imbalance and a method for data alignment to reduce non-local access. The paper also presents three criteria for partitioning and a systematic approach for mapping data and computation to processors. Additionally, a new pre-fetching procedure is introduced to prevent redundant non-local accesses.
41736	4173647	Optimal Data Acquisition for Statistical Estimation.	In this paper, we examine the problem of a data analyst purchasing data from strategic agents in order to accurately estimate a statistic. The agents incur private costs to reveal their data, which can be correlated with their data. Our focus is on linear unbiased estimators, and we develop a mechanism that is both individually rational and incentive compatible. This mechanism minimizes the worst-case mean-squared error of the estimation, while also adhering to a budget constraint. We provide a closed-form solution for this optimal mechanism and also extend our findings to the case of acquiring data for parameter estimation in regression analysis, where private costs can correlate with the dependent variable but not the independent variables.
41737	41737104	Medium access control with channel state information for large sensor networks	Traditionally, random access protocols have been designed with simple models for the physical layer. However, a new reception model has been introduced that incorporates the channel states of transmitting users and allows for multiple simultaneous successes. This model assumes that each user has access to their own channel state and proposes a variant of Slotted ALOHA protocol for medium access where the transmit probability is based on the channel state. The maximum stable throughput of a finite user system is derived and optimal schedulers are obtained for simple reception models. The concept of asymptotic stable throughput is also introduced, which is the maximum stable throughput as the number of users increases. The use of channel state information in schedulers is studied for sensor networks, specifically in the context of CDMA networks. This application shows the benefits of using channel state information to vary the transmit probability.
41738	41738121	A computational model for watermark robustness	Multimedia security schemes often use a combination of cryptography and information hiding techniques, such as steganography or watermarking. These schemes have various applications, such as dispute resolution, proof of ownership, and fingerprinting. To ensure their effectiveness, formal security definitions are necessary. However, existing definitions for information-theoretic and computational security cannot be directly applied to watermarking schemes due to their differences. Additionally, current definitions for watermark security have conceptual flaws. In this paper, the authors propose a formal framework and definitions for watermark robustness, the key security property of watermarking schemes. These definitions address the limitations of previous proposals and allow for compatible security proofs with cryptographic definitions. 
41739	4173977	Towards security policy decisions based on context profiling	The growing use of personal mobile devices has led to the creation and consumption of valuable and sensitive data by ordinary users. However, without proper access control policies, this data can be disclosed in unintended ways. While some applications and services offer security and privacy policies, users may not understand or be able to adjust them to their needs. This paper suggests that context information, such as a device's location, can be used to infer appropriate access control policies. Three usage scenarios are described to support this argument, and the concept of "familiarity" is proposed as a measure to determine suitable policy settings. The paper also shares the results of a study using context observations from two participants' devices over a period of time.
41740	4174025	A visual token-based formalization of BPMN 2.0 based on in-place transformations	This paper presents a formalization of the Business Process Model and Notation (BPMN) standard, which defines the execution semantics for process instances. Existing formalizations are incomplete and require mappings to other languages, while this formalization uses in-place graph transformation rules that directly update the models. The formalization is documented visually using BPMN syntax and has been extensively verified. It can be used to complement the existing standard and identify potential issues, providing a useful tool for the maintainers of the standard and vendors to verify conformance.
41741	4174111	Improved IPsec performance utilizing transport-layer-aware compression architecture.	The growth of internet communication has led to a need for better security measures for transmitting information over public networks. The Internet Protocol Security (IPsec) standard was developed to address this issue. However, IPsec solutions often result in slower processing times and larger packet sizes. To improve both security and performance in IPv6 networks, a new compression architecture called Efficient Secure Pipe (ES_PIPE) has been proposed. Unlike traditional IPsec compression methods, ES_PIPE is transport-layer-aware and can compress different types of packets. Experimental results show that ES_PIPE reduces transmission time by 20-30% and improves the efficiency of encryption and decryption processes by 20%.
41742	4174245	Design of Length-Compatible Polar Codes Based on the Reduction of Polarizing Matrices	Length-compatible polar codes are a type of polar codes that can be used for various lengths using a single encoder and decoder. This paper presents a method for constructing length-compatible polar codes by reducing the 2n × 2n polarizing matrix proposed by Arikan. The conditions for a reduced matrix to support a polar code of a given length are analyzed and used to construct these codes through codeword-puncturing and information-refreezing. These codes have low complexity and can be encoded and decoded similarly to a polar code of length 2n. The results show that these codes provide a performance gain of 1.0 - 5.0 dB compared to randomly punctured codes when using successive cancellation decoding.
41743	4174399	Compute-and-forward: Harnessing interference with structured codes.	The use of centralized encoder and decoder in communication systems involves a channel matrix, which represents a set of linear equations that can be transformed into parallel channels. In multiuser networks, interference is seen as creating linear equations of codewords, and the goal of the receiver is to collect a complete set of such equations. A new relaying technique, called compute-and-forward, uses structured codes to reliably compute functions over channels. This allows the relays to efficiently recover linear functions of codewords without fully decoding them, thus removing the effects of noise at the relay. This technique has been applied to a Gaussian relay network with interference and has shown to achieve better rates compared to other techniques in certain situations.
41744	41744210	The SPRIGHT algorithm for robust sparse Hadamard Transforms	This paper discusses the problem of computing a K-sparse N-point Hadamard Transform (HT) from noisy time domain samples, where K is sub-linearly scaled to N. The SPRIGHT algorithm is proposed to recover the sparse HT coefficients in a stable manner, even with the presence of additive Gaussian noise. It is shown that the K-sparse HT can be reconstructed from noisy samples with a small error probability and the same sample and computational complexity as in the noiseless case. Numerical experiments also confirm the small big-Oh constants in the complexity of the SPRIGHT algorithm.
41745	417456	A trace semantics for Petri nets	The concept of trace is extended to apply to Petri nets, resulting in a poset of generalized traces associated with each net. The trace languages defined by Petri nets are characterized, and it is shown that this characterization also applies to Winskel's general event structures and stable event structures. This means that in this framework, stable event structures, general event structures, and Petri nets form a hierarchy in terms of expressive power.
41746	4174653	Getting Rid Of Links In Hierarchical Radiosity	Hierarchical radiosity with clustering is a popular algorithm for calculating global illumination in complex environments. However, it has been difficult to use for intricate scenes due to the large number of transport coefficients (links) that need to be stored between surfaces. In this paper, a modified shooting method is proposed to solve the radiosity equation without the need for storing links. This reduces memory consumption significantly, although computation time may increase. The error behavior of this new method is compared to the traditional gathering approach, and the relationship between global error and local error threshold is analyzed. Caching can also be used to reduce computation time.
41747	4174770	Warped K-Means: An algorithm to cluster sequentially-distributed data	Many data-generating devices, such as motion sensors and eye trackers, produce large amounts of sequential data. This data often needs to be compressed for classification, storage, and retrieval purposes. Traditional clustering algorithms are not effective for this type of data as they do not consider the sequential nature of the data. To address this issue, a new method called Warped K-Means (WKM) has been developed. This algorithm incorporates a hard sequentiality constraint in the clustering process, resulting in improved accuracy and reduced computational cost. WKM has been successfully applied to various tasks, including human activity segmentation and classification, outperforming other state-of-the-art techniques.
41748	41748102	Face Matching and Retrieval Using Soft Biometrics	Soft biometric traits, such as gender and facial marks, are not distinct enough on their own for accurate face recognition. However, when combined with face matching scores, they can improve overall accuracy. In certain applications, like visual surveillance, soft biometric traits are even more valuable in cases where a face is partially covered or in a non-frontal pose. These traits can also differentiate between identical twins and provide descriptive evidence in court cases. A method has been developed to automatically detect facial marks using active appearance models and blob detection. Experiments using various databases have shown that incorporating soft biometric traits can enhance the performance of face recognition systems.
41749	4174936	KoNKS: konsensus-style network koordinate system	A network coordinate system assigns virtual coordinates to nodes in a network based on their distance from each other, allowing for accurate estimation of network latency without direct communication between nodes. These systems have many practical applications, but can also be vulnerable to attacks where malicious peers report false coordinates. Previous secure schemes have been developed to protect against these attacks, but a new type of attack has recently been discovered. To address this issue and prevent an "arms race" of constantly developing new defenses, a new network coordinate system called KoNKS has been proposed. KoNKS aims to achieve consensus among nodes by limiting the influence of malicious peers and preventing them from appearing closer than they actually are. It has been shown to be as accurate as previous systems and has low processing and communication overhead.
41750	417503	Secure Group Communication Using Robust Contributory Key Agreement	Contributory group key agreement protocols are useful for small peer groups as they generate group keys based on contributions from all group members. These protocols are resistant to various attacks and offer strong security properties such as key independence and perfect forward secrecy. This paper introduces a robust contributory key agreement protocol that can handle any sequence of group changes. It is based on Group Diffie-Hellman and uses a group communication system with Virtual Synchrony semantics. The protocol provides both Virtual Synchrony and Group Diffie-Hellman security properties in the presence of node failures, network partitions, and other events. A secure group communication service, Secure Spread, was implemented using this protocol. It is compared to a centralized group key management protocol with similar security properties to demonstrate its practicality.
41751	4175121	Metric Learning Based Structural Appearance Model for Robust Visual Tracking	The success of a visual tracker depends heavily on its ability to accurately model appearance. Recently, there has been a growing interest in using sparse representation for appearance modeling. However, most existing methods use reconstruction errors to compute the observation likelihood, which can lead to poor performance when faced with significant appearance variations. This paper proposes a new approach that uses a metric learning based structural appearance model to better match different appearances. The structural representation is obtained through multiscale max pooling of weighted local sparse codes. An online multiple instance metric learning algorithm is then used to learn a discriminative and adaptive metric, reducing the impact of misaligned training examples. The proposed method, implemented within a Bayesian inference framework, outperforms existing methods in terms of accuracy and effectiveness, as demonstrated through comprehensive experiments on challenging image sequences.
41752	417523	Single Image Dehazing with White Balance Correction and Image Decomposition	Dehazing a single image is a difficult task due to its ambiguous nature. Existing dehazing methods often make assumptions and use prior knowledge to improve the results, but they rarely consider the impact of the imaging process itself, such as white balance and metering. This can result in color distortion and under-exposure in foggy scenarios. To address these issues, a new approach is proposed in this paper which corrects white balance and decomposes the image into two components, reflex lightness and ambience illumination. These components are then used to improve the dehazing process and produce a high quality image. Experimental results show that this method outperforms current state-of-the-art methods in terms of contrast and color accuracy.
41753	4175352	Combined Retrieval Strategies for Images with and without Distinct Objects	The paper discusses the design of an all-season image retrieval system that can handle images with and without distinct objects. The system uses a neural network trained with the BPTS algorithm to pre-classify images into distinct-object or no-distinct-object categories based on visual contrasts and spatial information. For images with distinct objects, an attention-driven retrieval strategy is used, while images without distinct objects are processed with a fusing-all retrieval strategy. The combination of these two approaches results in improved performance. 
41754	4175452	Quantifying the Benefits of SSA-Based Mobile Code	This paper discusses the use of Static Single Assignment (SSA) form as an alternative transportation format for Java code, and its potential benefits for just-in-time compilers. The authors integrated support for SSA-based code representation into Jikes RVM, a Java execution environment, and compared its performance to traditional JVM bytecode. Results show improvements in both compilation time and code quality, suggesting that SSA-based representations offer advantages for just-in-time compilation. This study validates the potential of SSA form as a superior alternative to virtual machines for high-performance Java code generation.
41755	4175590	Video-Based Self-positioning for Intelligent Transportation Systems Applications.	Traffic congestion is a common issue in urban areas, leading to increased research in automatic traffic management systems and congestion pricing. A crucial aspect of these systems is lane prediction and self-positioning of vehicles on the road. To address this, a new problem of vehicle self-positioning using dashboard camera footage is introduced. This involves predicting the number of lanes and localizing the vehicle within those lanes. A model is formulated that uses the video as a key observation, making it more effective than existing low-level vision-based techniques. The model takes into account the number of lanes and vehicle position as parameters, making use of high-level semantic knowledge. This approach is tested on real-world videos and proven to be successful.
41756	4175651	Mapping deontic operators to abductive expectations	Deontic concepts and operators, which represent norms, have been widely used in legal reasoning and normative multi-agent systems. The EU-funded SOCS project has developed a language for specifying agent interactions in open multi-agent systems, with a declarative and operational semantics. The SOCS framework uses this specification directly for verification. This paper proposes mapping deontic operators to expectations in the SOCS social framework, supported by a similarity between their abductive semantics and Kripke semantics. The aim is to use the computational machinery of the SOCS framework for specifying and verifying systems using deontic operators.
41757	4175715	1600 faults in 100 projects: automatically finding faults while achieving high coverage with EvoSuite	Automated unit test generation techniques typically focus on either identifying violations of automated oracles or producing comprehensive test suites. However, the use of search-based testing (SBST) in conjunction with automated oracles has not been extensively explored, as it becomes less efficient with a large number of testing targets. This paper presents a search-based approach, implemented in the EvoSuite tool, that successfully achieves both objectives at once. An empirical study using EvoSuite on 100 open source projects revealed that SBST is able to efficiently detect faults while also producing comprehensive test sets for any chosen coverage criterion. The study also showed that EvoSuite was able to detect twice as many failures related to undeclared exceptions compared to traditional random testing methods, indicating its potential for uncovering real faults in software projects. This highlights the need for further research in improving automated oracles and for incorporating tools like EvoSuite into software development processes to ensure clean program interfaces.
41758	4175821	Tree transducers with external functions	This paper discusses the computational abilities of two types of tree transducers: macro tree transducers and attributed tree transducers. These transducers allow for the invocation of external functions during rewriting processes, making them more powerful than traditional transducers. The main finding of the paper is that macro tree transducers with external function calls can be characterized in terms of attributed tree transducers with external function calls. Additionally, the paper defines two classes of tree functions that can be generated using these transducers, and proves that these classes are equal to the class of primitive recursive tree functions. This demonstrates the strength of these transducers in terms of computational power.
41759	4175926	Ensembles of nested dichotomies for multi-class problems	Nested dichotomies are a statistical method used in logistic regression to solve complex classification problems with multiple categories. They are represented as binary trees and involve breaking down the problem into simpler dichotomies. This approach is more accurate than directly applying other classification techniques like C4.5 or logistic regression to multi-class problems. In the standard approach, the choice of the tree is based on domain knowledge, but an alternative is to treat all trees as equally likely and create an ensemble classifier. This approach produces even more accurate results and also generates class probability estimates. It is a useful method for applying binary classifiers to multi-class problems.
41760	4176037	Ramsey number of paths and connected matchings in Ore-type host graphs.	The Schelp conjecture states that in any 2-coloring of a graph with 3n-1 vertices and minimum degree at least (3n-1)/4, there is a monochromatic path of length 2n. This conjecture has been extended to graphs with an Ore-type condition, where the sum of the degrees of any two non-adjacent vertices is at least half of the total number of vertices. A new proof has been proposed, showing that a connected matching of size n can be found in any 2-coloring of such graphs. This result has been used to prove an asymptotic version of the conjecture for paths.
41761	4176156	A Lagrangian Approach To Dynamic Interfaces Through Kinetic Triangulation Of The Ambient Space	This paper presents a new Lagrangian approach for modeling dynamic interfaces between different materials in two-dimensional simulations. The approach maintains a two-dimensional triangulation that includes the one-dimensional description of the interfaces, allowing for topology changes to be detected through face inversions. Each face is labeled with the type of material it contains, and the triangulation and labels are updated consistently during deformation using kinetic data structures. This approach offers an alternative to other popular methods, such as the level set and particle level set methods, and has been shown to be reliable in difficult situations. Numerical experiments demonstrate the efficiency of the approach, and potential for extension to three dimensions is discussed.
41762	4176267	Non-linear book manifolds: learning from associations the dynamic geometry of digital libraries	The traditional approach to designing virtual libraries is to replicate the physical space they are based on. However, a new paper suggests automatically capturing the actual space of the books and creating a non-linear book "manifold." This raises questions about whether the focus should be on the books themselves or the users. Experiments on a real digital library show that the user-focused approach is a strong competitor to the book-focused one. Additionally, the geometric layers of the manifold provide significant benefits for retrieval and visualization. For example, the topological layer allows for more effective "Manifold association rules," which outperform conventional rules. This approach improves various aspects of association rule mining, including computational efficiency and accuracy.
41763	4176355	Boosted Density Estimation Remastered.	There has been an increase in the use of iterative approaches for boosted density estimation and sampling. However, these methods often rely on heavy assumptions on the iterates, which can be unrealistic or difficult to verify. This contrasts with the original boosting theory, where weaker assumptions would suffice. In this paper, it is shown that a weak learner, in the original boosting theory sense, is all that is needed to achieve boosting for density estimation. The convergence rates presented in the paper are better and rely on weaker assumptions compared to existing methods. Additionally, the model fit is shown to belong to exponential families, and a new variational characterization of $f$-divergences is obtained. Experimental results demonstrate improved performance compared to AdaGAN, with smaller architectures. 
41764	4176423	Index coding via linear programming	Index Coding is a topic that has gained attention recently due to its applications in video-on-demand and wireless communication. It encodes information as an undirected graph and the key parameter is the broadcast rate, which is the average cost of communication per bit. Previous bounds on this rate were derived indirectly from other Index Coding capacities, but these did not provide much insight into the behavior of the rate. This study introduces a new hierarchy of linear programs that directly analyze the broadcast rate and improve upon previous bounds. This also leads to the discovery of new graphs with significant differences in their broadcast rates and the first nontrivial approximation for the rate. The proofs utilize tools from extremal graph theory.
41765	4176524	Secure Hash-and-Sign Signatures without the Random Oracle.	A new signature scheme is proposed that is secure against chosen message attacks, assuming a variant of the RSA conjecture. Unlike other schemes, it does not rely on signature trees and instead uses the "hash-and-sign" paradigm. The cryptographic hash function used is well defined and reasonable, rather than being modeled as a random oracle. The security proof is constructed in three steps: first, a random oracle model is used, then it is replaced with a hash function that meets strong computational assumptions, and finally, it is shown that these assumptions are reasonable based on standard intractability assumptions.
41766	4176630	Imitation learning with hierarchical actions	In this paper, the authors discuss the power of imitation in learning new skills through observing a mentor. They propose a new model for goal-based imitation that utilizes action hierarchies and relies on sample trajectories of the mentor's actions, rather than requiring explicit mentor actions. The model is tested on a large-scale grid world task and shows rapid learning by combining hierarchical actions to achieve subgoals and reach a desired goal state. This approach of hierarchical imitation can significantly improve learning, especially in large state spaces, compared to learning without a mentor or without an action hierarchy.
41767	4176729	Identity based threshold ring signature	Threshold ring signature schemes allow a group of t entities to automatically recruit n – t entities to create a publicly verifiable signature on behalf of the entire group while maintaining the anonymity of the actual signers. This is useful for ad-hoc groups like mobile ad-hoc networks. The paper introduces an identity based threshold ring signature scheme that is secure and compatible with trusted authorities. It is the first of its kind and is also the most efficient ID-based and threshold ring signature scheme in terms of required pairing operations.
41768	4176854	Scalable Web Content Attestation	The web is a major platform for sharing information, but recipients have no way of verifying the integrity of the content they receive. This paper introduces the Spork system, which uses the Trusted Platform Module (TPM) to tie the web server's integrity state to the content delivered to browsers. This allows clients to verify that the content is coming from a functioning server. The paper discusses the design and implementation of Spork and its Firefox validation extension, as well as the challenges of scaling the delivery of mixed static and dynamic content using TPM hardware. Empirical analysis shows that Spork can handle a large number of clients and deliver integrity-measured content with manageable overhead. 
41769	4176967	Chinese Microblog Sentiment Classification Based on Deep Belief Nets with Extended Multi-Modality Features	This paper introduces a DBN model and a multi-modality feature extraction method for Chinese micro blogging sentiment classification. The model incorporates traditional features for document classification as well as comments from micro blog posts. The integration of these features is represented as an input vector for the DBN. The DBN model consists of stacked layers of RBMs, which can learn hidden structures from probability distribution samples of the data. A Class RBM layer is added for final sentiment classification. The results show that this deep learning method outperforms traditional models such as SVM or NB, highlighting the effectiveness of DBN for short-text sentiment classification with the proposed feature extension method.
41770	4177080	Tractable Bayesian Learning of Tree Belief Networks	The paper introduces decomposable priors, a type of prior for tree belief nets that allows for tractable Bayesian learning with complete observations. This means that the posterior can be analytically determined in polynomial time. The tractability is achieved through two main results: the integration of factored distributions over spanning trees in a graph, and the use of compactly parametrized product of Dirichlet distributions as tree parameter priors. These results not only enable exact Bayesian learning, but also allow for a new class of tractable latent variable models where the likelihood of a data point is calculated through an average of different tree structures. 
41771	4177136	Effective Category and Measure in Abstract Complexity Theory (Extended Abstract)	The Operator Speed-up, Gap, and Compression Theorems have been strengthened by utilizing an efficient form of the Baire Category Theorem. This results in more powerful versions of these theorems. Additionally, it has been proven that all complexity classes of recursive predicates have a negligible amount of effective measure in the space of recursive predicates. Conversely, the class of predicates with a complexity above a given recursive threshold has a significant amount of recursive measure in the class of recursive predicates.
41772	4177254	Imprecise probabilistic query answering using measures of ignorance and degree of satisfaction	Conditional probabilistic logic programming involves answering a query using either a probability interval or a precise probability obtained through the maximum entropy principle. While the former can be uninformative and the reliability of the latter can be questionable with imprecise prior knowledge, this paper proposes methods to measure if a probability interval or a single probability is sufficient for answering a query. These methods include measuring the ignorance of a probabilistic logic program with respect to a query, which reflects the reliability of a precise probability for the query, and measuring the probability that the exact probability falls within a given interval. These measures are shown to satisfy certain properties and are demonstrated through a case study.
41773	417736	The Hardness and Approximation Algorithms for L-Diversity	There are two main categories of solutions for preserving privacy when publishing data: theoretical and heuristic. The theoretical category guarantees low information loss, but is limited to the k-anonymity principle which has been shown to be vulnerable to privacy attacks. On the other hand, heuristic solutions may incur high information loss in some cases, but have been shown to perform well on real data. While many heuristic algorithms have been developed for advanced privacy principles, this study focuses on the popular l-diversity principle. The study shows that finding optimal l-diverse generalizations is a difficult problem, but proposes an approximation algorithm with a non-trivial bound on information loss. Experiments with real data support the effectiveness and efficiency of the proposed solution.
41774	4177440	Process attributes in bio-ontologies.	Biomedical processes are important for understanding the functioning of organisms and are often included in biomedical terminologies and ontologies. However, accurately representing the attributes of these processes, such as rates and regularities, has been a challenge. To address this issue, a design pattern has been proposed that is compatible with upper ontology frameworks. It is based on the idea that process attributes can be described by how repeated parts of a process contribute to the overall process. This approach has been applied to modelling heart beating processes and has shown that logical definitions of process attributes are feasible. However, the expressivity of description logic languages limits the ability to fully define these attributes, indicating the need for primitive entities in ontology frameworks. Overall, this highlights the importance of formal upper-ontology frameworks for ensuring consistency and interoperability in representing biomedical processes and their attributes.
41775	4177525	A comparison of multiobjective evolutionary algorithms with informed initialization and kuhn-munkres algorithm for the sailor assignment problem	This paper compares the performance of two multiobjective evolutionary algorithms, NSGA-II and SPEA2, with informed initialization on large instances of the Sailor Assignment Problem for the United States Navy. The informed initialization includes special solutions from an extended version of the Kuhn-Munkres algorithm, which is typically used for solving single valued linear assignment problems. The extension allows it to be used for single objective instances of the sailor assignment problem. The results of the evolutionary algorithms are compared to the Kuhn-Munkres extension, providing a benchmark for their performance.
41776	41776107	Finding Associations in Composite Data Sets: The CFARM Algorithm	This paper introduces a composite fuzzy association rule mining mechanism (CFARM) for identifying patterns in datasets with composite attributes, which are attributes that can have multiple values following a common schema. The focus is on generating fuzzy association rules based on the properties of these composite attributes, specifically in the context of analyzing nutrients in grocery data sets. The paper begins with a background and related work review, followed by a formal definition of CFARM concepts. The algorithm is then described and evaluated using both real and synthetic data sets. Overall, CFARM proves to be effective in identifying patterns in datasets with composite attributes.
41777	4177729	On the Portability of Prolog Applications	The lack of portability in Prolog programs has been a major concern for programmers. Despite the ISO standard set in 1995, it has not fully addressed the issue for larger applications. In response, YAP and SWI-Prolog have developed a compatibility framework since 2007 to allow code to run on various Prolog systems without migration. The implementation and effectiveness of this framework have been evaluated through its use in several libraries and a significant application. This approach aims to make Prolog programming more convenient and efficient by reducing the need for adaptation and migration of programs.
41778	4177823	3D Object Modeling and Recognition from Photographs and Image Sequences	The chapter introduces a method for representing rigid 3D objects using local affine-invariant descriptors and spatial relationships between their surface patches. This approach combines geometric constraints from different views with a normalized representation of appearance to aid in object modeling and recognition. It is applied in two domains: photographs, where models are created and recognized from a small number of images, and video, where dynamic scenes with multiple moving objects are segmented and matched using 3D models for efficient indexing and retrieval. This approach offers a new method for handling complex and cluttered scenes in both photography and video.
41779	417790	Labelled natural deduction for a bundled branching temporal logic	We present a complete and reliable method for reasoning in a bundled branching temporal logic called BCTL*, which is a modified version of CTL*. Unlike CTL*, BCTL* does not have the limit-closure property in its validity semantics. We offer both classical and intuitionistic versions of our labelled natural deduction system for the until-free version of BCTL* and analyze the intuitionistic system to show that derivations can be reduced to a normal form. This allows us to prove the consistency of the system using purely syntactical methods for both the intuitionistic and classical versions. 
41780	4178035	Matrix partitions of perfect graphs	The M-partition problem involves determining if a given graph G can be divided into m parts based on the rows and columns of a symmetric m by m matrix M, with specific adjacency rules based on the values in M. This problem is a generalization of graph coloring and homomorphisms and is often studied in perfect graphs. The paper focuses on M-partitions in perfect graphs and identifies a class of "normal" matrices for which the problem can be solved using a finite set of forbidden induced subgraphs. However, there are also matrices that do not fall into this class and can result in more complex M-partition problems. Some of these problems can be solved in polynomial time, while others are NP-complete and difficult to classify. 
41781	417819	Combined Discriminative Training For Multi-Stream Hmm-Based Audio-Visual Speech Recognition	This paper explores the use of discriminative training for a multi-stream hidden Markov model (HMM) based audio-visual speech recognizer (AVSR). The authors propose joint training of the two streams, in contrast to previous approaches that trained each stream separately. Experiments on a 20-speaker one-hour test set show a 22% relative gain in AVSR performance compared to models with separate training, and a 50% relative gain over the maximum-likelihood models. On a noisy test set, there is a 21% relative gain over models with separate training, representing a 30% improvement over the maximum-likelihood baseline. These results demonstrate the effectiveness of discriminative training for AVSR.
41782	4178247	A Fuzzy Logic Approach to Wrapping PDF Documents	This paper addresses the challenge of wrapping PDF documents, which is necessary for effective text data management. The proposed method utilizes fuzzy logic to handle the uncertainty inherent in the structure and presentation of PDFs. A bottom-up hierarchical approach is used, where a PDF wrapper is defined by specifying group type definitions that dictate the desired structure of groups of tokens. These definitions include fuzzy conditions based on spatial and content predicates. An algorithm for wrapper evaluation has been developed and implemented in a visual wrapper generation system. Experimental results have shown the system's accuracy and applicability to PDF documents from different domains.
41783	4178334	Querying Graph Databases	Graph data is becoming increasingly popular as a way to represent various types of databases. Many recursive queries in relational databases can be expressed as graph traversals. This paper introduces a language for searching through graph-like databases using extended regular expressions. The language allows for the creation of a partial order on the paths used for searching, as well as the ability to nondeterministically cut off low priority tuples. An algebra for partially ordered relations and an algorithm for computing path queries are also presented. This language has potential applications in hypertext databases, such as the World Wide Web.
41784	4178438	Web Communities: Models and Algorithms	Recent research has focused on improving web search engines by developing new techniques to enhance recall and precision. However, few studies have addressed the problem of identifying web communities. Most approaches use spectral or hierarchical algorithms, which overlook the fact that web communities are social networks with unique statistical properties. This paper analyzes web communities by studying the evolution of hubs and authoritative pages. By understanding how authors behave in relation to popular pages, the paper proposes a method for identifying relevant properties for specific topics. Experiments have confirmed the effectiveness of this model and technique. 
41785	4178549	Analyzing and Predicting Emoji Usages in Social Media.	Emojis are a popular form of graphical expression used in social media to convey emotions. They go beyond text and improve communication effectiveness. Machine learning advancements now allow for automatic composition of text messages with emojis. However, understanding and predicting emoji usage is a complex task. To address this, a dataset of emojis and tweets was created and their usage was systematically studied. This led to the development of a multitask multimodality gated recurrent unit (mmGRU) model, which leverages text, image, and user information to predict emoji categories and positions. The model showed significant improvements in accuracy (+9.0% for category and +4.6% for position). Case studies were also conducted to further understand the usage of emojis in social media.
41786	4178660	Emotional talking agent: System and evaluation	This paper presents a system for creating emotional audio-visual speech for a 3-D talking agent using the PAD emotional model. A GMM-based method is used to predict changes in acoustic features based on the PAD values, and a parametric framework is developed for emotional facial expression synthesis. Perceptual evaluations were conducted to understand the impact of vocal and facial expressions on emotion and to assess the effectiveness of the emotional talking agent in human-computer speech communications. The study found that the combination of audio and visual modalities has a strong reinforcing effect on emotion perception and that users prefer a multimodal interface for better comprehension of emotion. The results also demonstrate the success of the PAD-based emotional talking agent synthesis system.
41787	4178740	Hermes: agent-based middleware for mobile computing	Hermes is a middleware system that allows for the design and execution of activity-based applications in distributed environments. It supports mobile computation and allows application domain experts to focus on designing activity workflow without needing to consider the distributed environment. A context-aware compiler is responsible for generating mobile agents from a workflow specification. Hermes has a component-based, agent-oriented system with a three-layer software architecture and can be customized for specific application domains by adding domain-specific component libraries. The middleware layer, compilers, libraries, services, and other tools make up a general programming environment that has been proven successful in both industrial control and bioinformatics applications. In these applications, mobile agents are used for tasks such as product tracing, self-healing, data collection, service discovery, and simulating biological systems. 
41788	4178872	Horizon-Independent Optimal Prediction with Log-Loss in Exponential Families	. The focus of this paper is on the study of online learning using regular parametric models and logarithmic loss.2. Hedayati and Bartlett (2012b) found that a Bayesian prediction strategy with Jeffreys prior and sequential normalized maximum likelihood (SNML) are optimal if the latter is exchangeable and the optimal strategy can be calculated without knowing the time horizon beforehand.3. The authors investigated which families have exchangeable SNML strategies and found that for one-dimensional exponential families, the exchangeability can only occur for the Gaussian, Gamma, and Tweedie exponential family of order 3/2.4. This paper provides a solution to an open problem and highlights the importance of exchangeability in online learning.5. Keywords: SNML Exchangeability, Exponential Family, Online Learning, Logarithmic Loss, Bayesian Strategy, Jeffreys Prior, Fisher Information.
41789	4178965	Constructive cryptography --- a new paradigm for security definitions and proofs	Constructive cryptography is a new approach to defining and proving the security of cryptographic schemes. It involves constructing a resource with certain security properties from another, weaker resource. This is done through the use of a simulator, which allows for composability – meaning that a protocol created by combining multiple secure steps is also secure. This is different from traditional game-based and attack-based definitions of security. Constructive cryptography allows for a deeper understanding of security notions and the modular and composable design of protocols. It also separates the understanding of cryptography's goals from the technical definitions and proofs, making it useful for teaching and protocol design. 
41790	4179014	W3QS - A System for WWW Querying	W3QL is a high level language similar to SQL that allows users to access data and services on the World-Wide Web. It is declarative, meaning that a query is used to specify a graph to be matched with portions of the WWW. This includes nodes corresponding to web pages and edges representing hypertext links. Complex conditions can be applied to nodes and their relationships, and W3QL can also utilize existing search services like AltaVista. The language is also extensible, allowing users to use their own data analysis tools. W3QS is a system that manages W3QL queries and can be accessed through the WWW or a programming interface. Multiple interfaces are available, including graphic interfaces and templates for commonly used queries.
41791	4179111	Online control of enumeration strategies via bat algorithm and black hole optimization.	Constraint programming is a problem-solving approach that models problems as a series of variables and constraints. These variables have a range of possible values and the solving process involves assigning values to them to find potential solutions. The efficiency of this process depends on the selection of an enumeration strategy, which determines the order in which variables and values are chosen. This is a difficult task as the behavior of strategies is unpredictable and varies depending on the problem. To address this, recent research has focused on using a combination of strategies and evaluating them during the search process. This is known as online control of enumeration strategies, which can be seen as an optimization problem. In this paper, two new systems based on nature-inspired metaheuristics are proposed: bat algorithm and black hole optimization. These approaches show improved performance compared to previous work on online control, as demonstrated through experiments on various benchmarks.
41792	417925	Hyper-heuristics: a survey of the state of the art.	Hyper-heuristics are a set of methods that aim to automate the development of heuristic techniques for solving difficult computational search problems. This research field focuses on creating more widely applicable search methodologies. The term "hyper-heuristic" was first used in 2000 to describe techniques for choosing heuristics in combinatorial optimization, but the idea of automating heuristic design dates back to the 1960s. Hyper-heuristics refer to methods for selecting or generating heuristics to solve computational search problems. There are two main categories of hyper-heuristics: heuristic selection and heuristic generation. Unlike traditional approaches, hyper-heuristics operate on a search space of heuristics rather than directly on the search space of solutions. This paper provides an overview of the origins, types, and current research trends in hyper-heuristics, as well as potential directions for future research.
41793	4179337	Toward the automatic control of robot assembly tasks via potential functions: the case of 2-D sphere assemblies	The authors present a method for controlling automated assembly tasks using artificial potential functions. They focus on generating actuator commands for a robot manipulator to move rigid-body parts from disassembled to assembled configurations. The approach is applied to 2D sphere assemblies and a primitive constructive theory is presented for controlling this class of tasks. Preliminary computer simulations show promising results for the proposed approach.
41794	41794133	Determining trust in media-rich websites using semantic similarity	The growth of multimedia content on the Web has made it an integral part of people's lives. However, it is important for users to be able to determine the trustworthiness of the information they access. This is especially crucial since users typically rely on search engines like Google or Yahoo! to provide them with information. While these search engines may prioritize popular content, it may not always be trustworthy. To address this issue, a new method has been proposed to determine the trustworthiness of websites based on the similarity of their multimedia content with established and trusted websites. This method allows for dynamic computation of the trust level of websites across different domains, reducing reliance on traditional user feedback. The experimental results demonstrate the effectiveness of this approach.
41795	4179524	Exploring the underlying structure of haptic-based handwritten signatures using visual data mining techniques.	This paper discusses the analysis of haptic-based handwritten signatures using visual data mining techniques. The aim is to create virtual reality spaces using nonlinear transformations and optimizing for minimal information loss. The study compares the effectiveness of genetic programming and classical optimization methods in constructing these visual spaces using large haptic datasets. Different distance functions and classifiers are also explored to determine their impact on the representation accuracy and discrimination power of the visual spaces. Results show that both methods are able to effectively represent the relationships between haptic data objects and their classes, with classical optimization outperforming genetic programming in terms of mapping error and discrimination power. 
41796	41796117	Efficient Routing in Networks with Long Range Contacts	Long Range Contact graphs are defined by an underlying network topology and an extra link connecting each node to a distant neighbor, chosen randomly according to a probability distribution. These graphs are a good model for the small world phenomenon, and we investigate their use in greedy routing, a distributed routing protocol. We give bounds on the expected number of steps required for routing in a ring augmented with links chosen using the r-harmonic distributions, showing a 驴(log2 n)-bound for the 1-harmonic distribution. This suggests that the model of Kleinberg can be simplified by using a ring instead of a mesh. We also compare the performance of different distributions in terms of diameter and routing. Finally, we show how to define a probability distribution for any network and study the performance of greedy routing in the augmented network. 
41797	4179765	Demarcation of bacterial ecotypes from DNA sequence data: A comparative analysis of four algorithms	Microbiologists in a variety of fields, such as systematics, epidemiology, and biotechnology, would benefit from being able to identify closely related but ecologically distinct populations of bacteria. To aid in this task, several algorithms have been developed. This paper examines the effectiveness of four of these algorithms in correctly identifying ecotypes from sequence data, including information on the habitats where the bacteria were isolated. The algorithms were tested on synthetic sequences and data from Bacillus strains isolated from Death Valley. The results showed that one algorithm, Ecotype Simulation, performed significantly better than the others, but was also the least efficient.
41798	4179851	An evaluation of intelligent agent based innovation in the wholesale financial services industry	The success of wholesale financial services hinges on the development of flexible e-business models and strategies, as well as innovative systems for knowledge management and customer relationship management. Australian corporations in this sector face challenges and the paper proposes a research model to assess the impact of emerging intelligent agent technologies. This will help firms achieve international competitiveness by investigating and monitoring the use of these technologies and identifying successful approaches for adoption. The integration of e-business strategy, finance, agent architectures, and knowledge technologies offers a new solution to the challenges faced by the wholesale financial services industry. Agent architectures allow for dynamic and responsive systems that can adapt to the highly competitive global financial environment.
41799	41799155	Discriminative Density Propagation for 3D Human Motion Estimation	The article describes a new algorithm for estimating 3D human motion in videos using a discriminative approach. This means it does not rely on a predictive observation model, but instead uses a large motion capture database and a 3D human model to synthesize training data. The algorithm learns to predict conditional state distributions for 3D body pose tracking, without the need for a generative 3D model. It also improves mixing and initialization of generative inference algorithms. The paper presents three main contributions: establishing density propagation rules, proposing algorithms for learning multimodal state distributions, and demonstrating the effectiveness of the method in real and motion capture-based test sequences.
41800	4180034	Interconnect agnostic checkpoint/restart in open MPI	HPC applications at scale must be able to handle faults in order to utilize current and future HPC systems. MPI level transparent checkpoint/restart fault tolerance is a popular choice for developers who do not want to modify their code. However, previous MPI implementations have struggled to support a wide range of interconnects, especially shared memory. This paper introduces a new method for implementing checkpoint/restart coordination algorithms that is independent of the interconnect type. This allows applications to be checkpointed on one set of interconnects and restarted on a different set, adapting to changes in the cluster environment. Performance results for HPC applications using this approach are also presented.
41801	4180116	Computing pure strategy nash equilibria in compact symmetric games	This article discusses the complexity of computing pure strategy Nash equilibria (PSNE) in symmetric games with a fixed number of actions. The authors focus on "compact" representations, where the number of players can be exponential in the representation size. They show that determining the existence of PSNE in general is an NP-complete problem when utility functions are represented as arbitrary circuits. However, for games with two actions, a PSNE always exists and can be found in polynomial time. They then introduce a specific compact representation for piecewise-linear utility functions and provide polynomial-time algorithms for finding a sample PSNE, counting the number of PSNEs, and finding social-welfare-maximizing equilibria. This representation is extended to parameterized families of symmetric games, allowing for efficient methods of answering questions about the family without solving each game individually. 
41802	4180265	Boundary graph grammars with dynamic edge relabeling	eNCE graph grammars are a type of graph grammar that generate graphs with edge labels, in addition to node labels. These edge labels, along with the NCE feature, introduce new properties to the grammar. This is particularly evident in boundary eNCE (B-eNCE) grammars, which do not have the context-sensitive feature of "blocking edges" found in other eNCE grammars. B-eNCE grammars also have a Chomsky normal form and a Greibach normal form, and their languages can be characterized in terms of regular tree and string languages. It is proven that the class of (boundary) eNCE languages is larger than the closure of the class of (boundary) NLC languages under node relabelings. Similar results are found for linear eNCE grammars.
41803	4180324	Offline/realtime traffic classification using semi-supervised learning	The task of identifying and organizing network traffic based on application type is difficult due to the constantly changing nature of applications, particularly those that aim to go undetected. Traditional methods such as using port numbers are becoming less effective, leading to a reliance on deep packet inspection. However, this approach is resource-intensive, leading researchers to propose a new method of classifying traffic based on unique flow characteristics. This paper introduces a semi-supervised classification method that can handle both known and unknown applications, using a small number of labeled and a larger number of unlabeled flows for training. The evaluation of this approach on real network traffic shows high accuracy rates and the need for retraining only when there are significant changes in network usage patterns. Prototype systems are also implemented to demonstrate the feasibility of this method. 
41804	4180433	(s|qu)eries: Visual Regular Expressions for Querying and Exploring Event Sequences	In this paper, the authors introduce (s|qu)eries, a visual query interface for analyzing event sequence data using regular expressions. Many domains rely on finding patterns in event sequences to gain insights, but current systems have limitations in their expressiveness, workflow, and scalability. (s|qu)eries addresses these issues by providing a touch-based interface that allows users to build and visualize complex queries in an approachable way. This supports iterative and exploratory workflows, as well as debugging. The system was validated through interviews with data scientists who regularly analyze event sequence data. 
41805	4180521	Graph Isomorphism, Color Refinement, and Compactness.	Color refinement is a widely used technique to prove that two given graphs are not isomorphic. It is considered to be a highly efficient method, but it does not work for all graphs. A graph is said to be amenable to color refinement if the procedure successfully distinguishes it from any non-isomorphic graph. Babai et al. (1980) have demonstrated that random graphs have a high probability of being amenable. It has been further determined that the applicability of color refinement can be precisely determined by recognizing amenable graphs in \({O((n+m)\log n)}\) time, where n and m are the number of vertices and edges in the input graph.
41806	4180661	The Trouble with Long-Range Base Pairs in RNA Folding	RNA prediction accuracy is hindered by long-range base pairs, and a recent study analyzed the distribution of these pairs in known RNA structures. Surprisingly, long-range base pairs are actually overrepresented in these structures, challenging the idea that they are systematically overpredicted. Instead of kinetic effects, the study suggests that a modification to the expected accuracy model for RNA secondary structures could improve prediction. This modification includes a span-dependent penalty, which has shown to improve maximum expected accuracy structure predictions. The prevalence of long-range base pairs also suggests that RNA structures do not follow the polymer zeta property, which has implications for the performance of sparsified RNA folding algorithms.
41807	418072	Three-dimensional modeling from two-dimensional video	The paper introduces a surface-based factorization method for recovering the 3-D structure of a rigid object from a 2-D video sequence. The method uses polynomial patches to describe the object's shape and parametric 2-D motion models to track its projections in the image plane. By factorizing a rank 1 matrix, the method is able to simultaneously estimate the 3-D shape and motion parameters. This approach is an extension of the original factorization method by Tomasi and Kanade (1992) and simplifies the tracking process by focusing on regions with a single set of motion parameters. Experimental results demonstrate the effectiveness of this method in real-life video sequences.
41808	4180834	Towards a Compositional SPIN	This paper discusses the use of automated assume-guarantee verification with learning in the SPIN tool. This technique can complement existing state-reduction methods, allowing SPIN to handle larger systems. The authors present a "light-weight" approach for evaluating the benefits of learning-based assume-guarantee reasoning within SPIN, and experimented with different versions of this technique. They also introduce a new heuristic for generating component assumptions when their environment is unavailable. The effectiveness of learning-based assume-guarantee reasoning is demonstrated through an example of a resource arbiter for a spacecraft. While this approach currently has performance overheads, the authors believe it can be integrated into SPIN for improved memory savings.
41809	4180926	Exploiting Simd Parallelism With The Cgis Compiler Framework	Desktop PCs now have various parallel processing units, making it a challenging task for programmers to develop applications that take advantage of this parallelism. To do so efficiently, programmers need to have in-depth knowledge about the hardware. CGiS is a data-parallel programming language that simplifies this process by providing a unified abstraction for both graphics processing units (GPUs) and vector processing units of CPUs. The CGiS compiler framework virtualizes the differences in capability and accessibility, allowing for easy mapping of the abstract programming model to these targets. This approach has been successfully applied to CPUs and has now been extended to SIMD instruction sets. Experimental results have shown that real-world applications can be easily implemented with CGiS and produce efficient code.
41810	4181025	Pegasos: primal estimated sub-gradient solver for SVM	We introduce a new stochastic sub-gradient descent algorithm for solving the optimization problem in Support Vector Machines (SVM). Our algorithm requires a smaller number of iterations to achieve a desired level of accuracy compared to previous methods. The number of iterations also scales linearly with the regularization parameter and the runtime is dependent on the number of non-zero features in each example. This makes our algorithm ideal for learning from large datasets, particularly in text classification problems. It can also be applied to non-linear kernels, but in this case, the runtime is dependent on the size of the training set. Our algorithm shows a significant increase in speed compared to previous SVM learning methods.
41811	4181155	A Relation-Based Schema for Treebank Annotation	This paper introduces a new annotation schema for treebanks, focusing on its use in creating a corpus of Italian sentences. The schema differentiates between arguments and modifiers, providing a more precise representation of predicate-argument structure and subcategorization. The accuracy of the schema relies on the methods used to define the relations, which consist of three components: morpho-syntactic, functional, and semantic. The authors provide evidence for the effectiveness of these tripartite structures through examples encountered while developing the Italian treebank.
41812	4181238	A posteriori error estimates for nonconforming finite element methods for fourth-order problems on rectangles	The a posteriori error analysis for conforming finite element discretisations of the biharmonic problem for plates is well-established, but nonconforming discretisations are easier to implement in practice. However, the a posteriori error analysis for the Morley plate element is unique because two edge contributions from an integration by parts cancel out. This property is not present in popular nonconforming finite element schemes such as the nonconforming rectangular Morley, incomplete biquadratic, and Adini finite elements. To address this issue, a new methodology using a conforming discrete space on macro elements is introduced to prove the reliability and efficiency of an explicit residual-based error estimator. This technique is successfully applied to the Morley triangular finite element, with numerical experiments confirming its reliability and efficiency on uniform and graded tensor-product meshes.
41813	4181324	Architectural evolution of FamiWare using cardinality-based feature models	Ambient Intelligence systems are constantly evolving with the introduction of new devices and technologies. To manage these changes, a software product line engineering process has been developed for FamiWare, a family of middleware for ambient intelligence environments. This process uses cardinality-based feature models and clonable features to handle the structural variability of these systems. The process automates the management of feature changes and propagates them to the FamiWare middleware code. It is able to handle thousands of features, which would be impossible to manage manually. The process also calculates the effort needed to make changes in customized products. Two operators have been defined to assist with these tasks. The approach has been validated through case studies, showing that it can successfully handle changes in FamiWare configurations with thousands of features.
41814	4181437	Inference with separately specified sets of probabilities in credal networks	We have developed new methods for making predictions in credal networks, which are graphs that represent relationships between variables using sets of probabilities. These networks reflect strong independence between variables. Our approach involves using separate sets of probabilities and we have proven that making predictions in polytrees is a difficult problem. To address this issue, we have developed techniques that make the process more efficient by examining the separability of these sets of probabilities. This can greatly reduce the computational effort required, especially in polytrees. 
41815	4181562	General lower bounds for evolutionary algorithms	Evolutionary optimization, specifically genetic optimization, is a widely used framework for optimizing problems. It is known for being user-friendly, robust, and not requiring derivatives. However, it is also known to be slow. Recent research has shown that the convergence rate of popular evolution strategies cannot be faster than linear, and the constant in the linear convergence decreases quickly as the dimension increases. This limitation is not only present in evolution strategies, but also in comparison-based algorithms like the Hooke & Jeeves algorithm and the simplex method. However, using the full ranking information of the population instead of just selections can lead to faster convergence rates, potentially providing superlinear convergence results. 
41816	418162	3D Warp Brush: Interactive Free-Form Modeling on the Responsive Workbench	The 3D warp brush is a new method for creating shapes in a virtual reality environment. It operates on triangle meshes and combines the efficiency of explicit mesh representations with implicit modeling operators. The brush has an adjustable area of influence and offers different warp functions such as drag, explode, and whittle. A unique aspect is the ability to convert meshes into 3D warp brushes in real-time. The use of a Responsive Workbench and two-handed interaction makes it easy for users to modify surfaces and create desired shapes. Examples of models created with this method showcase its effectiveness.
41817	4181777	Attribute generation based on association rules	A decision tree is a useful tool for accurately classifying data and keeping the tree size small. To improve the performance of a decision tree, new attributes and values can be added during data pre-processing. However, current methods for creating new attributes are time-consuming and require prior knowledge of the data. To address this issue, a new approach is proposed that extracts knowledge on relevant attributes through association rules from the training data. The new attributes and values are then generated based on these rules. Experiments show that this approach is effective in improving the performance of decision trees.
41818	4181856	Intelligent Social Learning	The paper discusses social learning, which is the process by which individuals acquire new information through exposure to others in a common environment. This can occur through social facilitation, where individuals are influenced by the presence of others, or through imitation. The paper presents a general definition of social learning and analyzes the cognitive processes involved in social facilitation and imitation. It also highlights the lack of a systematic understanding of social learning and proposes a continuum of cognitive complexity for these processes. The paper draws upon a cognitive model of social action and discusses how this approach can be useful in agent systems applications.
41819	418196	Pairing the Volcano	Isogeny volcanoes are mathematical structures composed of elliptic curves connected by l-isogenies. Researchers have previously developed algorithms for navigating these graphs, but they were unable to predict the direction of a step before taking it. This led to a trial-and-error approach, resulting in many unnecessary steps. In this paper, the authors propose a more efficient method for finding points of order l on the volcano that generate a horizontal or ascending isogeny. Their method is faster than previous ones, and they also explore the case of 2-isogeny volcanoes and derive a new invariant. Their benchmarks show that their algorithm is faster than previous methods for computing the endomorphism ring of an elliptic curve.
41820	4182044	An intelligent agent-based scheme for vertical handover management across heterogeneous networks.	The advancements in technology have led to the widespread use of mobile devices and innovative solutions for various applications and services. As the demand for constant and seamless connectivity increases, network operators are required to integrate different types of wireless and cellular networks. This integration necessitates the ability for users to freely move between networks, known as vertical handover. However, due to the involvement of various technologies, the issue of vertical handover needs to be addressed. Several solutions have been proposed but most lack intelligence and adaptability, resulting in packet loss and delay. This paper suggests an intelligent cooperative agent based approach using a knowledge plane to address this problem. Agents are introduced in mobile devices and access points to collect information from the environment and make handover decisions. A selection function is also introduced to choose the best network for handover. The proposed approach is validated through simulations.
41821	4182190	Aavp: An Innovative Autonomic Architecture For Virtual Network Piloting	The Internet has experienced significant growth in recent years, leading to the need for new technologies and applications. However, the size and scale of the Internet has also created challenges, making it necessary to design a Next Generation Internet. One solution is network virtualization, which allows for different network architectures and protocols to be used on a shared physical infrastructure. While this offers many advantages, it also adds complexity to network systems. To address this, an autonomic computing framework is proposed, which allows for self-management of virtual resources. Testing on a real test-bed has shown the effectiveness of this framework in maintaining a desired quality of service.
41822	4182222	Service management for multi-operator heterogeneous networks	The emergence of heterogeneous radio networks, which aim to provide ubiquitous services, requires a comprehensive management approach. In this paper, we propose an integrated management approach for end-to-end service management in a multi-operator environment. Our solution can be easily integrated into existing web-based management platforms, making it a flexible and scalable option. This approach allows for efficient management of services across various networks, ensuring a seamless experience for users. 
41823	4182347	A CIM Extension for Peer-to-Peer Network and Service Management	Peer-to-peer (P2P) networks and services are becoming more prevalent in the networking industry, particularly for enterprise solutions. As a result, there is a need for an open management approach that ensures quality of service (QoS) parameters are met. This paper focuses on the design of a management framework for P2P networks and services. A management information model has been developed, based on the standard CIM model, to capture the various functional, organizational, and topological aspects of the P2P model. This model is essential for effectively managing P2P networks and services.
41824	418247	Multipub: Latency And Cost-Aware Global-Scale Cloud Publish/Subscribe	Topic-based pub/sub is a popular way to communicate between loosely connected entities in distributed systems. To handle varying communication demands, pub/sub services can be deployed in the cloud and distributed across multiple regions for faster dissemination. However, the design and deployment of such middleware can impact communication latency and cloud costs. To address this, the paper proposes MultiPub, a flexible middleware that can dynamically reconfigure the communication layer to achieve a predefined maximum latency while minimizing costs. Experiments show that MultiPub can effectively reduce latency and save costs compared to traditional approaches. 
41825	4182529	Development of Fault-Tolerant Software Systems Based on Architectural Abstractions	This paper explores the use of architectural abstractions in developing fault-tolerant software systems. These abstractions represent different aspects of fault tolerance, such as error detection and handling, and can be instantiated into concrete components and connectors. By formally specifying the structural and behavioral properties of these abstractions, the process of verifying and validating software architectures can be automated. The paper discusses how these abstractions, along with a recursive process, can aid in the modelling and analysis of fault-tolerant software systems. The effectiveness of this approach is demonstrated through a case study of a critical real-time application. Overall, the use of architectural abstractions can help manage the complexity of incorporating fault tolerance into systems. 
41826	4182626	Fifth workshop on software engineering for large-scale multi-agent systems (SELMAS)	As technology continues to advance, software is becoming more and more embedded in our daily lives, leading us towards a future of ambient computing. Multi-agent systems (MAS) are a key technology in creating and managing large-scale distributed systems. In recent years, there has been a focus on improving the design and implementation of MAS through software engineering research. However, ensuring the reliability of these large MAS remains a challenge. The Fifth Workshop on Software Engineering for Large-Scale Multi-Agent Systems (SELMAS 2006) aims to bring together experts from various fields to discuss the latest technologies and approaches for creating dependable MAS. 
41827	4182717	A Batch-Authenticated and Key Agreement Framework for P2P-Based Online Social Networks.	Online social networks (OSNs) like Facebook and MySpace have become popular platforms for people to share their interests with friends. However, the security and privacy of these networks are major concerns. To address this issue, a security framework has been proposed for authenticating multiple users simultaneously in P2P-based OSNs. The framework includes three batch authentication protocols that use different cryptographic systems such as one-way hash function, ElGamal proxy encryption, and certificates. These protocols offer various advantages, such as lower computational cost, exchange of more information among users, and guaranteeing non-repudiation of transactions. The proposed framework does not require a centralized authentication server, making it suitable for extending OSNs with batch verifications. The security of the protocols has been formally proven against passive adversaries and impersonator attacks. They also meet important security requirements such as mutual authentication, reputation, community authenticity, non-repudiation, and flexibility. This makes the framework an appropriate choice for P2P-based OSNs.
41828	4182835	Near-Optimal Evasion of Convex-Inducing Classifiers	Classifiers are commonly used to identify and prevent miscreant activities. In this study, we examine how an adversary can manipulate a classifier by making strategic queries in order to avoid detection at a low cost. We build upon previous research by Lowd and Meek (2005) and extend it to include convex-inducing classifiers. Our algorithms are able to generate undetected instances with minimal cost using a polynomial number of queries, without needing to reverse engineer the decision boundary. This research sheds light on the vulnerabilities of classifiers and provides insights for improving their effectiveness against malicious manipulation.
41829	4182968	Evaluating tax sites: an evaluation framework and its application	In recent years, governments have been implementing e-services to provide faster and more advanced services to their citizens. One important aspect of these e-services is tax filing and payment, and this paper examines the key factors that contribute to a successful tax website. The goal is to create an evaluation framework that takes into account the needs of citizens, which can be used by both government officials and website developers to improve the quality of services provided to citizens. This framework has the potential to enhance the overall user experience and efficiency of tax websites.
41830	4183077	Multimodal representation, indexing, automated annotation and retrieval of image collections via non-negative matrix factorization	The availability of large image collections on the internet has led to the incorporation of non-visual data such as text descriptions, comments, ratings, and tags. These additional data can enhance the analysis of image content and improve its performance. This paper introduces a new method, based on non-negative matrix factorization, to create multimodal representations of images that combine visual features with text information. This approach identifies common factors in different types of data and integrates them into the same representation space. The effectiveness of this method was evaluated in image indexing and search tasks, and the results showed significant improvement compared to other methods using singular value decomposition.
41831	4183114	Finding a Sun in Building-Free Graphs	A recent study showed that determining whether a graph contains a sun is a difficult problem. However, for building-free graphs, this can be solved in O(min{mn 3, m 1.5 n 2}) time and the sun can be found in the same time. Building-free graphs include perfect graphs such as Meyniel graphs, as well as imperfect graphs. This class also encompasses other graph classes, making it a useful generalization. Additionally, a vertex elimination method is presented for (building, gem)-free graphs, which are a type of distance hereditary graphs and a subset of (building, sun)-free graphs.
41832	4183261	Perception of Material from Contact Sounds	Contact sounds play a crucial role in how we perceive materials in virtual environments. Researchers studied the relationship between material perception and factors that influence the creation of contact sounds. They found that an auditory decay parameter, which reflects the sound's rate of fading, strongly influenced how people perceived an object's material. In experiments where participants were asked to compare the similarity of synthesized sounds based on material or length, the decay rate had a greater impact than the sound's fundamental frequency. This was further confirmed in a study where participants were asked to categorize sounds into different material types. The results suggest that a simplified model of material can be used in virtual auditory environments, with the decay rate being a key factor in creating realistic material perception.
41833	4183338	Breaking up is hard to do: An evaluation of automated assume-guarantee reasoning	Finite-state verification techniques can be hindered by the state-explosion problem, where the number of states in a system grows exponentially. Assume-guarantee reasoning is a proposed approach to mitigate this issue by breaking down a system into smaller subsystems and analyzing them individually. This allows for a more efficient and cost-effective verification process. However, assumptions about the environment in which a subsystem operates must be provided, which has traditionally been a difficult manual task. Recent advances in automatically generating these assumptions have prompted a study to compare assume-guarantee reasoning with monolithic verification. The results showed that in only a few cases, assume-guarantee reasoning was able to verify larger systems than monolithic verification, highlighting the need for further research and experimental evaluation in this area.
41834	4183492	The right algorithm at the right time: comparing data flow analysis algorithms for finite state verification	Finite state verification is becoming increasingly important in proving properties about software. In this process, analysts go through different modes of operation, such as exploratory, fault finding, and maintenance. During the exploratory mode, inconsistencies are often found due to flaws in the properties or the software itself. This leads to the analyst shifting to a fault finding mode, where they need counter example traces to determine the cause of the inconsistency. As the system becomes more stable, maintenance mode is entered and re-verification is needed to ensure consistency. It is believed that having different algorithms optimized for each mode of use would be most beneficial for the analyst.
41835	4183548	Scalable fault tolerant protocol for parallel runtime environments	The use of high performance computing platforms is increasing in order to tackle larger and more complex problems. This has led to a need for parallel runtime environments that can adapt to changing hardware and library requirements. This paper focuses on the design of a scalable and fault tolerant protocol for communication within these environments. The protocol supports message transmission across multiple nodes and includes a self-healing topology to prevent failures. Formal protocol verification has been conducted for both normal and failure cases. Multiple routing algorithms have been implemented, with the rule-based variant being the most effective for damaged or incomplete topologies.
41836	4183669	Identifying redundancy and exposing provenance in crowdsourced data analysis.	The presented system allows analysts to use paid crowd workers to explore data sets and build upon their insights. Crowd workers can easily perform basic analysis tasks, but may generate redundant explanations. To efficiently use the crowd's work, analysts must be able to identify and consolidate redundant responses and determine the most plausible explanations. The paper demonstrates various techniques to assist analysts in using crowdsourced explanations, such as utilizing multiple workers to detect redundancies and capturing explanation provenance through tasks and an interface for filtering and selecting the most plausible explanations.
41837	418375	On Regulatory and Organizational Constraints in Visualization Design and Evaluation.	Problem-based visualization research offers clear guidance for understanding and designing for user needs, but lacks guidance on other external factors that can impact visualization design and evaluation. This gap can leave researchers and practitioners vulnerable to unexpected constraints that may affect the validity of evaluations or result in project failure. This article focuses on two types of external constraints, regulatory and organizational, and discusses how they can impact visualization design and evaluation. The authors suggest strategies for identifying, mitigating, and evaluating these constraints using a design study methodology, and provide a healthcare case study as an example. By incorporating external constraints into visualization design and evaluation, researchers and practitioners can improve the effectiveness and success of their solutions, especially in industries where these constraints are prevalent.
41838	4183837	A process calculus for Mobile Ad Hoc Networks	The @w-calculus is a process calculus used to formally model and reason about Mobile Ad Hoc Wireless Networks (MANETs) and their protocols. It accurately captures key features of MANETs, such as the ability of nodes to broadcast messages within their transmission range and move in and out of other nodes' transmission range. The @w-calculus separates a node's communication and computational behavior from its physical transmission range. The calculus has a formal operational semantics and is a conservative extension of the @p-calculus. It also has a symbolic semantics and a corresponding notion of symbolic bisimulation equivalence. The @w-calculus is useful for developing and analyzing formal models of MANET protocols.
41839	4183956	Digital stereo edges from zero crossing of second directional derivatives	The facet model is used for step edge detection. It involves analyzing the pixel values in a neighborhood and interpreting them relative to the underlying gray tone intensity surface. A pixel is marked as a step edge if there is a negatively sloped zero crossing of the second directional derivative in the direction of a nonzero gradient at the pixel's center. To determine this, the underlying gray tone intensity surface is estimated using a functional form with discrete orthogonal polynomials up to degree three. This allows for easy computation of the appropriate directional derivatives. Compared to other operators, the zero crossing of the second directional derivative performs the best for edge detection, followed by the Prewitt gradient operator, and the Marr-Hildreth zero crossing of the Laplacian operator performs the worst.
41840	4184039	A Methodology and Toolkit for Deploying Reliable Security Policies in Critical Infrastructures.	The rapid development of Information and Communication Technologies (ICT) has led to the emergence of intelligent and autonomous systems in critical infrastructures. These systems are being integrated into various domains such as transportation, communication, finance, commerce, and healthcare through different types of networks. However, ensuring the security and protection of sensitive data and services in these critical infrastructures is a major challenge. To address this issue, the authors propose a methodology for deploying and monitoring trusted access control policies. This methodology incorporates formal and semiformal techniques to specify, verify, implement, reverse-engineer, validate, assess risk, and optimize access control policies. To facilitate this process, the authors introduce their system SVIRVRO. An example is provided to demonstrate the effectiveness of this methodology.
41841	418411	On the Differential Geometry of 3D Flow Patterns: Generalized Helicoids and Diffusion MRI Analysis	The article discusses the use of dense locally parallel 3D curves in medical imaging, computer vision, and graphics. These curves can be found in various structures such as white matter fibre tracts, textures, fur, and hair. The authors propose a differential geometric approach to characterize these structures by examining the behavior of the associated 3D frame field and its curvature functions. They also introduce the use of a generalized helicoid model as an osculating object and establish a connection between its parameters and the curvature functions. This allows for the creation of parametrized 3D vector fields that can approximate these patterns. The approach is applied to the analysis of diffusion MRI data, showcasing the benefits of considering the full differential geometry.
41842	4184278	The Hamilton-Jacobi Skeleton	The eikonal equation and its variations are important in computer vision and image processing, as it is used for continuous versions of mathematical morphology, stereo, and shape-from-shading. Its numerical simulation can be challenging due to the formation of singularities, but there are classical approaches from Hamiltonian physics that have not been explored in computer vision. This paper presents a new algorithm for simulating the eikonal equation that has advantages in tracking shocks, as well as an efficient method for detecting shocks by measuring outward flux and applying an energy conservation principle. Several numerical examples, including complex shape skeletons, are provided to illustrate the approach.
41843	4184340	Tubular Surface Segmentation for Extracting Anatomical Structures From Medical Imagery	This work presents a model and algorithm for automatically extracting tubular structures from medical imagery. The model is able to fit various anatomical structures, such as fiber bundles in the brain and blood vessels in computed tomography angiograms. The extraction of these structures has potential benefits in the diagnosis of diseases such as schizophrenia and cardiovascular issues. The proposed tubular model has advantages over existing methods, including fewer degrees of freedom and a more efficient representation in 4-D. The algorithm can also detect and evolve branches of tubular trees. Testing on datasets shows promising results when compared to expert segmentations. 
41844	41844131	Visual Attention Accelerated Vehicle Detection in Low-Altitude Airborne Video of Urban Environment	The airborne vehicle detection system aims to reduce collisions and traffic congestion caused by the increasing number of vehicles. Unlike stationary systems, airborne systems mounted on unmanned aircrafts or satellites have a wider view angle and higher mobility. However, detecting vehicles in aerial videos is challenging due to complex scenes and platform movement. To address this, a new two-stage method is proposed, consisting of attention focus extraction and vehicle classification. The first contribution is a fast attention focus extraction algorithm that detects candidate vehicle regions for quicker computation. The second contribution is a simple and efficient hierarchical classification process using the AdaBoost learning algorithm to reduce false alarms. Experimental results show that this method outperforms other algorithms in terms of detection rate and false positive rate while meeting real-time application requirements.
41845	4184524	Does JavaScript software embrace classes?	JavaScript is the dominant programming language for the Web and is used to create various applications such as mail clients and office applications. These applications can contain hundreds of thousands of lines of code and rely on informal class abstractions to manage complexity. However, there has been little research on how these class abstractions are used in JavaScript. To address this gap, a study was conducted on 50 popular JavaScript applications from GitHub. The study identified four types of JavaScript software: class-free, class-aware, class-friendly, and class-oriented. These categories represent varying levels of usage of classes, with class-oriented systems being the least common at 8%. Understanding how these classes are used is important for improving programming environments and structure libraries to better support developers.
41846	418469	Personalized Web search for improving retrieval effectiveness	Current Web search engines are designed to cater to the needs of all users, without considering the specific requirements of individual users. Personalization of Web search involves customizing the search results based on the interests of each user. A new method is proposed to learn user profiles from their search histories, which are then used to enhance the effectiveness of Web search. The technique involves learning a user profile from their search history and a general profile from a category hierarchy. These profiles are combined to map the user's query to a set of relevant categories, providing context to disambiguate the query words. The search is then conducted using both the user's query and the set of categories. Various algorithms for profile learning, category mapping, and fusion are evaluated, showing that this personalized approach is both effective and efficient.
41847	418474	A Multi-View Embedding Space for Modeling Internet Images, Tags, and Their Semantics	This paper addresses the issue of modeling Internet images and associated text or tags for various tasks such as image-to-image search, tag-to-image search, and image annotation. The authors utilize canonical correlation analysis (CCA) to map visual and textual features to a latent space and incorporate a third view that captures high-level image semantics. They present two methods for training this three-view embedding: supervised, using ground-truth labels or search keywords, and unsupervised, using semantic themes obtained through tag clustering. To ensure accurate retrieval while maintaining scalability, multiple visual features are combined and explicit nonlinear kernel mappings are used. A specially designed similarity function is used for retrieval, which outperforms the traditional Euclidean distance. The proposed system shows promising results and outperforms baseline methods on large-scale Internet image datasets.
41848	4184834	Approximation algorithms for metric facility location and k-Median problems using the primal-dual schema and Lagrangian relaxation	This article discusses approximation algorithms for two common optimization problems in facility location: the metric uncapacitated facility location problem and the metric k-median problem. These algorithms have a low running time of O(m logm) and O(m logm(L + log (n))) respectively, where m and n are the total number of edges and vertices in a complete bipartite graph of cities and facilities. The main techniques used are a new approach to the primal-dual schema and the application of Lagrangian relaxation to develop efficient approximation algorithms. These algorithms have guarantees of 3 and 6 for the uncapacitated facility location problem and the k-median problem respectively.
41849	4184969	Secure Overlay Network Design	As Internet security threats continue to increase, new overlay network architectures have been proposed to secure privileged services. These architectures involve a defense perimeter where only servelets are allowed to pass, and end users must be authorized to communicate with access points (APs). The goal is to minimize the number of APs that could be compromised by an attacker. In this paper, the authors present a polynomial-time algorithm for designing the connections between APs and servelets in the average-case model, and also show that a star-shaped design is optimal when the probability of AP failure is at least 1/2. In the worst-case model, they use combinatorial set theory to provide bounds on the number of APs that a perfectly failure-resistant design can support. These results provide a theoretical foundation for practical secure overlay network design. 
41850	4185058	Allocating online advertisement space with unreliable estimates	The article discusses the problem of optimally allocating online ad space to advertisers with limited budgets. The objective is to find an algorithm that can efficiently use estimated keyword frequencies to make near optimal decisions, while also maintaining a good worst-case competitive ratio. This is important in real-world situations where search engines have stochastic information that can accurately estimate search query frequencies, but may also have unpredictable spikes. The approach is a black-box one, using an oracle to recommend advertisers based on the estimates. The algorithm provides two performance guarantees, which can be adjusted by a parameter, and is applied to two online problems.
41851	41851134	Spanners in sparse graphs	The concept of a t-spanner in a graph is introduced, where the distance between any two vertices is at most t times their distance in the original graph. If the t-spanner is required to be a tree, it is called a tree t-spanner. In 1998, Fekete and Kremer proved that the problem of determining if a planar graph has a tree t-spanner is polynomial time solvable for t ≤ 3, but NP-complete for larger t values. They also posed the question of whether the problem is polynomial time solvable for every fixed t ≥ 4. In this work, the authors resolve this open problem and extend the solution to a larger class of sparse graphs. They show that the problem is fixed parameter tractable for planar graphs of bounded treewidth, as well as for a more general class of graphs called apex-minor-free graphs. However, the problem remains NP-complete for K6-minor-free graphs, indicating that the restriction to apex-minor-free graphs is necessary for tractability.
41852	4185263	Random classification noise defeats all convex potential boosters	Boosting algorithms, such as AdaBoost and LogitBoost, can be seen as using gradient descent to minimize a potential function based on the margins of a data set. However, a recent study has shown that these algorithms are highly affected by random classification noise. This means that even with techniques like early stopping or weight regularization, these boosters cannot accurately learn a data set with a nonzero random noise rate. This is in contrast with other boosting algorithms that do not fall into the convex potential function framework and can still achieve high accuracy in the presence of random noise. 
41853	4185347	Mining the Student Assessment Data: Lessons Drawn from a Small Scale Case Study	This paper discusses a case study on educational data mining (EDM) using data from an online assessment where students received tailored feedback after each question. The focus is on analyzing the effectiveness of the questions and feedback in meeting individual student needs. Despite using traditional data mining techniques such as clustering, classification, and association analysis, it was difficult to obtain meaningful and insightful results due to the small dataset and well-defined problems. This case study highlights the challenges in applying DM to educational data and the need for more advanced approaches. 
41854	4185448	Applying SAT Solving in Classification of Finite Algebras	The classification of mathematical structures is important for pure mathematics research, but it is a meticulous task. Automated techniques can aid in this process, with many focusing on the quantitative side such as counting isomorphism classes. However, a bootstrapping algorithm has been developed for qualitative classification, producing theorems that describe unique properties for isomorphism classes. To fully verify the classification, a range of problems must be proven, which can be challenging for traditional automated theorem provers. This paper discusses the use of Boolean satisfiability solving to generate fully verified classification theorems in finite algebra, and presents various methods for encoding these problems to improve the bootstrapping algorithm. Experimental evidence shows the effectiveness of these methods.
41855	4185575	Composite model-checking: verification with type-specific symbolic representations	Automated verification methods based on state exploration have made significant progress in areas such as hardware design, improving testing and validation processes. Symbolic model-checking, which uses compact data structures like Binary Decision Diagrams (BDDs) to check for safety and liveness properties, has been successful in this field. However, these techniques have not been as successful in software systems due to limitations in dealing with infinite-state programs and efficiently representing different variable types. A new approach has been proposed that combines BDD and arithmetic constraint representations to overcome these limitations. This composite model-checking strategy can be extended to other symbolic representations and also includes approximation techniques to address the undecidability of model-checking on infinite-state systems. This approach has been demonstrated to be effective in analyzing software specifications with a mix of booleans, integers, and enumerated types. 
41856	4185613	Unifying Justifications and Debugging for Answer-Set Programs.	Viegas Damasio et al. (2013) introduced a method to construct propositional formulas for logic programs that encode provenance information. These formulas can be used to extract justifications for a given interpretation, but do not provide information on why the interpretation is not an answer-set. In contrast, Gebser et al. (2008) use meta-programming transformations to debug logic programs and identify necessary changes for an interpretation to be an answer-set. The authors propose a unified approach that combines these two methods, using meta-programming transformations to generate answer-set programs that can compute provenance and debugging models directly. This approach is more efficient and generalizes the debugging one, as any error can be traced back to a provenance but not the other way around. The authors developed a proof-of-concept tool to demonstrate the effectiveness of their approach.
41857	4185725	Argumentation based decision making for autonomous agents	This paper proposes an argumentation based framework to help agents make decisions within a modular architecture. The framework is dynamic and takes into account the context in which the agent is operating, allowing for adaptability in a changing environment. To address incomplete information in open environments, abduction has been integrated into the framework. This is especially useful in situations where the agent is facing a dilemma and needs additional information to make a decision. Additionally, a personality theory for agents has been developed within the same framework, adding an element of individuality to the decision making process based on principles from Cognitive Psychology.
41858	4185822	On the Pedagogically Guided Paper Recommendation for an Evolving Web-Based Learning System	This paper discusses a unique recommender system for suggesting papers in a web-based learning environment. The system is based on the observation of learners and their behaviors, allowing the most suitable papers to survive and be recommended. It also introduces a pedagogically layered similarity between read and recommended papers, taking into consideration both interest and pedagogical suitability. The third aspect is the annotation of papers with temporal sequences of learners' behaviors, which helps maintain objectivity and integrity while providing deeper insights into their knowledge levels and enabling 'just-in-time' recommendations for e-learning support.
41859	4185981	Investigation of the Social Predictors of Competitive Behavior and the Moderating Effect of Culture.	Research has found that Competition is a strong motivator for changing behavior, but there is limited understanding of its effectiveness and how it is impacted by culture. This study examined the predictors of Competition using three social influence measures: Reward, Social Comparison, and Social Learning. The results showed that Reward has the greatest influence on Competition, followed by Social Comparison, but Social Learning has no significant impact. Culture was also not found to moderate the relationship between these constructs. This suggests that designers of gamified applications can use Reward, Social Comparison, and Competition together to motivate behavior change in both individualistic and collectivistic cultures. 
41860	4186015	Combining Online Algorithms for Acceptance and Rejection	 derive a combined algorithmResource allocation and admission control are essential tasks in communication networks, often needing to be performed in real-time. These problems are typically approached under either a benefit or cost model, seeking to maximize accepted requests or minimize rejected requests, respectively. However, algorithms designed for these different objectives can be vastly different. This study aims to combine algorithms from both models to achieve good results for both objectives. The proposed approach involves using an algorithm with a competitive ratio of cA for accepted requests and cR for rejected requests, resulting in a combined algorithm with a competitive ratio of O(cRcA) for rejection and O(cA) for acceptance. This method can also be extended to a collection of k algorithms.
41861	41861126	On profiling blogs with representative entries	This paper explores the challenge of finding relevant blogs in a rapidly growing blogosphere. To address this issue, the authors propose a new problem of profiling a blog by selecting a set of representative entries. This allows for more efficient and accurate searching and matching, as it avoids dealing with the large number of frequently updated and noisy entries in a blog. The proposed entry selection algorithm is guided by three principles - anomaly, representativeness, and diversity - and is found to be highly efficient and effective in classification accuracy. By using fewer than 20 representative entries, comparable accuracy can be achieved compared to using all entries. 
41862	4186227	Stochastic Convex Optimization with Multiple Objectives.	If you want to change your name on a website or platform, you can use the "Report an Issue" link to make the request. This link is usually located in the support or help section of the website. By clicking on it, you can send a message to the website's customer service team explaining the reason for your name change and providing any necessary documentation. They will review your request and make the necessary changes if approved. This is a simple and efficient way to request a name change on a website or platform.
41863	41863239	A neural network approach to smarter sensor networks for water quality monitoring.	Environmental monitoring is becoming more advanced with the use of large-scale and affordable sensor networks that can operate reliably and autonomously for extended periods of time. However, sophisticated analytical instruments like chemo-bio sensors have limitations in terms of the number of samples they can take. To overcome this, multiple sources of information can be coordinated to maximize their deployment lifetime. In a study conducted at the River Lee in Cork, Ireland, a neural network was used to control the sampling frequency of a phosphate analyzer based on inputs from rainfall radar images and a water depth sensor. The results showed that this approach can effectively improve the efficiency of the sensor network, even with limited training data.
41864	4186476	Estimating Mixture Models via Mixtures of Polynomials.	Mixture modeling is a technique that combines simple models with weighting to make them more expressive. The Expectation Maximization (EM) algorithm is commonly used to update and improve mixture models, but it does not guarantee global convergence due to the non-convex nature of the likelihood function. Method of moments approaches have been developed to provide global guarantees for some mixture models, but they are not applicable to all types of mixture models. In this study, a new framework called Polymom is introduced, which is based on the method of moments and allows for easy derivation of estimation procedures similar to EM. The framework uses a Generalized Moment Problem and solves it using semidefinite optimization and computer algebra. This approach allows for insights and tools from convex optimization and computer algebra to be applied to statistical estimation problems, with promising results shown in simulations. 
41865	4186536	Biasing evolving generations in learning classifier systems using information theoretic measures	This paper discusses the use of information-theoretic concepts to improve the evolution process in Learning Classifier Systems. By considering the potential information contained in individuals within each generation, the Sufficiency measure of a rule is used as an indicator of usefulness. This measure is then incorporated into the XCS algorithm during the early stages of each run to guide the selection process. Initial simulations demonstrate that this approach reduces the effort required to solve the 20-input multiplexer problem. Overall, the incorporation of information-theoretic ideas shows promise in improving the efficiency and effectiveness of Learning Classifier Systems.
41866	4186659	Inter-receiver fairness: a novel performance measure for multicast ABR sessions	In a multicast ABR service, connections are limited to the rate of the bottleneck link in the distribution tree, which can lead to unfairness among receivers with different preferred operating rates. To address this issue, a paper proposes allowing the connection to operate at a higher rate, taking into account each receiver's loss tolerance. A measure for inter-receiver fairness is developed and a technique for determining the optimal rate is presented. The paper also suggests using multiple virtual circuits for a single multicast session to improve fairness. Examples are provided to demonstrate the concept in various network scenarios.
41867	41867220	The Asymptotics of Ranking Algorithms	The article discusses the problem of supervised ranking, where the goal is to rank sets of candidate items returned in response to queries. Traditional statistical methods for this task require complete rankings from individuals, which is not practical in real-world scenarios. As a result, researchers have focused on modeling partial preference data, such as pairwise comparisons of items. However, the article shows that commonly used methods for this approach are not consistent, even in low-noise settings. To address this issue, the authors propose a new approach based on aggregation of partial preferences and develop empirical risk minimization procedures. These procedures are shown to be consistent through theoretical analysis and experimental results.
41868	41868132	Efficient message passing interface (MPI) for parallel computing on clusters of workstations	Parallel computing on clusters of workstations and personal computers has great potential due to its utilization of existing hardware and software. Parallel programming environments provide an easy way to express parallel computation and communication, with a recent industrial standard being the Message Passing Interface (MPI). However, existing implementations on clusters suffer from poor performance in the collective communication part due to being built on top of a point-to-point communication layer. In this paper, the authors present an efficient design and implementation of the collective communication part in MPI, optimized for clusters of workstations. This system consists of an MPI-CCL layer and a User-Level Reliable Transport Protocol (URTP), integrated with the operating system via a kernel extension mechanism. Results from performance measurements on a collection of IBM RS/6000 workstations connected via a 10-Mbit Ethernet LAN show that the system's performance is limited by interactions between the kernel and user space rather than the LAN's bandwidth. 
41869	4186941	Synthesis of Petri Nets with Whole-Place Operations and Localities.	This paper discusses the use of synthesising systems from behavioural specifications as a means of creating implementations that are automatically correct and do not require expensive validation processes. The systems are modelled using Petri nets and the specifications are given in the form of step transition systems. The focus is on synthesising Petri nets with whole-place operations and localities, which can express a wide range of system behaviours such as inhibiting actions, resetting local states, and maximal executions. The synthesis problem has been solved for specific net classes and a general approach has been developed for tau-nets. This paper adapts the synthesis techniques for wpol-nets, based on the concept of a region of a transition system.
41870	4187017	Parallel Shortest Path Algorithms for Solving Large-Scale Instances	This study explores the efficiency of the Delta-stepping parallel algorithm in solving the single source shortest path problem on large-scale graphs with non-negative edge weights. The performance of the algorithm is tested on the Cray MTA-2, a high-end shared memory system with features that aid in parallel implementation of irregular algorithms. The results show significant speedup compared to sequential algorithms for low-diameter sparse graphs. For example, the algorithm can solve a directed scale-free graph with 100 million vertices and 1 billion edges in less than ten seconds on 40 processors, with a speedup of close to 30. These are the first reported performance results for a shortest path problem on realistic graph instances with billions of vertices and edges.
41871	418715	DANCo: Dimensionality from Angle and Norm Concentration	Recently, determining the intrinsic dimensionality of a dataset has become increasingly important. However, current methods are unreliable when the dataset has a high intrinsic dimensionality and is nonlinearly embedded in a higher dimensional space. To address this issue, a new robust estimator is proposed in this paper. It utilizes information from normalized nearest neighbor distances and angles between neighboring points, and also provides closed-forms for the Kullback-Leibler divergences of the respective distributions. Experiments on synthetic and real datasets demonstrate the effectiveness and robustness of this algorithm compared to existing methods. 
41872	4187214	Dialogue games that agents play within a society	This paper discusses the use of argumentation and dialogue games in artificial agent societies in order to manage conflicts and social influences. While there has been significant focus on developing theoretical models for multi-agent systems, there is a lack of understanding of the computational implications and systemic impact of using argumentation in agent societies. To address this, the paper presents a framework for argumentation-based negotiation (ABN) that takes into account the societal structure and provides agents with strategies for resolving conflicts. Experiments show that arguing can be beneficial for agent societies, particularly when resources are limited and agents have imperfect knowledge. Negotiating social influences can also improve agent performance.
41873	4187344	An Anytime Algorithm for Optimal Coalition Structure Generation	Coalition formation is a crucial aspect of multi-agent systems, where autonomous agents come together to efficiently achieve their individual or collective goals. However, the problem of determining which coalitions to form in order to achieve a specific goal is challenging due to the large number of possible solutions. To address this, an anytime algorithm has been developed that uses a novel representation of the search space and a branch-and-bound technique to efficiently search through sub-spaces. This algorithm has been tested with a new input distribution and has been shown to outperform previous algorithms in terms of solution quality and efficiency. It is also capable of providing a guaranteed solution within a specific bound, even if interrupted before completing its search.
41874	4187434	Using constraints and process algebra for specification of first-class agent interaction protocols	Current approaches to multi-agent interaction involve defining protocols as a set of possible interactions and hard-coding decision-making processes into agent programs. This creates problems such as a strong coupling between agents and protocols, limitations on the protocols agents can use, and the inability to compose protocols at runtime. To fully utilize the potential of multi-agent systems, it is important to have protocols exist as first-class entities that can be inspected, referenced, composed, and shared. This paper proposes a framework, RASA, which treats protocols as first-class entities. It also presents a formal language for specifying agent interaction protocols as first-class entities, allowing for agents to reason about and compose more complex protocols at runtime. 
41875	4187581	Characterizing effective auction mechanisms: insights from the 2007 TAC market design competition	The article discusses the 2007 TAC Market Design competition and categorizes the entries. It then compares these entries and examines the relationship between market dynamics, auction rules, and adaptive strategies through post-tournament experiments. Based on this analysis, the paper speculates on the design of successful auction mechanisms, both in the specific competition and in a broader context.
41876	4187647	An Approach to Specifying Coordinated Agent Behaviour	Team oriented programming involves various methods for organizing teams of agents and coordinating their actions to reach specific objectives. The SimpleTeam framework is designed to facilitate this type of programming. It allows for the creation of team plans that outline the tasks of a group of agents or sub-teams working towards a common goal. The framework also includes tools for managing concurrency and handling exceptions. An example application is provided to demonstrate the benefits of this approach in simplifying the design of coordinated behavior in multi-agent systems.
41877	4187744	Equilibrium analysis in cake cutting	Cake cutting is an important concept in fair division, where a divisible good needs to be divided among agents with different preferences. The main criteria for fairness are proportionality and envy-freeness, and existing protocols aim to achieve these outcomes if all agents follow the protocol. However, it is not guaranteed that all agents will follow the protocol, leading to a need to study equilibria of cake cutting protocols. The Dubins-Spanier procedure, which ensures a proportional allocation, is studied in this paper, and it is shown that every envy-free allocation can be mapped to a pure Nash equilibrium in the corresponding moving knife game. Furthermore, every pure Nash equilibrium in the moving knife game results in an envy-free allocation of the cake. The game also has an epsilon-equilibrium that is both epsilon-envy-free and independent of the tie-breaking rule.
41878	41878107	Inter-organization networks: implications of access control: requirements for interconnection protocol	Inter-Organization Networks (IONs) are formed when two or more distinct organizations connect their internal computer networks. These networks facilitate the exchange of data and resources between organizations, such as cad/cam data between manufacturers and subcontractors, software distribution from vendors to users, and customer input to suppliers' order-entry systems. This paper examines the technical implications of interconnecting networks across organization boundaries. It argues that traditional network design criteria of connectivity and transparency are not sufficient for IONs, as access control is a primary high-level requirement. This requires a scheme based on non-discretionary control to allow organizations to limit connectivity and make network boundaries visible. New interconnection protocols are needed to support these access control requirements, and message-based gateways are a promising solution.
41879	4187917	An Integrated View on Rules and Principles	The differences between rules and principles in the field of law have long been acknowledged, with rules appearing to directly lead to their conclusion while principles provide reasons for their conclusions. This has led to a debate over whether rules and principles should be strictly distinguished based on their logical structure. Dworkin argues for a strict distinction, while others, such as Soeteman, disagree. The authors of this paper propose an integrated view in which rules and principles have the same logical structure but behave differently in reasoning. They support this view with a formalization using Reason-Based Logic and provide three ways of reconstructing reasoning by analogy based on their integrated view.
41880	4188013	Design and analysis of an asymmetric mutation operator	Evolutionary algorithms are a type of randomized search heuristics that use fitness to guide their search. However, in practical applications, problem-specific knowledge can be used to improve the search process. For problems with bit strings of finite length, good solutions tend to have either very few one-bits or very few zero-bits. To address this, a specific mutation operator has been developed and analyzed. This operator is designed to be effective in these situations, but it also has some limitations. Analytical results have been obtained for both specific examples and general function classes to understand the strengths and weaknesses of this mutation operator. 
41881	4188158	Sum-Product Networks: A New Deep Architecture	The complexity of the partition function is a major challenge in graphical model inference and learning. To address this, researchers have developed a new type of deep architecture called sum-product networks (SPNs). These networks are directed acyclic graphs that use variables, sums, and products as nodes. If an SPN is complete and consistent, it can represent the partition function and all marginals of a graphical model. Learning algorithms for SPNs have been proposed using backpropagation and EM, and experiments have shown that they can outperform standard deep networks in terms of speed and accuracy. Additionally, SPNs have potential connections to the structure of the cortex. 
41882	4188233	Precoding and Power Optimization in Cell-Free Massive MIMO Systems.	Cell-free Massive multiple-input multiple-output (MIMO) is a wireless communication system that uses a large number of small, low-cost, and low-power access points (APs) connected to a central network controller. This system does not have traditional cellular boundaries and each user is served by all APs at the same time. The basic linear precoding schemes used in this system are conjugate beamforming, zero-forcing, and maximum ratio transmission. This technology has the potential to significantly increase spectral efficiency and enhance coverage and capacity in wireless networks.
41883	4188311	Autonomous Background Coordination Technology for Timely Sensor Connection in Wireless Sensor Networks	Wireless sensor networks (WSNs) are becoming increasingly popular for collecting data in various military and civil applications. However, in an indoor factory monitoring environment, the constant need for sensor relocation and addition to accommodate changes in production lines can affect the scalability of the network due to limitations of the wireless nodes. To address this issue, a two-tier autonomous decentralized architecture has been proposed, with sensors forming the first layer and routers forming the second layer. The routers form communities to coordinate and share information, allowing for efficient switching of sensor loads and timely expansion of sensors. Simulation results demonstrate that this background coordination technology improves sensor connection time and increases the number of connected sensors in the network.
41884	4188479	Extracting Places and Activities from GPS Traces Using Hierarchical Conditional Random Fields	This paper discusses the importance of learning human behavior patterns from sensor data for accurate activity inference. It proposes a system that uses hierarchically structured conditional random fields to extract a person's activities and significant places from GPS data. Unlike existing techniques, this approach considers high-level context to detect significant places. Experiments show significant improvements and the system is able to accurately estimate a person's activities using data from other individuals. This highlights the robustness and effectiveness of the proposed system in understanding human behavior.
41885	4188576	Model counting: a new strategy for obtaining good bounds	Model counting is a problem in computer science where we need to calculate the number of solutions to a given propositional formula. It is a more general version of the NP-complete problem of propositional satisfiability, making it both useful and difficult to solve. However, a new approach has been developed that involves adding carefully chosen constraints to the formula in order to reduce the solution space. These constraints are randomly chosen XOR or parity constraints and have been proven to provide accurate model counts with high probability. This method has been shown to be effective for solving difficult combinatorial problems and provides high-confidence guarantees on the quality of the results.
41886	4188646	Rollup: Non-Disruptive Rolling Upgrade with Fast Consensus-Based Dynamic Reconfigurations.	Rolling upgrade is a process of upgrading servers in a distributed system in a progressive manner to minimize service downtime. This requires a well-designed protocol for cluster membership to ensure the system's availability during the upgrade. Existing protocols, such as CoreOS etcd, rely on a primary server for both reconfiguration and data storage, which can cause disruptions when the primary needs to be replaced. The proposed protocol, called Rollup, uses a candidate leader only for reconfiguration and scalable biquorums for service requests, resulting in a fast and non-disruptive upgrade. While Rollup does not provide a complete coordination service, it offers an 8-fold speedup compared to protocols relying on a primary for reconfiguration. This has been demonstrated through theoretical analysis and experiments on physical and virtual machine clusters. 
41887	4188745	Semantics for null extended nested relations	The nested relational model is an extension of the flat relational model that allows for complex objects to be modeled by relaxing the first normal form assumption. Previous research on this model has focused on defining data structures and query language, while integrity constraints have been characterized by subclasses and normal forms. This paper introduces null extended data dependencies to define the semantics of nested relations with null values. A generic null value is used to formalize incomplete information, and a preorder is defined to measure the information content of nested relations. The extended chase procedure is developed to test and infer from null extended data dependencies, and it is shown to generalize the classical chase procedure. This approach also allows for the capture of losslessness in nested relations and extends the semantics of flat relations.
41888	4188869	Improved equilibria via public service advertising	 guarantees any improvement.The paper discusses the concept of "public service advertising campaigns" as a means to encourage players in natural games to follow low-cost equilibria. However, the assumption that everyone will follow the advice is unrealistic. Instead, the paper considers the question of how effective such campaigns can be if only a fraction of the population actually follows the advice, and only temporarily. The analysis is done for various types of games, including network design, scheduling, and party affiliation games. The results show that for some games, a random fraction of the population following the advice can lead to a significant improvement, while for others, there is a strict threshold that must be met. For certain games, no fraction guarantees any improvement. 
41889	4188967	The Second Rewrite Engines Competition	The Second Rewrite Engines Competition (REC) took place during the 7th Workshop on Rewriting Logic and its Applications (WRLA 2008). Five systems, ASF+SDF, Maude, Stratego/XT, TermWare, and Tom, participated in the competition. The organization and conduct of the competition are discussed and the main results and conclusions are presented.
41890	4189049	SIMULTANEOUS REGISTRATION, SEGMENTATION AND MODELLING OF STRUCTURE IN GROUPS OF MEDICAL IMAGES	In this article, the authors propose an algorithm for extracting information from groups of medical images of the same anatomy. The algorithm simultaneously segments and registers the images, creating a model of their structure and correspondences. This framework models the tissue fractions rather than the expected intensity in each voxel, allowing for decoupling from imaging details. The algorithm compares the current image to a reconstructed image generated from the model tissue fractions and current intensity distributions for each tissue type. The algorithm is described in detail and results are presented from its application to a set of brain MR images.
41891	4189154	Automatic interpretation and coding of face images using flexible models	Face images are complex and challenging to interpret due to their high variability. This variability is caused by factors such as individual appearance, 3D pose, facial expression, and lighting. To address this issue, a compact parametrized model of facial appearance has been developed. This model takes into account all sources of variability and includes both shape and gray-level appearance. It is created by analyzing a training set of face images and can be used for tasks like image coding, person identification, 3D pose recovery, gender recognition, and expression recognition. A robust multiresolution search algorithm is used to fit the model to new face images, allowing for the accurate location of facial features and recovery of shape and appearance parameters. This model has been tested on a database of 690 face images with varying 3D pose, lighting, and expression, and has shown good performance in all tasks.
41892	4189260	Timeliness Evaluation of Intermittent Mobile Connectivity over Pub/Sub Systems.	Mobile systems often have intermittent connectivity and asynchronous data transfer, making it challenging to guarantee timely communication between mobile users. To address this, Queueing Network Models (QNMs) can be used to model system performance and provide accurate solutions for metrics such as response time. In this paper, the authors propose a model for end-to-end response time in mobile systems using QNMs and a publish/subscribe middleware. This model incorporates intermittent connectivity by modeling user connections/disconnections with an ON/OFF queueing system. The model is validated through simulations with real-world workload data, showing a deviation of less than 5% from actual performance. This approach can be used to improve the QoS of mobile systems in various application scenarios.
41893	418934	Efficient Subwindow Search: A Branch and Bound Framework for Object Localization	Object recognition systems typically only determine if an object is present or not, but do not provide information on its location. To estimate the location, a sliding window approach can be used, but this is computationally expensive. A new branch and bound scheme is proposed in this paper that efficiently maximizes a variety of quality functions over all possible subimages. This method is faster than exhaustive or sliding window search and allows the use of previously considered too slow classifiers for localization. The resulting systems achieve state-of-the-art performance on various data sets and competitions. 
41894	4189448	Multiresolution modeling and visualization of volume data based on simplicial complexes	A new method for representing and visualizing scattered volumetric datasets is proposed. These datasets are a sample of a scalar field defined over a three-dimensional domain, which can be visualized as a hypersurface in four dimensions. The method uses a multiresolution model, where the domain is split into tetrahedra and the scalar field is approximated with increasing precision. This is achieved through an adaptive incremental approach based on the coherence of the scalar field. This model allows for efficient extraction of isosurfaces at different levels of resolution, as well as progressive and multiresolution rendering. The method is evaluated on various scattered datasets with positive results.
41895	4189532	Parallel mining of closed sequential patterns	Sequential pattern mining is an important data mining task with many practical applications. The closed sequential pattern, which contains all the information of the complete pattern set but is more compact, is the most useful variation. However, there is currently no parallel method for mining closed sequential patterns. In this paper, the authors propose a new algorithm, called Par-CSP, for parallel mining of closed sequential patterns on a distributed memory system. Par-CSP uses dynamic scheduling and selective sampling to minimize interprocessor communication and address load imbalance. Experiments on a 64-node Linux cluster showed that Par-CSP performs well on different datasets. 
41896	4189633	The software model checker Blast	Blast is a tool used for verifying temporal safety properties in C programs. It can either prove that a program satisfies a safety property or provide an execution path that violates it. This is achieved through the use of lazy predicate abstraction and interpolation-based predicate discovery. The paper discusses how Blast has been applied in two case studies - one for statically proving memory safety and the other for generating test suites that guarantee coverage of a specific predicate. The tool has been shown to provide automated, precise, and scalable analysis for C programs.
41897	4189749	Compositional Shape Analysis by Means of Bi-Abduction	The article discusses the challenges of accurately and efficiently analyzing mutable data structures in automatic program verification and introduces shape analysis as a solution. Shape analysis uses a compositional method where each procedure is analyzed independently, and utilizes a restricted form of separation logic to over-approximate data structure usage. The method is based on a generalized form of abduction, called bi-abduction, which jointly infers missing portions of state and untouched portions of state. The article reports on case studies and implementation of the analysis algorithm, showcasing its potential for automation and scalability. The article also makes technical contributions in proof procedures and analysis algorithms, but its main contribution is demonstrating the increased automation possible with abductive inference.
41898	4189898	Decidability of inferring inductive invariants.	Induction is a commonly used method for verifying the correctness of hardware and software systems. This involves modeling the system using logical formulas and then using a decision procedure to check if a logical formula serves as an inductive safety invariant for the system. The main challenge in this approach is determining the inductive invariant, also known as invariant inference. This paper focuses on the problem of inferring first-order inductive invariants by restricting the language of candidate invariants. The authors present cases where this problem can be solved in infinite languages, as long as certain restrictions are applied and a suitable well-quasi-order is defined. They also show that there are some cases where this is not possible, and propose a framework for constructing infinite languages while ensuring the decidability of the invariant inference problem. The paper provides examples of applying this approach to programs that manipulate linked-lists and distributed protocols.
41899	4189962	Relational access to Unix kernel data structures.	State of the art kernel diagnostic tools such as DTrace and Systemtap provide a procedural interface for analyzing kernel tasks. However, this work argues that a relational interface to kernel data structures can offer additional benefits for kernel diagnostics. The Pico COllections Query Library (PiCO QL) Linux kernel module uses a domain specific language to define a relational representation of accessible Linux kernel data structures. This includes a parser to analyze the definitions and a compiler to implement an SQL interface for querying the data structures. Unlike other tools, PiCO QL does not require kernel instrumentation and imposes no overhead when idle. It only accesses relevant kernel data structures to answer the input queries. Examples of Linux kernel queries are provided to showcase PiCO QL's usefulness in identifying system resources, security vulnerabilities, and performance issues.
41900	41900258	Deep Features For Text Spotting	The aim of this project is to identify text in natural images. This is achieved through two steps: detecting word regions in the image and recognizing the words within these regions. The authors propose a Convolutional Neural Network (CNN) that can perform both tasks efficiently. The CNN has a unique architecture that allows for feature sharing and outperforms previous methods in text detection, character classification, and bigram classification. The authors also make technical changes to traditional CNN architectures, including no downsampling and multi-mode learning. They also present a method for automatically generating annotations from Flickr data. All these components are combined to create a state-of-the-art text spotting system, which is evaluated on two standard benchmarks and shows improvement over previous methods.
41901	41901194	Online Spatial Event Forecasting in Microblogs.	Event forecasting using social media data streams has numerous practical applications. While existing methods can accurately predict temporal events like elections and sports, they struggle with forecasting spatiotemporal events such as civil unrest and influenza outbreaks. To address this challenge, the authors propose novel batch and online approaches for spatiotemporal event forecasting on platforms like Twitter. These models take into account the evolving spatial features and their correlations over time. The proposed inference algorithms optimize the model parameters, and a dynamic programming technique is used to calculate the alignment likelihood of tweet sequences. The effectiveness of the approach is demonstrated through experiments in two different domains.
41902	4190232	Decoupling function and anatomy in atlases of functional connectivity patterns: Language mapping in tumor patients.	This paper discusses the construction of an atlas that summarizes the functional connectivity characteristics of a cognitive process in a population of individuals. The atlas uses a low-dimensional embedding space derived from a diffusion process on a graph of fMRI time course correlations. It is independent of the anatomical space and can represent functional networks with varying spatial distribution in a population. The atlas is represented by a common prior distribution for all subjects. An algorithm is developed for fitting this model to observed data. The method is successfully applied to a language fMRI study, identifying coherent and functionally equivalent regions across subjects. It is also able to map functional networks from a healthy population to individuals with language network disruptions caused by tumors.
41903	4190358	Data Lifecycle Challenges in Production Machine Learning: A Survey.	Machine learning is a crucial tool for extracting knowledge from data and solving complex tasks. However, the accuracy of a machine learned model is heavily influenced by the quality of the data it is trained on. To address this, there is a need for robust processes and tools to analyze, validate, and transform data in large-scale machine learning systems. Based on Google's experience in developing a production machine learning platform, this article discusses the challenges in data management and explores relevant literature from the data management and machine learning communities. The three main areas of focus are data understanding, data validation and cleaning, and data preparation, with an emphasis on the constraints that arise depending on when and by whom the problems are encountered in the model's lifecycle.
41904	419042	On a class of vertices of the core.	For supermodular TU-games, the vertices of the core are the marginal vectors. This also applies to games with a distributive lattice of feasible coalitions, which are determined by a hierarchy among players. A broader class of core vertices, called min-max vertices, can be found by minimizing or maximizing the coordinates of a core element in a given order. A simple formula, applicable for connected hierarchies and under certain restrictions, can be used to calculate these vertices without solving an optimization problem. The conditions for two different orders to produce the same vertex for all games are identified, and it is shown that balanced games with more than four players have core vertices that are not min-max vertices.
41905	419051	Quasi- and pseudo-inverses of monotone functions, and the construction of t-norms	The article discusses properties of quasi- and pseudo-inverses of non-decreasing real functions. A method for constructing a commutative, fully ordered semigroup on the unit interval is presented, using a given triangular norm T and non-decreasing function f. A similar construction using the pseudo-inverse results in an operation bounded by the minimum, but may violate associativity. Sufficient conditions for creating new t-norms from a given one and a non-decreasing function f are also discussed, along with examples. The article was published in 1999 by Elsevier Science B.V. and is protected by copyright.
41906	419069	HoIP: Haptics over Internet Protocol	This paper introduces a new application layer protocol called Haptics over Internet Protocol (HoIP) that enables low latency communication of haptic data for applications like telesurgery. To maintain stability in the control loop, the protocol has a target round trip delay of 5 ms or lower. HoIP uses adaptive sampling strategies, multithreaded architecture, and UDP implementation to achieve this low latency without compromising on the quality of haptic perception. The paper discusses the frame structure and design choices of HoIP, as well as the implementation and processing delay measurements using real haptic devices and HAPI software. Results show that HoIP has a maximum processing delay of 0.6 ms at both the transmitter and receiver side. 
41907	4190761	Selecting informative features with fuzzy-rough sets and its application for complex systems monitoring	The use of intelligent pattern recognition applications is hindered by high dataset dimensionality. To address this issue, a redundancy-removing step is often performed, using rough set theory (RST) as a dataset pre-processor. However, RST relies on crisp datasets and can result in loss of important information. To overcome this, a new feature selection technique is proposed that uses a hybrid variant of rough sets and fuzzy-rough sets. This approach retains dataset semantics and allows for the creation of clear and readable fuzzy models. Results from applying this technique to complex systems monitoring demonstrate its superiority over conventional methods such as entropy, PCA, and random selection. 
41908	4190841	Spatially varying registration using Gaussian processes.	The paper proposes a new technique for registering spatially-varying data using Gaussian process priors. This method involves modifying the spectrum of the Gaussian process based on a user-defined tempering function, resulting in a non-stationary process with varying levels of smoothness in different areas. Unlike other approaches, this method only affects the prior model and does not require any changes to the registration algorithm. This allows for the application of this method to various registration techniques, such as spline-based models and statistical shape or deformation models. The effectiveness of this method is demonstrated by registering cone beam CT images with high noise levels, resulting in improved accuracy and robustness. 
41909	41909129	How android app developers manage power consumption?: an empirical study by mining power management commits.	The popularity of the Android platform has led to a large number of Android applications being developed. Developers must consider power consumption management during the design and implementation process in order to maintain usability. With the availability of public repositories like GitHub, developers can access a wealth of information on common power management strategies used in Android applications. This paper presents an empirical study of power management commits in Android applications, with a focus on those from F-Droid. The study identifies 468 power management commits from 154 different applications belonging to 15 categories, and categorizes them into 6 groups based on open card sorting. The study also highlights the varying dominant power management activities across different categories of Android applications.
41910	41910237	Animating wrinkles on clothes	This paper presents a method for generating realistic wrinkles on clothes without using a fine mesh or requiring large computational resources. The method takes into account the area conservation property of cloth and uses a user-defined wrinkle pattern to modulate the deformation of individual triangles. This allows for the use of small in-plane deformation stiffnesses and a coarse mesh, resulting in a fast and robust cloth simulation. Additionally, the ability to design wrinkles on different types of deformable models makes this method versatile for creating synthetic images. The method can also be applied to other types of wrinkling phenomena, as it is based on geometric principles.
41911	4191173	Visually Dynamic Presentation of Proofs in Plane Geometry - Part 1. Basic Features and the Manual Input Method	We have developed a new way of presenting proofs in plane geometry using visually dynamic mediums like computer displays. This method involves creating a single diagram that is animated and updated with each step of the proof, making it easier to understand and follow. Our system, "Java Geometry Expert" (JGEX), offers two methods for creating these dynamic presentations: manual input and automatic. In this first part of our work, we introduce the main features of our method and explain the manual input method, which utilizes mouse clicks to create the dynamic diagram and proof text. This visually dynamic approach solves the issue of identifying geometry elements and makes the proof more visually appealing and intuitive.
41912	4191244	Improved algorithms for min cut and max flow in undirected planar graphs	This study focuses on the min st-cut and max st-flow problems in planar graphs, both in static and dynamic settings. The first algorithm presented can compute a min st-cut in O(n log log n) time for an undirected planar graph with two given vertices s and t. The same bound is achieved for computing a max st-flow in an undirected planar graph. These are the first algorithms to break the long-standing O(n log n) barrier for these problems. A fully dynamic algorithm is also presented, capable of maintaining min st-cut and max st-flow values in an undirected planar graph with edge insertions and deletions, with a time complexity of O(n(2/3) log(8/3) n) per operation. This algorithm is based on a new dynamic shortest path algorithm for planar graphs, which is also of independent interest. This is the first non-trivial dynamic algorithm for min st-cut and max st-flow.
41913	419131	Even more on advice on structuring compilers and proving them correct: changing an arrow	The author discusses their paper "On Representation of Data Types" and the concept of representation correctness in relation to compiler correctness. They note similarities between their work and previous works by Gaudel (1980), Broy and Wirsing (1980), McCarthy and Painter (1967), Burstall and Laudin (1969), Morris (1973), and ADJ (1979). However, they realize that an arrow has a different direction and ultimately conclude that their approach is correct. Their paper defines representations in terms of derivers and discusses the derived C-algebra ~A. The author also introduces the concept of a representation of data types T~ ,E by T~,E'' and defines it in relation to a homomorphism r.
41914	4191494	Handle and Hole Improvement by Using New Corner Cutting Subdivision Scheme with Tension	The Doubly Linked Face List (DLFL) is a new structure that allows for easy manipulation of 3D mesh models. It is topologically robust, meaning it always results in valid surfaces. This paper explores the relationship between DLFL and subdivision algorithms, with three main contributions. First, a new corner cutting scheme is introduced that allows for more control over the shape of the subdivided surface. Second, an efficient algorithm is developed for implementing this scheme on the DLFL structure. This ensures that the topological robustness is maintained. A comparison with the popular Catmull-Clark scheme shows that the new corner cutting scheme produces better results.
41915	419156	A Maximum Entropy Tagger with Unsupervised Hidden Markov Models	This article presents a new tagging model that combines a hidden Markov model (HMM) with unsupervised learning and maximum entropy modeling. This approach reduces the cost of building taggers with limited resources, such as no dictionary and a small annotated corpus. The model was tested on English POS tagging and Japanese word segmentation tasks, and showed significant improvement in accuracy when trained with a small annotated corpus. In addition, the English POS tagger achieved better results (96.84%) compared to other state-of-the-art taggers when a large annotated corpus was available. This new approach has potential for improving tagging accuracy with limited resources.
41916	4191650	The Unsplittable Stable Marriage Problem	The Gale-Shapley algorithm is a well-known method for solving the stable marriage problem. This paper explores its use in the many-to-many stable marriage problem, also known as the stable allocation or ordinal transportation problem. A variant of the algorithm is presented that is suitable for "ordinal" assignment problems. It is similar to a bicriteria approximation algorithm by Shmoys and Tardos for scheduling on unrelated parallel machines with costs. This algorithm finds an unsplit (non-preemptive) stable assignment that ensures each job is assigned at least as well as it could be in any fractional stable assignment, and each machine is congested by no more than the processing time of the largest job.
41917	41917125	Normalized incremental subgradient algorithm and its application	The paper introduces a new incremental optimization algorithm, called NIS, for minimizing the sum of multiple component functions. This algorithm is specifically designed for problems where the component functions have common local minima. The NIS algorithm is implemented incrementally and can be used in distributed systems. It works by normalizing the subgradients of each component function, resulting in better convergence compared to other traditional methods. The convergence of the NIS algorithm is proven and its effectiveness is demonstrated through two applications: solving convex feasibility problems and distributed maximum likelihood estimation. The algorithm is also tested on numerical examples related to source and node localization in wireless sensor networks, showing its efficiency and effectiveness. 
41918	4191851	An analytical model for SMAC protocol in multi-hop wireless sensor networks.	The SMAC protocol in multi-hop wireless sensor networks (WSNs) uses an active/sleep dynamic and contention backoff scheme. To analyze its performance, we model each node as a single server queue with server shutdown and use a two-dimensional continuous-time Markov chain to represent each node's states. This allows us to calculate metrics such as average packet loss ratio, network throughput, average packet delay, and average power consumption. The accuracy of the analytical model is verified through experiment comparisons with simulation results. The model also allows us to study the tradeoff between energy efficiency and quality of service (QoS) requirements and determine optimal parameters like duty cycle, mean active period, and buffer size in multi-hop WSNs.
41919	4191950	Lifting integer variables in minimal inequalities corresponding to lattice-free triangles	Two recent studies by Andersen et al. and Borozan and Cornuéjols have identified the minimal inequalities of a system with two rows, two free integer variables, and nonnegative continuous variables. These inequalities, known as split cuts or intersection cuts, are derived using maximal lattice-free convex sets. To use these inequalities for two rows of a general simplex tableau, the system must be extended to include integer variables and lifting functions must be developed to determine the coefficients of the integer variables in the corresponding inequalities. The authors of this paper analyze the lifting of minimal inequalities from lattice-free triangles, which can be classified into three categories. They prove that the lifting functions are unique for two of the categories, while for the third category, a fill-in inequality can yield minimal inequalities under certain conditions. The paper also presents conditions for the fill-in inequality to be extreme.
41920	4192022	Ramsey-type theorems for lines in 3-space.	The article discusses geometric Ramsey-type statements, which provide guarantees on the size of cliques or independent sets in graphs and hypergraphs induced by incidence relations between lines, points, and reguli in 3-space. The authors prove three main statements: one about the size of a clique or independent set in the intersection graph of n lines in R-3, one about subsets of lines that are stabbed by one line, and one about subsets of lines that lie on a regulus. These results are obtained using geometric incidence bounds and Turan-type results. The article also includes an intermediate result about points incident to a single algebraic curve. The proofs of these statements also yield polynomial-time algorithms.
41921	419216	Competitive Diffusion on Weighted Graphs.	This paper discusses the use of a weighted graph to model a social network, where individuals are represented as vertices and their relationships are represented as edges with varying levels of importance. In the competitive diffusion game, players choose a vertex as a starting point to spread their idea through the network, with the goal of maximizing the sum of weights of infected vertices. The paper focuses on the computational problem of determining whether a pure Nash equilibrium exists in a given graph, presenting both negative and positive results for different types of graphs. The problem is shown to be difficult for certain graph classes, but solvable in polynomial time for others.
41922	4192232	In-Core Computation of Geometric Centralities with HyperBall: A Hundred Billion Nodes and Beyond	The paper discusses the problem of determining the most central nodes in a social network. Many centrality measures have been proposed, but traditional methods for computing them are not efficient for very large graphs. The authors propose a method that leverages HyperLogLog counters, allowing for fast and accurate approximation of centrality measures while reducing the memory required. This approach is suitable for processing networks with billions of nodes and can be parallelized for even faster computation. 
41923	4192385	An Efficient Algorithm for Solving Pseudo Clique Enumeration Problem	The problem of identifying dense structures in a graph is important in data mining and engineering. Cliques are a common way to represent dense structures due to their simplicity. Pseudo cliques, which are subgraphs with a lower number of edges than a clique, are a natural extension of cliques. This paper focuses on a specific type of pseudo clique where the edge-to-vertex ratio is above a given threshold. The authors propose a polynomial time algorithm based on reverse search to enumerate all pseudo cliques in a graph. Computational experiments show the effectiveness of the algorithm on both randomly generated and practical graphs.
41924	4192410	Dynamic Planar Convex Hull with Optimal Query Time	The dynamic maintenance of the convex hull of a set of points in the plane is a significant problem in computational geometry. A new data structure is introduced that allows for efficient point insertions and deletions in O(log n) time, and various convex hull queries in O(log n) time. The data structure uses O(n) space and has applications in solving the k-level problem and the redblue segment intersection problem with connected segments.
41925	4192531	On the decidability and finite controllability of query processing in databases with incomplete information	This paper focuses on studying how to answer queries over relational databases with integrity constraints (ICs) under the open-world assumption (OWA). The types of ICs considered are functional dependencies and inclusion dependencies, while the query languages considered are conjunctive queries (CQs), union of conjunctive queries (UCQs), and their variations with negation and inequality. The paper presents results on the decidability and finite controllability of OWA query answering, including identifying the decidability/undecidability boundaries and studying OWA query answering over finite and unrestricted databases. These results also have implications for problems such as implication of ICs and query containment under ICs. The paper closes two long-standing open problems in query containment. The findings have potential applications in areas such as view-based information access, ontology-based information systems, data integration, data exchange, and peer-to-peer information systems.
41926	4192678	Partitioning similarity graphs: a framework for declustering problems	The paper presents a new similarity-based technique for declustering data in parallel computing environments. The proposed method takes into account factors such as query distribution, data types and sizes, and partition-size constraints. It uses a similarity graph and a max-cut partitioning approach to allocate frequently accessed data-items to different disks, improving overall performance. The method is flexible and can be applied to Grid Files at the data page level. Detailed experiments show that it is effective in adapting to different query and data distributions, and outperforms traditional mapping-function-based methods. 
41927	4192775	Fuzzy MCDM approach for selecting the best environment-watershed plan	This paper discusses the use of a fuzzy decision support system in multi-criteria analysis for selecting the best plan alternatives in environment-watershed topics. The system uses the fuzzy analytic hierarchy process (FAHP) method to determine the weightings of criteria based on subjective perception. A questionnaire was used to gather input from 15 experts in the field. The system takes into account subjectivity and vagueness in the decision-making process and provides an overall performance value for each alternative. A case study from Taiwan is used to demonstrate the effectiveness of the approach in evaluating five alternatives for an environment-watershed plan. This system can be useful in destination planning and promoting the sustainability of watershed tourism resources.
41928	4192829	Offline/online attribute-based encryption with verifiable outsourced decryption	In the era of big data, service providers often store data in third-party cloud systems, such as social networking websites, to protect its security and privacy. However, this presents new challenges in allowing authorized access to specific parts of the data without decryption. Attribute-based encryption (ABE) offers a solution by allowing fine-grained access control based on user attributes, but it can be computationally expensive, especially for mobile devices. To address this issue, a new ABE scheme is proposed that uses bilinear groups in prime order and combines offline/online key generation and encryption with verifiable outsourced decryption. This reduces the computational burden on devices and is secure against chosen-plaintext attacks. Simulation results show its effectiveness in reducing computational costs. 
41929	41929120	I/O cost minimization: reachability queries processing over massive graphs	This paper discusses the challenge of answering reachability queries on massive graphs that cannot fit entirely in main memory. Existing studies have used indexing techniques, but they are not suitable for handling such large graphs due to high I/O costs. To minimize these costs, the paper proposes a Yes-Label scheme and a heap-on-disk data structure for traversing the graph. Additionally, the paper suggests partitioning the heap-on-disk to ensure sequential I/Os. These approaches are shown to effectively answer multiple reachability queries and have been tested on both synthetic and real graphs, proving their efficiency.
41930	4193052	Variance Reduced Stochastic Gradient Descent with Neighbors	Stochastic Gradient Descent (SGD) is widely used in machine learning, but its slow convergence can be a problem. To address this, variance reduction methods like SAG, SVRG, and SAGA have been proposed, achieving linear convergence. However, these methods require either full gradient computations at pivot points or storing per data point corrections, which can slow down the process. To improve speed and convergence, this paper explores algorithms that utilize the structure of the training data to share and reuse information about past stochastic gradients across data points. This leads to faster optimization in the early stages. Additionally, the paper provides a unified convergence analysis for a group of variance reduction algorithms, referred to as memorization algorithms, and includes experimental results that support the theory.
41931	41931120	Deterministic Annealing for Unsupervised Texture Segmentation	This paper presents a mathematical framework for deterministic annealing and mean-field approximation for partitioning, clustering, and segmentation problems. It outlines an efficient optimization approach and provides proof of convergence. The framework has potential applications in computer vision, pattern recognition, and data analysis. The paper also introduces a new method for unsupervised texture segmentation that uses statistical tests to measure homogeneity. This method formulates texture segmentation as a pairwise data clustering problem with a sparse neighborhood structure. The paper compares different clustering objective functions derived from invariance principles and evaluates the performance of the algorithms on a dataset of micro-texture mixtures and real-world images.
41932	4193214	Small stretch spanners in the streaming model: new algorithms and experiments	The article presents deterministic algorithms for computing small stretch spanners in the streaming model. A (α, β)-spanner of a graph is a subgraph that preserves distances within a certain factor. The algorithms can handle graphs with unknown numbers of vertices and edges and only one pass over the data is allowed. The authors demonstrate how to compute a (k, k-1)-spanner in O(1) processing time per edge/vertex, with O(n1+1/k) edges and memory space. The algorithms can also be adapted for limited internal memory. An experimental study supports the practical value of this approach.
41933	4193326	Splitting argumentation frameworks: an empirical evaluation	In a recent paper, Baumann demonstrated that splitting results, similar to those known for logic programs and default logic, can also be applied to Dung argumentation frameworks (AFs). This means that under certain conditions, an AF can be divided into two parts, allowing for more efficient computation of extensions. Specifically, this process involves computing an extension for one part, modifying the other part, and then combining the two extensions. The paper then presents an empirical evaluation of the effects of splitting on extension computation, showing that it can greatly improve algorithm performance.
41934	4193411	Cross-Referenced Dictionaries and the Limits of Write Optimization.	Dictionaries are a commonly studied data structure that allow for various operations such as insertions, deletions, and membership queries. In a RAM, these operations take O(log N) time on N elements. Dictionaries can also be cross-referenced, which involves creating multiple dictionaries on the same set of tuples. In external memory, using B-trees can achieve optimal insertions and deletions, but write optimization techniques have been developed to improve the insertion and deletion costs. However, for deletions in cross-referenced dictionaries, write optimization does not offer a significant advantage. In fact, a lower bound has been established that shows optimal cross-referenced dictionaries cannot match the write optimization bound for insert-only dictionaries. This highlights the limitations of write optimization techniques in certain situations.
41935	4193557	From Reference Frames to Reference Planes: Multi-View Parallax Geometry and Applications	The paper introduces a new method for analyzing the geometry of multiple 3D points in uncalibrated images. This method involves decomposing the projection of the points onto a reference plane and then re-projecting them onto the camera's image plane. This eliminates the need for internal camera calibration and orientation, simplifying the analysis. The use of a reference plane also allows for simpler tri-focal constraints and leads to useful applications in 3D scene analysis, such as New View Synthesis. The method also allows for partial reconstruction using partial calibration information. Overall, this new framework offers a simpler and more intuitive approach to analyzing 3D scene points from multiple images.
41936	4193638	Complete arcs in PG(2,25): The spectrum of the sizes and the classification of the smallest complete arcs	This paper reports on a thorough computer search that determined the smallest size of a complete arc in PG(2,25) to be 12. It also found that complete 19-arcs and 20-arcs do not exist, thus fully determining the spectrum of complete arc sizes in this projective geometry. The authors also provide a classification of the smallest complete arcs, with 606 non-equivalent 12-arcs identified and their automorphism groups and geometrical properties studied. The success of the search was aided by using projective equivalence properties to limit the search space and avoid generating duplicate arcs.
41937	4193747	Assessing the Need for Referral in Automatic Diabetic Retinopathy Detection	This paper discusses the use of emerging technologies in healthcare to improve efficiency and reduce costs. Specifically, it focuses on the use of image recognition for the screening of diabetic retinopathy, a complication of diabetes that can lead to blindness. Previous research has focused on identifying specific lesions related to DR, but this may not be enough to determine if a patient should be referred for further treatment. The paper introduces a new algorithm that combines results from multiple lesion detectors to make a more accurate decision on patient referral. Results show that this approach achieved a high level of accuracy without the need for additional normalization techniques.
41938	4193836	Evolutionary regression modeling with active learning: an application to rainfall runoff modeling	Computer simulations have become a common way to study complex real-world phenomena when controlled experiments are not feasible. However, these simulations are computationally expensive, leading to the use of surrogate models such as neural networks and kernel methods. These models are cost-effective and useful for tasks like optimization and sensitivity analysis. To address the model calibration problem in rainfall runoff modeling, a new automated approach has been developed that incorporates advances in machine learning such as hyperparameter optimization, model type selection, and active learning. This makes the method more useful for domain experts.
41939	4193944	Linear antenna array synthesis using fitness-adaptive differential evolution algorithm	This article discusses the use of an adaptive Differential Evolution (DE) algorithm to optimize the spacing between elements in non-uniform linear antenna arrays. The goal is to produce a radiation pattern with minimal side lobe level and controlled null placement. DE is a highly effective optimization method with few control parameters and easy implementation. Two simple adaptation schemes are used to regulate DE's control parameters, F and Cr. The algorithm successfully solves three difficult design problems and outperforms other metaheuristics like Genetic Algorithm, Particle Swarm Optimization, Memetic Algorithms, and Tabu Search. The results demonstrate the effectiveness of the proposed algorithm in achieving the optimization goal.
41940	41940215	Families of Alpha- Beta- and Gamma- Divergences: Flexible and Robust Measures of Similarities	This paper presents an extension and overview of Alpha-, Beta- and Gamma- divergences, discussing their fundamental properties and showing that there are families of these divergences with consistent properties. The authors establish links and correspondences among the divergences through nonlinear transformations, such as generating Beta- divergences from Alpha- divergences and vice versa. Additionally, a new class of Gamma- divergences is shown to be generated from both Beta- and Alpha- divergences. The paper also explores the connections between these divergences and Tsallis and Renyi entropies, highlighting their information theoretic interpretations. Overall, this paper provides a comprehensive understanding of the properties and relationships between these divergences.
41941	4194113	CA4IOT: Context Awareness for Internet of Things	The Internet of Things (IoT) will connect billions of sensors, creating an opportunity for a sensing-as-a-service platform. With a large number of sensors collecting similar information and continuous advancements in sensor technology, selecting the most appropriate sensors for a given problem will be a challenge. This paper proposes the Context Awareness for Internet of Things (CA4IOT) architecture, which automates the task of selecting sensors based on the problem at hand. It focuses on configuring filtering, fusion, and reasoning mechanisms for sensor data streams collected by selected sensors. The goal is to provide more useful information than the raw data streams, based on the user's submitted problems.
41942	4194246	Validating the AMULET Microprocessors	The AMULET group at the University of Manchester has been working on designing asynchronous microprocessors for the past ten years. These designs have become increasingly advanced and complex. However, ensuring that these designs are correct on the first try is a difficult task. Therefore, the group has been exploring various verification and validation techniques to improve the quality of their designs and reduce the number and severity of bugs in the final product. 
41943	4194323	Myopic Sparse Image Reconstruction With Application To Mrfm	We offer a solution for improving image clarity when the convolution operator or point spread function (PSF) is only partially known. By using small perturbations and finding principal components, we can better understand the uncertainty in a high dimensional space. Our method is based on the assumption that the image is sparse, which is commonly seen in magnetic resonance force microscopy (MRFM) images. Using a Bayesian Metropolis-within-Gibbs sampling framework, our algorithm outperforms other methods like the alternating minimization (AM) algorithm for sparse images. We demonstrate the effectiveness of our approach on real MRFM data of a tobacco virus. 
41944	4194422	The impact of data accuracy on system learning	This paper examines the relationship between database accuracy and system learning in information systems. The study focuses on a basic model featuring a database, rulebase, and embedded machine learning approach. The results show that the accuracy of the information system affects the quality of individual rules, and this impact can be determined by accounting for database inaccuracy. In some cases, this impact is monotonic. Additionally, the accuracy of the system can also impact the order of importance of rules within the rulebase. This highlights the importance of considering database accuracy in the design and behavior of learning systems.
41945	4194541	String hashing for linear probing	Linear probing is a popular method for implementing dynamic hash tables, where all keys are stored in a single array. When a key is received, it is hashed to a location and then consecutive locations are probed until the key or an empty location is found. In a study presented at STOC'07, it was found that with standard 2-universal hashing, the expected number of probes is Ω(log n). However, using 5-universal hashing can reduce this to a constant number of probes. This method is not available for complex domains such as variable length strings, so a solution is to first use collision-free hashing into a simpler intermediate domain and then apply the complicated hash function. This study found that an expected constant number of linear probes can be achieved as long as the second hash function is 5-universal and the first hash function has O(1) expected collisions with each key. This allows for a smaller intermediate domain, resulting in a simpler and faster overall hash function. The study also looked at how the overhead from linear probing decreases as the array size increases and the implications of storing strings directly as intervals in the array.
41946	4194628	Exploring alternatives for representing and accessing design knowledge about enterprise integration	Enterprise integration involves finding ways for different legacy applications to communicate and work together effectively. This can be a challenging task due to the complexity and specificity of each integration effort. However, there are some tools that can help, such as Business Process Models (BPM) and Enterprise Integration Patterns (EIP). These tools provide designers with abstract descriptions of common integration tactics. However, they may have different perspectives and can be difficult to align. To address this issue, a new approach is proposed that uses the theory of speech acts to bridge the gap between BPM and EIP. This approach includes a re-representation of EIP as speech acts, and a mapping between speech acts and tasks in BPM. This approach has been successfully applied in real-world cases.
41947	4194739	The 2009 Edsger W. Dijkstra prize in distributed computing	The ACM-EATCS Edsger W. Dijkstra Prize in Distributed Computing was established to recognize exceptional research papers in the field of distributed computing that have had a significant impact on the theory or practice of the subject for at least ten years. This award highlights the importance of understanding the principles of distributed computing and their lasting influence on the field. 
41948	4194840	Automating the analysis of design component contracts	Software patterns are a new way of designing software to solve problems within a specific context. These patterns describe the structure and collaboration among components in a software design. The promise of this approach is to simplify the construction of software systems and reduce costs by reusing previous experience. However, it also introduces challenges in ensuring the reliability and integrity of these complex systems. To address this, a formal model is needed to analyze pattern-based designs and their interactions. This paper presents a formal framework for analyzing object-oriented systems with pattern-based designs as building blocks. A case study is provided to demonstrate the approach in a distributed system. This framework can help ensure the integrity of software systems and reduce costs in their construction.
41949	4194923	Engineering multi-agent systems with aspects and patterns	Objects and agents are important concepts in software engineering, with agents being more complex as they have additional concerns such as beliefs, goals, and behavior properties. Multi-agent systems typically include different types of agents and objects, and a structured approach is needed for their composition. This paper compares an aspect-based approach and a pattern-based approach for building multi-agent software, both with the goals of aligning high-level agent models with object-oriented designs, separating agency concerns, and allowing for flexible and easy-to-maintain systems. The authors demonstrate the practicality of these approaches through a real-life example, the Portalware system.
41950	4195025	Analytic Tangent Irradiance Environment Maps for Anisotropic Surfaces	This paper discusses the use of spherical harmonic irradiance maps in environment-mapped rendering for anisotropic surfaces. While this technique is commonly used for Lambertian isotropic surfaces, it is not suitable for materials that are anisotropic and require shading based on local tangent direction. The authors propose an extension of spherical harmonic irradiance maps to anisotropic surfaces, using the diffuse term of the Kajiya-Kay model. They provide an analytic formula for the diffuse BRDF in terms of spherical harmonics, showing faster decay than Lambertian reflectance. This formula can be easily implemented in existing code for real-time rendering and has potential applications in offline rendering for fibers.
41951	41951118	Hierarchical Iterated Local Search for the Quadratic Assignment Problem	Iterated local search (ILS) is a stochastic method that combines a perturbation step with a local search algorithm. This article proposes a new way of hybridizing ILS by using it as the embedded local search in another ILS. This nesting of local searches can be further iterated, creating a hierarchy of ILS. The paper explores this idea in the context of solving the quadratic assignment problem and shows that the hierarchical ILS outperforms a "flat" ILS in terms of efficiency and effectiveness. These results suggest that hierarchical ILS has potential for other applications and should be further investigated.
41952	4195215	Ruling analysis and classification of torn documents	This paper presents a new ruling classification method for document clustering. Unlike previous methods that focus on removing ruling lines, this method analyzes ruling lines for reassembling document snippets. It uses a novel Fourier feature and Support Vector Machines to classify lines into void, lined, or checked categories. The method also includes accurate line localization using projection profiles and robust line fitting. Evaluation on real-world document snippets shows an F-score of 0.987, and a synthetic dataset achieves an F-score of 0.931. The dataset used for evaluation is publicly available for benchmarking purposes. 
41953	4195313	Enumerating Maximal Bicliques from a Large Graph Using MapReduce	In this study, the authors focus on finding the maximum number of bipartite cliques (bicliques) in a large graph, which is important for data mining in social networks and bioinformatics. They introduce new parallel algorithms for the MapReduce platform and test them using Hadoop MapReduce. Their approach involves breaking the graph into smaller subgraphs and processing them simultaneously. This is made possible by minimizing redundancy and balancing the load on different reducers. The results of their experiments show that the algorithms can handle large graphs with millions of edges and tens of millions of bicliques, which is a significant achievement in this field.
41954	4195417	Fast and accurate processor models for efficient MPSoC design	The development of embedded software for upcoming MPSoC architectures is becoming increasingly challenging due to growing system complexity and greater software content. Traditional ISS-based validation methods are no longer feasible due to this complexity. To address this issue, the article introduces an approach of abstract processor modeling for multiprocessor architectures. This model combines computation modeling, an abstract RTOS, and accurate interrupt handling to create a versatile and multifaceted processor model with various levels of features. This model is then integrated into a system model using automatic generation and compilation, allowing for rapid design space exploration and validation. Experimental results on a multiprocessor mobile phone platform show the benefits of this approach, with a simulation speed of 300MCycles/s and less than 3&percnt; error. The results also demonstrate the trade-off between speed and accuracy at different levels of abstraction, providing guidance for future processor model designers.
41955	4195544	The PITA system: Tabling and answer subsumption for reasoning under uncertainty.	Probabilistic Logic Programming (PLP) is a field that combines probability and logic programs to represent uncertainty in real world domains. This has led to the development of languages such as Independent Choice Logic, LPADs, Problog, and PRISM. These languages share a similar distribution semantics, but computing the probability of queries in PLP programs is complex due to the need to combine non-exclusive explanations. PRISM reduces this complexity by restricting the form of programs it can evaluate, while Possibilistic Logic Programs use a simpler metric of uncertainty than probability. The PITA system, originally designed for LPADs, has been adapted to efficiently support both restricted PLP and Possibilistic Logic Programs. PITA uses tabling with answer subsumption and can be parameterized to support different variations of PLP, such as PITA(IND,EXC) for PRISM and PITA(COUNT) for counting different explanations. PITA has been shown to be competitive with PRISM and is available in XSB version 3.3.
41956	41956232	Extraction of rules from discrete-time recurrent neural networks	This paper discusses the importance of extracting symbolic knowledge from trained neural networks and encoding it directly into networks before training. This allows for the exchange of information between symbolic and connectionist representations. The focus is on the extraction of rules from recurrent neural networks, which can be trained to classify strings of a regular language. These rules can be extracted in the form of deterministic finite-state automata using clustering algorithms. The study compares the generalization performances of different extracted models and introduces a heuristic for choosing the best model to approximate the learned regular grammar. 
41957	419574	Reflections on simultaneous impact	The problem of resolving simultaneous impacts in collision response modeling has not been adequately addressed by existing algorithms. These algorithms fail to fulfill five important physical requirements, leading to issues such as non-symmetry preservation, loss of kinetic energy, and inability to handle break-away. To overcome these limitations, a new generalized impact model has been proposed that combines the strengths of two popular approaches. This model satisfies all the identified requirements, including symmetry preservation, kinetic energy conservation, and break-away capabilities. Additionally, a complementary restitution model has been developed to address the problem of inelastic collapse. The proposed algorithm has been successfully applied to large-scale simulations of impacting rigid bodies, including frictional impacts. It has also been validated through physical experiments, accurately capturing the behavior of vertically oscillated granular materials.
41958	4195854	Occlusion-Aware Reconstruction And Manipulation Of 3d Articulated Objects	The article introduces a method for recovering complete 3D models of articulated objects using structure-from-motion techniques. By capturing 3D point cloud models of the object in two different configurations, the joint axes can be accurately determined and classified as revolute or prismatic. This allows a robotic system to manipulate the object along its joint axes and exercise its degrees of freedom. The models are occluded-aware, meaning the robot can plan paths to unseen parts of the object. The algorithm does not require prior knowledge of the object and can handle non-planar objects and scenes. Experiments with a PUMA 500 robotic arm demonstrate the effectiveness of the approach on various objects with different joint types.
41959	419591	Living up to Expectations: Computing Expert Responses'	This paper discusses the importance of truthful and informative responses in cooperative man-machine interaction. It emphasizes the need for a system to modify its response if it might mislead the user. The focus is on identifying and avoiding potentially misleading responses by considering the types of informing behavior typically expected of an expert. The paper proposes a formal account of different types of assertions that should be included in responses to questions about achieving a goal, in addition to a simple answer, in order to prevent the questioner from being misled.
41960	4196021	On the complexity of sparse elimination	Sparse elimination is a method used to solve polynomial systems that generate zero-dimensional ideals. It works by considering the Newton polytope, or geometric object, of the polynomial instead of its total degree. A monomial basis for the coordinate ring is defined using a mixed subdivision of the Newton polytopes, and a sparse resultant matrix is constructed to compute a multiplication map and find common zeros. The size of the monomial basis is equal to the mixed volume, a measure of complexity, and the computation of this basis is equivalent to computing the mixed volume. The algorithms used have a worst-case complexity proportional to the volume of the Minkowski sum, and new bounds are established in terms of sparsity parameters. A lower bound on mixed volume in terms of Euclidean volume is also proved, which has relevance beyond the context of sparse elimination. 
41961	4196186	Average Whenever You Meet: Opportunistic Protocols for Community Detection.	The article discusses an asynchronous communication model, where one edge is randomly activated in each round and its endpoints can exchange messages and perform computations. A random process is studied, where the first time a vertex is an endpoint of an active edge, it chooses a random number and in subsequent rounds, the values of the endpoints are updated to their average. The authors show that if the graph has a two-community structure, the values held by the nodes will reflect this structure during a certain phase of the process. This analysis requires new concentration bounds on random matrices and is used to design opportunistic protocols for recovering community structure with low computational costs.
41962	419623	Kernel-based machine learning for fast text mining in R	Kernel-based machine learning methods have made significant progress in processing text data using string kernels and suffix arrays. The kernlab package offers a wide range of already implemented algorithms and infrastructure for these methods. Additionally, by utilizing the text mining capabilities of the tm package, R users can efficiently process, visualize, and group large collections of text data. The focus of these packages is on the effectiveness of different types of string kernels in performing these tasks.
41963	419634	Multimodal authentication based on random projections and source coding	This paper presents an authentication framework for independent modalities using binary hypothesis testing and source coding with random projections. The source coding allows for the reconstruction of multimodal signals at the decoder using authentication data, while random projections are employed to address security, privacy, robustness, and complexity concerns. The performance of the authentication system is analyzed in both direct and random projection domains, with an asymptotic performance approximation compared to exact solutions. The effectiveness of modality fusion on the authentication system is also demonstrated. 
41964	4196415	Balancing accountability and privacy using e-cash (extended abstract)	The concept of electronic cash (e-cash) allows users to withdraw coins from the bank and spend them anonymously and without being traced. However, for certain applications such as tax purposes, it is necessary to set limits on the amount of money that can be spent anonymously. This paper introduces a new e-cash system that allows for these limits to be enforced without the need for a trusted party. The system ensures anonymity as long as the user does not double-spend a coin or exceed the publicly-known spending limit with any merchant. This system is based on a compact e-cash system and is secure under the same assumptions in the random-oracle model. It is also efficient, with only O(ℓ+k) bits needed to store 2ℓ coins and the complexity of the withdrawal and spend protocols being O(ℓ+k). This allows for a balance between accountability and privacy, which is not possible with regular cash.
41965	4196533	Bayesian Generalized Kernel Mixed Models	The proposed methodology is a fully Bayesian approach for generalized kernel mixed models (GKMMs), which are extensions of generalized linear mixed models using a reproducing kernel. The regression vector of the model is assigned a mixture prior, allowing for sparsity and aiding in Bayesian computation. A Markov chain Monte Carlo (MCMC) algorithm is developed using data augmentation, reversible jump method for model selection, and Bayesian model averaging for posterior prediction. This approach can be related to the Karhunen-Loève expansion of a Gaussian process (GP), leading to a flexible approximation method for GPs. 
41966	4196645	Symbolic execution of concurrent objects in CLP	The concurrent objects model involves objects with dedicated processors in a distributed environment where communication is through asynchronous method calls. This poses a challenge in symbolic execution due to the combination of object-oriented features, concurrency, and backtracking. To address this, the paper proposes a CLP-based approach where the OO program is transformed into an equivalent CLP program with built-ins for handling concurrency. These built-ins include primitives for synchronization and scheduling. Symbolic execution of the transformed program then becomes a standard sequential execution of CLP. A prototype implementation in the PET system demonstrates the feasibility of this approach.
41967	4196771	Conditional termination of loops over heap-allocated data.	Static analysis, a method used to analyze computer programs for potential errors, often does not take into account the values stored in the heap, making it less accurate. In this article, the authors propose a solution for accurately tracking heap-allocated data in Java programs by using ghost variables instead of directly tracking object fields and array contents. This allows for more efficient heap-insensitive analysis while still providing information on the original heap-allocated data. If certain conditions cannot be proven, the authors suggest using aliasing preconditions to ensure the program will terminate. Experimental results demonstrate that this method greatly improves accuracy with a reasonable overhead.
41968	4196889	A Technique For Recursive Invariance Detection And Selective Program Specialization	This paper discusses a method for optimizing programs by reducing checks within cycles using Program Transformation and Abstract Interpretation. The technique involves using an abstract interpreter to detect simple invariants and then using program transformation to simplify predicates within the cycle. This allows for optimizations beyond what can be achieved with a single pass of the abstract interpreter. The paper presents a class of programs that can benefit from this technique and provides examples and evaluations in areas such as floundering detection and reducing run-time tests in automatic logic program parallelization. The technique can be implemented using existing tools, making it a valuable addition to the compiler design. 
41969	4196980	Roadmap-Based level clearing of buildings	This paper presents a roadmap-based method for multi-agent search in buildings or multi-story environments. The method involves creating a graph representation of the environment, known as a roadmap, which allows for feasible paths to be encoded. The roadmap is divided into regions, such as levels, and specific strategies are designed for each region to efficiently cover and clear the environment. This approach offers guarantees on coverage and the minimum number of agents required. It is also capable of handling complex and realistic environments, unlike other methods that are limited to simple 2D environments. 
41970	4197041	Small-space analogues of Valiant's classes	The uniform circuit model of computation uses the width of a boolean circuit to represent the space complexity of a function. Similarly, in Valiant's algebraic model, the width of an arithmetic circuit can be used as a measure of space. The class VL is introduced as an algebraic variant of deterministic log-space L. In the uniform model, the definition of VL is equivalent to that of VPSPACE at polynomial width. To define algebraic variants of non-deterministic space-bounded classes, "read-once" certificates for arithmetic circuits are introduced. It is shown that polynomial-size algebraic branching programs can be expressed as a read-once exponential sum over polynomials in VL, and that VBPs are stable under these sums. Additionally, it is shown that read-once exponential sums over a restricted class of constant-width arithmetic circuits fall within VQP, making it the largest known subclass of poly-log-width circuits with this property.
41971	4197177	Robust fuzzy control for uncertain discrete-time nonlinear Markovian jump systems without mode observations	This paper focuses on the robust fuzzy control problem of uncertain discrete-time nonlinear Markovian jump systems without mode observations. It utilizes the Takagi and Sugeno (T-S) fuzzy model to represent the system with parameter uncertainties and Markovian jump parameters, resulting in an uncertain Markovian jump fuzzy system (MJFS). A stochastic fuzzy Lyapunov function (FLF) is used to analyze the robust stability of the uncertain MJFS, incorporating both system modes and membership functions. A mode-independent state-feedback control design is then developed using a non-parallel distributed compensation (non-PDC) scheme to ensure stochastically stability for all uncertainties. The conditions for robust stability and mode-independent stabilization are formulated as linear matrix inequalities (LMIs), which can be efficiently solved using existing optimization techniques. A simulation example is also presented to demonstrate the effectiveness of the proposed method.
41972	41972141	Adaptive document image binarization	The article introduces a new approach to document image binarization, where the page is divided into smaller components such as text, background, and pictures. It addresses issues caused by noise, illumination, and different types of degradation. Two new algorithms are used to determine a local threshold for each pixel. The performance of the method is evaluated using test images with ground-truth, evaluation metrics for textual and synthetic images, and a weight-based ranking system. The experimental results demonstrate that the proposed method performs well in different scenarios and outperforms other existing techniques.
41973	4197331	Meta-Learning for Semi-Supervised Few-Shot Classification.	Few-shot classification is a type of learning algorithm that trains a classifier using only a few labeled examples. Recently, there has been progress in this field using meta-learning, where a parameterized model is trained on different classification problems, each with a small labeled training set and corresponding test set. This research aims to advance this paradigm by incorporating unlabeled examples within each episode. The study considers two scenarios: one where all unlabeled examples belong to the same classes as the labeled examples, and another more challenging situation with additional distractor classes. To address this, Prototypical Networks are extended to use unlabeled examples when creating prototypes and are trained in an end-to-end manner. Experiments on different benchmarks show that this approach can improve predictions due to the inclusion of unlabeled examples. A new split of ImageNet, with a hierarchical structure, is also proposed for further evaluation.
41974	4197430	An innovative feature selection using fuzzy entropy	The paper presents a new approach for feature subset selection, which is divided into two phases. The first phase focuses on reducing the algorithm run time by finding the best number of clusters in the dataset using silhouette value and calculating entropy fuzzy measures for each feature. In the second phase, the goal is to select a feature subset that meets certain boundaries to achieve high accuracy. The method is tested on various datasets and results show that it is able to select the minimum number of features without significantly impacting the final classification accuracy, making it a promising approach for feature selection. 
41975	4197547	Lia: A Location-Independent Transformation For Asocs Adaptive Algorithm 2	Artificial Neural Networks (ANNs) with fixed topologies often face challenges during learning. However, ANNs that use dynamic topologies have shown promise in overcoming these issues. Adaptive Self-Organizing Concurrent Systems (ASOCS) is a type of learning model with dynamic topologies. This paper introduces Location-Independent Transformations (LITs) as a way to efficiently implement dynamic topologies in parallel hardware. LITs create location-independent nodes that can compute network outputs using local information, allowing for the addition and deletion of nodes during learning. The paper presents the Location-Independent ASOCS (LIA) model as an LIT for ASOCS Adaptive Algorithm 2, and provides formal definitions for LIA algorithms. This not only describes LIA, but also provides a formal description of basic ASOCS mechanisms in general. 
41976	4197632	Principles for robust evaluation infrastructure	The "Cranfield" evaluation method has been used for almost 50 years and has played a crucial role in the development of large-scale information retrieval systems. Recent investigations have uncovered flaws in some experiments, leading to better understanding and prevention of these issues in future work. Based on their research and observations, the authors propose principles for designing new evaluation infrastructure. This statement highlights the importance of systematic evaluations in identifying strengths and limitations of retrieval systems and how this knowledge can inform future experiments and infrastructure design.
41977	4197721	On the Graph Traversal and Linear Binary-Chain Programs	Grahne et al. have introduced a graph algorithm for evaluating recursive queries, consisting of two phases. The first phase transforms a linear binary-chain program into equations with predicate symbols. The second phase involves constructing a graph and traversing relevant paths to produce answers. A new algorithm has been developed which is faster than Grahne's, by reducing the search space and generating answers directly from previously found answers and associated path information. This results in a linear time complexity for both acyclic and cyclic data. 
41978	419786	CloudNet: dynamic pooling of cloud resources by live WAN migration of virtual machines	Virtualization technology has greatly expanded the possibilities for resource management, allowing for the easy migration of virtual machines (VMs) within a local area network (LAN). This has led to a shift from allocating resources on a single server to managing pools of resources in a data center. The next step in this evolution is expected to be the migration of VMs over wide area networks (WANs), which will allow for the provisioning of resources across multiple data centers. The CloudNet architecture, consisting of cloud computing platforms and a virtual private network (VPN)-based network infrastructure, aims to facilitate this transition. It offers optimized support for live WAN migration of VMs, with a set of optimizations that reduce memory migration time by 65% and decrease bandwidth consumption by 50%. This has been successfully evaluated on an operational cloud platform spanning the continental US.
41979	4197937	A Structure-preserving Clause Form Translation	Resolution theorem provers are commonly used to find proofs for theorems, but they often convert the theorem into a clause form first. However, this conventional translation can obscure the structure of the formula and make it much longer. To address this issue, a non-standard clause form translation has been developed that preserves more of the formula's structure and avoids the exponential increase in length. This approach can also be combined with replacing predicates by their definitions before converting to clause form. Additionally, a new method called lock resolution has been created specifically for this non-standard clause form translation, resulting in a significant reduction in search space and time for one example. These techniques make resolution theorem provers more appealing for program verification, as the theorems involved in this field are often simple but tedious for humans to prove.
41980	4198020	A Decidable Quantified Fragment Of Set Theory With Ordered Pairs And Some Undecidable Extensions	This paper discusses a fragment of set theory with restricted quantification, which includes pair related quantifiers and constructs. The goal is to explore its potential applications in knowledge representation. The decision problem for this language has a non-deterministic exponential time complexity, but for formulae with shorter quantifier prefixes, it becomes NP-complete. Despite this restriction, important set-theoretic constructs, particularly those related to maps, can still be expressed. The paper also presents some undecidable extensions of the language, such as those involving the operators domain, range, image, and map composition. Overall, this paper provides insights into the complexity and expressiveness of this fragment of set theory.
41981	4198130	Dynamical Systems and Stochastic Programming: To Ordinary Differential Equations and Back	This paper examines the relationship between two types of models for biological systems: ordinary differential equations (ODE) and stochastic and concurrent constraint programming (sCCP). The authors define methods for converting between the two types of models and study their properties. They find that the mapping from sCCP to ODE preserves rate semantics for biochemical models and explore the invertibility of the mappings. They also investigate if the models produced by the mappings have the same dynamics, and provide examples where this property fails in the inverse direction. Overall, the paper highlights the importance of considering both types of models in understanding the behavior of biological systems.
41982	4198232	Optimal call admission and preemption control for public safety communications	Public safety communication systems use trunked mobile radio systems. The Engset model is commonly used to model these systems. This paper discusses the optimal call admission and preemption control for these systems using the Engset model. The system considers two classes of users, high and low priority. The optimal call admission policy for low priority users is a state-dependent threshold-based policy, and the optimal call preemption policy for low priority users is a threshold-based policy. This analysis provides a framework for efficient and effective management of public safety communication systems.
41983	419832	Exclusion and Guard Zones in DS-CDMA Ad Hoc Networks	The main issue in direct-sequence code-division multiple-access (DS-CDMA) ad hoc networks is preventing a near-far problem. This can be addressed by using guard zones, such as an exclusion zone and a carrier-sense multiple-access (CSMA) guard zone. The exclusion zone represents the minimum physical separation between mobiles, while the CSMA guard zone deactivates potentially interfering mobiles beyond the exclusion zone. This paper analyzes DS-CSMA networks with these guard zones, considering a finite network with a uniform clustering spatial distribution. The analysis uses a closed-form expression for the outage probability in the presence of Nakagami fading, taking into account the network geometry. The tradeoffs between exclusion zones and CSMA guard zones are explored for DS-CDMA and unspread networks, with the spreading factor and guard-zone radius providing design flexibility for achieving specific levels of outage probability and transmission capacity. The advantage of an exclusion zone is that it allows for a constant number of active mobiles, leading to higher transmission capacities compared to a CSMA guard zone.
41984	419846	Integration of domain knowledge in the form of ancillary map data into supervised classification of remotely sensed data	In recent years, machine learning and data mining have become popular in remote sensing applications, particularly in land cover and vegetation mapping. This paper introduces a new method that incorporates additional information (domain knowledge) in supervised classification of land cover using high dimensional remote sensing data. The traditional classification schemes used for ecological or land use purposes often have classes that are difficult to distinguish in remote sensing data. This paper presents a technique that utilizes prior probabilities in a decision tree classification algorithm to effectively include ancillary data sources and improve classification accuracy. This method is based on recent developments in statistics and machine learning, allowing for robust estimates of class membership without penalizing rare classes. 
41985	4198512	On mining complex sequential data by means of FCA and pattern structures.	Data sets in today's world are becoming increasingly complex and diverse, making data mining essential for real-world applications. This study focuses on analyzing "complex" sequential data using Formal Concept Analysis (FCA) and its extension, pattern structures. These structures, based on a subsumption operation, are used to mine complex data such as sequences or graphs. By utilizing projections, the study shows how meaningful patterns can be identified and computing efficiency can be improved. The method was applied to a French healthcare data set on cancer and the results, including annotations and analysis from a physician, are presented. The study highlights the importance of data mining and FCA in analyzing and understanding complex data sets.
41986	4198633	Privacy-preserving mobile crowd sensing in ad hoc networks.	Mobile Crowd Sensing (MCS) is a new paradigm that aims to outsource sensing data collection to the owners of mobile devices. This approach has the potential to revolutionize the traditional methods of collecting and processing sensing data. However, the widespread use of MCS raises concerns about privacy, especially when using third-party infrastructures. To address these concerns, this paper proposes three protocols for privacy preservation in ad hoc networks. The Privacy-Preserving Summation (PPS) protocol protects the privacy of Sensing Service Consumers (SSCs), while the Privacy-Preserving Difference Rank Computation (PPDRC) protocol ensures the privacy of Mobile Device Owners (MDOs). Additionally, the Approximate K-Nearest Neighbor with Privacy Preservation (AKN2P2) protocol is proposed to identify the k-nearest neighbors without compromising the privacy of either the MDOs or SSCs. The paper also includes performance evaluations of these protocols under different settings.
41987	4198758	Subspace-based linear multi-step predictors in type 1 diabetes mellitus.	Managing diabetes can be a challenge, as it requires balancing insulin dosage and food intake to maintain healthy blood glucose levels. This paper proposes a multi-step data-driven approach to predict blood glucose levels in the future, which can help patients make informed decisions about their treatment. The method uses physiological models and clinical data from 14 type 1 diabetic patients to develop predictors based on subspace identification methods. The results show that the predictors have a mean population prediction error standard deviation of 19.17 mg/dL, 37.99 mg/dL, 50.62 mg/dL, and 58.06 mg/dL for 30 min, 60 min, 90 min, and 120 min ahead predictions.
41988	4198866	Analysis and design of admission control in Web-server systems	Service control nodes, such as web sites and Mobile Switching Centers, can be represented as server systems with multiple servers handling incoming requests. To prevent overload, various admission control mechanisms are typically implemented. This paper focuses on the modeling of a service control-node from a control perspective. The queue is assumed to follow an M/G/l-system and is represented by a nonlinear flow model, with a simplified discrete-time model used for analysis and design. An admission control system using a PI-controller and anti-reset windup feature is developed and its stability is proven. Discrete-event simulations are used to verify the results from the simplified queue model. 
41989	4198930	Constructive Negation by Pruning and Optimizing Higher-Order Predicates for CLP and CC Languages	In this article, the authors discuss different forms of negation in constraint logic programming using the program's completion approach. They introduce a new method called constructive negation by pruning, which has a correct and complete operational semantics based on Kunen's three-valued logic. The authors place emphasis on a full abstraction result that allows for a deeper understanding of the operational behavior of CLP programs with negation. They also provide further insights derived from this approach.
41990	41990105	A field trial of privacy nudges for facebook	The article examines the issue of regretful online disclosures by Internet users and proposes two modifications to the Facebook interface to help users be more conscious of their content and audience. A 6-week field trial with 28 Facebook users was conducted to evaluate the effectiveness of these modifications. The results showed that reminders about audience can prevent unintended disclosures without causing inconvenience, but introducing a time delay before posting may be perceived as both helpful and annoying. Some participants found the modifications helpful, while others found them unnecessary or intrusive. The article discusses the implications and challenges of designing and evaluating systems to assist users with online disclosures.
41991	4199140	A compendium of formal techniques for software maintenance	This paper discusses the importance of software maintenance in practical software engineering and highlights how it has been neglected by theoretical computer scientists. It provides an overview of formal techniques that have been developed to assist in the software maintenance process, specifically in reverse engineering and re-engineering. The paper also suggests that maintaining specifications instead of programs could be more beneficial in the future. The described work serves as a foundation for a collaborative project that is exploring various other aspects of software maintenance. 
41992	4199225	Developing and validating predictive decision tree models from mining chemical structural fingerprints and high-throughput screening data in PubChem.	Advances in high-throughput screening (HTS) techniques and compound libraries have enabled the rapid testing of millions of compounds for potential use in drug development. However, the large amount of data produced by HTS assays makes it difficult to identify promising compounds. In this study, Decision Trees (DT) models were developed to analyze HTS results based on chemical structure fingerprints. These models were effective in discriminating compound bioactivities in four assays, including those for 5HT1a agonists, antagonists, and HIV-1 RT-RNase H inhibitors. Further evaluation showed that the models could also be used for virtual screening and complement traditional methods for selecting potential drug candidates.
41993	4199367	Sequential Design of Experiments via Linear Programming	The multi-armed bandit problem is a well-known dilemma in decision theory that involves balancing the exploration and exploitation of a system. In this paper, the focus is on a variant of this problem where the exploration phase is costly and occurs before the exploitation phase. The goal is to find an inexpensive exploration strategy that optimizes the exploitation objective. This problem is NP-Hard, but the paper presents a polynomial time approximation algorithm for it. The approach involves a linear program rounding technique and can also be applied to other related problems in sensor networks. The resulting exploration policies are sequential and do not revisit any arm, making them desirable in scenarios where multiple explorations can happen simultaneously. 
41994	41994101	Reasoning About Approximate Match Query Results	Approximate match predicates are essential for data cleaning operations and various types have been used within a declarative data cleaning framework. These techniques return pairs of tuples with a score indicating their level of similarity. The problem of estimating parameters for planning purposes in these algorithms is addressed in this paper. Precise knowledge of the result size and score distribution is crucial for decision making in identifying similar tuples, which is important for data cleaning. The proposed solution strategies are compliant with the declarative framework and are evaluated for performance. The experimental results validate the expectations and provide insights into the quality and performance of the estimation framework. These techniques can be easily implemented in data cleaning systems.
41995	419950	Effective trust management through a hybrid logical and relational approach	This paper discusses the deployment of logical trust management systems in enterprise environments using a relational database management system (DBMS). The authors present a framework for managing trust management policies and describe a procedure for compiling credentials into dynamic views within the database. They also propose a hybrid algorithm for efficiently enumerating user capabilities. The paper includes an evaluation of a prototype implementation and suggests that their approach can be generalized to other trust management approaches. The authors argue that despite the lack of attention to deployment strategies, their framework offers a practical solution for managing trust in authorization.
41996	4199617	Selecting the right objective measure for association analysis	The paper discusses the use of objective measures such as support, confidence, interest factor, correlation, and entropy in evaluating the interestingness of association patterns. However, these measures can sometimes conflict with each other, and data mining practitioners may not be aware of better alternatives. The paper suggests examining key properties in order to select the most suitable measure for a specific application. A comparative study of 21 measures from various fields is also presented, showing that each measure has its own strengths and weaknesses. The paper also highlights two scenarios where existing measures become consistent with each other. Finally, an algorithm is proposed to help domain experts select the best measure for their needs.
41997	4199748	Verifying Second-Level Security Protocols	A second-level security protocol relies on an underlying protocol to achieve its purpose. While the verification of traditional authentication protocols has become standard, second-level protocols pose new challenges. These include formalizing references to the underlying protocols, adjusting the threat model, and defining new goals. These issues have been addressed using Isabelle and the Inductive Approach. The effectiveness of this approach is exemplified by its successful application to a certified e-mail delivery protocol developed by Abadi et al.
41998	4199843	Finding partitions of arguments with Dung's properties via SCSPs	The concept of forming coalition structures allows agents to combine their efforts in order to achieve a shared goal. The authors propose exploring homogeneous groups with distinct lines of thought, and extend the Dung Argumentation Framework to incorporate coalitions of arguments. This involves partitioning the initial set of arguments into subsets, where each subset represents a different line of thought with the same inherited property as Dung's framework. The use of (soft) constraints is suggested as a formal method to address NP-complete problems in weighted argumentation, with semiring algebraic structures used to model different optimization criteria for the coalitions. The authors demonstrate this approach using JaCoP, a Java constraint solver, on a small-world network.
41999	419993	Deconstructing datacenter packet transport	pFabric is a datacenter fabric design that focuses on providing high performance for high-priority flows while also maximizing overall network utilization. This is achieved through a minimalistic approach, using simple mechanisms at each switch and eliminating most buffering. The design relies on switches making locally and greedy decisions based on priority information in the packet header, without requiring any flow state or rate estimates. As a result, rate-control is rarely necessary and all flows start at line-rate, only slowing down in extreme cases of congestion. Simulation results with realistic workloads and topologies demonstrate that this straightforward design achieves near-optimal flow completion times and network utilization.
411000	41100014	The design of bug fixes	Software engineers have multiple options when it comes to fixing bugs, and the choice they make can have various implications for both practitioners and researchers. These include the risk of introducing new bugs, the location of the bug fix in the code, and whether the fix addresses the root cause or just a symptom. A recent study conducted with 40 engineers, data from 6 bug triage meetings, and a survey of 326 engineers revealed that non-technical factors, such as the software's proximity to release, play a significant role in bug fixing decisions. The study also suggests potential improvements in bug prediction and localization methods. 
411001	41100136	Importance sampling based discriminative learning for large scale offline handwritten Chinese character recognition.	This paper presents a discriminative learning framework for large-scale classification tasks using importance sampling. The framework assigns weights to samples based on their importance and uses three methods to calculate these weights for learning a modified quadratic discriminant function (MQDF). These methods include rejection sampling, boosting algorithm, and minimum classification error (MCE) rule. The proposed framework focuses on cursive samples, which are typically more difficult to classify, and achieves higher accuracy with lower computational complexity compared to the traditional maximum likelihood estimation (MLE) rule. Experiments on Chinese handwritten character datasets show promising results, demonstrating the effectiveness of the proposed framework.
411002	4110028	Synchronization Strings: List Decoding for Insertions and Deletions.	The study focuses on codes that can tolerate both insertions and deletions in a received codeword. These codes, called $L$-list-decodable, have an efficient algorithm that can report a list of $L$ codewords that contain the original one. The authors use the concept of synchronization strings to show that there exist efficient codes with a constant alphabet and sub-logarithmic list sizes, even when the fraction of insertions can be arbitrarily large. The results also reveal a significant asymmetry between the impact of insertions and deletions on error-correction, with deletions affecting the code's rate while insertions are borne by the adversary. The study also provides tight bounds on the parameters of these codes, showing that the alphabet size needs to be exponentially large in the gap to capacity. This is in contrast to the Hamming error model, where a polynomial alphabet size suffices for unique decoding.
411003	41100313	Large-scale factorization of type-constrained multi-relational data	Statistical modeling of large multi-relational datasets is becoming increasingly popular, with applications including knowledge bases like DBpedia, Freebase, YAGO, and Google Knowledge Graph. These datasets contain millions of entities, hundreds and thousands of relations, and billions of relational tuples. Collective factorization methods, particularly tensor approaches, have been successful in scaling up to these datasets using the alternating least squares (ALS) algorithm. This paper introduces an extension to the RESCAL tensor factorization method that considers relational type-constraints, which define the logic of relations by excluding certain entities from subject or object roles. The proposed approach is shown to be scalable and outperforms RESCAL without type-constraints in both runtime and prediction quality on large datasets.
411004	41100450	Representing Knowledge in Robotic Systems with KnowLang.	Building intelligent robotic systems is exciting but also incredibly difficult. Researchers have found that using a logical approach can help achieve robot intelligence, but there is still a struggle to connect this abstract logic with real-world meaning. This paper introduces KnowLang, a new formal language designed for knowledge representation in a specific type of intelligent robotic system called ASCENS. These systems, known as Autonomic Service-Component Ensembles, are made up of mobile, intelligent, and open-ended groups of service components that can reason locally and in a distributed manner. These service components contain rules, constraints, and self-adaptation mechanisms, allowing them to acquire and process knowledge about themselves, other components, and their environment. The paper also includes a case study showcasing how KnowLang can be used to represent knowledge in a robotic system.
411005	41100528	An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry	Software product lines have become increasingly popular in industry for creating related products, maximizing reuse, and utilizing variable and configurable options. However, as software becomes more dynamic and requires adaptable features, the need for systems to support runtime adaptation is growing. This is especially true for embedded systems, ecosystems, service-based applications, and self-adaptive systems. Traditional software product line architectures are not equipped to handle such dynamic conditions, leading to the development of Dynamic Software Product Lines (DSPLs). While these approaches attempt to address the challenges of runtime variability, they are still in an early stage of development. In this research, we provide an overview of the current techniques and state of the art in DSPLs, as well as the challenges and solutions needed to support runtime variability in these systems.
411006	4110061	Extending the UMIOP specification for reliable multicast in CORBA	OMG has released a specification for unreliable multicast in CORBA applications called UMIOP, which is based on the best-effort IP Multicast protocol. However, fault-tolerant and groupware applications often require more strict guarantees for message delivery, such as reliable multicast with FIFO, causal, or total ordering. Currently, OMG does not have a specification for meeting these requirements. To address this gap, the ReMIOP protocol was proposed as an extension to UMIOP, providing a reliable multicast mechanism in CORBA middleware. Performance tests were conducted to compare ReMIOP, UMIOP, and UDP sockets for IP multicast communication, showing the additional costs of implementing reliable and unreliable multicast at the middleware level.
411007	41100743	Byzantine Fault-Tolerant MapReduce: Faults are Not Just Crashes	MapReduce is a commonly used system for running important tasks like scientific data analysis. However, research has shown that unexpected faults can occur during these jobs, potentially corrupting the results. While MapReduce runtimes like Hadoop can handle crashes, they are not equipped to handle arbitrary or Byzantine faults. To address this issue, a new MapReduce algorithm and prototype have been developed to tolerate these types of faults. An experiment showed that this approach uses twice the resources of Hadoop, which is still more efficient than other fault-tolerance methods. This added cost is deemed acceptable for critical applications that require a high level of fault tolerance.
411008	41100835	RStore: A Distributed Multi-Version Document Store	This article discusses the problem of storing a large number of versions of keyed documents in a distributed environment while efficiently answering retrieval queries. The need for such a system is increasing in various application domains. The article explores the design space and trade-offs involved in building such a system, and proposes a novel system architecture with tuning knobs to adapt to different data and query workloads. The system acts as a layer on top of a distributed key-value store and uses algorithms for efficient data partitioning and handling new versions. Extensive experiments show that the system outperforms standard baselines by orders-of-magnitude.
411009	41100929	Reconstruction of neural action potentials using signal dependent sparse representations	In this study, a method is presented to construct a sparse representation dictionary for neural action potentials using the K-SVD algorithm and Discrete Wavelets Transform. This dictionary is then used in a Compressive Sensing (CS) framework to recover the neural signal. The proposed approach outperforms non-signal dependent CS recovery algorithms by achieving the same quality of reconstruction with 2.5 times fewer measurements. It also increases the signal to noise and distortion ratio (SNDR) by 6 dB compared to non-signal dependent methods. The recovered signal is evaluated using spike sorting techniques, which show clear separation of spike clusters even at low compression ratios. This method has implications for hardware implementation of compressed sensing, as it can reduce power and chip area by the same order.
411010	41101025	Compact Multicast Routing	A compact multicast scheme is a routing method in a distributed network where any source can send messages to a chosen group of targets. The amount of space needed to store the routing table on each node is balanced with the stretch factor, which is the maximum ratio between the cost of the multicast route and the cost of a steiner tree for the same target nodes. Different versions of the problem were studied, including labeled, name-independent, and dynamic, where the goal is to minimize both the cost of the multicast tree and the cost of control messages for updating the tree as nodes join and leave the multicast service.
411011	41101122	Abstract Transformers for Thread Correlation Analysis	The authors propose a new method for accelerating the analysis of concurrent programs that use shared memory. The focus is on thread correlation, which involves identifying relationships between the local states of different threads and the global state. This is crucial for verifying properties of concurrent programs. However, tracking these correlations is expensive, especially due to the cost of applying abstract transformers. The proposed technique involves using footprints and memoization to compute abstract transformers more efficiently. The technique has been implemented in a concurrent shape analysis framework and has been used to prove several properties of complex concurrent programs. Empirical results show a significant reduction in analysis time.
411012	41101218	Complexity of Computing with Extended Propositional Logic Programs	The paper introduces the concept of an F-program, where F is a collection of formulas. These programs are a generalization of standard logic programs, allowing for formulas other than atoms to be used in rules. Examples of F include the set of all atoms, literals, Horn clauses, and clauses with varying numbers of literals. The paper explores the complexity of computing with F-programs, including determining the existence and membership of stable and minimal answer sets. The complexity of reasoning involving these notions is also studied, and several open problems are listed.
411013	41101383	Stochastic Models Of Slc Hr Sar Images	The paper introduces two algorithms for extracting texture primitive features from Single Look Complex (SLC) and Polarimetric Synthetic Aperture Radar (PolSAR) SLC data. These algorithms use a Gauss-Markov Random Field (GMRF) model to capture the spatial correlation in the data. One algorithm uses a complex GMRF model to characterize the spatial relationship of SLC SAR data, while the other extends this model to capture inter-band correlation between polarimetric channels. The authors use a Bayesian approach to effectively handle model fitting and selection. The algorithms are tested on a polarimetric E-SAR L band scene of Mannheim, Germany and the results are presented.
411014	41101444	An active camera system for acquiring multi-view video	The system discussed is designed to capture multi-view video of a person moving through an environment. It utilizes a real-time tracking algorithm to adjust the parameters of multiple active cameras, ensuring that the person remains centered in each view. The result is a set of synchronized and time-stamped video streams, providing multiple perspectives of the person's movements.
411015	41101599	A hierarchical characterization of a live streaming media workload	In this article, the authors present a detailed analysis of live streaming media content on the Internet. They studied over 3.5 million requests over a 28-day period, focusing on client, session, and transfer levels. They found that the interactions between users and objects are different for live and stored objects, with live objects being more driven by the object itself rather than the user. They also observed a Zipf-like pattern in user interest for a specific object, compared to the classic Zipf-like popularity of objects for a user. The variability in transfer lengths was found to be due to the stickiness of clients to a particular live object. The authors also suggest that the characteristics of live media access workloads are likely dependent on the type of live content being accessed. This is supported by the strong temporal correlations observed in the study, which can be attributed to the synchronizing impact of live content on access patterns. The authors present a model for live media workload generation based on their findings, which is implemented in Gismo. 
411016	41101684	Multi-layer Virtual Transport Network Design.	Service overlay networks and network virtualization are used to overcome deficiencies of the Internet, such as resiliency, security, and quality of service guarantees. However, most overlay/virtual networks are only used for routing and tunneling, instead of providing scoped transport flows with mechanisms for error and flow control and resource allocation. This results in limited network resource allocation and utilization. To address these limitations, a multi-layer approach to virtual transport network (VTN) design is proposed. This allows for dynamic scope management, improving application and network management. The multi-layer VTN design partitions the network into smaller scopes, achieving better performance compared to traditional single-layer designs. Simulation and experimental results demonstrate the effectiveness of this approach in enhancing network performance.
411017	4110177	Entropy-preserving cuttings and space-efficient planar point location	Point location is the process of organizing a polygonal subdivision into a data structure to efficiently determine which cell contains a given query point. The structure should minimize the expected query time, taking into account the probabilities of the point being in each cell. The entropy of the probability distribution is a key factor in determining the lower bound on the expected search time. The number of edges in the subdivision is a lower bound on the required space, but there is currently no method that can achieve both a query time of H + &Ogr;(H) and space of &Ogr;(n). This paper introduces entropy-preserving cuttings to achieve a query time of H + &Ogr;(H) with only &Ogr;(n log* n) space.
411018	41101882	Distributed error confinement	The article discusses error confinement in distributed applications, where only nodes directly affected by a fault are allowed to deviate from their correct behavior. This is impossible if an adversary can cause arbitrary faults, but the article introduces a new measure called agility to quantify fault tolerance. Broadcast algorithms are proposed that guarantee error confinement with optimal agility, which can be used in more complex systems. Previous studies on fault locality were not error confined and allowed a wide range of behaviors to be considered correct. The article also presents a new technique for analyzing the "cow path" problem.
411019	41101940	On Dual-Rail Control Logic for Enhanced Circuit Robustness	Ultra low-power design and energy harvesting applications require digital systems to operate under extremely low voltages. To achieve this, sub-threshold operation mode is used, which balances dynamic and static power consumption. However, this mode results in large delay variations, making asynchronous circuits necessary. But even these circuits can be vulnerable due to timing assumptions that are no longer valid. To address this, a paper presents an automated approach for synthesizing robust controllers for sub-threshold digital systems using dual-rail implementation. This eliminates the need for inverters, which can cause timing issues. Dual-rail controllers have minimal overhead and can even be faster than single-rail solutions. The presented synthesis techniques are efficient and can be applied to large controllers, as shown in benchmarks.
411020	41102015	A measurement-based analysis of multihoming	Multihoming is a networking strategy used by stub networks to enhance reliability and performance. However, there is limited understanding of the tangible benefits and how to fully utilize them. This paper aims to quantify the potential benefits of multihoming for both high-volume content-providers and enterprises. Results show that multihoming can significantly improve performance, with a potential 40% performance penalty for choosing the wrong providers. There is also evidence of diminishing returns when using more than four providers. The paper also provides guidelines for choosing ISPs and practical strategies for using multiple connections to achieve optimal performance. Additionally, an analysis of the reliability benefits of multihoming is provided.
411021	4110212	Utilizing Design Information in Aspect-Oriented Programming	Traditionally, aspect-oriented languages use pointcut designators to select specific parts of a program based on explicit lexical information, which can limit adaptability and result in hard-coded and implementation-specific code. To address this issue, design intentions can be used instead, which are represented by annotated design information that describes the behavior and meaning of program elements. This paper analyzes four techniques used in object-oriented languages to associate design information with program elements and discusses how it can be used in the weaving process of aspect-oriented languages. The paper also proposes language abstractions to support the composition of aspects with design information, and shows how the aspect-oriented language Compose* can be extended to support design information. The application of design information to improve the reusability of aspects is demonstrated, and the paper concludes with related works, a discussion, and conclusions. 
411022	41102254	Context adaptation of mamdani fuzzy rule based systems	Context adaptation involves adjusting a fuzzy rule based system (FRBS) to a specific context by modifying the meanings of linguistic terms and the fuzzy sets within the system. This is achieved through the use of operators that are optimized to balance interpretability and accuracy. The operators adjust the universe of input and output variables and modify the core, support, and shape of the fuzzy sets in the system. The parameters for the operators are chosen through a genetic optimization process. This approach has been applied to Mamdani fuzzy systems in two different domains, regression and data modeling. 
411023	4110231	Pattern Based Integration of Internet of Things Systems.	The Internet of Things (IoT) is a network of physical devices that can exchange data through sensors, actuators, and connectivity. It has a significant impact on society as more systems are becoming based on IoT. However, one of the main challenges is integrating the heterogeneous systems within the same communication network. Various studies have addressed this issue at different levels, but the approaches are scattered and not cohesive. This chapter presents a comprehensive and systematic approach for identifying and addressing integration concerns in IoT system architecture. It uses a pattern-based approach to provide generic solutions for common integration issues. The approach is illustrated through the example of integrating IoT systems in smart city engineering.
411024	41102411	Cache-aware timing analysis of streaming applications	There is a growing interest in designing hardware and software specifically for streaming applications, which process continuous streams of data. These applications are found in various devices and the timing analysis problem arises when determining performance metrics and mapping them onto hardware architectures. Previous work has neglected micro-architectural features such as caches, but a new framework has been presented that accurately models the evolution of instruction caches and the impact of previous data items on execution time. This framework combines program analysis techniques with mathematical methods for analyzing streaming applications, resulting in more accurate estimations of timing and buffer size. Experiments with an MPEG-2 encoder show that this approach is efficient, scalable, and leads to better results.
411025	411025233	Worst-case temperature analysis for different resource availabilities: a case study	Three-dimensional chip integration can lead to high on-chip temperatures, which can affect the reliability and performance of real-time systems. To prevent overheating, dynamic thermal management methods are used, but their impact on system temperature must be considered when designing real-time systems. This paper proposes a framework for calculating the worst-case temperature of a system with varying resource availabilities, using real-time and network calculus models. Case studies on an advanced multimedia system demonstrate the effectiveness of this framework in analyzing the effects of dynamic frequency scaling and thermal-aware scheduling techniques on system temperature. 
411026	41102644	The design of a stream cipher LEX	This paper introduces the concept of leak extraction from a block cipher and applies it to the AES cipher. The result is a new stream cipher called LEX, which is based on AES and is significantly faster than AES in both software and hardware. This demonstrates the effectiveness and practicality of leak extraction in improving cipher performance.
411027	41102794	Static Analysis and Software Assurance	Computer networks have experienced significant growth in the last ten years, which has highlighted the importance of security. However, a major challenge in computer security is the software assurance problem, which deals with the fact that even our most trusted software, including security software, can contain bugs. The speaker in this talk will discuss how static analysis, a technique for examining software without actually running it, can help address this problem. They will also share their recent experiences with using static analysis tools for detecting vulnerabilities and will identify some remaining challenges in the field, as well as potential areas for future research.
411028	411028167	Principles and applications of continual computation	Automated problem solving involves using computational resources to solve given problems. This process involves applying effort in real time to generate a solution, which marks the end of problem solving. However, continual computation goes beyond this by proactively allocating computational resources to potential future challenges. This approach also considers idle time and its allocation in different settings. The article presents various applications that demonstrate the potential of continual computation in practical tasks. © 2001 Elsevier Science B.V. All rights reserved.
411029	41102981	10-bit programmable voltage-output digital-analog converter	The paper discusses the development of a compact and low-power 10-bit floating-gate digital-to-analog converter (FGDAC) using nonvolatile floating-gate voltage references. This new design eliminates the limitations of traditional charge amplifier voltage-output DACs, resulting in improved accuracy and reduced element spread. The FGDAC was fabricated using a 0.5 micrometre CMOS process with a total area of 0.0522 mm2. Experimental results demonstrate that it can achieve INL and DNL values of less than plus or minus 0.5 LSB (0.68 mV). This FGDAC allows for programmable linear or nonlinear conversion with high precision.
411030	41103041	A semantic environment model for crowd simulation in multilayered complex environment	This paper discusses the challenges and importance of modeling environments in crowd simulation, particularly in complex and multilayered environments. The authors propose a three-tier semantic model for representing the environment, which includes geometric, semantic, and application levels. This model allows for better interactions between individuals and the virtual environment. A modified continuum crowd method is then applied to this model to simulate realistic behaviors of large crowds in complex environments such as buildings and subway stations. The method is tested in two synthetic urban spaces and the results demonstrate the effectiveness of the semantic environment model in providing accurate information for crowd simulation.
411031	41103165	Stabilizing Locally Maximizable Tasks in Unidirectional Networks Is Hard	A distributed algorithm is considered self-stabilizing if it can recover from faults and attacks without external intervention in a finite amount of time. This paper discusses the problem of constructing self-stabilizing solutions for locally maximizable tasks in uniform unidirectional networks. The authors show that deterministic self-stabilization is impossible in these networks, and that the "silence" property (where communication is fixed from a certain point in the execution) cannot be guaranteed. However, they present a series of generic protocols that can be used for all locally maximizable tasks. These include a deterministic protocol with polynomial time and space complexity, and two probabilistic protocols with expected polynomial time complexity. 
411032	4110323	A Generalized Semantics for Concurrent Constraint Languages and their Abstract Interpretation	The authors propose a framework for abstract interpretation of concurrent constraint languages, using the concept of abstraction between constraint systems. They define an abstract program for a concrete program, with the execution of the abstract program performing the abstract interpretation. This framework is based on a denotational semantics of concurrent constraint languages, where each agent is seen as a closure operator. The authors demonstrate the practical implementation of this analysis through a reexecution algorithm, and also show how suspension analysis can be incorporated into the framework. This extends a previous framework for abstract interpretation of concurrent logic languages.
411033	4110334	Towards the use of cad models in VR applications	Large industrial companies have a goal of creating integrated information systems to manage their projects. These systems include 3D visualization of models that are realistic enough to be used for virtual prototyping, design review, and training. However, there are challenges in producing Virtual Reality (VR) models from Computer-Aided Design (CAD) models. To address this, an application called ENVIRON (ENvironment for VIRtual Objects Navigation) was created to allow real-time interaction and an immersive experience with large industrial engineering models. This application was developed to meet the growing need for using VR in the industry, particularly with CAD models.
411034	41103436	Authenticated system calls	System call monitoring is a technique used to detect and control compromised applications by checking each system call at runtime. A new approach to this is introduced, using authenticated system calls which are augmented with extra arguments and a cryptographic message authentication code (MAC). This extra information is used by the kernel to verify the system call and its policy. The application is automatically generated with authenticated system calls through an installer program that uses static analysis to generate policies and rewrites the binary. The paper presents the approach, a prototype implementation, and experimental results showing its effectiveness in protecting against compromised applications at a reasonable cost.
411035	41103519	On Constrained Spectral Clustering and Its Applications	Constrained clustering algorithms, such as -means and hierarchical clustering, have been extensively studied, but handling a large number of constraints in these algorithms is known to be difficult. One solution is to use spectral clustering, which is still an area of ongoing development. This paper proposes a new approach for constrained spectral clustering that explicitly incorporates Must-Link and Cannot-Link constraints into the optimization problem. This method has several practical advantages, including the ability to specify the level of confidence in the constraints and a guarantee of how well the constraints are satisfied. It can also be solved efficiently and inherits the benefits of spectral clustering. Empirical results demonstrate the effectiveness of this approach, including its potential for transfer learning using constraints. 
411036	41103626	Re-tiling polygonal surfaces	This paper introduces an automatic method for creating surface models at different levels of detail, which is important for interactive graphics and rendering complex scenes. The method involves connecting new vertices over the surface, using surface curvature to determine vertex distribution, and smoothly interpolating between models. The key concept is creating an intermediate model called the mutual tessellation, which combines original and new vertices. The original vertices are then removed and the surface is locally re-triangulated to match the original connectivity. This technique has been successfully applied to various types of surface models. The paper also discusses relevant computer graphics categories and subject descriptors.
411037	41103723	Correctness proof for database reconstruction algorithm.	The rise in the use of databases for storing important information has also led to an increase in computer crimes involving databases. Despite some research and practical applications in digital forensics for databases, this area has not received much attention and lacks a defined model. This paper introduces an algorithm for reconstructing database information for forensic purposes and provides proof of its correctness. The algorithm uses the current database and a log of modifications to determine the data in a relation at a specific time. The paper also discusses inverse functions for relational algebra operators and introduces the concept of relational algebra log and value blocks. The majority of the paper is dedicated to explaining the proof of correctness for the database reconstruction algorithm.
411038	41103817	A Systematic Approach to the Development of Event Based Applications	LECAP is a new framework proposed for building event-based applications. It offers several advantages over existing approaches, such as supporting a while-parallel language, dynamic binding of programs to events, stepwise development, and composition of specifications. The event-based architectural style is recognized for its ability to develop large and complex systems by loosely coupling components, making it popular in various environments. However, current approaches to developing event-based applications lack support for reasoning about correctness. LECAP aims to address this issue by providing a compositional and stepwise approach to specifying and verifying event-based applications. 
411039	41103963	Starburst SSD: an efficient protocol for selective dissemination	Starburst is a routing-based protocol designed for efficient data dissemination in sensor networks. It uses a routing hierarchy to quickly and reliably deliver data to nodes that meet specific criteria. The protocol dynamically adapts its delivery policy based on the number of nodes requiring updates, using direct routing for a small number of nodes and more efficient algorithms for larger numbers. Dynamic beacon selection algorithms also improve scalability and fault tolerance. Starburst has been successfully implemented and evaluated on two routing protocols, showing a significant reduction in transmission cost and latency for small node subsets. Tests on a real-world testbed also validate its effectiveness.
411040	411040110	On the communication and streaming complexity of maximum bipartite matching	In this work, the authors focus on understanding the communication and space complexity of the maximum bipartite matching problem. They introduce the concept of an ε-matching cover, which is a sparse subgraph of the original graph that preserves the size of maximum matching within an additive εn error. They show that a 2/3-approximation can be achieved with a message of size O(n) by constructing a 1/2-matching cover with additional properties. They also consider a restricted version of the problem and show that a 3/4-approximation can be achieved with linear space. Additionally, they design a deterministic one-pass streaming algorithm for a specific scenario, which is the setting of a well-known randomized algorithm for online bipartite matching. This is the first deterministic algorithm for this scenario with only O(n) space complexity.
411041	41104146	Muse: Mapping Understanding and deSign by Example	Information integration involves designing relationships between two schemas, known as schema mappings. This is a complex task, and while automated tools can suggest potential mappings, there are few tools available to help designers understand and create alternative mappings. Muse is a mapping design wizard that uses data examples to assist designers in refining mappings towards the desired specification. It guides designers in two important components of mapping design: specifying grouping semantics for data sets and choosing among alternative interpretations for ambiguous mappings. Muse uses familiar databases and short sequences of small examples to infer the desired semantics and make the design process easier. Experiments with Muse on publicly available schemas have been successful.
411042	41104252	A collaborative decision-making model for orientation detection.	Orientation detection is a crucial aspect of both biological vision and machine vision. The Hubel-Wiesel model, which suggests that orientation selectivity in simple cells is due to overlapping receptive field centers, has some limitations. To address these limitations, this paper proposes a collaborative decision-making approach using a double-layer neural network. The first layer estimates the relative position of a contrast edge, while the second layer uses a least square optimization to determine orientation. This approach is not only flexible but also applicable to image processing. Statistical and simulation experiments show that this model is efficient and effective, and can explain visual illusions. Additionally, it outperforms other image processing algorithms on natural images. The neural mechanism of this model is in line with neurobiological findings, making it suitable for higher level visual tasks.
411043	411043247	Robust wireless video streaming using hybrid spatial/temporal retransmission	The paper discusses two main challenges faced by wireless video streaming applications: bandwidth demands and timing constraints. To address these challenges, the authors propose a hybrid spatial/temporal retransmission protocol which uses overhearing nodes as relays to retransmit failed packets. This approach increases individual throughput and overall network capacity. The protocol also includes a Time-based Adaptive Retransmission strategy to meet timing constraints by dynamically determining whether to transmit or discard a packet based on its retransmission deadline. Results from evaluations on both a testbed and in real-world scenarios show that this hybrid approach improves streaming performance, particularly in busy networks, under fading conditions, and for mobile users.
411044	4110449	Functional ASP with Intensional Sets: Application to Gelfond-Zhang Aggregates.	This paper introduces a variant of Answer Set Programming (ASP) with evaluable functions that allows for a fully logical treatment of aggregates. This is achieved by incorporating a new type of logical term, intensional sets, which can be used as predicate or function arguments and can be nested within other intensional sets. This approach has several advantages over other semantics for aggregates, including a compositional semantics and the ability to explicitly define aggregates within the logical language. The proposed semantics for aggregates is also shown to coincide with the one defined by Gelfond and Zhang for the Alog language, when restricted to that syntactic fragment.
411045	41104538	Idea Inheritance, Originality, and Collective Innovation.	This paper discusses the process of creating new products through the combination of previous ideas. While previous studies have focused on patent citations, this paper examines the impact of originality on popularity and practicality through the analysis of collaborations in an online virtual community. The authors propose a new method for measuring innovation based on the distance between 3D shapes. This research sheds light on the creative process and the role of technology in promoting creativity. It also highlights the potential for open innovation, where individuals can collaborate and evolve designs without belonging to the same organization or exchanging money.
411046	41104666	Construction and analysis of ground models and their refinements as a foundation for validating computer-based systems	In order to make the verified software challenge proposed by Hoare and Misra have practical impact, it is necessary to include rigorous definitions and analysis before code development. This involves creating ground models, or blueprints, that describe the desired application of the programs. These models must be linked to the code in a traceable and checkable way, and the relevant properties must be refined in a stepwise manner. The Abstract State Machines (ASM) method is a reliable system development discipline that can bridge the gap between informal requirements and executable code by combining application-centric modeling with mathematically verifiable detailing. This allows for compile-time verification of the code.
411047	41104735	A logic programming approach to knowledge-state planning: Semantics and complexity	The article introduces a new planning language called K, which is based on logic programming principles. It allows for describing transitions between states of knowledge rather than fully described states of the world, making it suitable for planning under incomplete knowledge. K also supports default principles through negation as failure. It is flexible, as it can also represent transitions between states of complete knowledge. This allows for natural and compact problem representation. The article provides a thorough analysis of K's computational complexity, showing that it can handle various planning problems, including standard and secure planning. These findings serve as the basis for the DLVk system, which implements K using the DLV logic programming system. 
411048	41104814	Compressed Sensing of Approximately-Sparse Signals: Phase Transitions and Optimal Reconstruction	Compressed sensing is a technique used to measure signals that are sparse, meaning they only have a small number of significant components. However, most signals are only approximately sparse, meaning that while they have a small number of relevant components, the others are not exactly zero but close to it. In this study, the researchers use a Gaussian distribution to model these approximately sparse signals and examine their compressed sensing with dense random matrices. They use replica calculations to determine the optimal reconstruction error for these signals and analyze the performance of the G-AMP algorithm. They also propose a special "seeding" design for measurement matrices to improve the algorithm's optimality.
411049	41104941	Componentising a Scientific Application for the Grid	Grid applications are complex and managing this complexity can be a challenge. A promising solution is component-based development, which is gaining popularity in the grid community. To evaluate its effectiveness, a case study was conducted on reengineering a high performance numerical solver into a component-based grid application. The chosen component model is a specialized version of the Fractal model designed for grid environments. The results of the study show that componentization has improved the application's ability to be modified and reused without sacrificing performance. This provides evidence that component-based development can be a successful approach for building and evolving grid applications. 
411050	411050175	Traceback-Based Optimizations for Maximum a Posteriori Decoding Algorithms	MAP decoding is an important tool for turbo coding and other advanced feedback-based algorithms. However, in order to use these techniques in resource-constrained systems, it is necessary to limit the complexity of their implementation while maintaining their superior performance. One way to achieve this is by incorporating traceback information into the MAP algorithm, which can simplify the computational requirements. This approach also offers the potential for new architectural variants for the decoder, each with its own advantages depending on the memory hardware and number of trellis states. These enhancements can significantly reduce computational complexity without sacrificing performance. 
411051	41105124	Bounds on Redundancy in Constrained Delay Arithmetic Coding	In this paper, the authors discuss the issue of a finite delay constraint in an arithmetic coding system. This constraint can lead to significant delays in encoding or decoding due to the nature of the coding process. To overcome this problem, the authors propose a method of inserting fictitious symbols, which results in a redundancy in the coding rate. They derive an upper bound for this redundancy and show that it decreases exponentially as the delay constraint increases. This makes their method more effective than existing block to variable methods. The authors also demonstrate the practical application of their results in compressing English text.
411052	41105232	Using the mobile application EDDY for gathering user information in the requirement analysis	The success of product design relies heavily on understanding the knowledge and requirements of the target users. Many user-centered design studies have been conducted over the years and can be used as a resource by developers. This paper introduces a framework called EDDY, which aims to aid in the development of mobile applications for data collection. These applications can be useful for studies that involve users collecting information themselves, such as Cultural Probes and Experience Sampling Method. The paper evaluates the benefits of using EDDY for these studies and compares it to traditional methods.
411053	41105376	Dynamic Bayesian network based interest estimation for visual attentive presentation agents	The paper discusses a user study that was conducted to compare two methods of estimating users' interest in a multimodal presentation based on their eye gaze. The study took place in a virtual showroom where 3D agents presented product items and adapted their performance according to users' attentiveness. The system analyzed eye movements in real-time to determine users' attention and visual interest towards interface objects. Previous research used algorithms based on the time spent looking at an object, but this was not effective for dynamic presentations. The paper proposes using dynamic Bayesian networks to consider the context of the object of interest, resulting in more timely and appropriate responses from the presentation agents. The benefits of this approach are demonstrated theoretically and empirically.
411054	41105415	Performability assessment by model checking of Markov reward models	This paper introduces efficient methods for model checking Markov reward models, particularly for evaluating the performability of computer-communication systems. The authors propose the use of the logic CSRL for specifying performability measures, which offers flexibility and allows for numerical evaluation of various measures. The use of CSRL also helps reduce the size of Markov reward models that need to be analyzed. The paper presents background information on both Markov reward models and CSRL, including their syntax and semantics. It also discusses a duality result between reward and time. The paper then presents five numerical algorithms for verifying time- and reward-bounded until-properties, a key operator in CSRL. A case study is provided to demonstrate the versatility of this approach. 
411055	411055129	Efficient on-the-fly algorithms for the analysis of timed games	The paper introduces a new efficient on-the-fly algorithm for solving timed game automata with a focus on reachability and safety properties. It is a symbolic extension of a previous algorithm for model-checking finite-state systems. The algorithm can terminate before fully exploring the state-space and utilizes zones as a data structure for efficient steps. The paper also suggests optimizations and methods for obtaining time-optimal winning strategies for reachability games. The algorithm is evaluated through experiments, showing promising performance results.
411056	41105647	Learning to Map between Ontologies on the Semantic Web	Ontologies are essential for the success of the Semantic Web, as they allow for the publication of machine understandable data. However, with the distributed nature of the Semantic Web, data will come from various ontologies, making information processing across them challenging without knowing the semantic mappings between elements. Manually finding these mappings is not feasible, and thus, the development of tools to assist in the ontology mapping process is crucial. Glue is one such system that uses machine learning techniques to find mappings between two ontologies. It employs multiple learning strategies and incorporates commonsense knowledge and domain constraints to improve matching accuracy. Experiments have shown that glue proposes highly accurate semantic mappings in various real-world domains.
411057	41105733	A-Ordered Tableaux	The use of A-orderings of literals in resolution proof procedures has been extensively studied, but in tableau proof procedures it has only recently been introduced by the authors of this paper. The paper presents a completeness proof for A-ordered ground clause tableaux, which is easier to follow than previous proofs. This technique is then extended to the non-clausal and non-ground cases, and an ordered version of Hintikka sets is introduced. It is shown that regular A-ordered tableaux are a proof confluent refinement, but when combined with connection refinements, they become an incomplete proof procedure. The paper also introduces regular A-ordered first-order NNF tableaux and discusses their completeness and implementation.
411058	41105861	Conditions for Resolving Observability Problems in Distributed Testing	This paper discusses the issues of controllability and observability that can arise when using a test architecture with multiple remote testers. These problems often require external coordination messages to be used during testing. The goal is to create a test or checking sequence that is free from these problems without relying on external coordination. The paper explores conditions that make it possible to construct such a sequence and provides procedures for creating subsequences that eliminate the need for external coordination messages. This allows for more efficient and effective testing of systems that meet these conditions.
411059	4110593	Cross-Modal Supervision For Learning Active Speaker Detection In Video	This paper proposes a method for using audio to supervise the learning of active speaker detection in videos. The Voice Activity Detection (VAD) technique is used to guide the learning of a vision-based classifier in a weakly supervised manner. The classifier utilizes spatio-temporal features to capture upper body motion related to speaking, such as facial expressions and hand gestures. The authors also introduce a person-specific model to improve the generic model for active speaker detection. Furthermore, the paper demonstrates the online adaptation of the generic model to previously unseen speakers using audio (VAD) for weak supervision. This approach overcomes the lack of clean training data by leveraging temporal continuity. Overall, this is the first system to learn and automatically adapt to new speakers in a different dataset using audio-visual data. The work showcases the potential of multi-modal data for unsupervised learning by transferring knowledge from one modality to another.
411060	41106027	The tradeoffs of fused memory hierarchies in heterogeneous computing architectures	The rise of general purpose computing on graphics processing units (GPGPU) has led to a convergence of consumer and high-performance computing markets. Many top-ranked HPC systems now include GPU accelerators, but previous connections between the CPU and GPU through the PCIe bus have limited scalability. A new trend towards integrating the CPU and GPU has removed this bottleneck and created a unified memory hierarchy. This trend is examined through AMD's Fusion Accelerated Processing Unit (APU) as a testbed, comparing its performance, power consumption, and programmability to discrete GPUs. The unified memory hierarchy offers potential tradeoffs for high-performance scientific computing.
411061	41106172	Combining Multi-robot Exploration and Rendezvous	This study focuses on the problem of exploring an unknown environment using two mobile robots. The objective is to achieve a quick rendezvous between the robots while maximizing their speed in exploring the environment. The main challenge is to minimize the reliance on communication for the rendezvous. This involves identifying unique potential rendezvous locations, ranking them based on their uniqueness, and synchronizing with the other robot to meet at one of these locations at a specific time. These tasks must be performed simultaneously while exploring and mapping the environment. The proposed approach combines the exploration and rendezvous tasks by considering the cost and uniqueness of potential rendezvous locations. Results show that this approach improves the efficiency of the joint tasks compared to using uniqueness alone.
411062	4110625	Effort Games and the Price of Myopia	Effort Games are a game-theoretic model used to study cooperation in open environments. It is based on the principal-agent problem from economic theory, where a central authority (the principal) tries to incentivize agents to exert effort towards completing a common project. The probability of completing a task is higher when effort is exerted, but there is a cost for the agent. This domain can be modeled as a normal form game, with payoffs based on probabilities of tasks and a boolean function defining successful completion. The Price of Myopia is proposed as a measure of the influence of rationality on the principal's minimal payments. It is computationally complex to test dominant strategies and find reward strategies, and these problems are at least as hard as calculating the Banzhaf power index. However, in certain restricted domains, these problems can be solved in polynomial time. The article also provides simulation results for specific types of effort games.
411063	4110632	On worst-case allocations in the presence of indivisible goods	In this study, we examine a problem of fairly allocating a set of goods to a group of n agents. While it is known that proportional allocations are possible in the case of infinitely divisible goods, this is not always the case with indivisible goods. We focus on the algorithmic and mechanism design aspects of this problem, and identify a lower bound for the value that each agent can receive. We also develop a polynomial time algorithm for finding such allocations and explore the design of truthful mechanisms. Our findings show that while a deterministic mechanism cannot achieve a truthful $\frac{2}{3}$ -approximation, a simple algorithm can achieve a constant approximation when the number of goods is limited. We also provide a negative result for randomized mechanisms under certain conditions. 
411064	41106425	A Novel Cryptographic Algorithm Based on Iris Feature	Biometric cryptography is a method that uses biometric characteristics to encrypt data, increasing its security and addressing the limitations of traditional cryptography. A new algorithm is proposed in this paper, using the iris as the biometric feature due to its high accuracy. The algorithm extracts a 256-dimensional textural feature vector from the iris image using 2-D Gabor filters. The data is then encrypted and decrypted using add/subtraction operations and the Reed-Solomon error-correcting algorithm. Experimental results prove the effectiveness of this system.
411065	411065177	Shared Linear Encoder-Based Multikernel Gaussian Process Latent Variable Model for Visual Classification	This paper introduces a new multiview learning method based on the Gaussian process latent variable model (GPLVM). Unlike previous GPLVM methods, this approach considers a back constraint and uses a linear projection to map multiple observations to a consistent subspace before projecting them onto the latent variable space with a Gaussian process prior. Additionally, a multikernel strategy is used to design the covariance matrix, making it more adaptive for data representation. To improve classification, a discriminative prior is also incorporated into the learned latent variables. Experimental results on three real-world databases demonstrate the effectiveness and superiority of this method over existing approaches.
411066	41106661	Very deep feature extraction and fusion for arrhythmias detection.	The electrocardiogram (ECG) is a widely used tool for diagnosing diseases related to abnormal heart rhythm. However, detecting these abnormalities is challenging due to the complexity and noise of ECG signals. To address this, researchers propose a deep convolutional neural network (VDCNN) using small filters to reduce noise and improve performance. They also introduce multi-canonical correlation analysis (MCCA) and the Q-Gaussian multi-class support vector machine (QG-MSVM) for better feature learning and classification. Results show that this approach outperforms other methods in accurately differentiating between normal and abnormal heartbeats without any noise filtering or pre-processing.
411067	41106750	Super-recursive Algorithms and Modes of Computation	In contemporary computer science, computers and computer networks can function in two modes: functional recursive mode and functional super-recursive mode. Some researchers argue that interactive computation is more powerful than Turing machines, while others believe that the Church-Turing Thesis still holds. The disagreement is due to the fact that traditional computability theory does not consider real world factors such as time and space. However, it has been proven that even a finite system of interacting recursive automata or algorithms can achieve super-recursive power in real world conditions. This paper explores the importance of considering modes of information processing in the design of efficient distributed hardware and software systems. 
411068	41106817	Automatic protocol reverse-engineering: Message format extraction and field semantics inference	Understanding the command-and-control (C&C) protocol used by a botnet is crucial for identifying potential malicious activity. However, these protocols are often undocumented, making it difficult to defend against botnets. Automatic protocol reverse-engineering techniques allow for a better understanding of these protocols and are essential for security applications. This approach involves analyzing the program that implements the protocol and extracting accurate and complete information, even for encrypted protocols. The proposed approach, called Dispatcher, was used to analyze the previously undocumented C&C protocol of MegaD, a major spam botnet responsible for a significant portion of Internet spam. 
411069	41106929	Techniques for Edge Stratification of Complex Graph Drawings	The authors of this paper propose a method for exploring graph layouts in order to reduce visual complexity and improve clarity. This approach involves dividing the layout into layers with desired properties, using heuristics. These layers can then be combined and explored by the user to gradually reveal more details. A user study was conducted to test the effectiveness of this approach, and an experimental analysis was performed on popular graph drawing algorithms to evaluate the number of layers and their correlation to the number of crossings in a graph layout. The authors found that their approach was useful for exploring graph layouts, and suggest that the number of layers may be a reliable measure of visual complexity. However, the method may not be efficient for larger and more complex layouts, and further research is needed to extend and improve this approach. 
411070	41107031	Integer-grid maps for reliable quad meshing	Quadrilateral remeshing is a process used to improve the quality of a mesh, particularly in computer graphics, by transforming irregularly shaped polygons into regular quadrilaterals. This is achieved through global parametrization, which allows for explicit control over irregular vertices and smooth distribution of distortion. However, current techniques are not reliable when applied to real-world input data, often resulting in non-injectivities and therefore unusable quadrilateral meshes. In this paper, a new convex Mixed-Integer Quadratic Programming (MIQP) formulation is proposed to ensure the resulting map is within the class of Integer-Grid Maps, guaranteeing a quad mesh. To overcome the computational challenges, two additional optimizations are proposed: a complexity reduction algorithm and singularity separating conditions. These improvements result in a more reliable and accurate remeshing process, allowing for the global search of high-quality coarse quad layouts. 
411071	41107155	Animation of deformable models using implicit surfaces	This paper discusses a method for creating and animating complex deformable models using implicit surfaces. These surfaces act as an additional layer around moving and deforming structures, providing a smooth definition of the object's surface and efficient collision detection. The implicit layer deforms to create accurate contact surfaces between colliding objects, and a physically-based model is used to calculate collision responses. This approach also allows for easier control of an object's volume through the use of local controllers. Two applications are presented to demonstrate the effectiveness of this technique: animating characters with implicit flesh and modeling soft, inelastic substances with constant volume during animation. 
411072	41107257	Multivalued mappings, fixed-point theorems and disjunctive databases	This paper discusses the meaning of disjunctive programs and databases and how they lead to multivalued mappings and fixed points. Various fixed-point theorems for these mappings are explored, some of which are already familiar and some of which are new. The concept of a normal derivative of a disjunctive program is introduced, which is a type of logic program that can be determined by the disjunctive program. This makes it easier to find fixed points using established methods. The paper also explains how fixed points of multivalued mappings are related to fixed points of single-step operators from normal derivatives, which can simplify the construction of models for disjunctive databases. The paper also presents a collection of known results on fixed points of single-valued mappings for reference. The paper concludes by discussing potential challenges and future research topics related to this work.
411073	41107362	Continuous and Parallel LiDAR Point-Cloud Clustering	In the world of the Internet of Things, the need for analyzing large amounts of data generated by high-rate sensors at the edge of infrastructures is crucial. One example is LiDAR technology, which can detect objects with high precision in large areas. This data, known as point clouds, can support automated functions in distributed systems. The problem of clustering point clouds is key in extracting useful information from this data. A proposed solution is Lisco, which is a continuous clustering algorithm that can be parallelized for efficient processing. Its parallel version, P-Lisco, can be used on different computing architectures and has shown to be more efficient and scalable compared to traditional methods in experimental evaluations. 
411074	41107416	Accelerating Multiagent Reinforcement Learning by Equilibrium Transfer.	Equilibrium-based multiagent reinforcement learning (MARL) is an important approach that uses equilibrium solution concepts from game theory to guide agents' decision-making. However, existing algorithms for this approach have difficulty scaling due to the high computational cost of finding equilibria. This paper introduces the concept of "equilibrium transfer," where previously computed equilibria are reused when agents have a low incentive to deviate from them. This leads to a new framework called equilibrium transfer-based MARL, which is shown to significantly accelerate learning (up to 96.7% reduction in time) and achieve higher rewards compared to other algorithms. This framework also scales well as the state and action space grows and the number of agents increases. 
411075	411075188	The effectiveness of task-level parallelism for high-level vision	 The paper discusses the issue of slow execution in large production systems, also known as rule-based systems, and how this limits their usefulness in both practical applications and research. 2. Previous attempts to speed up these systems have focused on match parallelism, but it has been found that this only provides limited speed-ups. 3. The paper suggests that task-level parallelism, which involves breaking down the system into high-level tasks, could provide much larger speed-ups when combined with match parallelism. 4. The authors use a mature research system called SPAM to investigate task-level parallelism and report promising speed-ups of over 12 fold using 14 processors. 5. They also discuss their methodology for selecting and applying task-level parallelism and the potential benefits of using shared virtual memory in this implementation. 6. Task-level parallelism has not been well-studied in the literature, but the authors believe it could be a valuable tool for improving the performance of large-scale production systems.
411076	41107629	Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and Blind Unmixing of Hyperspectral Images.	The process of hyperspectral unmixing involves estimating the number of endmembers present in a scene, which is crucial for accurately extracting their spectral signatures and determining the abundance fractions of pixels. This is typically done in two separate steps, but a new approach presented in this paper combines the two tasks into one using a multiple constrained optimization framework. This is achieved through a low-rank and sparse non-negative matrix factorization method, where a l(1)/l(2) norm penalty term is used to promote low-rankness. The proposed approach is validated through experiments with simulated and real data, showing its effectiveness in accurately estimating the number of endmembers.
411077	41107738	Sparsity Pattern Recovery Using Fri Methods	The search for a sparse representation of signals has been a popular topic in recent years. Various methods have been developed to solve this problem, including relaxing a non-convex optimization problem and applying the finite rate of innovation (FRI) theory, which uses algebraic techniques based on Prony's algorithm. Recent advancements in this theory have shown the potential to recover sparse representations beyond the previously established uniqueness limits. This paper focuses on applying these methods to signals that are sparse in the union of Fourier and canonical bases, specifically in the case of the union of DCT and Haar basis. An extension is proposed that takes advantage of the even symmetry of cosine functions and operates on the observed vector in a dual domain. The performance of this new approach is compared to other state-of-the-art algorithms through simulations, showing its superiority in various scenarios.
411078	4110781	Automatic synthesis of filters to discard buffer overflow attacks: a step towards realizing self-healing systems	Buffer overflows are a common target for network-based attacks and are often used by worms to spread. While some techniques, such as StackGuard, have been developed to protect servers from these attacks, they can cause the server to crash and make its service unavailable. However, a new approach has been developed that learns the characteristics of attack inputs and filters them out in the future without changing the server code. This allows for faster recovery from attacks without the need for server restarts. Testing has shown that this approach is effective in generating accurate filters for most buffer overflow attacks.
411079	41107914	Capti-speak: a speech-enabled web screen reader	 Capti-Speak is a speech-enabled screen reader designed to alleviate the frustrations of web browsing for people with vision impairments. It uses a custom dialog model to interpret speech utterances and translates them into browsing actions, providing audio feedback. In a user study with 20 blind subjects, Capti-Speak was found to be significantly more usable and efficient compared to regular screen readers, particularly for ad-hoc browsing, searching, and navigating to desired content. This shows the potential of augmenting screen readers with speech input interfaces to improve the browsing experience for those with limited keyboard shortcut vocabulary or unfamiliarity with webpage structures.
411080	41108052	"Press Space to Fire": Automatic Video Game Tutorial Generation.	The concept of tutorial generation for games is presented as an AI problem. This involves creating tutorials that can effectively teach players how to play games. Different approaches to tackling this problem include generating written explanations of game rules, designing helpful game levels, and producing demonstrations of gameplay using agents that mimic human behavior. The General Video Game AI framework is proposed as a valuable tool for addressing this issue.
411081	41108113	Feature analysis for modeling game content quality.	This paper explores the potential for automatic game content generation to enhance player entertainment by tailoring the player experience in real-time. The authors analyze the relationship between level design parameters in platform games and player experience, using a neuroevolutionary preference learning method to predict players' preferences and emotions based on game content. They conduct experiments on a modified version of Super Mario Bros, using statistical parameters and frequent sequences of level elements as features. The results suggest that a smaller feature window decreases prediction accuracy, and that models built on a combination of features outperform those using partial information. This research highlights the importance of understanding the relationship between game content and player experience for effective game personalization.
411082	41108256	RORI-based countermeasure selection using the OrBAC formalism	As threats against information systems become more advanced, it is becoming increasingly difficult for security administrators to detect and respond to attacks. Implementing strong security policies is an effective way to protect systems, but it requires expertise and knowledge. While strong policies can serve as powerful countermeasures, inappropriate policies can have disastrous consequences for an organization. To combat complex attacks, current research suggests using multiple countermeasures, but the methodology is often unclear or difficult to implement. This paper presents a structured approach for evaluating and selecting optimal countermeasures based on the return on response investment (RORI) index. A real case study of a mobile money transfer service is used to demonstrate the applicability of the model, and the service, security policies, and countermeasures are expressed using the OrBAC formalism.
411083	4110832	Maximum rate single-phase clocking of a closed pipeline including wave pipelining, stoppability, and startability	Aggressive design techniques, such as using level-sensitive latches and wave pipelining, are being utilized to improve the performance of digital systems. However, the optimal clocking problem for these designs is challenging to solve due to its nonconvex solution space. Current algorithms use linear programs to solve a simpler case, but a new algorithm called Gpipe has been developed to efficiently determine the maximum single-phase clocking rate for a closed pipeline with a specified degree of wave pipelining. Additionally, a method for introducing or increasing wave pipelining while maintaining the clock rate has been discovered, as well as techniques for stopping and restarting the pipeline under stall conditions without losing data or affecting testing capabilities. 
411084	41108430	Specifying User Interfaces for Runtime Modal Independent Migration	The evolution of computing systems has shifted from a procedural approach to a problem-oriented one. In the future, computer usage will revolve around specific services rather than platforms or applications. These services should be accessible through a uniform interface, independent of technology. This paper proposes a framework for runtime migration of user interfaces by using general interface descriptions in XML and converting them with XSLT. This will allow for easy extension of services to different devices and modalities. A proof of concept for this approach is demonstrated through the conversion of a joystick in a 3D virtual environment to a 2D dialog-based user interface.
411085	41108590	Scalable multimedia content analysis on parallel platforms using python	In today's world, there is a high demand for web-scalable solutions for analyzing multimedia content due to the dominance of consumer-produced media. One approach to achieving scalability is through mapping application computation onto parallel platforms. However, this poses challenges such as increased code complexity, limited portability and required low-level knowledge of hardware. To address this, a Python-based framework called PyCASP has been developed, which automatically maps computation onto various parallel platforms, making it easier to develop and scale applications. PyCASP follows a systematic, pattern-oriented approach and allows for easy prototyping and efficient performance. Illustrations of this framework are provided through three different multimedia content analysis applications, showcasing its automatic portability and scalability while allowing for high-level language prototyping and efficient low-level code performance.
411086	41108631	Toward a deeper understanding of the relationship between interaction constraints and visual isomorphs	Interaction and manual manipulation are important for problem solving according to cognitive science literature. Different types of interactions or constraints on interactions can make a problem seem easier or harder. However, the visual analytics community has not fully utilized this knowledge for analytical problem solving. This paper proposes that constraints on interactions and visual representations can affect the effectiveness of problem solving strategies. A user study was conducted using a mathematic game called number scrabble to test this hypothesis. The game has an optimal visual isomorph and the study aimed to see if participants could find it and analyze their strategies. Results showed that interaction constraints do impact problem solving and certain constraints can increase the chance of finding the optimal isomorph. 
411087	41108714	An Acoustic Identification Scheme for Location Systems	Pervasive computing applications often require location awareness to effectively integrate technology into daily life. This is achieved through a location system using sensors to determine a user's position and provide relevant information. In this paper, an acoustic-based location system is proposed, using a set of microphones connected to a central server. Mobile users can produce acoustic signals through standard speakers on their devices to locate themselves through the system. The design focuses on using multi-frequency symbols for robust and unique identification of users. Experiments were conducted to test the system's ability to recognize and decode signals from different users in the same area at various distances.
411088	4110884	Investigating gesture and pressure interaction with a 3d display	This article explores the use of a mobile device as a multifunctional input and output for a stereoscopic 3D television display. The combination of gestural and haptic input (touch and pressure) is utilized to navigate multimedia and TV content, with visual feedback enhancing the user experience. A user evaluation was conducted to compare these prototypes with traditional devices for multimedia interaction, revealing the benefits and providing design guidelines. This approach allows for efficient and engaging interaction with complex information spaces, making use of both the mobile device and the 3D TV display.
411089	411089195	Text segmentation via topic modeling: an analytical study	This paper discusses a new approach to text segmentation using the latent Dirichlet allocation (LDA) topic model. This method not only identifies segment boundaries, but also provides information on the topic distribution within each segment. This can be useful for tasks such as segment retrieval and discourse analysis. The proposed approach performs better than a standard baseline method and outperforms most unsupervised methods on a benchmark dataset. This highlights the potential of using topic modeling for text segmentation.
411090	41109020	Mapping Simple Polygons: The Power of Telling Convex from Reflex	The exploration of a simple polygon by a robot is studied using a visibility graph. This graph has a vertex for each vertex of the polygon, and an edge between two vertices if they can see each other. The robot is capable of ordering the vertices it sees and determining whether the angle between them is convex or reflex. An upper bound on the number of vertices is known. The general result is that a robot can always determine the base graph of a locally oriented, arc-labeled graph. Combining this with other techniques, it can reconstruct the visibility graph of the polygon. Multiple identical robots can also solve the weak rendezvous problem by positioning themselves to mutually see each other. 
411091	4110916	Algorithms and data structures for flash memories	Flash memory is a type of memory that can store data even when the power is turned off. It is commonly used in handheld devices such as phones and cameras because it is compact and can hold a lot of information. However, it has some limitations - it can only be erased in large blocks and can only be erased a limited number of times. This means that special techniques and structures are needed to effectively use flash memory. These techniques help to update data efficiently, reduce the number of erasures, and evenly distribute wear on the memory blocks. This survey discusses these techniques, many of which have only been described in patents until now.
411092	41109248	Enhancing interactive web applications in hybrid networks	There are multiple options for mobile internet users, including high bandwidth cellular data services and WiFi. However, WiFi can be intermittent, making it difficult for vehicles to support interactive applications like web search. The Thedu system was created to address this issue by using aggressive prefetching and m2m transfers to transform web search into a one-shot request/response process. A prototype was deployed on the DieselNet testbed in Amherst, MA and showed that Thedu can deliver four times as many relevant web pages with a mean delay of 2.3 minutes in areas with high AP density. However, in rural areas with sparsely deployed APs, m2m routing may improve the number of relevant responses delivered, but with a significantly higher mean delay of 6.7 minutes. 
411093	41109364	Global Optimization for Value Function Approximation	Value function approximation methods have been widely used in various applications, but they often lack error bounds. In this paper, a new approximate bilinear programming formulation is proposed for value function approximation, using global optimization. This formulation guarantees strong a priori guarantees on both robust and expected policy loss by minimizing specific norms of the Bellman residual. While solving a bilinear program optimally is NP-hard, the Bellman-residual minimization itself is also NP-hard, making it unavoidable. The paper presents optimal and approximate algorithms for solving bilinear programs, which offer a convergent generalization of approximate policy iteration. The behavior of these algorithms under incomplete samples is also briefly analyzed. The effectiveness of the proposed approach is demonstrated through consistent minimization of the Bellman residual on simple benchmark problems.
411094	41109491	Efficient Algorithm for Modified Local Polynomial Time Frequency Transform	This paper introduces new algorithms for analyzing non-stationary signals with multiple components. The algorithms use a modified local polynomial time frequency transform and divide the signal into segments to estimate parameters. This approach reduces the computational complexity by minimizing the overlap between segments. The use of adaptive window lengths also improves the time-frequency resolution for each component. These new algorithms are more efficient than previously reported methods and can accurately analyze non-stationary signals with multiple components. 
411095	411095117	A New Courseware Diagram for Quantitative Measurement of Distance Learning Courses	Web-based distance learning offers flexibility for students, but lacks interaction between students and the instructor. To address this issue, a systematic assessment mechanism is proposed to enhance interaction and improve student learning performance. This mechanism utilizes courseware diagrams, which combine tools from conceptual mapping and influence diagrams. The courseware diagram has two main components: a course flow chart that can be systematically built to provide remedial courses for students based on their performance, and the ability to adjust course content based on students' learning performance. This user-friendly mechanism allows for prompt feedback from students and can be easily used by both instructors and students.
411096	41109681	Audio-augmented paper for the therapy of low-functioning autism children	This paper introduces a prototype of audio-augmented paper for use in therapy for low-functioning autism children. The prototype allows therapists to record audio on regular paper using tangible tools, making it easy to share between therapist and child. The prototype is specifically designed to help therapists engage children in storytelling activities. An initial pilot study was conducted to test the effectiveness of the prototype.
411097	41109753	Cooperative navigation in robotic swarms.	This study focuses on cooperative navigation for robotic swarms in an event-servicing scenario where robots need to service events at specific locations. The goal is to develop a system where robots can inform each other about events and guide each other to event locations using delay-tolerant wireless communications. This approach takes advantage of the swarm's redundancy, distribution, and mobility. The algorithm is tested in two scenarios, both in simulation and on real robots: a single searching robot finding a target while others perform their own tasks, and collective navigation where all robots navigate between two targets. The results show that this algorithm allows for efficient navigation and the emergence of a robust dynamic structure within the swarm.
411098	41109855	Insider Attack Identification and Prevention Using a Declarative Approach	A process is a series of steps that use data to achieve a specific goal. Insiders who have access to data and annotations can carry out attacks on the privacy and security of the process. These attacks can be difficult to identify as they are hidden among non-malicious steps. Process and attack models are defined as directed graphs based on data flow. An attack is successful if it satisfies certain conditions and has a similarity match with the process model. A declarative approach is proposed for vulnerability analysis, using logic rules to define valid attacks. Possible ways for agents to carry out these attacks are generated, and improvement opportunities are identified and exploited to eliminate vulnerabilities. The improved process is then evaluated until all possible ways of carrying out the attack are thwarted. 
411099	41109931	An approach to adapt service requests to actual service interfaces	The development of service oriented architectures has led to the creation of frameworks that allow for self-adaptive service compositions with dynamic binding. This means that a developer can specify an abstract service at design time and a concrete implementation will be selected at run time. However, in an open world setting, where services are provided by different organizations, there may be mismatches between the interfaces or protocols of services. To address this issue, researchers have identified possible mismatches and developed basic mapping functions that can be used to solve them. These mapping functions can be combined into scripts which can be executed by a mediator to adapt the operation request and ensure communication between services with different interfaces or protocols. The process of creating these scripts can also be partially automated.
411100	41110015	Load-balancing and caching for collection selection architectures	Modern Web search engines are facing the challenge of dealing with the rapid growth of the Internet. To handle this, they have adopted distributed organizations, where documents are distributed among servers and queries are answered in a parallel and distributed manner. One way to reduce the computing load in this setup is through the use of collection selection, which balances the quality of results with the cost of solving queries. This paper explores the relationship between collection selection, load balancing, and caching in a distributed search engine. It proposes a load-driven collection selection strategy and a novel caching policy that improves the effectiveness of results. The combination of these strategies results in a system that can retrieve two thirds of the top-ranked results with only one fifth of the computing workload compared to a baseline centralized index.
411101	41110135	Using a Natural Language Understanding System to Generate Semantic Web Content	The article discusses the research and development of a system called OntoSem, which automatically creates detailed semantic annotations for text and makes it available on the Semantic Web. This system has been under development for over fifteen years and uses a special ontology and lexicon to translate English text into a custom knowledge representation language. The authors faced challenges in adapting OntoSem for the Semantic Web, but were able to develop a translation system, OntoSem2OWL, which converts the text meaning representations into the Semantic Web language OWL. They also used OntoSem and OntoSem2OWL to create a web service called SemNews, which processes news summaries and publishes structured representations of their meaning.
411102	41110227	Experimental Demonstration of a Hybrid Privacy-Preserving Recommender System	Recommender systems help merchants suggest products to customers based on their preferences. However, these systems often have privacy vulnerabilities. The ALAMBIC framework was created to protect customer privacy and merchant interests. It is a hybrid recommender system that uses different techniques to make recommendations. One unique aspect is that customer data is divided between the merchant and a third party, preventing either from accessing sensitive information. Experimental results show that this system can still perform well and be user-friendly. User testing also reveals positive reactions to the privacy measures in place.
411103	41110346	Decision knowledge triggers in continuous software engineering.	Decision knowledge is an important aspect of software development, which includes information about the decisions made, the problems they address, and the rationale behind them. However, it is often not integrated into the development process due to the additional effort required and the lack of perceived short-term benefits. Continuous software engineering provides a solution to this problem by incorporating decision knowledge management into daily practices such as code commits and task management. This paper proposes ways to encourage developers to capture and utilize decision knowledge during these practices, specifically by packaging distributed knowledge, making tacit decisions explicit, and ensuring consistency between decisions.
411104	41110450	Texture-based visualization of unsteady 3D flow by real-time advection and volumetric illumination.	 This paper presents a method for visualizing unsteady 3D flow using dense textures, with a focus on computational efficiency and visual perception. The technique utilizes a 3D graphics processing unit (GPU) to efficiently handle logical 3D grid structures through 2D textures. The final display is achieved through slice-based direct volume rendering, with two options for volumetric illumination (gradient-based and line-based). Different perception-guided rendering methods are also considered to address issues of clutter and occlusion. Performance measurements and results are discussed to showcase the effectiveness of this approach. 
411105	41110528	Dynamic logic for plan revision in intelligent agents	This paper introduces a dynamic logic specifically designed for a version of the agent programming language 3APL that operates on propositions. 3APL agents have both beliefs and plans, and executing a plan can alter an agent's beliefs. The unique feature of 3APL agents is their ability to revise plans during execution, making it difficult to analyze them using traditional methods like structural induction. Therefore, the authors propose a dynamic logic that takes into account the plan revision aspect of 3APL agents. They also provide a complete and sound set of axioms for this logic. 
411106	41110655	Interval-valued fuzzy strong S-subsethood measures, interval-entropy and P-interval-entropy.	The paper introduces a new concept called fuzzy interval-entropy, which provides a value within the closed subinterval of [0, 1]. This concept is used to define interval-valued fuzzy strong S-subsethood measures, which are then used to build fuzzy interval-entropies. To ensure that the results are intervals, the paper utilizes total orders for comparison. The concept of P-interval-entropy is also introduced to address the problem of finding equilibrium points for interval-valued negations with respect to total orders. An example is provided to illustrate the application of these concepts and the results are compared to those obtained using classical interval-valued entropy, which provides a real number instead of an interval.
411107	41110722	Sonar Sensor-Based Efficient Exploration Method Using Sonar Salient Features and Several Gains	The paper presents a new method for exploring unknown environments using sonar sensors. It proposes a "sonar salient feature" (SS-feature) to accurately map the environment by extracting circle features from salient convex objects. The SS-feature is incorporated into an extended Kalman filter-based simultaneous localization and mapping (SLAM) framework. To efficiently explore the environment, a strategy is used that considers driving cost, expected information, and localization quality. This reduces unnecessary exploration and leads to more accurate pose estimation. The method was tested in various experiments and was found to be effective in building accurate maps autonomously with sonar sensors in different home environments.
411108	411108115	Crawling the infinite web	Web pages that are generated dynamically upon request and contain links to other dynamically generated pages pose a problem for search engine crawlers due to the limited resources available for indexing. To address this issue, probabilistic models for user browsing in "infinite" Web sites have been proposed and studied. These models aim to predict how deep users go while exploring Web sites, and can be used to estimate how deep a crawler must go to download a significant portion of the visited content. Real data on page views in several Web sites show that, in theory and practice, a crawler only needs to go a few levels, about 3 to 5 clicks away from the start page, to reach 90% of the visited pages.
411109	41110941	Topic-driven multi-type citation network analysis	In this paper, the authors discuss the use of automated citation analysis in ranking authors in scientific fields. While previous methods have used content-based or citation network link analyses, this paper presents a novel approach that combines both methods with topical link analyses. This integrated probabilistic model considers citations among papers, authors, affiliations, and publishing venues in a single model. The application of Topical PageRank is also introduced to account for researchers' expertise in different domains. The authors also explore the impact of weighting various factors in a heterogenous link analysis of the citation network. Experimental results show that the multi-type citation graph and the use of Topical PageRank can improve performance, and heterogenous link analysis with parameter tuning performs even better.
411110	411110120	The effectiveness of automatic text summarization in mobile learning contexts	Mobile learning is a popular method of learning that utilizes the advantages of mobile devices and technology to allow learners to access information anytime and anywhere. However, it also comes with its own set of challenges, particularly in delivering and processing learning content. To address this, a study was conducted to explore the use of automatic text summarization as a tool for reducing the amount of textual content in mobile learning. While text summarization can condense the most important ideas, it may also affect the overall meaning of the content. The study focused on the effectiveness of automatic text summarization in mobile learning, and the results showed that it can effectively generate helpful summaries. This highlights the importance of properly summarizing learning content to accommodate the unique features of mobile devices. 
411111	41111175	Centralized Adaptive Routing for NoCs	As the number of applications and programmable units in CMPs and MPSoCs increases, the Network-on-Chip (NoC) has to handle diverse and time-dependent traffic loads. To address this, NoC load-balanced, adaptive routing mechanisms have been introduced, which are more efficient than traditional oblivious routing schemes for hardware implementations. However, current adaptive routing schemes rely on local or regional congestion signals rather than the global state of the system. This paper presents a novel paradigm of NoC centralized adaptive routing, specifically designed for mesh topology. This approach continuously monitors the global traffic load and adapts routing decisions to improve load balancing. The implementation is scalable and lightweight, outperforming distributed adaptive routing schemes in terms of load balancing and throughput.
411112	41111256	WCET analysis with MRU cache: Challenging LRU for predictability	The article discusses cache analysis for calculating Worst-Case Execution Time (WCET) and how most previous work has focused on the LRU replacement policy. However, commercial processors often use non-LRU policies as they are more efficient and still perform well. The article specifically looks at the MRU policy used in Intel Nehalem processors and how it has been underestimated due to existing cache analysis techniques not matching well with MRU. The authors propose a new classification, k-Miss, to better capture MRU behavior and use formal conditions and efficient techniques to determine k-Miss memory accesses. Results show that the proposed MRU analysis is precise and efficient, making it a good candidate for cache replacement policies in real-time systems.
411113	41111332	Psychophysics Testing Of Bionic Vision Image Processing Algorithms Using An Fpga Hatpack	The Monash Vision Group is developing a bionic eye that will involve implanting stimulation tiles on the primary visual cortex of the brain. In preparation for the first human trial in 2014, the group is exploring different image processing techniques and user interfaces. A special device called the FPGA Hatpack has been created to simulate the visual experience of a bionic eye recipient for normally sighted individuals. This Hatpack has been used to test three different methods for selecting luminance thresholds, and the results have shown varying levels of performance and areas for improvement. These findings are important for improving the overall functionality of the bionic eye.
411114	41111452	A document retrieval system for man-machine interaction	The article discusses an automatic document retrieval system designed for the IBM 7094. The system is capable of processing English texts and search requests using statistical, syntactic, and semantic procedures. It is organized around a central supervisor that calls on different subroutines to alter processing sequences and matching criteria. This flexibility allows for a variable amount of information to be produced in response to a search request and enables user interaction by allowing them to change their search requirements. The system also allows for evaluation of different analytical procedures by comparing retrieval results under different processing conditions.
411115	41111559	TimeFlip: Using Timestamp-Based TCAM Ranges to Accurately Schedule Network Updates	Network configuration and policy updates are necessary but can cause temporary disruptions if not performed carefully. Accurate time coordination has been proposed as a solution to reducing these disruptions, but implementing it presents challenges. A practical method, called TimeFlips, is introduced in this paper to address this issue. TimeFlips use a timestamp field in a TCAM entry to implement atomic bundle updates and coordinate network updates with high accuracy. It is shown that with enough flexibility in scheduling, a single TCAM entry and a single bit can be used to encode a TimeFlip while maintaining a high level of accuracy.
411116	41111646	A dynamic joint protocols selection method to perform collaborative tasks	This paper discusses the issue of static selection of interaction protocols in multi-agent systems, which limits openness, dynamic behavior, and integration of new protocols. To address this issue, the authors propose a method for agents to select protocols at runtime when interacting with each other. This allows for more flexibility and adaptability in multi-agent interactions. The paper outlines the concepts and mechanisms used for this dynamic selection process.
411117	41111729	The Development of Hopping Capabilities for Small Robots	This paper discusses the creation and evolution of small hopping robots that are specifically designed for traversing difficult terrain and exploring unknown environments, particularly in low gravity environments such as outer space. These robots use a discontinuous motion, where they jump, stop to recharge, and then continue on their journey. The development of these robots is described through the different prototypes that have been created, showcasing the importance of steering, jumping, and self-righting capabilities. The final prototype also includes wheels for precise movement after landing. The authors share their lessons learned during the development process and provide detailed descriptions and images of the various prototypes, which can be applied to the design of other jumping or hopping robots.
411118	41111899	Face transfer with multilinear models	Face Transfer is a technique that allows for the mapping of one individual's facial movements onto the facial animations of another person. It works by extracting speech-related mouth movements, expressions, and 3D pose from video footage and using this data to create a detailed 3D textured face for the target individual. The model accounts for the target's unique facial expressions and visemes, and can be easily edited to change these features or even the target's identity. This process is made possible by a multilinear model that separates the different attributes of the face, allowing for independent variation. Face Transfer also addresses common issues with creating this type of model, such as securing accurate data and minimizing errors.
411119	41111982	System diagnosis with smallest risk of error	This article discusses fault diagnosis in multiprocessor systems. Each processor can test its neighbors, but faulty processors may give incorrect results. The goal is to accurately identify the status of all processors, with a constant probability of failure for each processor. The article presents efficient diagnosis algorithms for complete bipartite graphs and simple paths, which provide the highest probability of correctness. This is the first time such reliable fault diagnosis has been achieved for these systems without any assumptions about the behavior of faulty processors. The article was published in 1998 by Elsevier Science B.V. and all rights are reserved.
411120	41112023	Refining non-taxonomic relation labels with external structured data to support ontology learning	This paper introduces a method for enhancing ontology learning systems by incorporating external knowledge sources like DBpedia and OpenCyc. The method uses verb vectors extracted from large amounts of unstructured text to suggest labels for unknown relations in domain ontologies. This is achieved through the creation of a knowledge base that includes verb centroids, mappings between concept pairs, and ontological knowledge from external sources. By applying semantic inference and validation, the accuracy of relation labels is improved. The effectiveness of this hybrid method is compared to other methods that solely rely on corpus data or reasoning and external sources.
411121	4111216	Generating implications for design through design research	Human-computer interaction (HCI) emphasizes the importance of designing technology around the needs and behaviors of users, based on social science research. However, a major challenge for designers is translating this research into practical design ideas. Despite efforts to bridge this gap, there is still a lack of understanding about the knowledge that informs design decisions. Through interviews with 12 experienced HCI design researchers, the roles and types of design implications, as well as the process of generating and evaluating them, were explored. A framework was developed to guide the generation of design implications, which revealed a wider range, additional sources, and evaluation criteria. These findings have implications for interaction design research.
411122	41112236	Surmounting BPM challenges: the YAWL story	Over the past decade, the field of Business Process Management (BPM) has undergone significant changes. Various proposals for business process modelling and execution have been introduced, but not all have endured. The Workflow Patterns Initiative was created to establish a more organized method for comparing and developing languages. This resulted in the creation of YAWL, a new workflow language based on distilled patterns. In this paper, the authors discuss the position of YAWL in relation to the evolution of BPM and the current challenges facing the field. This highlights the importance of structured approaches in BPM and the need for continued development in this area.
411123	411123223	A local Tchebichef moments-based robust image watermarking	This paper presents a new content-based watermarking scheme that uses Tchebichef moments to protect against geometric distortions and common image processing operations. The scheme combines feature extraction using Harris-Laplace detector with watermark embedding in non-overlapped disks that are invariant to scaling, translation, and rotation. The watermark is embedded in the magnitudes of Tchebichef moments via dither modulation, ensuring robustness against image processing operations and allowing for blind detection. Simulation results using Stirmark demonstrate the effectiveness of the proposed method in comparison to other image watermarking schemes. 
411124	41112436	Quantitative Model Refinement as a Solution to the Combinatorial Size Explosion of Biomodels	The practice of building a large system by progressively refining an initial abstract specification is common in software engineering, but not widely used in systems biology. This approach involves starting with a high-level model of a biological system and gradually adding more specific details about its components and reactions. In this study, the focus is on data refinement, where specific species in the model are substituted with multiple subtypes. The authors demonstrate how this refined model can be systematically obtained from the original one. As an example, they apply this methodology to a previously developed model for the eukaryotic heat shock response, refining it to include details about the acetylation of heat shock factors. Despite the increased complexity of the refined model, the authors show that their approach allows for minimal computational effort while preserving the experimental fit and validation of the model.
411125	41112584	Revocable Fingerprint Biotokens: Accuracy and Security Analysis	The paper discusses the challenges of using biometrics in security applications due to the inability to change biometric data if a database is compromised. To address this issue, the concept of revocable or cancelable biometric-based identity tokens (biotokens) is introduced. This approach separates biometric data into two fields, one of which is encoded and one that supports approximate matching, to enhance both privacy and security. The paper presents an adapted algorithm for fingerprint recognition that shows an average decrease in Equal Error Rate of over 30%, providing improved security and privacy. The approach also addresses the issue of protecting small fields in the biometric data.
411126	41112673	Strong multidesignated verifiers signatures secure against rogue key attack	Designated verifier signatures (DVS) and multidesignated verifiers signatures (MDVS) are important tools in electronic voting and contract signing. DVS allows a signer to create a signature that can only be verified by a specific entity chosen by the signer, while MDVS extends this by allowing the signer to choose multiple designated verifiers. In this paper, the authors investigate the security of MDVS against rogue key attacks, where a designated verifier tries to forge a signature that passes verification by another designated verifier. They propose a new construction that does not rely on the knowledge of the secret key assumption, and a generic construction of strong MDVS. These contributions aim to strengthen the security of MDVS and make it a more reliable primitive in electronic applications. 
411127	41112759	Universal authentication protocols for anonymous wireless communications	This paper discusses a new approach to secure roaming protocols, which allow a user to access a foreign server while authenticating both the server and the user's home server. The traditional approach involves all three parties, but the proposed approach only requires the involvement of the user and the foreign server. Two protocols are proposed, one with better efficiency and user anonymity and the other with strong user anonymity. These protocols can be used universally, regardless of the user's home or foreign domain, making the system less complex. The paper also presents a practical solution for user revocation, which is a challenging issue in two-party roaming with strong user anonymity. These solutions can be applied in various types of roaming networks.
411128	411128113	Security Analysis And Modification Of Id-Based Encryption With Equality Test From Acisp 2017	At the 2017 Australasian Conference on Information Security and Privacy, Wu et al. presented a method for identity-based encryption that aims to prevent insider attacks. However, our analysis shows that their approach is vulnerable to attacks and does not meet the claimed security standards. To address this issue, we propose a modified version of their method.
411129	41112965	Packet classification using tuple space search	Routers need to quickly sort and forward packets in order to efficiently perform functions like firewalls and QoS routing. This involves matching packets against a database of filters and forwarding them based on priority. Current filter schemes are not suitable for large databases. A new algorithm called Tuple Space Search (TSS) maps filters to tuples and uses a hash table for faster searching. Techniques are also introduced to improve the search process. In testing, TSS showed significant speedup and is the only scheme that allows fast updates and search times. An optimal algorithm called Rectangle Search is also described for two-dimensional filters.
411130	41113074	Transforming examples into patterns for information extraction	Information Extraction (IE) systems are commonly used to extract specific information from text using pattern matching. However, adapting these systems to a new subject domain can be time-consuming and expensive as it requires building a new pattern base. To address this issue, a strategy for building patterns from examples is described. This approach allows for quicker adaptation to new domains by having the user provide examples and their corresponding logical form entries, which the system then transforms into patterns using meta-rules. This eliminates the need for manual construction of patterns, making the process more efficient. 
411131	41113146	What input errors do you experience? Typing and pointing errors of mobile Web users	Small devices, like PDAs, are popular for accessing the Web, but they face challenges due to limited interface bandwidth. This can lead to difficulties for users, such as small keyboards and limited pointing devices. While there is little research on the input difficulties caused by these limitations, anecdotal evidence suggests that there are similarities between able-bodied users of the mobile Web and users with motor impairments on desktop computers. This paper presents a study on the input errors of mobile Web users, identifying six types of typing errors and three types of pointing errors that are shared between both user domains. The study suggests that solutions used for motor impaired desktop users could also be beneficial for mobile Web users, as they face similar challenges despite using different input devices. 
411132	41113234	Automatic sign translation	This paper discusses the challenge of automatically translating signs in natural environments, which contain a large amount of information. The authors present their current system for detecting and translating Chinese signs into English, using two data-driven machine translation methods: Example Based Machine Translation (EBMT) and Statistical Machine Translation (SMT). They compare the results of these methods, which were trained on a small bilingual sign corpus and bilingual glossary, and find that EBMT produces more accurate translations while SMT is better at recognizing new patterns. They are also working on a multi-engine system that can learn from data and combine the results of EBMT and SMT.
411133	41113314	SmartLabel: an object labeling tool using iterated harmonic energy minimization	Labeling objects in images is crucial for various visual learning and recognition applications, but manual labeling is time-consuming and prone to errors, making it difficult for large image databases. Semi-supervised learning algorithms, like Gaussian random field (GRF), can help by incorporating unlabeled data, but their one-shot nature limits their performance. This paper presents a new tool, SmartLabel, that uses four innovations to semi-automatically label objects in images: soft labeling, spatial constraints in graph construction, iterated harmonic energy minimization, and relevance feedback for human interaction. Results on six datasets show that SmartLabel outperforms GRF with minimal user input and significantly improves labeling accuracy.
411134	41113442	Multimode Locomotion Via Superbot Robots	This paper introduces a versatile and customizable robot that can adapt to various modes of locomotion through reconfigurable modules. Each mode is optimized for specific characteristics such as environment, speed, turning ability, energy efficiency, and resilience to failures. The Superbot robot, inspired by MTRAN and CONRO, is used as an example to demonstrate this concept. Experimental results, including both real-life tests and simulations, have shown the effectiveness of this approach. The Superbot can move in various ways, such as forward, backward, turning, sidewinding, maneuvering, and traveling up to 500 meters on a flat surface with just its battery. In simulations, it can perform as different creatures and travel at a speed of 1.0 meter per second on flat terrain with less than 6W per module. It can also climb slopes of up to 40 degrees. 
411135	411135130	Building Association-Rule Based Sequential Classifiers for Web-Document Prediction	Web servers collect and store information about users' browsing behavior in web logs. This data can be used to create statistical models that predict the users' next requests based on their current behavior. However, analyzing these logs can be challenging due to their large size and sequential nature. While previous research has proposed various methods for building association-rule based prediction models using web logs, there has been no comprehensive study on the effectiveness of these methods. This paper compares different types of sequential association rules for web document prediction and identifies two important factors - the type of antecedents of rules and the criterion for selecting prediction rules. Based on this comparison, the authors propose a best overall method and test it on real web logs.
411136	41113647	Privacy Preserving Record Linkage via grams Projections	Record linkage is a commonly used technique in data mining applications, but with the growing amount of available data, concerns about privacy have arisen. In this paper, the authors propose a new approach for private record linkage using secure data transformations and differential privacy. Their method involves embedding private frequent variable length grams from the original data and using personalized thresholds for matching individual records. This approach offers stronger privacy guarantees and better scalability compared to existing techniques, while still maintaining comparable utility. 
411137	411137103	Client-Centric Adaptive Scheduling of Service-Oriented Applications	This paper introduces a client-centric computing model that enables service-oriented applications to be executed in a more adaptive manner. This model allows for tasks to be distributed between the client and network side, making it more flexible and able to adjust to changes in the environment. It is also expected to improve scalability, performance, and privacy control. The paper presents scheduling algorithms and rescheduling strategies for the model and experiments demonstrate that it can significantly improve the execution of service-oriented applications. Overall, this model offers a more efficient and customizable approach to executing these types of applications. 
411138	41113814	Structured design with mathematical proofs	Structured design is a commonly used approach in software design, but it has limitations in ensuring design correctness. Formal methods, which involve using mathematical notation and reasoning, offer a way to address this issue. These methods can detect inconsistencies and ambiguities in a system and can be automated using computers. This paper introduces a formal structured design method with mathematical proofs and provides an example of its use in producing a provable design. The method integrates correctness verification into each step of the design process, making it different from traditional methods. This not only improves the quality of the design, but also allows for the verification of its correctness. 
411139	4111397	Torben: A Practical Side-Channel Attack for Deanonymizing Tor Communication	The Tor network is widely used for anonymous communication online, but its security is being increasingly studied by researchers, including those from academia, industry, and even nation-states. While previous attacks on Tor have been proposed, they have had low accuracy and high false-positive rates, making them impractical for widespread use. This paper presents a new attack, called Torben, which is more reliable and less intrusive than previous attacks. It takes advantage of the fact that web pages can be manipulated to load content from untrusted sources, and that the size of request-response pairs in encrypted communication can reveal information about the web pages being visited. In testing, Torben was able to accurately detect web page markers with over 91% accuracy and no false positives. 
411140	41114024	Economics of a supercloud.	A Supercloud is a type of Infrastructure-as-a-Service (IaaS) known as a "CrossCloud" that allows users to have direct control over cloud deployments, even across multiple different cloud providers. This goes beyond the capabilities of federated or hybrid clouds. Users can perform privileged operations such as live migration of virtual machine instances across different cloud providers with varying virtual machine monitors, networking, and storage infrastructure. The Supercloud also allows for simultaneous deployments across various combinations of cloud providers and the ability to change placement at any time. It provides virtual machine, storage, and networking with a full set of management operations, allowing for optimized performance across different cloud providers.
411141	411141140	Delayed reinforcement learning for adaptive image segmentation andfeature extraction	Object recognition is a complex process that involves multiple levels of algorithms. These systems typically lack feedback between levels, making it difficult to ensure their effectiveness. A new approach using "delayed" reinforcement learning has been introduced to address this challenge. This method involves learning the parameters of a multilevel system used for object recognition based on feedback from the highest level. Through experimental validation, this approach has shown improved results in recognizing 2D objects by controlling feedback in a systematic manner. This approach has the potential to solve a longstanding problem in the field of computer vision and pattern recognition.
411142	41114238	On the performance of Dijkstra's third self-stabilizing algorithm for mutual exclusion and related algorithms	In 1974, Dijkstra introduced the concept of self-stabilizing algorithms and presented three algorithms for mutual exclusion on a ring of n processors. The third algorithm, while the most interesting, was non-intuitive and its worst case complexity (the upper bound on the number of moves until it stabilizes) was unknown. In 1986, a proof of its correctness was presented, but the complexity question remained open. In this paper, the authors solve this question and prove an upper bound of $${3\frac{13}{18} n^2 + O(n)}$$ and a lower bound of $${1\frac{5}{6} n^2 - O(n)}$$. They use potential functions and amortized analysis in their approach. The paper also presents a new three-state algorithm for mutual exclusion with a tight bound of $${\frac{5}{6} n^2 + O(n)}$$. In 1995, Beauquier and Debas presented a similar three-state algorithm with an upper bound of $${5\frac{3}{4}n^2+O(n)}$$ and a lower bound of $${\frac{1}{8}n^2-O(n)}$$. The authors improve upon this algorithm with an upper bound of $${1\frac{1}{2}n^2 + O(n)}$$ and a lower bound of $${n^2-O(n)}$$. Overall, their algorithm out
411143	41114329	An approach to v&v of embedded adaptive systems	V&V techniques are crucial for ensuring the reliability of high assurance systems. However, the inclusion of embedded adaptive components in these systems has made conventional V&V methods less effective in handling uncertainties caused by environmental changes. This is particularly concerning in safety-critical applications like flight control systems, where these changes must be carefully observed and understood. To address this issue, a non-conventional V&V approach is proposed in this paper, specifically designed for online adaptive systems. It involves a novelty detection technique based on Support Vector Data Description and online stability monitoring tools based on Lyapunov's Stability Theory. Case studies using NASA's Intelligent Flight Control System demonstrate the success of this approach.
411144	41114478	Language model-based sentence classification for opinion question answering systems	This paper proposes a language model-based approach with a Bayes classifier for classifying opinionative and factual sentences in an opinion question answering system. The proposed technique significantly outperforms current standard classification methods, with an improved accuracy of 93.35%. The motivation for this task comes from the desire to provide tools for analyzing attitudes and feelings in online resources. The system aims to present multiple answers to the user based on opinions derived from different sources. This can be achieved through a document retrieval component and a sentence retrieval component, but an accurate classification component is necessary to detect and classify opinionative and factual sentences. 
411145	41114510	IS-Label: an independent-set based labeling scheme for point-to-point distance querying	The article discusses the problem of finding the shortest path or distance between two vertices in a graph, which has many practical uses. Several indexes have been developed for this purpose, but they can only handle graphs with up to 1 million vertices. This is insufficient for the increasingly large real-world graphs, such as social networks and Web graphs. The authors propose a new labeling scheme based on the independent set of a graph, which is shown to be able to handle much larger graphs than existing indexes.
411146	41114614	Distributed Maximal Clique Computation	Maximal cliques are important substructures in graph analysis. However, most existing algorithms for computing maximal cliques are either sequential and cannot scale, or are parallel but suffer from skewed workload. In this paper, a distributed algorithm is proposed for computing maximal cliques on a share-nothing architecture. The problem of skewed workload distribution is effectively addressed, resulting in reduced time complexity for computing maximal cliques in real-world graphs. Additionally, algorithms for efficiently maintaining the set of maximal cliques when the graph is updated are also devised. The effectiveness of these algorithms is verified through experiments on various real-world graphs from different application domains.
411147	41114742	Context-awareness in software architectures	Context-awareness is becoming increasingly important in adaptable systems, and as such, there is a need for formal models and notations to incorporate this aspect into higher levels of modelling. This paper proposes a formal approach to designing context-aware systems that is well integrated with existing concepts and techniques for software architectures. This approach uses a set of primitives to model context as a first-class entity and explicitly address context-awareness as an additional dimension of architectural elements. An image search system is used as an example to illustrate this approach.
411148	41114826	Biasing the overlapping and non-overlapping sub-windows of EEG recording	EEG recording requires subjects to sit still for a couple of hours and perform various mental, computational, motor imaginary or other tasks. This process can be tedious and complicated, as it is difficult for subjects to maintain concentration and there may be accidental muscle activity. This can make it challenging to extract useful information for classification, especially in longer task durations. To improve classification accuracy, this study examines the impact of different time segments on performance. The results suggest that the middle and end parts of the recording are more important for accurate classification, while the earlier segments may have lower performance. 
411149	41114923	Curve fitting by fractal interpolation	Fractal interpolation is a method used to represent data with an irregular or self-similar structure. While most studies focus on using this technique for functions, it can also be applied to curves. A new method has been introduced that offers a more efficient way to represent curves compared to existing methods. Results show that this new method produces smaller errors or better compression ratios, making it a useful tool for curve fitting. 
411150	4111502	On the Security of Tandem-DM	We have provided the first proof of security for Tandem-DM, a well-known method for creating a cryptographic hash function from a block cipher. Our proof shows that when Tandem-DM is used with AES-256, it is resistant to collision attacks from adversaries making less than 2120.4 queries. We have also proven that Tandem-DM has a good level of preimage resistance. This is significant because Tandem-DM is one of only two known constructions that can create a hash function with provable resistance to collisions, making it a practical and desirable option for secure data encryption.
411151	41115157	Training a sentence planner for spoken dialog: the impact of syntactic and planning features	Spoken dialog systems rely on the dialog manager to perform both domain specific tasks and general dialog functions. To separate knowledge about language from domain specific knowledge, natural language generation techniques can be used. However, the natural language generator must be customized for each application. This study introduces a new method for training the natural language generator automatically and explores the impact of domain specific and general features on performance. The results show that while general features have the greatest impact, incorporating domain specific features can improve performance without sacrificing the benefits of automatic domain customization through training.
411152	41115236	Robust coordination in large convention spaces	In multi-agent systems (MAS), regulating the behavior of autonomous agents is important to solve coordination problems and minimize conflicts. Social conventions can be used by agents to coordinate, but agreeing on conventions without a central authority is challenging. A new spreading-based convention emergence mechanism is proposed in this paper to help agents agree on the best convention when there are multiple options. This mechanism is applied to a problem with a large convention space, specifically finding a common vocabulary for agents to communicate without ambiguity or inconsistencies. The approach is shown to be scalable and robust to unreliable communication in large and changing scenarios.
411153	41115346	Computing the communication costs of item allocation	Multiagent systems involve groups of agents working together to accomplish a task or allocate resources. One way to efficiently distribute these resources is through auctions, which allow agents to communicate their private values for the resource to a central decision maker. The communication requirements of different auction methods vary, and this study examines the use of entropy as a measure of communication bandwidth. A new method for measuring bandwidth usage is also presented, using dialogue trees to represent all possible communication between two agents. The study also provides new guidelines for choosing the most effective auction method, based on analysis of various auctions and their communication requirements. These guidelines differ from previous recommendations and take into account the number of bidders and the sample space of their valuations.
411154	41115428	Designing a Programming Language for Home Automation	The AutoHAN project at the Cambridge Computer Laboratory is developing programming languages suitable for use in home automation and networking. These languages will greatly impact the usability of electronic devices and expand the potential users who can benefit from psychology of programming research. The design process and criteria for these languages are still ongoing and this paper outlines the current progress and plans for further research. Home networking technologies are becoming more prevalent and standardization efforts are underway. This presents a significant challenge for the psychology of programming community. While there has been some previous research on programming home appliances, it is not typically considered relevant to programming language design. However, with the increasing complexity of home automation, these insights may become more valuable in the future.
411155	41115519	Content-Based Cross Search for Human Motion Data using Time-Varying Mesh and Motion Capture Data	This paper introduces a cross search scheme for two types of 3D human motion data: time-varying mesh (TVM) and motion capture data. TVM is a sequence of 3D mesh models created from multiple-view images, allowing for recording of shape, color, and motion of real-world objects. Efficient retrieval systems are crucial for practical TVM archiving, but current systems require additional TVM generation for queries, which is time-consuming. Motion capture data, commonly used for capturing 3D human motion, has a different data structure and is incompatible with TVM. The proposed retrieval system utilizes a modified shape distribution algorithm to enable cross-referencing between TVM and motion capture data.
411156	411156161	Sketch-on-Map: Spatial Queries for Retrieving Human Locomotion Patterns from Continuously Archived GPS Data	This article proposes a system for retrieving human movement patterns using tracking data collected from a GPS receiver over a large area and extended period of time. A clustering algorithm is used to segment the data based on the person's navigation behavior. Queries can be made using sketches on a map displayed on a computer screen, with five different types of queries available. Algorithms are implemented to analyze the sketch, identify the query, and retrieve results. A graphical user interface allows for hierarchical querying and visualization of intermediate results. The sketch-based user interaction strategy makes querying for movement patterns easy and clear. 
411157	41115755	Autonomous visual model building based on image crawling through internet search engines	This paper presents a method for automatically creating visual semantic concept models from Internet search engine data without manual labeling. The process involves crawling images from search engine results and using a generalized Multiple-Instance Learning (MIL) framework to model the results as "Quasi-Positive Bags". An algorithm called "Bag K-Means" is used to find the maximum Diverse Density (DD) without negative bags, and a "Bag Fuzzy K-Means" algorithm is proposed to improve the model's accuracy. Experimental results show that this method can accurately learn models for specific concepts and outperforms the original Google Image Search.
411158	41115862	The Time Index: An Access Structure for Temporal Data	The authors introduce a new indexing technique called the time index, designed to enhance the performance of certain types of temporal queries. This index allows for efficient retrieval of objects that are valid within a specific time period and supports the temporal WHEN operator and aggregate functions. The time index is also extended to improve the performance of the temporal SELECT operator, which retrieves objects that meet a specific condition during a given time period. The paper describes the indexing technique, search and insertion algorithms, and a method for processing a temporal JOIN operation. A simulation comparing the time index to other temporal access structures is also presented. 
411159	4111598	A Framework For Dynamic Ebusiness Negotiation Processes	The business world is becoming increasingly customer-focused, leading to a rise in demand for personalized products and services. This has created a need for an open and dynamic environment for eBusiness processes in order to support new business models. While there has been previous research on negotiation, most of it has focused on simpler forms such as auctions and bilateral negotiations. However, more complex negotiations are necessary for the formation of supply networks and personalized requests in today's business landscape. Negotiation processes are often intertwined with other business processes, making it crucial to have a framework that can handle this complexity. This paper proposes a framework that incorporates existing research on negotiations and expands it to meet the demands of modern eBusiness applications. The framework consists of five components that work together to support dynamic negotiation processes: negotiation requirements, structure, process, protocol, and strategy.
411160	41116041	CCS with Hennessy's merge has no finite-equational axiomatization	The paper confirms a conjecture made by Bergstra and Klop in 1984 by showing that the process algebra obtained by adding an auxiliary operator proposed by Hennessy in 1981 to Milner's Calculus of Communicating Systems is not finitely based modulo bisimulation equivalence. This means that Hennessy's merge cannot replace the left merge and communication merge operators proposed by Bergstra and Klop, if a finite axiomatization of parallel composition modulo bisimulation equivalence is desired. In other words, the additional operator does not provide a simpler or more efficient way of defining parallel composition.
411161	41116143	Axiomatizing the Least Fixed Point Operation and Binary Supremum	The least fixed point operation on continuous functions on complete partially ordered sets can be described by iteration algebras, which have a finite set of axioms. By combining this with the binary supremum operation on continuous functions on complete semilattices, a finite set of axioms can be found for these operations. This also leads to complete infinite equations and finite implications.
411162	41116219	Equational theories of tropical semirings	This paper discusses the equational theories of exotic semirings, which are semirings with underlying carrier sets that are subsets of the set of real numbers and with operations of minimum or maximum as sum and addition as product. Examples include the (max,+) semiring and the tropical semiring. The paper shows that none of these exotic semirings has a finite basis for its equations, and this also applies to the underlying commutative idempotent weak semirings. The paper provides characterizations of the equations, decidability results, descriptions of free algebras, and relative axiomatization for these commutative idempotent weak semirings.
411163	41116327	Rules for Abstraction	The paper discusses the use of ion techniques for verifying reactive systems, which aim to combine automatic and interactive proof techniques. It focuses on homomorphic abstraction, which involves using proof rules in Lamport's Temporal Logic of Actions. The authors argue that a logical formalization of abstraction can lead to more refined techniques, and present two new techniques for verifying liveness properties over abstract models. The ultimate goal is to provide a theoretical basis for the integration of different proof techniques in the verification of reactive systems. 
411164	41116419	New routing techniques and their applications.	This paper discusses two new routing techniques that can be used to implement efficient routing schemes for different types of graphs. The first technique is for unweighted graphs with n nodes and m edges, and uses Õ(1/ε n2/3) space per vertex and Õ(1/ε)-bit headers to route messages between any pair of vertices on a (2+ε,1)-stretch path. This is an improvement compared to previous techniques that use Õ(n5/3) space. The second technique is for weighted graphs with a normalized diameter of D, and uses Õ(1/ε n1/3log D) space per vertex and Õ(1/ε log D)-bit headers to route messages on a (5+ε)-stretch path. This is an improvement over previous techniques that use Õ(n4/3) space. The paper also presents routing schemes for unweighted and weighted graphs with different stretch values, providing more efficient options compared to previous techniques. These new techniques have been proven to be almost optimal in terms of space usage.
411165	41116544	Fast generation of multiple resolution instances of raster data sets	The paper discusses efficient algorithms for studying raster data sets at multiple resolutions in GIS applications. These algorithms involve generating coarser resolution rasters from a fine resolution raster. The paper describes an algorithm that can solve this problem in Θ(N) time when the data fits in the main memory of a computer. Two additional algorithms are presented for handling larger data sets in external memory. One algorithm requires O(sort(N)) data transfers, while the other requires O(scan(N)) transfers. A variant of the problem is also studied where a connected subregion is handled instead of the full input raster. An algorithm for this variant runs in Θ(U log N) time in internal memory and can be adapted for external memory using O(sort(U)) data transfers. The efficiency of two of the presented algorithms is demonstrated through implementation and practical use. 
411166	41116625	Independent Range Sampling, Revisited Again.	The range sampling problem involves storing a set of points with associated weights in a structure that allows for efficient extraction of random samples from a given query range. This problem was first studied in 2014 by Hu, Qiao, and Tao, and later by Afshani and Wei. The initial research focused on unweighted and dynamic versions of the problem in one dimension, while the latter considered weighted and unweighted versions in 3D for halfspace queries. The authors present three main results and insights, including the possibility of efficient data structures with expected query time, efficient worst-case query bounds by allowing approximate weight proportionality, and a conditional lower bound that shows one of these concessions is necessary. This leads to a significant gap between the expected and worst-case query time for 3D range sampling queries, with the former resulting in near-linear space and polylogarithmic query time, and the latter requiring near-linear space and query time close to $n^{2/3}$. This is the first known major gap between expected and worst-case query time for a range searching problem.
411167	41116786	Simultaneous placement and scheduling of sensors	The article discusses the challenge of monitoring spatial phenomena using wireless sensors with limited battery life. The problem involves deciding where to place sensors to best predict unsensed locations while also considering when to activate them to maximize performance within power constraints. Traditionally, these two problems have been approached separately, but the authors propose a new algorithm, ESPASS, that simultaneously optimizes placement and scheduling. The algorithm has been proven to provide a constant-factor approximation to the optimal solution. It also allows for a smooth power-accuracy tradeoff and is effective in complex settings where sensing quality is measured. Empirical studies show significant improvements in performance compared to separate placement and scheduling methods. 
411168	41116816	The Geometry of Differential Privacy: The Small Database and Approximate Cases.	This work focuses on the trade-offs between accuracy and privacy in linear queries over histograms, which includes contingency tables and range queries. The goal is to find a differentially private mechanism that minimizes the mean squared error compared to the true answer. For pure differential privacy, previous work has provided an approximation guarantee. This study extends this guarantee to (epsilon, delta)-approximate differential privacy and also considers the small database setting. By using statistical estimation techniques, they are able to achieve a polylogarithmic approximation to the optimal for both pure and (epsilon, delta)-privacy. This study also explores the accuracy gap between pure and approximate privacy notions. Finally, they establish a connection between hereditary discrepancy and private mechanisms, leading to the first polylogarithmic approximation to the hereditary discrepancy of a matrix.
411169	411169191	Cops and robbers on geometric graphs	Cops and robbers is a game where one robber is chased by a set of cops on a graph. The cop number, or minimum number of cops needed to catch the robber, is studied for geometric graphs. These graphs consist of points on a 2D plane with a certain distance between them. It is proven that the cop number for connected geometric graphs in a 2D plane is at most 9, with an example of a graph with a cop number of 3. The upper bound for random geometric graphs is improved for graphs that are dense enough. It is also shown that in the connectivity regime, if the distance between points is below a certain threshold, the cop number is 1 with high probability. On the other hand, if the distance is above a certain threshold, the cop number is at least 1 with high probability.
411170	41117064	A golden-block-based self-refining scheme for repetitive patterned wafer inspections	This paper introduces a new method for identifying defects in two-dimensional wafer images with repetitive patterns. The technique utilizes prior knowledge and has the ability to learn and create a database of golden blocks from the wafer images themselves. This database is refined and modified when used for further inspections. The use of golden blocks eliminates the need to recalculate periods and building blocks for new wafer images with the same pattern. The proposed algorithm results in a significant reduction in processing time and storage overhead. New building blocks can also be derived directly from existing golden blocks, improving the overall quality of defect detection.
411171	411171102	Fast Algorithms for Testing Fault-Tolerance of Sequenced Jobs with Deadlines	Queue-based scheduling systems execute jobs in a predetermined sequential order, but faults can cause delays by forcing jobs to re-execute. Therefore, it is crucial to determine in real-time whether the scheduled jobs are fault-tolerant, meaning they will meet their deadlines regardless of faults. This allows for quick decisions, such as admitting urgent jobs without compromising the overall schedule's fault tolerance. The goal of this work is to create efficient algorithms for testing the fault tolerance of sequenced jobs in the presence of transient faults. The algorithms are exact and run in linear time, allowing for real-time decisions, and are based on different fault models that specify the allowed fault patterns and the time frame for restarting failed jobs. 
411172	411172113	A linear time algorithm for consecutive permutation pattern matching.	Order-isomorphic sequences are those where the elements in one sequence are in the same order as the elements in another sequence. A linear time algorithm has been developed to check if a given sequence contains a subsequence that is order-isomorphic to a given pattern. This is called consecutive permutation pattern matching and is easier than the general permutation pattern matching problem which is known to be NP-complete. The algorithm has a time complexity of O(n+m) if the pattern can be sorted in O(m) time, otherwise it is O(n+mlogm). The algorithm is based on the Knuth-Morris-Pratt string matching algorithm.
411173	41117322	A Universal Integral	This content discusses the introduction of a general integral that can be defined on any measurable space, based on a minimal set of axioms. This integral operates on measures that are monotone set functions and measurable functions with a range within the unit interval. The concept of integral equivalence is introduced, which leads to a significant type of general integral known as the universal integral. Various types of this integral, including extremal ones, are described and characterized. 
411174	41117449	Splittable traffic partition in WDM/SONET rings to minimize SONET ADMs	SONET ADMs are a major cost factor in WDM/SONET rings. Various studies have proposed heuristics to minimize the number of ADMs needed by partitioning traffic and allowing it to be transferred between different wavelengths. This approach, known as splittable traffic streams, has been shown to potentially reduce the number of ADMs required. This paper discusses two variations of the minimum ADM problem with splittable traffic streams: one with prespecified routings and one without. Both variations are proven to be NP-hard, but the paper proposes heuristics with approximation ratios of 5/4 and 3/2, respectively.
411175	4111752	Journal Self-Citation Viii: An Is Researcher In The Dual Worlds Of Author-Reader And Author-Institution	This paper examines the ethical implications of journal editors requesting authors to cite papers from the same journal in which they are submitting an article. The author argues that this issue must be considered within the context of two relationships: author-reader and author-institution/community. These relationships highlight that an author cannot exist in isolation and is constrained by the institutional system in which they operate. Therefore, the request for citations can be seen as a way to foster a positive author-reader relationship and contribute to the production of diverse knowledge. Ultimately, the paper concludes that this request is ethically important for maintaining good relationships and promoting valuable knowledge in the global community. 
411176	41117650	Special issue on programming based on actors, agents and decentralized control.	The AGERE! workshop, held in conjunction with the ACM SPLASH conference since 2011, aims to bring together researchers in programming systems, languages, and applications based on decentralized control paradigms such as actors, active/concurrent objects, and agents. The goal is to promote the use of these paradigms in developing software for complex, real-world applications. The workshop is a continuation of previous workshops on Object-based Concurrent Programming, and has become increasingly relevant as concurrency and distribution have become commonplace in programming. This special issue presents extended and enhanced versions of selected papers from AGERE! 2011 and 2012, which have undergone additional review cycles to promote the development and adoption of high-level programming paradigms centered around concurrency.
411177	41117714	Multiscale morphological segmentation of gray-scale images.	The paper presents a new method for segmenting gray level images using multiscale morphology. The approach is similar to the watershed algorithm, but uses multiscale morphological closing and opening to gradually fill or clip dark and bright features on the image. The algorithm identifies valid segments at each scale based on three criteria and integrates them in the final result. It consists of two passes, with a preprocessing step to simplify small scale details, and detects potential features contributing to segment formation. The algorithm is tested on synthetic and real images and compared to other methods, with a quantitative measure of performance.
411178	41117821	A Smart Home Agent For Plan Recognition Of Cognitively-Impaired Patients	Assisting individuals with cognitive deficiencies in a smart home presents complex challenges, including plan recognition. To address this issue, a formal framework based on lattice theory and action description logic has been proposed. This framework reduces uncertainty in predicting an agent's behavior by generating new implicit extra-plans. This approach provides an effective solution for plan recognition in a smart home, allowing for assistance to be provided to those with cognitive deficits. The framework has been implemented in a smart home laboratory and will be validated through experimentation with real-life scenarios. 
411179	41117950	Large scale system evaluations using PHY abstraction for LTE with OpenAirInterface	This paper outlines the methodology for expected effective SINR mapping (EESM) and mutual information effective SINR mapping (MIESM) based PHY abstraction on the OpenAirInterface (OAI) LTE platform. The authors discuss the calibration process for these techniques using the OAI link level simulator, and then validate the calibrated scheme using the OAI system level simulator. The paper also covers the methodologies for different transmission modes of LTE, including transmission modes 1, 2, and 6 with various channel types. The results demonstrate that proper calibration is necessary for accurate results, and that the implemented PHY abstraction in the OAI system level simulator can significantly increase simulation speed while maintaining accuracy. The use of OAI, which implements the LTE release 8.6, also allows for portability to other LTE simulators.
411180	41118011	Guard zone based D2D underlaid cellular networks with two-tier dependence	D2D communication is a promising feature in 5G networks due to its potential to enhance network efficiency. However, its implementation in current cellular networks can cause interference between D2D and macrocell tiers. To address this issue, a D2D underlaid cellular network is proposed, where D2D users can access the cellular spectrum with guard zones to avoid interference from macro base stations and other D2D transmitters. Using spatial models and analytical results, the D2D and cellular coverage probability as well as the D2D area spectral efficiency are studied. An optimal guard zone radius is derived to maximize the D2D network throughput. Simulation results confirm the accuracy of the proposed analytical expressions.
411181	41118136	Enhanced formulations and branch-and-cut for the two level network design problem with transition facilities.	This article introduces the Two Level Network Design Problem with Transition Facilities (TLNDF), which combines network design and facility location aspects. The objective is to find a minimum cost subtree connecting two types of customers while serving primary customers with a primary subtree that is embedded in a secondary subtree. This problem is modeled on an extended graph with additional arcs representing facility installation. A cut set based model is proposed and theoretical results are presented on relating cut set inequalities on the extended graph to the original graph. A branch-and-cut algorithm is developed and its efficiency is confirmed through a computational study. 
411182	41118231	Exact solution of the robust knapsack problem.	The uncertain variant of the knapsack problem involves items with weight ranges and a limit on the number of items with unexpected weights. A dynamic programming algorithm is proposed for this problem, with techniques to decrease its complexity. The performance of this algorithm is compared to other exact algorithms for robust optimization problems through computational analysis.
411183	41118328	Performance Evaluation of Reverse Engineering Relational Databases into Extended Entity-Relationship Models	This article discusses a method for automatically converting a relational database into an extended Entity-Relationship (EER) schema. The focus is on two key factors for evaluating the performance of this process: the extent to which it can be fully automated, and its efficiency. The latter is analyzed through time complexity analysis of the pseudo-code algorithms used in the reverse engineering process. By addressing these issues, the method aims to provide a high level of automation and efficiency in converting databases into EER schemas.
411184	41118468	A knowledge-based object modeling advisor for developing quality object models	Object models, also known as class diagrams, are commonly used in information system requirements to illustrate classes, attributes, operations, and relationships. However, developing high-quality object models can be difficult, particularly for inexperienced analysts in business environments. To address this challenge, a knowledge-based system has been developed as an extension to an open source CASE tool. This system uses an ontology of quality problems, based on a conceptual model quality framework, related empirical studies, and analysis patterns to provide recommendations for improving the quality of object models. An empirical evaluation of the prototype has shown that the system is effective in improving the semantic quality of object models, particularly in terms of model completeness. 
411185	41118527	Emotions and Multimodal Interface-Agents: A Sociological View	Creating user-friendly human-computer interfaces is crucial for the effective use of computer technology. As information systems become more complex, interface design has evolved to incorporate advanced concepts such as agent technology. Emotions play a significant role in human-computer interactions, making emotional agents increasingly important. The use of multimodal interfaces has also become more prevalent, allowing for more powerful communication between humans and computers. Combining the concepts of agents and multimodality can be beneficial, and sociological theories on emotion can provide valuable insights. The DFG-research project "Sozionik" uses sociological theory to enhance computational systems, specifically hybrid societies. Incorporating sociological perspectives can lead to a more accurate understanding of emotions in interface design, as emotions are also influenced by social norms and rules. Therefore, a sociologically-based user model and an agent's "social self" model are necessary for effective emotional agent design.
411186	41118639	Quantification of small cerebral ventricular volume changes in treated growth hormone patients using nonrigid registration.	Nonrigid registration is a technique used to measure small changes in the volume of anatomical structures over time. In this study, a nonrigid registration algorithm was used to measure changes in brain ventricle volume in a group of patients receiving growth hormone replacement therapy and a control group. The results showed a statistically significant difference in volume changes between the two groups. The accuracy of the measurements was validated by comparing them to previously published estimates and by determining the precision from three consecutive scans of volunteers. The results also showed a high level of shape correspondence between manually segmented ventricles and those obtained through segmentation propagation.
411187	411187107	Mesh Connected Computers with Fixed and Reconfigurable Buses: Packet Routing and Sorting	Mesh connected computers have become popular due to their unique features. This paper discusses two variations of the mesh model - one with fixed buses and the other with reconfigurable buses. These models have been extensively researched and we provide solutions for important problems related to packet routing and sorting. We present lower and upper bounds for routing on a linear array and k驴k routing and sorting on a 2D mesh. Our improved algorithms for 1 驴 1 routing and matching sorting algorithm show better time bounds than existing ones. We also introduce greedy algorithms for various routing and sorting problems, which have better average performance and matching lower bounds. We also demonstrate that sorting can be achieved in logarithmic time on a mesh with fixed buses.
411188	41118861	Studying compiler optimizations on superscalar processors through interval analysis	This paper explores the relationship between compiler optimizations and the performance of superscalar processors. Using interval analysis, the authors break down execution time into different components and study how compiler optimizations affect each component. This helps to understand the impact of optimizations on out-of-order processor performance. The analysis reveals interesting insights and provides suggestions for future research in this area. The authors also compare the effects of compiler optimizations on out-of-order and in-order processors. Overall, this paper highlights the complex interaction between compiler optimizations and microarchitecture, and the importance of considering this relationship in optimizing processor performance.
411189	41118976	INTERACTIVE FUZZY PROGRAMMING BASED ON FRACTILE CRITERION OPTIMIZATION MODEL FOR TWO-LEVEL STOCHASTIC LINEAR PROGRAMMING PROBLEMS	This article discusses two-level linear programming problems with random variable coefficients in objective functions and constraints. It proposes using the concept of chance constrained programming to transform these problems into deterministic ones, based on a fractile criterion optimization model. The article also introduces the use of fuzzy goals for objective functions and presents a fusion of stochastic and fuzzy approaches called interactive fuzzy programming. An illustrative numerical example is provided to demonstrate the effectiveness of this method.
411190	41119026	A robust analysis, detection and recognition of facial features in 2.5D images	The article discusses a method for recognizing 3D faces using 2.5D images. The method involves smoothing and converting 3D mesh face images to 2.5D range images. The nose-tip, as the most prominent feature on the face, is detected using corner points and curvedness values. Other facial landmarks are then located and a facial graph is generated. The feature space is described as a 3D function that maps depth values to pixel intensities. The extracted features are of dimensionality [1ź×ź21] and are used to classify face images using Multilayer Perceptron (MLP) and Support Vector Machines (SVM). The method has achieved maximum recognition rates of 75-87.5% on various databases.
411191	41119160	Human Face Recognition using Gabor based Kernel Entropy Component Analysis.	This paper introduces a new method, called Gabor wavelet based Kernel Entropy Component Analysis (KECA), which combines Gabor wavelet transformation (GWT) and KECA for improved face recognition. The GWT is used to extract discriminative facial features that can handle variations in illumination and facial expressions. Then, KECA is extended to include cosine kernel function, and applied to the extracted features to obtain only the most relevant eigenvectors with positive entropy contribution. These features are then used for image classification using distance measures such as L1, L2, Mahalanobis, and cosine similarity. The method was tested on various face databases and showed promising results in recognizing both frontal and pose-angled faces.
411192	411192134	Word-Level Script Identification Using Texture Based Features	Script identification is an important area of research in document image analysis. It is necessary for many post-processing tasks such as document sorting, machine translation, and text searching in multilingual environments. To accurately recognize scripts, a robust word-level handwritten script identification technique has been proposed in this paper. It uses texture-based features, including Histograms of Oriented Gradients (HOG) and Moment invariants, to identify words in seven popular scripts. The technique has been tested on 7000 handwritten words and achieved an overall accuracy of 94.7% using 5-fold cross-validation. The chosen classifier, Multi-Layer Perceptron (MLP), was also tested with different folds and epoch sizes. This paper is an extended version of a previous work and presents significant results in identifying complex and varied scripts.
411193	41119334	Face Synthesis (FASY) System for Generation of a Face Image from Human Description	The paper discusses a new approach to generating a face based on a human-like description. The FASY System is a database retrieval and face generation system that is currently being developed. It has the capability to generate a requested face that is not found in the existing database, allowing for continuous expansion. This new concept has the potential to enhance the accuracy and diversity of face generation.
411194	41119429	Examples of Independence for Imprecise Probabilities	This paper discusses the concept of independence for imprecise probabilities. The authors argue that there are multiple definitions of independence that are relevant in different scenarios. They provide simple examples to illustrate the different definitions and their relationships. Overall, the paper aims to clarify the meaning of independence for imprecise probabilities.
411195	41119513	Modelling and inference with Conditional Gaussian Probabilistic Decision Graphs	Probabilistic Decision Graphs (PDGs) are graphical models that represent a joint probability distribution using a decision graph structure. They can capture specific independence relations that other graphical models cannot, making them more efficient for certain operations. Previous PDGs were only defined for discrete variables, but we can now extend them to include continuous variables by assuming a Conditional Gaussian joint distribution. This allows for efficient inference to be carried out.
411196	41119644	Adaptive multi-task monitoring system based on overhead prediction	The growth and success of the Internet has made managing and monitoring ISP networks a complex process. To address this, a new monitoring system has been proposed that can adapt to network conditions and varying traffic. It utilizes an optimization method that predicts overhead and uses a weighted utility function to handle multiple monitoring tasks. To evaluate its performance, an experimental methodology is presented and the system is tested on different parameters. The results show that the proposed system effectively adjusts to changing network conditions and can handle multiple monitoring tasks efficiently. 
411197	41119710	A comprehensive solution to the XML-to-relational mapping problem	The use of relational database management systems (RDBMSs) for storing and querying XML data has gained attention due to their reliable data management services. However, there is a mismatch between the relational and XML data models, requiring the XML data to be shredded and loaded into relational tables before querying. Existing solutions for this problem are limited and often tied to a specific backend and use proprietary mapping languages. To address these limitations, ShreX has been developed as a comprehensive and end-to-end solution for XML-to-relational mapping. ShreX uses annotations to an XML Schema for mapping, making the process simpler and allowing for various mapping strategies to be combined. It also provides automatic document shredding and query translation capabilities and is portable across different database backends.
411198	41119842	Prioritizing risk pathways: a novel association approach to searching for disease pathways fusing SNPs and pathways.	Complex diseases are influenced by both mutated risk genes and environmental factors. Traditional methods for identifying susceptibility genes only consider a single-gene disease model, while pathway-based approaches take into account the joint effects of genetic factors and biological networks. With the availability of high-throughput SNP datasets and human biologic pathways, bioinformatics methods can be used to search for risk pathways associated with complex diseases. The proposed approach, called Prioritizing Risk Pathways fusing SNPs and pathways (PRP), uses a risk-scoring measurement to prioritize risk biologic pathways. This approach was applied to five complex diseases and revealed both shared and specific risk pathways, which was supported by literature research. Overall, this approach provides a new perspective for understanding the pathogenesis of complex diseases and generating new hypotheses.
411199	41119927	Reliability and route diversity in wireless networks	In this study, the issue of communication reliability in wireless networks is examined in the context of fading environments using the outage probability approach. The disconnect probability, which measures the likelihood of a transmission not being correctly received by any other node in the network, is calculated for one and two dimensional random networks. The end-to-end reliability of multi-hop transmissions is also evaluated using the outage probability metric, and algorithms are developed to determine the most reliable route while considering power constraints or minimum energy usage with a reliability constraint. Additionally, the relationship between outage probability and transmission power is explored, with and without route diversity.
411200	41120021	Completeness bounds and sequentialization for model checking of interacting firmware and hardware	The trend of implementing complex system management functions in firmware (FW) requires support for verifying FW within its hardware (HW) environment. A unified HW-FW model can help identify commonly-occurring interaction patterns between HW and FW, allowing for more efficient verification. This paper introduces a bounded model checking (BMC)-based methodology for FW verification, using static analysis techniques to determine the completeness bound. By exploiting commonly-occurring FW code patterns and sequentializing concurrent FW and HW code, software model checkers can be directly applied for verification. The authors have implemented a completeness bound analyzer and sequentializer to automate this process, and successfully evaluated the tool on three real FW benchmarks with multiple correctness properties. 
411201	41120134	On the Linear Convergence of the ADMM in Decentralized Consensus Optimization	Decentralized consensus optimization is a collaborative approach where agents in a connected network work together to minimize the sum of their individual objective functions over a shared decision variable. Information exchange between neighbors is limited in this process. The alternating direction method of multipliers (ADMM) is often used for this problem, involving iterative computation at each agent and information exchange between neighbors. This method has been found to be efficient and powerful. This paper establishes the linear convergence rate of ADMM for this problem when the local objective functions are strongly convex. The convergence rate is determined by the network topology, properties of local objective functions, and algorithm parameters. This result not only serves as a performance guarantee, but also provides guidance for speeding up the convergence of ADMM.
411202	4112023	Constraint Handling In Multi-Objective Evolutionary Optimization	This paper presents a new method for handling constraints in multi-objective evolutionary algorithms. It uses adaptive penalty functions and distance measures to modify the objective space, allowing the algorithm to evolve optimal solutions from both feasible and infeasible spaces. The search in the infeasible space focuses on individuals with better objective values and low constraint violations. The number of feasible individuals in the population guides the search process towards finding more feasible or optimum solutions. The proposed method is easy to implement and does not require parameter tuning. It has been tested on various constrained multi-objective problems and has shown promising results. 
411203	41120324	Friends or foes? on planning as satisfiability and abstract CNF encodings	Planning as satisfiability is a method used to find parallel step-optimal plans, implemented in tools like SATPLAN. However, a limitation of this approach is proving the absence of plans of a certain length. To address this, the idea of using solution length preserving abstractions of the original planning task is explored. This is promising as the abstraction may have a smaller state space, similar to successful methods in model checking. However, when evaluated empirically, even handmade abstractions do not improve the performance of SATPLAN. This is due to the fact that in many cases, the shortest resolution refutation for the abstracted planning task is longer than that of the original task. This suggests a fundamental weakness in the approach and calls for further investigation into the relationship between transition-systems, abstractions, and SAT encodings.
411204	41120411	HMC: verifying functional programs using abstract interpreters	Hindley-Milner-Cousots (HMC) is an algorithm that simplifies the process of verifying safety properties of higher-order functional programs. It does this by breaking down the problem into interprocedural analysis for first-order imperative programs. This is achieved by using the type structure of the functional program to generate logical refinement constraints, which are then transformed into a first-order imperative program and an invariant. This invariant is used with an invariant generator for first-order imperative programs to verify the safety of the source program. HMC has been implemented and has shown promising results in verifying OCAML programs using two imperative checkers. By combining type-based and state-based reasoning, HMC offers a fully automatic way to verify programs written in modern programming languages. 
411205	41120525	A Flexible Stochastic Automaton-Based Algorithm for Network Self-Partitioning	The article presents a flexible and distributed algorithm for network partitioning using stochastic automata. This algorithm is able to find the optimal k-way partition for various cost functions and constraints in directed and weighted graphs. The article discusses the motivation for the distributed partitioning problem, introduces the algorithm, and shows that it has a high probability of finding the optimal partition. It also explains why the algorithm is efficient and provides examples of its performance. The article concludes by exploring the potential applications of this algorithm in mobile/sensor classification, fault-isolation in power systems, and control of autonomous vehicles.
411206	41120630	Enhancing Product Detection With Multicue Optimization For Tv Shopping Applications	Smart TVs offer a more immersive viewing experience by allowing consumers to watch TV, use applications, and access the internet. However, the current technology still lacks the ability for seamless interaction with the content being streamed, particularly in TV-enabled shopping. This means that if a consumer sees a product they are interested in purchasing while watching a TV show, they must switch to a different screen or device to make the purchase. To address this issue, a multicue product detection framework is proposed for TV shopping. This framework utilizes appearance, topological, and spatio-temporal cues to detect complex products, improving the precision of the results. This approach allows for easier and more convenient purchasing for consumers through their smart TV.
411207	411207103	Correlation-Assisted Imbalance Multimedia Concept Mining And Retrieval	In recent years, there has been a significant increase in multimedia data due to the rise of social media and smart devices. This has made the task of mining and retrieving useful information from multimedia data, which includes texts, images, and videos, more important. However, the large amount of data and the gap between low-level features and high-level concepts make this task challenging. To address this, understanding the correlations among classes can help bridge this gap. Additionally, many real-world datasets have imbalanced class distributions, where minority instances represent important concepts such as frauds, intrusions, and unusual events. Despite efforts to address this issue, imbalanced concept retrieval remains a difficult problem. This paper proposes a new model that analyzes the correlation between retrieval scores and labels to improve imbalanced concept mining and retrieval, even with low scores from minority classes. Experiments on benchmark datasets show promising results for this framework.
411208	4112081	Detection Of Partial-Band Noise Interference In Slow Fh/Qpsk Systems	This paper presents a detection algorithm for detecting unknown partial-band noise interference (PBNI) in slow frequency-hopped spread spectrum (SFH-SS) systems. The algorithm uses a simple hypothesis testing approach and a derived threshold level to detect the PBNI after dehopping. It utilizes the outputs of two correlators used for QPSK demodulation and sums them over one hop duration. The probability of detection (P-D) is formulated as a performance measure for the detector, and both analytical and simulation results demonstrate its effectiveness in detecting PBNI even at low interference power. Furthermore, the algorithm can also be applied to detect multitone interference (MTI) in SFH-SS systems. 
411209	41120986	A Semidefinite Relaxation Approach for Beamforming in Cooperative Clustered Multicell Systems With Novel Limited Feedback Scheme	This paper discusses suboptimal cooperative beamforming strategies to maximize the user sum rate in a multicell system with limited feedback. The problem is decoupled into subproblems using the uplink-downlink duality theorem. A semidefinite relaxation technique is used to convert the nonconvex problem into a convex semidefinite programming problem, which can be efficiently solved in polynomial time. A novel limited feedback scheme based on compressive sensing is proposed to obtain high-quality channel state information (CSI). The paper also considers a scenario where base stations have different estimates of the same CSI and investigates the channel quantization criterion. Numerical results show the effectiveness of the proposed beamforming algorithm and limited feedback schemes.
411210	41121033	An abstract machine for efficiently computing queries to well-founded models	The well-founded semantics is a popular approach because it is a type of skeptical semantics that considers unknown atoms instead of assigning them a true or false value. This allows for more efficient computation and makes it suitable as a basis for non-monotonic reasoning. The Warren Abstract Machine (WAM)-based abstract machine, known as the SLG-WAM, uses tabling to efficiently compute the well-founded semantics and requires extensions to handle non-ground normal logic programs. These extensions include representing answers that are neither true nor false and implementing delay and simplification operations. The efficiency of this implementation is demonstrated through a theorem and performance results.
411211	41121142	Self-regulating finite automata	Self-regulating finite automata are introduced and discussed in this paper. These automata use a sequence of rules from previous moves to regulate their rule usage. The concept of turns, where a new self-regulating sequence of moves begins, is given special attention. Two infinite hierarchies of language families are established based on the number of turns, and it is shown that these hierarchies align with those resulting from parallel right linear grammars and right linear simple matrix grammars. The paper also compares these hierarchies and suggests exploring self-regulating pushdown automata, noting that they do not result in infinite hierarchies like those achieved by self-regulating finite automata.
411212	41121294	Learning convex combinations of continuously parameterized basic kernels	This study focuses on finding the optimal kernel for minimizing a regularization error functional, commonly used in regularization networks and support vector machines. The kernel is assumed to be a combination of basic kernels, such as Gaussian kernels, which are continuously parameterized by a compact set. The study proves that there is always an optimal kernel that can be expressed as a convex combination of at most m+1 basic kernels, where m is the sample size. It also provides a necessary and sufficient condition for a kernel to be optimal. The study's proof is constructive and leads to a greedy algorithm for learning the optimal kernel. Preliminary numerical simulations are presented to demonstrate the algorithm's properties.
411213	41121339	Online space-variant background modeling with sparse coding.	The paper proposes a sparse coding method for background modeling in videos. The model is based on dictionaries that are constantly updated as new data is provided by a video camera. The background is considered as noisy data with local and global changes over time due to various factors. To capture these changes, a space-variant analysis is used where a dictionary of atoms is learned for each image patch. At run time, each patch is represented by a linear combination of these atoms and any changes are detected when the atoms are not sufficient to represent the patch. The proposed method shows good performance on different scenarios and can also detect periodic changes caused by natural illumination. It can be used as a component in change detection systems.
411214	41121431	Automated 3D lymphoma lesion segmentation from PET/CT characteristics	PET using <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">18</sup> F-FDG is the most effective method for detecting lymphoma due to its high sensitivity and specificity. However, accurately delineating tumors on PET images remains a challenge. Most methods use intensity-based strategies, but recent approaches have incorporated anatomical information to improve accuracy. In this study, the authors propose a fully automated method for lesion detection and segmentation using a combination of machine learning and hierarchical image models. This approach achieved a detection rate of 92% and mean sensitivity and specificity of 0.73 and 0.99, respectively, without any user interaction. This method shows promise for improving the efficiency and accuracy of PET imaging for lymphoma patients.
411215	41121526	Multizone soundfield reproduction in reverberant rooms using compressed sensing techniques	We present a method for reproducing a multizone soundfield in a reverberant room by determining the acoustic transfer function (ATF) between a loudspeaker and the desired region using a limited number of microphones. This is achieved by assuming a sparse soundfield in the Helmholtz solution domain and using a compressed-sensing approach to find the ATF. This allows for the optimal characterization of the original sound using scarce sound pressure measurements. The resulting ATF is then used to derive the optimal loudspeaker filter that minimizes reproduction error over the entire region. Simulations show that this method greatly reduces the number of required microphones and enables accurate reproduction across a wide frequency range.
411216	41121687	The Finite Capacity Dial-A-Ride Problem	The Capacitated Dial-a-Ride problem involves finding the shortest tour for a vehicle to deliver objects to their specified destinations while adhering to a maximum capacity of k objects at any given time. Previous approximation guarantees for this problem were limited to specific cases, but a new algorithm with an approximation ratio of O(sqrt{k}) has been developed for special instances on a class of tree metrics. For general metric spaces, the ratio is O(sqrt{k} log n log log n). A 2-approximation algorithm is also provided for the problem on a line metric. Additionally, a new framework is considered where the vehicle can leave and pick up objects at intermediate locations, and an O(1) approximation algorithm is designed for tree metrics. The ratio between the values of optimal solutions for the two versions of the problem is also studied, with a maximum possible value of \Omega(k^{2/3}). 
411217	411217115	Construction of an annotated corpus to support biomedical information extraction.	Information Extraction (IE) is a text mining tool used to automatically identify important biomedical events from large collections of documents. Understanding the syntax and semantics of verbs and nominalized verbs is crucial for IE, and annotated corpora can be useful for training. A new annotation scheme has been developed for gene regulation events, which includes identifying all participants and assigning them semantic roles and biological concept types. The Gene Regulation Event Corpus (GREC) contains 240 MEDLINE abstracts annotated by biologists. It is unique in the biomedical field as it includes not only core relationships, but also other important details such as location, time, and environmental conditions. The corpus has already been used to develop a lexical resource for biomedical text mining and can also be used to train IE components. It is freely available for academic use.
411218	41121825	Optimal resource allocation for cost and reliability of modular software systems in the testing phase	Reliability is a crucial aspect of commercial software and is measured by the number of failures that occur during development. To improve reliability, a comprehensive test plan must be in place to ensure all requirements are tested. However, software testing is often constrained by limited time and resources, making it important for project managers to allocate testing resources effectively among different modules. This paper presents an optimal resource allocation problem for modular software systems during the testing phase. The goal is to minimize development costs while achieving a desired level of reliability with a fixed amount of testing effort. The proposed optimization algorithm, based on the Lagrange multiplier method, is supported by numerical examples and sensitivity analysis. This allows for the identification of key parameters and their impact on achieving the desired reliability objective. Overall, the proposed approach aims to efficiently allocate limited testing resources and improve software reliability.
411219	411219113	Minimum Edge Ranking Spanning Trees of Threshold Graphs	The minimum edge ranking spanning tree problem (MERST) is a challenging problem in graph theory, where the goal is to find a spanning tree with the lowest possible edge ranking in a given graph. This problem is known to be NP-hard for general graphs. However, a new study has found that for threshold graphs, which have practical applications, there exists a polynomial time algorithm to solve MERST. This is a significant result as it is the first non-trivial graph class in which MERST can be solved in polynomial time.
411220	41122041	One Graph to Rule Them All Software Measurement and Management	The software architecture of a system is its fundamental organization and includes the relationships between components and the system's environment. A holistic model is necessary for managing the architecture of a large software system and ensuring continuous integration and verification of all artifacts. A unified graph-based approach has been proposed as a solution to this problem, which also allows for convenient and efficient project measurement. This approach can translate existing software metrics into a model that is independent of programming language and introduces new metrics that can cross language boundaries. The model can be implemented using existing tools such as graph databases, query languages, and algorithms.
411221	41122116	Wrapper maintenance: a machine learning approach	The rise of online information sources has led to a greater use of wrappers for extracting data from the web. However, there has been less focus on developing tools for maintaining these wrappers, which is important because web sources often change and cause the wrappers to malfunction. This article presents an algorithm that can learn structural information about data from positive examples, which can be used for wrapper maintenance tasks such as verification and reinduction. The algorithm was validated on 27 wrappers over a year and was able to detect 35 out of 37 changes, with a precision of 0.73 and recall of 0.95. The reinduction algorithm was also successful on ten web sources, achieving a precision of 0.90 and recall of 0.80 for data extraction. 
411222	41122291	Tools for transparent synchronous collaborative environments	Synchronous collaborative environments allow people in different locations to work together in the same virtual space. There are two ways to create a shared workspace: the collaborative-aware approach and the collaborative-unaware approach. The latter is more popular because it allows for the use of single-user applications. The authors of this paper focus on the collaborative-unaware approach and discuss their implementation of various tools such as latecomer support for Java applications, client synchronization to reduce data transmission delays, and lightweight multi-session support for multiple collaboration groups. These tools improve existing synchronous collaboration systems and make them more realistic, comprehensive, and versatile. 
411223	41122366	Multi-level Hashing for Peer-to-Peer System in Wireless Ad Hoc Environment	Peer-to-peer (p2p) systems have become increasingly popular, especially with the rise of mobile technologies. However, the resource constraints and node mobility in these systems pose challenges for researchers. Efficient content distribution and discovery are crucial for p2p protocols, but routing in an ad hoc environment can be expensive. This paper proposes a solution using multi-level hashing for wireless ad-hoc networks, which helps reduce routing overhead. Extensive simulations demonstrate the effectiveness and necessity of this approach. Overall, this paper addresses important issues in p2p systems and offers a promising solution for improving their performance in mobile environments.
411224	41122421	Analysis of source sparsity and recoverability for SCA based blind source separation	Sparse component analysis (SCA) has an important application in underdetermined blind source separation (BSS). This paper uses a probability framework to address the recoverability problem of underdetermined BSS using a two-stage SCA approach. The study considers a scenario where both the sources and mixing matrix are randomly generated. A recoverability probability estimate is presented, with a focus on the nonzero entry number of source column vectors. The concept of sparsity degree for a signal is defined, and its relationship with the sources' sparsity degree is established in terms of recoverability probability. This relationship can be used to ensure the performance of BSS. Simulation results have shown the effectiveness of the proposed probability estimation approach.
411225	41122557	A Semi-Supervised Svm Learning Algorithm For Joint Feature Extraction And Classification In Brain Computer Interfaces	Machine learning based Brain Computer Interfaces (BCIs) face challenges in using a limited amount of labelled data to build a classifier for a specific subject. BCI Competition 2005 addressed this challenge. An effective BCI system should be adaptive to handle dynamic variations in brain signals. One solution is to adjust the parameters while the system is used online. A new semi-supervised support vector machine (SVM) learning algorithm is introduced in this paper, where feature extraction and classification are jointly performed in iterations. This allows for using a small training set while maintaining high performance and shortening the initial calibration process. The algorithm can be used online to make the BCI system robust to signal changes. The algorithm's robustness to noise and convergence are analyzed, and it is applied to data from BCI Competition 2005, demonstrating its validity.
411226	411226103	Verification of Cache Coherence Protocols by Aggregation of Distributed Transactions	 This paper introduces a method for verifying the correctness of protocols and distributed algorithms. The method involves comparing the state graph of an implementation with a specification representing the desired abstract behavior. The specification is composed of atomic transactions, which are not necessarily atomic in the implementation. To address this, the method uses an aggregation function that combines the steps of each transaction in the implementation into a single atomic transaction in the specification. This paper demonstrates the effectiveness of the method by applying it to a directory-based cache coherence protocol for the Stanford FLASH multiprocessor. The protocol, which has over a hundred different implementation steps, is reduced to a specification with only six atomic transactions. This allows for easy proof of important properties such as data consistency. The method is also used to verify that the reduced protocol meets a desired memory consistency model, marking the first time a theorem-prover has been used to verify the correctness of a cache coherence protocol of this complexity.
411227	41122721	cSculpt: a system for collaborative sculpting.	Collaborative systems are commonly used for sharing work among people, but they are not well established in computer graphics. In this paper, a system for collaborative 3D digital sculpting is presented. This system allows multiple artists to work together on the same polygonal mesh, sharing their edits automatically and seamlessly. A merge algorithm is proposed to handle concurrent edits, which is fast, respects users' edits, and works for both geometry and appearance modifications. The algorithm is based on a multiresolution edit representation, making it suitable for both fine adjustments and large scale modifications. Tests showed that this algorithm outperforms previous methods for collaborative mesh editing.
411228	41122858	Architecture of Scalable Embedded Device Management System with Configurable Plug-In Translator	This paper discusses the challenges of the increasing heterogeneity of nodes in ubiquitous computer networks and proposes a system architecture to address this issue. The architecture includes two sub-systems - a Configurable Plug-In Translator System and a Dynamic Management Script Resolver System. The Translator System provides generalized interfaces to applications by using a script execution engine to absorb the heterogeneous low-level interfaces. The behavior of remote nodes is described using a script language. The Resolver System utilizes DNS to create a global scale script repository, taking advantage of DNS's scalability, robustness, and authority management capabilities. The integration of these two sub-systems allows for application software to easily access any remote device on the Internet.
411229	4112291	Multiple acoustic sources location based on blind source separation	This paper explores the use of blind source separation (BSS) with canonical correlation analysis (CCA) to locate multiple acoustic sources. The receiving array used is a sparse array with three subarrays. By using CCA on the receiving data, the separate components can be obtained. This allows for the calculation of time differences and the direction of arrival (DOA) for each acoustic source. The proposed method is able to reduce the impact of inter-sensor spacing and other factors, resulting in more accurate and stable results. Simulation results support the effectiveness of this approach. Overall, this new method offers a valuable contribution to the field of acoustic source location.
411230	4112303	Unsupervised Event Coreference Resolution.	Event coreference resolution is a crucial task in natural language processing, used in applications like information extraction and question answering. This article introduces new unsupervised, nonparametric Bayesian models for determining coreference clusters of event mentions from a group of unlabeled documents. By extracting various lexical, syntactic, and semantic features for each event mention, the models group together mentions with shared features (such as participating entities, location, and time). The main challenges in unsupervised event coreference include choosing a rich set of features and modeling events within and across documents. The first model presented combines the hierarchical Dirichlet process with the ability to capture uncertainty and incorporate a finite number of features. A new hybrid model also automatically infers the number of features and selects the most informative ones for the task. Evaluation shows significant improvement compared to baseline models for both within-document and cross-document event coreference.
411231	41123178	Delegation protocols for electronic commerce	The reliability of transactions in commercial and financial activities is essential in the electronic world, just as it is in real-life. However, many security mechanisms and protocols do not prioritize accountability, which is crucial in ensuring trust and integrity in these transactions. This is especially evident in delegation protocols. To address this issue, a new approach is proposed that explicitly includes accountability in delegation. This includes a new definition of semantics and a new protocol designed to eliminate the lack of accountability. By prioritizing accountability, electronic commerce can become more reliable and trustworthy in its transactions, similar to real-life commercial and financial activities.
411232	41123247	Real-Time Quadratic Sliding Mode Filter for Removing Noise.	This paper introduces a new sliding mode filter that effectively removes impulsive and high-frequency noise with a smaller phase lag compared to linear filters. It has improved performance in terms of overshoot and chattering, making it suitable for real-time applications. The filter uses a quadratic surface as its sliding surface and is based on a backward Euler discretization algorithm, which helps to prevent chattering. Experiments using an ultrasonic sensor and an optical encoder demonstrate the effectiveness of the filter. 
411233	4112338	The effectiveness of an optimized EPMcreate as a creativity enhancement technique for Web site requirements elicitation	This paper discusses the importance of creativity in requirements elicitation and the effectiveness of three creativity enhancement techniques (CET) in generating ideas for requirements. Two controlled experiments were conducted, with university students using either full EPMcreate, Power-Only EPMcreate, or traditional brainstorming to generate ideas for improving a high school's public website. The first experiment showed that Power-Only EPMcreate was the most effective in terms of both quantity and quality of ideas, followed by full EPMcreate and then brainstorming. The second experiment confirmed these results. In both experiments, a requirement idea was considered high quality if it was both new and useful.
411234	41123447	Presentation Models By Example	The article discusses the limitations of traditional interface builders and multi-media authoring tools in creating dynamic displays. These tools only support the construction of static displays with known components, making it difficult to create more complex displays. High-level user interface management systems and automated designers offer more sophisticated options, but they are not user-friendly as they require dealing with abstract concepts. In response, the authors introduce HandsOn, a GUI development environment that allows for the creation of complex displays with changing data through direct manipulation. HandsOn incorporates principles of graphic design, supports constraint-based layout, and uses Programming By Example techniques to simplify the design process. It also relies on a model-based language for representing displays and providing information for the tool to reason about. 
411235	41123519	Event Storage and Federation Using ODMG	The Cambridge Event Architecture (CEA) has enhanced its object-oriented, distributed programming environment by incorporating events through a language-independent interface definition language. This allows for the specification and publication of event classes. An extension to CEA has been developed using the ODMG standard to integrate the transmission and storage of events. This includes an ODL parser, event stub generator, metadata repository, and an event library for C++ and Java. The ODMG metadata interface enables clients to dynamically access interface specifications for event registration, allowing for new objects and independently developed components to interact seamlessly. Object database schemas can be used to define name services and interface traders, with type hierarchies allowing for matching between different domains. By utilizing metadata, contracts can be established between domains, facilitating event translation between heterogeneous systems.
411236	41123619	A Discrete-Time Polynomial Model of Single Channel Long-Haul Fiber-Optic Communication Systems	The paper discusses the need for a two-dimensional discrete-time input-output model to address physical impairments in long-haul DWDM systems and maximize system capacity. The proposed model is based on the Volterra series transfer function method and can be extended to multichannel systems. It considers fiber losses, frequency chirp, and photodetection, which are often ignored in current literature. The model also takes into account the effects of intersymbol interference, self phase modulation, intrachannel cross phase modulation, and intrachannel four wave mixing on system performance. The model is validated through SSF simulation and is used to develop a constrained coding scheme to mitigate these impairments. 
411237	4112372	Removal of 3D facial expressions: A learning-based approach	This paper discusses a method for recovering the neutral 3D face of a person when given a 3D face model with facial expressions. The proposed approach involves using a learning-based expression removal framework, which models expression residue and uses it to recover the neutral face. This is achieved through a two-step non-rigid alignment method and the use of two spaces, normal space and expression residue space. The problem of expression removal is then formalized as the inference of expression residue from normal spaces. The neutral face model is generated using a Poisson-based framework. Experiments on the BU-3DFED database show the success of this approach. 
411238	41123865	Cloud resource allocation schemes: review, taxonomy, and opportunities.	Cloud computing is a popular computing model that allows users to process data and run complex applications on a pay-as-you-go basis. As the demand for these cloud-based services increases, it becomes challenging to efficiently allocate resources while meeting the service-level agreement between providers and consumers. Factors such as resource heterogeneity, unpredictable workloads, and different objectives of cloud stakeholders make resource allocation even more complicated. As a result, there has been significant research to address these challenges. This paper provides a comprehensive review of existing cloud resource allocation schemes, highlighting their strengths and weaknesses. It also presents a thematic taxonomy to classify these schemes and analyzes them based on optimization objectives. Finally, it suggests opportunities for the development of optimal resource allocation schemes.
411239	41123988	An Intelligent Fingerprint-Biometric Image Scrambling Scheme	The article discusses a solution to prevent attacks and disruptions in biometric image transmission through the use of a challenge/response-based system. The proposed system involves an intelligent biometrics sensor that can receive challenges from the authentication server and generate encrypted responses using the Facial Recognition Technology (FRT). Additional secret keys, such as scaling factors and random phase masks, are also used to enhance the encryption security. The random phase masks are generated chaotically to further improve security. Experimental and simulation results have demonstrated the effectiveness of this system in ensuring the security and reliability of biometric image transmission. 
411240	41124030	Hybrid agent-based simulation for analyzing the National Airspace System	Hybrid agent-based simulation combines discrete-event and continuous-time models to accurately analyze large-scale complex systems like the National Airspace System (NAS). Its ability to incorporate different types of models makes it a reliable tool for evaluating system reliability and performance. However, there are important issues that must be addressed in order for it to be a valuable design and analysis tool. This paper discusses the development of hybrid agent-based simulation architectures for simulating the NAS, specifically using an object-oriented approach. It also explores methods for improving computational efficiency in updating the simulation, and compares their effectiveness.
411241	41124125	Displaced dynamic expression regression for real-time facial tracking and animation	The article presents an automatic method for real-time facial tracking and animation using a single video camera. This approach does not require calibration for each user and uses a generic regressor from public image datasets to accurately infer 2D facial landmarks and 3D facial shape from 2D video frames. The inferred landmarks are then used to adapt the camera matrix and user identity for better matching of facial expressions. This process is done in an alternating manner and quickly converges with increasing facial expressions observed in the video. The approach is shown to be robust and accurate, comparable to state-of-the-art techniques that require individual calibration, and runs at an average speed of 28 fps. The authors believe this approach is suitable for widespread use in consumer applications.
411242	4112426	Video stabilization based on a 3D perspective camera model	The paper introduces a new method for stabilizing video sequences using a 3D perspective camera model. This approach is more advanced than previous methods, as it can handle depth variations and large camera movements. The stabilization problem is formulated as a quadratic cost function with smoothness and similarity constraints, allowing for precise control and efficient optimization. Instead of recovering dense depths, an approximate geometry representation is used and warping errors are analyzed. The results show that visually plausible stabilization can be achieved even with planar structures. The approach is robust and efficient, as demonstrated by various experiments. 
411243	41124381	Multiple-Access Channels With Confidential Messages	The article discusses a discrete memoryless multiple-access channel (MAC) with confidential messages, where two users try to transmit both common and private information to a destination. Each user also receives channel outputs, which may reveal the confidential information sent by the other user. However, both users view each other as potential eavesdroppers and want to keep their confidential information secret. The level of secrecy is measured by the equivocation rate, and the article proposes a performance measure called the rate-equivocation tuple. The capacity-equivocation region, which includes all achievable rate-equivocation tuples, is studied for both the case of two confidential messages and one confidential message. Bounds are derived and a tradeoff between secrecy levels is demonstrated. The article also explores a class of degraded MACs and provides results for two example channels: binary and Gaussian MACs.
411244	411244815	Energy-efficient resource allocation in wireless networks with quality-of-service constraints	The article proposes a game-theoretic model to address the problem of joint power and rate control with quality of service (QoS) constraints in multiple-access networks. In this game, each user aims to maximize its own utility by choosing transmit power and rate while meeting QoS requirements. The QoS constraints are based on average source rate and delay, including transmission and queuing delays. The utility function focuses on energy efficiency, making it suitable for wireless networks with energy limitations. The Nash equilibrium solution for this non-cooperative game is derived, and a closed-form expression for the utility achieved is obtained. The article also explores the tradeoffs among throughput, delay, network capacity, and energy efficiency using this framework. Analytical expressions are provided for users' delay profiles, and the delay performance at Nash equilibrium is evaluated.
411245	41124535	Volumeshop: An Interactive System For Direct Volume Illustration	Illustrations have a significant impact on the learning process, whether they are used to teach medical procedures, anatomy, or technical devices. While there is a vast amount of volumetric data available, illustrations are typically created manually as static images, which can be time-consuming. The goal of this paper is to develop a dynamic three-dimensional illustration system that can directly operate on volume data. The system allows for interactive manipulation and exploration of single images, combining the traditional aesthetic appeal of illustrations with the flexibility of digital visualization techniques. This is achieved through a novel concept of direct multi-object volume visualization, which allows for control of the appearance of inter-penetrating objects. Additionally, the paper presents a unified approach for integrating various non-photorealistic rendering models and discusses several illustrative concepts, such as cutaways, ghosting, and selective deformation. A simple interface for specifying objects of interest through volumetric painting is also proposed. All of these methods are integrated into VolumeShop, an interactive application that utilizes hardware acceleration for direct volume illustration.
411246	41124619	Combining 2D and 3D views for orientation and relative position tasks	This study compares displays that combine 2D and 3D views to displays with only 2D or 3D views. The combination displays include orientation icons, in-place methods, and a new method called ExoVis. The researchers specifically looked at performance differences in 3D orientation and relative position tasks. The results showed that 3D displays are good for general navigation and relative positioning, while 2D/3D combination displays are better for precise orientation and position tasks. The combination displays performed as well or better than 2D displays. However, clip planes were not effective for 3D orientation tasks, but could be useful for tasks requiring only one slice. 
411247	41124763	Evaluation of prostate segmentation algorithms for MRI: the PROMISE12 challenge.	Prostate MRI image segmentation is important for various tasks in the clinical workup of prostate cancer, but evaluating algorithms on multi-center, multi-vendor and multi-protocol data has been challenging due to differences in scanners and image appearance. To address this, the Prostate MR Image Segmentation (PROMISE12) challenge was created, where 100 cases from 4 centers were included and 11 teams participated using various methods. Results showed that active appearance model based approaches outperformed other methods in accuracy and computation time, with Imorphics and ScrAutoProstate teams being the top performers. However, combining algorithms may lead to even better results. The challenge results are available online for further research. 
411248	41124853	Conditional shape models for cardiac motion estimation.	The authors propose a statistical shape model that can predict cardiac motion from a 3D end-diastolic CTA scan. The model combines atlas based segmentation and 4D registration using data from 4D CTA sequences. This can be useful for dynamic alignment of pre-operative CTA data with intra-operative X-ray imaging. As 4D imaging data is not widely available due to the use of prospective electrocardiogram gating techniques, the ability to predict motion from shape information is valuable. The accuracy of the predicted motion was evaluated using CTA scans of 50 patients, showing an average accuracy of 1.1 mm.
411249	41124994	Linear dimensionality reduction via a heteroscedastic extension of LDA: the Chernoff criterion.	The article introduces a new technique, called eigenvector-based heteroscedastic linear dimension reduction (LDR), for handling multiclass data. This technique is an extension of a two-class method that uses the Chernoff criterion, and improves upon the limitations of the widely used linear discriminant analysis (LDA) technique. The new method addresses the issue of heteroscedasticity by generalizing the between-class scatter to capture differences in variances. The authors demonstrate the effectiveness of this approach through experiments and a comparison with other dimension reduction techniques. They also propose a multiclass extension of the Chernoff criterion, which combines information from both class means and covariance matrices. This article provides a comprehensive overview of the proposed technique and its advantages for analyzing multiclass data.
411250	411250138	Three-dimensional modeling for functional analysis of cardiac images: a review.	The field of three-dimensional (3-D) heart imaging is rapidly advancing and holds great potential in improving clinical diagnosis and research on cardiovascular diseases. Various techniques have been developed to analyze images and extract parameters of cardiac shape and function. This paper provides a comprehensive overview of two decades of research on cardiac modeling, aiming to serve as a guide for both clinicians and technologists. It critically reviews the performance and clinical evaluation of these approaches, concluding that while they have the potential to enhance the diagnostic value of cardiac images, there are still challenges such as robustness, 3-D interaction, computational complexity, and clinical validation that need to be addressed.
411251	41125131	Type-Safe Feature-Oriented Product Lines	A feature-oriented product line is a group of programs that share a common set of features, which represent design decisions and configuration options. These features can be added to a program, resulting in the introduction of new structures and refinement of existing ones. The generation of programs in a feature-oriented product line is based on a user's selection of features, and a key challenge in this process is ensuring the correctness of all member programs. This is accomplished through a type system that checks the entire code base of the feature-oriented product line. A formal model of a feature-oriented Java-like language has been developed to demonstrate the effectiveness and completeness of this type system. 
411252	41125270	Topological segmentation in three-dimensional vector fields.	The article introduces a new method for segmenting three-dimensional vector fields, which involves replacing the original field with a segmented data set and using it to generate separating surfaces. The segmented data set is created by sampling the vector field with streamlines, and algorithms are developed to produce the separating surfaces. This method is particularly useful for generating local separatrices in the vector field, defined by a movable boundary region. The resulting partition can then be visualized using standard techniques, providing a higher level of abstraction for the vector field visualization.
411253	41125314	Multiplexed metropolis light transport	Global illumination algorithms using Markov chain Monte Carlo (MCMC) sampling are efficient for complex light transport scenes. These algorithms generate samples as a history of Markov chain states, distributed based on their contributions to the image. However, in light transport simulation, there is more information that can improve sampling efficiency. Multiple importance sampling (MIS) in bidirectional path tracing is an example of utilizing probability densities to construct paths with different estimators. Incorporating this information into MCMC sampling has been a challenge. This article introduces a novel MCMC sampling framework called primary space serial tempering, which combines MCMC sampling and MIS. A rendering algorithm, multiplexed Metropolis light transport, is also developed using this framework, resulting in comparable or better performance than more complex MCMC algorithms.
411254	41125478	A new framework for efficient password-based authenticated key exchange	Password-based authenticated key exchange (PAKE) protocols enable two users with a shared short password to establish a secure session key. These protocols need to be resistant to offline dictionary attacks, where an eavesdropper tries to match a password to observed transcripts. However, there are limited frameworks for constructing PAKE protocols in the standard model. A new methodology is proposed by abstracting and generalizing a protocol by Jiang and Gong, which allows for PAKE without the use of random oracles, in the common reference string model. This approach has several advantages over previous methods and can also be extended to be secure within the universal composability framework. This new protocol is more efficient than a previous one by Canetti et al. when using El Gamal encryption. 
411255	4112557	Accelerating partial-order planners: some techniques for effective search control and pruning	The authors propose several techniques to improve the practicality of well-founded partial-order planners. The first two techniques focus on search control while keeping costs low. One technique involves adjusting the A* heuristic used by ucpop to select plans, while the other prioritizes "zero commitment" plan refinements and uses LIFO prioritization. A more radical technique is the use of operator parameter domains to prune search, which involves computing these domains from operator definitions and initial and goal conditions. Experiments showed significant speedups for a variety of problems, with the hardest problems showing the greatest improvements. The authors provide Lisp code for their techniques and test problems in online appendices. 
411256	41125615	Finite-model theory—a personal perspective	Finite-model theory is a branch of mathematics that focuses on the logical properties of finite mathematical structures. This paper provides a personal perspective on the subject, with the author discussing his own history and interests, particularly stemming from his Ph.D. thesis. Some key topics covered include the differences between model theory for finite and infinite structures, the relationship between finite-model theory and complexity theory, 0-1 laws, and descriptive complexity theory. The paper aims to generate more interest in this field, as there has been a recent resurgence in its study.
411257	41125729	Coimplication and its application to fuzzy expert systems	Two-valued logic, also known as classical logic, assigns extreme truth values of 0 or 1 to a set of propositions based on consistency rules. In contrast, fuzzy logic or multiple-valued logic allows for truth values in the range of [0,1] to represent the degree of truth in a formula. However, many expert systems still primarily use two-valued logic and probability, even though the knowledge base often contains imprecise and vague information. In order to handle these uncertainties, a new approach using the equivalence relation for modus ponens and the concept of coimplication is proposed. This allows for a more accurate approximation of reasoning in fuzzy expert systems.
411258	4112582	The visibility-Voronoi complex and its applications	The VV(c)-diagram is a new hybrid diagram that combines the visibility graph and the Voronoi diagram of polygons. It is useful for planning paths for a robot moving among polygonal obstacles in the plane, with a focus on creating natural-looking paths that are short, smooth, and maintain a certain distance from obstacles. This diagram evolves from the visibility graph to the Voronoi diagram as the clearance parameter increases. Additionally, an algorithm has been developed to preprocess a scene of configuration-space obstacles and construct a VV-complex data structure. This structure allows for efficient planning of motion paths for any start and goal configuration, without explicitly constructing the VV(c)-diagram for each clearance value. The preprocessing time is O(n^2 log n) and the data structure can be queried with a Dijkstra search. A Cgal-based software package has been implemented for computing the VV(c)-diagram and has been used in various applications.
411259	41125978	Prediction and Predictability for Search Query Acceleration.	A commercial web search engine uses multiple servers to process search queries, and the response time is determined by the slowest server. Previous approaches focused on reducing the tail latency (high-percentile response time) of individual servers, but this is not effective for reducing extreme tail latency (99.99th percentile) when responses are aggregated from many servers. To address this, a new design space is proposed which considers when to predict query execution time and whether to override the prediction if predictability is low. By collecting dynamic features at runtime, the proposed framework is able to significantly reduce extreme tail latency and improve server throughput. This is evaluated in two scenarios: query parallelization and query scheduling on a heterogeneous processor, with results showing significant improvements compared to existing predictors.
411260	41126067	Uncovering hidden qualities - benefits of quality measures for automatically generated metadata	Digital libraries are increasingly using semantic techniques in their processes for metadata generation, search, and access. However, the quality of automatically generated metadata using these techniques is uncertain due to their statistical or collaborative nature. To ensure data quality in digital libraries, a user study was conducted to evaluate metrics for quality assessment and their usefulness for individuals during interaction. The study observed the interaction of domain experts in chemistry and presented them with three visualizations of sample semantic techniques, first without and then with quality information. The results showed that quality information is crucial not only for data curation in digital libraries but also for designing user-friendly interfaces for end-users. 
411261	4112610	Automatic task based analysis and parallelization in the context of equation based languages	This paper discusses a method for automatically parallelizing complex task systems with dependencies. It includes techniques for analyzing and representing dependencies, as well as a library-based approach for clustering, profiling, and scheduling tasks. The goal is to simplify the process of parallelizing these systems, which can be challenging and time-consuming. The approach has been implemented in the OpenModelica simulation environment and is able to handle shared memory multi-core and multi-processor systems. The paper also highlights the use of parallelism in mathematical modeling languages and how it can be effectively utilized. 
411262	41126246	Depth reduction for circuits of unbounded fan-in	This content discusses the limitations of constant depth circuits of size nlogO(1)n when using the basis of AND, OR, and PARITY. It is proven that these circuits are not more powerful than circuits of the same size and depth four. This concept is applied to other depth reduction theorems, showing that any set in AC0 can be recognized by a family of depth three threshold circuits with a size of nlogO(1)n. It is also noted that this size bound is the best possible when considering depth reduction over AND, OR, and PARITY. Overall, the results support the idea that most of the limitations of constant depth circuits still hold true.
411263	41126319	Finding temporal order in discharge summaries.	This paper suggests a method for automatically analyzing time-oriented clinical narratives, which would be useful for medical decision making, data modeling, and biomedical research. The proposed approach involves using a robust corpus-based method to identify temporal segments and their ordering within medical discharge summaries. The method considers a temporal segment to be a part of the text that does not have sudden changes in temporal focus. It uses a supervised machine-learning framework that incorporates a variety of linguistic and contextual features to determine temporal order. The results show an 83% F-measure in temporal segmentation and 78.3% accuracy in determining pairwise temporal relations. 
411264	41126452	Distributed computing with rules of thumb	In their research presented at ICS 2011, the authors focus on dynamic environments where computational nodes follow simple and unsophisticated rules of behavior, such as "best replying" and minimizing "regret." They investigate the conditions for convergence to an equilibrium point in these environments, specifically when nodes' interactions are not synchronized. This has applications in various fields including routing, congestion control, game theory, social networks, and circuit design. The authors also explore the relationship between their results and nontermination in distributed computing, the impact of scheduling on convergence, and the effects of asynchrony on no-regret dynamics.
411265	4112654	Multifractality in TCP/IP traffic: the case against	The discovery of long-range dependence in LAN and WAN packet data led to further research on multifractal behavior in TCP/IP traffic in WANs. However, the physical mechanisms behind this behavior have not been convincingly proven, leaving the question of whether multifractal traffic models are just a "black box" or if there is something real behind them. This paper examines the evidence for multifractal behavior in aggregate TCP traffic and finds that it is weak. The study also highlights misunderstandings in the literature about the scales at which multifractality has been claimed and explains other pitfalls that have led to an overestimation of the multifractal case. The paper argues for an alternative point process model that accurately reproduces the higher order statistics of the data without being multifractal. Overall, the paper suggests that the empirical evidence for multifractal behavior may be a misinterpretation due to limitations in statistical methodology.
411266	41126631	Evaluating combinations of ranked lists and visualizations of inter-document similarity	The article discusses the use of document clustering ideas in improving the accuracy of ranked lists in interactive systems. The study focuses on evaluating the effectiveness of these systems and constructing them in the best possible way. A user study is conducted to evaluate the combination of clustering and ranked lists in instance-oriented retrieval. Results show that although users prefer the combination, it does not necessarily improve effectiveness. In the second part of the study, a new approach that directly combines ranked lists with inter-document similarity information is developed and evaluated. This approach shows significant improvements in effectiveness and users are able to use the combined information effectively. The article also presents two prototype systems, AspInQuery and Lighthouse, which demonstrate the application of this approach.
411267	41126750	ESCAPE: Efficiently Counting All 5-Vertex Subgraphs.	Subgraph counting is a crucial technique in analyzing networks, especially in fields like bioinformatics and social networks. While triangle counting has been extensively studied, counting 4-vertex or 5-vertex patterns is a challenging task with few practical solutions that can handle large graphs. In this study, a new algorithmic framework is introduced that can efficiently count any small pattern in a graph. By breaking down the pattern into smaller ones and utilizing the graph's degree orientations, the algorithm avoids the combinatorial explosion faced by traditional methods. Only four specific subgraphs (three with less than 5 vertices) need to be enumerated to compute exact counts for all 5-vertex patterns. Empirical experiments show that this algorithm can handle graphs with millions of edges in minutes, making it the first practical solution for 5-vertex pattern counting at this scale. Additionally, a faster method for counting 4-vertex patterns is also introduced, which outperforms current state-of-the-art methods by ten times.
411268	41126842	Non-parametric Jensen-Shannon Divergence.	The problem of quantifying the difference between two distributions is common in machine learning and data mining. However, in many cases, we only have empirical data and do not know the true distributions or their form. This means we must assume a distribution or perform estimation in order to measure their divergence. This can be unsatisfactory for exploratory purposes, as the focus is on exploring the data rather than our expectations. In this paper, the authors propose a non-parametric method for measuring divergence between two distributions using cumulative distribution functions. This eliminates the need for estimation and allows for efficient calculation of divergences directly from data. Empirical evaluation shows that this method outperforms the current state of the art in detecting differences between distributions.
411269	41126948	Item Sets that Compress	Frequent item set mining faces a challenge of finding the most relevant frequent item sets due to the overwhelming number of results. This is caused by the similarity between large sets of frequent item sets. To tackle this issue, the MDL principle is used to determine the best set of frequent item sets that can compress the database the most. Four heuristic algorithms are proposed and tested, showing a significant reduction in the number of frequent item sets. Additionally, the approach is also effective in determining the optimal value for the min-sup threshold. 
411270	411270116	Integrating Meta-modelling Aspects with Graph Transformation for Efficient Visual Language Definition and Model Manipulation	Visual languages (VLs) are important tools for modeling various aspects of systems. In addition to standard languages like UML, there are also domain-specific languages that are more commonly used when there is strong tool support available. Different types of generators have been developed to create visual modeling environments based on VL specifications. To define a VL, both declarative and constructive approaches are used. The meta modeling approach is declarative, using classes of symbols and relations to define a VL and its properties. Constraints can also be used to further specify the language. Alternatively, a graph grammar approach can be used where graphs describe the abstract syntax of models and graph rules define the language grammar. This paper introduces a new concept of node type inheritance in algebraic graph grammars, which allows for a combination of declarative and constructive techniques in VL definition and model manipulation. Two examples, GENGED and AToM(3), demonstrate how VLs can be defined and manipulated using these techniques.
411271	41127121	Visdok: Ein Ansatz zur interaktiven Nutzung von technischer Dokumentation	Visdok is a multimedia presentation system that combines 3D visualization, animation, and hypertext navigation with a knowledge-based discourse planner in a client-server architecture. It allows for efficient access to existing information through the generation of menus and automatically created indices that are sensitive to the media and context. The Visdok system is particularly useful for product presentations and training purposes. 
411272	411272174	Robust Iris Localization and Tracking based on Constrained Visual Fitting	The article discusses a computer vision algorithm for localizing and tracking the iris. The localization algorithm helps the tracking algorithm by providing multiple starting points in case of tracking failure. The tracking is done using a robust method for fitting ellipses with added search constraints, resulting in improved accuracy compared to traditional methods. Experimental results show that the algorithm is quick, precise, and robust, making it suitable for human-machine interaction applications, especially for individuals with severe motor disabilities. 
411273	41127334	Domain Model Based Hypertext for Collaborative Authoring	Domain information models are used to represent the structural and semantic characteristics of information in different domains. This chapter discusses the creation of a domain model-based hypertext system and the benefits it offers. Experiences show that using semantic nets for domain-based hypertext can improve organization and maintain consistency in collaborative environments. This type of system, with an active management document, can also aid in coordinating authoring teams. Additionally, the structured hypertext feature allows for the automatic construction of HTML, making it applicable to the World Wide Web. The paper suggests methods for structuring and managing collaborative hypertext that can be directly applied to the web. 
411274	4112746	A “Joint+Marginal” Approach to Parametric Polynomial Optimization	We consider polynomial optimization problems on $\mathbb{R}^n$ with a parameter set $\mathbf{Y}$, and assume that all moments of a probability measure $\varphi$ on $\mathbf{Y}$ can be computed. We then present a series of semidefinite relaxations that converge to a probability measure encoding information about all global optimal solutions as the parameter varies. This allows for the approximation of various functional properties of the optimal solutions, such as their mean and persistency. Additionally, using the dual semidefinite relaxations, we can provide polynomial or piecewise polynomial lower approximations with $L_1(\varphi)$ or $\varphi$-almost uniform convergence to the optimal value function.
411275	41127572	Towards shorter task completion time in datacenter networks.	Datacenters are crucial for modern commercial operations and scientific applications, but existing scheduling frameworks are limited by only considering task or flow level metrics. This leads to suboptimal performance for delay-sensitive applications. To address this, the paper proposes TAFA (Task-Aware and Flow-Aware), which combines both metrics while avoiding their individual problems. This approach has been shown to significantly improve performance, reducing task completion time by over 35% in existing data center systems.
411276	41127629	Pegasus: coordinated scheduling for virtualized accelerator-based systems	Heterogeneous multi-core platforms, which consist of both general purpose and accelerator cores, are becoming more common. However, operating systems still view accelerators as specialized devices, making it difficult for applications to utilize all cores freely. The Pegasus system described in this paper offers a uniform resource usage model for all cores on such platforms by operating at the hypervisor level. Its scheduling methods efficiently share accelerators across multiple virtual machines, making them first class schedulable entities. An implementation of Pegasus using NVIDIA GPGPUs and x86-based host cores shows improved performance for applications by better managing resources. This results in performance improvements ranging from 18% to 140% over traditional GPU scheduling methods.
411277	41127721	Multishift Variants of the QZ Algorithm with Aggressive Early Deflation	The article discusses new variants of the QZ algorithm for solving generalized eigenvalue problems. These variants include an extension of the small-bulge multishift QR algorithm, which allows for the use of level 3 BLAS operations and efficient utilization of high performance computing systems. The authors also propose an extension of the aggressive early deflation strategy, which can identify and deflate converged eigenvalues earlier than traditional methods. Additionally, a new deflation algorithm for infinite eigenvalues is presented. These developments result in a significantly improved QZ algorithm implementation, as demonstrated by numerical experiments with random matrix pairs and real-world applications.
411278	41127847	Fcmac-Byy: Fuzzy Cmac Using Bayesian Ying-Yang Learning	The cerebellar model articulation controller (CMAC) is a neural network model known for its fast learning and simple computation. However, its rigid structure limits its ability to approximate certain functions. To address this issue, a new neural fuzzy CMAC called FCMAC-BYY is proposed. It uses Bayesian Ying-Yang (BYY) learning to optimize fuzzy sets and a truth-value restriction inference scheme to determine rule weights. Inspired by the Chinese philosophy of Ying-Yang, the FCMAC-BYY has better generalization ability, reduced memory requirements, and intuitive reasoning. Experiments on benchmark datasets show that it outperforms existing techniques.
411279	4112791	Web browser accessibility using open source software	A Web browser aims to provide a consistent user experience for accessing different types of information, but achieving universal accessibility and interactivity is still a long-term goal. Existing browsers only offer partial solutions for this, and integrating open source and free software components can be challenging due to their diverse implementation environments and incompatible standards. To address this, a middleware infrastructure called AMICO:WEB has been developed to enable access to a variety of components within a mainstream Web browser. This infrastructure allows for interoperability between different extension mechanisms and bridges the semantic differences between high-level Web APIs and low-level device-oriented APIs. Two scenarios are used to demonstrate the design decisions and benefits of AMICO:WEB for Web accessibility.
411280	41128039	Visualization of high-dimensional model characteristics	Inductive learning techniques are effective for creating explanatory models for large and complex data sets. However, these models can be challenging for users to interpret. To address this issue, a set of visualization methods has been developed to help users evaluate the quality of these models, compare different models, and identify areas for improvement. These methods include techniques such as high-dimensional data space projection, variable/class correlation, instance mapping, and model sampling. The effectiveness of these techniques is demonstrated through their application to various models built from a census data set. These visualizations aid in understanding and utilizing these complex models for better data analysis.
411281	41128160	On the bandwidth of the plenoptic function.	The plenoptic function (POF) is a useful tool for understanding problems in image and video processing, vision, and graphics. It is particularly helpful for image-based rendering, where the POF is used for sampling and interpolation. To accurately characterize the POF, it is important to consider the bandwidth. In this study, a model is used where band-limited signals are applied to smooth surfaces. It is found that the POF is not band limited unless the surfaces are flat. Rules are then developed to estimate the essential bandwidth for this model, taking into account factors such as maximum and minimum depths, maximum frequency, and maximum surface slope. With a multidimensional signal processing approach, this study can verify other important findings in POF processing and determine necessary sampling rates.
411282	41128257	Hardware-Accelerated Parallel-Split Shadow Maps	Shadow mapping is a commonly used technique for creating realistic shadows in real-time applications. However, it suffers from issues with aliasing, which can result in lower quality shadows. To address this, a new method called parallel-split shadow maps has been introduced. This method splits the view frustum into sections and generates a shadow map for each section, resulting in higher quality shadows in complex and large scenes. A fast and effective method for splitting the frustum is proposed, which reduces aliasing over the depth range. Additionally, hardware-accelerated processing is used to improve efficiency and eliminate extra rendering passes compared to standard shadow mapping.
411283	41128370	Property-driven design for swarm robotics	The paper proposes a new method, called property-driven design, for developing collective behaviors in swarm robotics systems. This method aims to overcome the limitations of the traditional code-and-fix approach, which can be time consuming and relies heavily on the designer's expertise. Property-driven design involves formally specifying desired properties for the system and using an iterative process to develop a model that satisfies these properties. The model is then implemented in simulation and on real robots. This approach helps to minimize the risk of developing a system that does not meet the desired properties and promotes the reuse of models across different hardware. The paper also presents a specific application of this method using Discrete Time Markov Chains and Probabilistic Computation Tree Logic. The method is demonstrated in the design and development of a swarm robotics system for aggregation. 
411284	41128419	A sequential Bayesian approach to color constancy using non-uniform filters	This paper introduces a new method for improving Bayesian color constancy by incorporating non-uniform filter measurements into the traditional technique. The non-uniform filter, with varying spectral sensitivity, is placed on the camera lens and the sensor measurements are sequentially integrated into the Bayesian probabilistic formulation. The main goals of this paper are to present a framework for incorporating non-uniform filter measurements and to demonstrate that these additional measurements can reduce the impact of the prior in Bayesian color constancy. The proposed method is tested using a filter with two different spectral sensitivities and is shown to outperform previous approaches in experiments on real data.
411285	41128546	Entropy-based gaze planning	The paper discusses an algorithm for identifying known objects in an unstructured environment through a monochrome television camera attached to a mobile observer. The algorithm utilizes an entropy map to guide the observer along an optimal trajectory that minimizes ambiguity and data collection. The recognition process is based on optical flow signatures generated by camera motion, which can be ambiguous. Gaze planning is used to improve discriminability, and a sequential Bayes approach is used to handle remaining ambiguity by accumulating evidence over time. Results from an experimental recognition system demonstrate the effectiveness of the algorithm on a variety of common objects.
411286	41128653	Minimizing counterexample with unit core extraction and incremental SAT	K Ravi has proposed a two-stage algorithm for minimizing counterexamples by eliminating irrelevant variables. However, the second stage of the algorithm, called BFL, is time-consuming due to its reliance on a SAT solver for each candidate variable to be eliminated. To address this issue, a faster algorithm has been proposed that uses unit core extraction and incremental SAT techniques. This allows for the elimination of multiple variables at once and avoids repeating searches in similar instances. Theoretical analysis and experiments have shown that this new approach is significantly faster than K Ravi's algorithm while still effectively eliminating irrelevant variables.
411287	41128776	Rate estimation for CABAC with low complexity	CABAC is an important technology in H.264/AVC for improving coding efficiency. However, its use in rate-distortion optimized encoding requires a lot of computations, particularly in the rate calculation for choosing the best coding mode. To address this issue, a fast rate estimation method for CABAC was proposed in this paper. The method utilizes two equations based on the probability state index and the match between the most probable symbol and the current binary to be coded. Simulation results showed that the proposed method can achieve significant run-time savings without any negative impact or additional memory requirements. Specifically, the method achieved an average run-time saving of 19.27% (up to 22.46%). 
411288	41128816	Finite-State Models for Computer Assisted Translation	Current automatic translation methodologies are not capable of producing high quality translations. However, certain techniques based on these methodologies can improve the efficiency of human translators. One such technique is the use of finite-state transducers, which have been successful in tasks like speech and handwriting recognition, and domain-specific machine translation. These models have advantages such as being able to learn from bilingual data and having efficient algorithms for parsing translations. A new approach called interactive search has been proposed, where the user can input a prefix of the desired translation and the system will produce the full translation in real-time. This approach has been tested on a corpus of printer manuals and has shown promising results, with human translators only needing to type 25% of the characters in the final translation. 
411289	4112894	Undecidability Results for Database-Inspired Reasoning Problems in Very Expressive Description Logics.	The field of knowledge representation is drawing inspiration from database theory, specifically in the areas of description logics and ontology languages. Interest has shifted from satisfiability checking to query answering, using query notions from databases such as conjunctive queries and path queries. The finite model semantics is becoming a viable alternative to the traditional unrestricted model semantics. This paper investigates database-inspired reasoning problems for highly expressive description logics, all featuring inverses, counting, and nominals. These problems involve role paths of unbounded length, resulting in non-locality. Undecidability has been established for all cases, including undecidability of finite entailment of unions of conjunctive queries for a fragment of SHOIQ and conjunctive queries for a fragment of SROIQ, both commonly used in ontology languages like OWL.
411290	41129026	Complex Pipelined Executions in OpenMP Parallel Applications	This paper introduces the idea of using directives in the OpenMP programming model to handle complex pipelined computations. These directives specify the order in which tasks should be executed, which is based on a name space that identifies the work being distributed by work-sharing constructs. This approach eliminates the need for programmers to manually create synchronization data structures and insert explicit synchronization actions in the code, which can make it difficult to understand and maintain. The compiler and OpenMP runtime library handle this process automatically. The proposal is demonstrated and evaluated with a multi-block example and is supported by the NanosCompiler for OpenMP.
411291	41129171	Opinion graphs for polarity and discourse classification	The article discusses the construction of opinion graphs that incorporate both opinions and discourse relations. This allows for a more comprehensive interpretation of text, as the graphs consider the interplay between opinions and discourse. By using this approach, polarity classification and discourse-link classification can be enhanced and improved. This inter-dependent framework has the potential to improve the performance of local polarity and discourse-link classifiers.
411292	41129281	Modified genetic algorithm-based feature selection combined with pre-trained deep neural network for demand forecasting in outpatient department.	The study proposes a novel approach for forecasting demand for key resources in a hospital's outpatient department (OPD) using a combination of a modified genetic algorithm (MGA) for feature selection and a pre-trained deep neural network (DNN). The approach is applied to real data from a hospital in China and is found to outperform other feature selection methods and demand forecasting models. The use of MGA and pre-trained DNN can provide valuable insights for staff scheduling and resource allocation in the OPD. This approach can be a useful tool for demand forecasting in hospitals and can improve the efficiency and accuracy of feature selection and forecasting compared to traditional methods.
411293	41129312	Methodology for Performance Evaluation of the Input/Output System on Computer Clusters	The growing use of high performance computing and its complex scientific applications requires more efficient Input/Output (I/O) systems. To ensure efficient use of I/O, this paper proposes a methodology for evaluating I/O performance on computer clusters with different configurations. This evaluation is crucial in understanding how different I/O subsystem setups affect application performance. The approach involves analyzing the I/O system at three levels: application, I/O system, and I/O devices. By varying system configurations and I/O operation parameters, the impact on performance is evaluated considering both the application and I/O architecture. This analysis also identifies factors that can be configured to improve I/O system performance and helps select the best configuration for a specific application.
411294	41129468	A decision support system for hospital emergency departments designed using agent-based techniques	This paper discusses an ongoing project that aims to create a model and simulation to assist hospital emergency department (ED) heads in making informed decisions. The model is an Agent-Based Model consisting of active agents representing people and passive agents representing services and systems. These agents interact using Moore state machines and the model includes the communication system and environment. The simulation, developed using NetLogo, evaluates the benefits of diverting non-urgent patients from the ED to primary care services. This project has the potential to aid ED decision-making and improve patient flow.
411295	4112955	A Branching-Process-Based Method to Check Soundness of Workflow Systems.	Workflow nets (WF-nets) are commonly used to model and analyze workflow systems. One important property of WF-nets is soundness, which ensures that the system is free of deadlocks and livelocks and that each task can be completed. Van der Aalst has proven that the soundness problem is decidable for WF-nets and that it is PSPACE-complete for bounded ones. This means that soundness can be determined by analyzing the reachability graph of the net. However, this approach is hindered by the state explosion problem. To address this issue, this paper proposes an algorithm to generate a finite prefix of the unfolding of a WF-net, known as basic unfolding. This technique can effectively save storage space and a necessary and sufficient condition is also proposed to decide soundness based on basic unfolding.
411296	411296232	A Supervised Learning and Control Method to Improve Particle Swarm Optimization Algorithms.	The paper introduces a new optimization method called adaptive particle swarm optimization with supervised learning and control (APSO-SLC) to improve the performance of traditional particle swarm optimization (PSO) by addressing issues such as difficult parameter setting and premature convergence. APSO-SLC uses strategies from machine learning and control fields to adaptively choose parameters and maintain diversity in the swarm. It treats PSO as a system to be controlled and models it as a dynamic quadratic programming model with box constraints. Recursive least squares with a dynamic forgetting factor is used to estimate the parameters and back diffusion and new attractor learning are implemented to prevent premature convergence. Experiments conducted on benchmark functions show that APSO-SLC outperforms traditional PSO in terms of simplicity, consistency, and overall performance. 
411297	41129750	The importance of trust in software engineers' assessment and choice of information sources	Engineers, specifically systems developers, primarily rely on their colleagues and internal reports for information. This is often explained by the principle of least effort, where engineers choose easily accessible sources rather than high-quality ones. However, this study argues that engineers also prioritize sources with a known or easily determinable trustworthiness. Trust is crucial in assessing the quality of information, as it is a perceived property. In a field study of a software design project, engineers placed more emphasis on quality-related factors when discussing and selecting information sources. They also tend to provide context for unfamiliar sources in conversation. Systems for managing knowledge and sharing expertise should consider these factors in order to accurately assess the credibility of information. 
411298	41129813	Are Digraphs Good for Free-Text Keystroke Dynamics?	Keystroke dynamics research has primarily focused on patterns found in fixed text, such as usernames and passwords. Digraphs and trigraphs have been found to be effective features in this context. However, there is growing interest in free-text keystroke dynamics, where the user can type anything they want. This raises the question of whether digraphs and trigraphs are still effective in this scenario. This paper aims to answer that question and finds that general digraphs and trigraphs are not as effective, and instead, word-specific ones are needed. The study also shows that the typing patterns for some words can vary depending on their position in a larger word. This is the first study to investigate these issues and could inform future research on features for free-text keystroke dynamics.
411299	41129935	Cross-language plagiarism detection	Cross-language plagiarism detection is the process of automatically identifying and extracting plagiarized sections in a multilingual setting. This involves analyzing a suspicious document and retrieving sections from a large, multilingual document collection that are found to be similar. The authors of this paper introduce a comprehensive retrieval process for cross-language plagiarism detection and review state-of-the-art solutions for two important subtasks. They also survey different retrieval models and compare the performance of three specific models: CL-CNG, CL-ESA, and CL-ASA. The evaluation is based on a large test dataset of 120,000 documents in six languages, and the results show that CL-CNG is the most effective model for ranking and comparing syntactically related texts across languages. CL-ESA performs well on arbitrary language pairs, while CL-ASA is best suited for "exact" translations but does not generalize well.
411300	41130017	Why Attackers Win: On the Learnability of XOR Arbiter PUFs.	Physically Unclonable Functions (PUFs) are being developed as a solution for secure storage and hardware authentication. However, vulnerabilities to Machine Learning (ML) attacks have been observed in one type of PUF, called arbiter PUFs. To counter these attacks, PUF manufacturers have shifted their focus to nonlinear architectures, specifically the XOR arbiter PUF with a large number of chains. This paper presents a learning framework for XOR arbiter PUFs, establishing a theoretical limit on the number of chains that can be learned in polynomial time with a certain level of accuracy and confidence. The paper also addresses the issue of noisy responses and concludes that no secure XOR arbiter PUF can currently be manufactured using existing IC technologies.
411301	411301150	Exploring presentation methods for tomographic medical image viewing	This paper discusses the challenges of presenting tomographic medical images on a computer screen due to the limitations of displaying multiple images at once. The focus is on magnetic resonance images and proposing filmless computer presentation methods. The study observes the traditional light screen environment and proposes solutions for meeting requirements in the computer environment. User feedback is obtained to determine feasibility and preferences, revealing three requirement categories: user control of film management, navigation of images and image series, and simultaneous availability of detail and context. The paper also introduces a framework for viewing tomographic medical images and presents solution directions for radiologist feedback. Results support the feasibility of proposed approaches and highlight the importance of presentation in medical imaging systems. 
411302	41130279	Exploring emotions and multimodality in digitally augmented puppeteering	The use of multimodal and affective technologies has raised new research questions in the development of expressive and engaging interactions. Two key challenges in this area are creating seamless multimodal systems for customized performance and content generation, and effectively utilizing emotional cues to create interactive loops. PuppetWall is a multi-user, multimodal system designed for digitally augmented puppeteering. It allows for natural interaction through hand movement tracking, multi-touch display, and emotion speech recognition. The system was evaluated with professional actors to explore expressive speech categories and identify challenges in tracking emotional cues from acoustic features. These findings have implications for the design of affective interactive systems.
411303	41130316	FixMyStreet Brussels: Socio-Demographic Inequality in Crowdsourced Civic Participation.	FixMyStreet (FMS) is a website that allows people to report environmental issues to the government, such as potholes and damaged pavements. This study examines the use of FMS in Brussels, Belgium, by analyzing 30,041 reports since it was established in 2013. The researchers found that there were significant differences in FMS use between the ethnically diverse districts in Brussels. They compared FMS use to sociodemographic indicators and social media data from Twitter. Their analysis showed that crowdsourced civic participation platforms may exclude low-income and diverse communities. These findings highlight the need for more inclusive design of such platforms in the future.
411304	41130468	Textile Interfaces: Embroidered Jog-Wheel, Beaded Tilt Sensor, Twisted Pair Ribbon, and Sound Sequins	Electronic textiles, or e-textiles, aim to merge electronics and computing with fabric. To further this goal, a team has developed a multi-use jog wheel, sound sequins made from PVDF film, and a tilt sensor using a hanging bead, embroidery, and capacitive sensing. In order to enable capacitive sensing over longer distances on the body, they have also created a twisted pair ribbon and tested its effectiveness compared to traditional sensing methods. The team shares their construction techniques and insights gained from this exploration of new e-textile interfaces and construction techniques in their Electronic Textile Interface Swatch Book.
411305	41130564	A near-linear-time algorithm for computing replacement paths in planar directed graphs	The replacement paths problem involves finding the shortest path from a starting point s to an ending point t in a directed graph, while avoiding a specific edge e in the graph. The fastest known algorithm for this problem is to remove each edge in the shortest path and compute the distance from s to t in the modified graph, with a running time of O(mn + n2 log n). This problem has applications in finding k simple shortest paths and in computing Vickrey pricing in distributed networks. A recent article presents a near-linear time algorithm for solving this problem in weighted planar directed graphs, improving the running time of related applications by almost a linear factor. The algorithm uses new ideas and a data structure that supports multisource shortest paths queries in planar directed graphs in logarithmic time. It can also be adapted to handle different variations of the problem, such as finding the actual replacement path or avoiding specific vertices instead of edges. 
411306	41130611	A parallel algorithm for finding a separator in planar graphs	Our algorithm presents a randomized parallel approach to finding a simple cycle separator in a planar graph. This separator has a size of O(√n) and can separate the graph so that the largest part contains at most 2/8 ċ n vertices. The algorithm has a time complexity of T = O(log2(n)) and can be run on P = O(n + f1+ε) processors, with n being the number of vertices, f being the number of faces, and ε being any positive constant. It is based on a sequential algorithm by Lipton and Tarjan, which has a time complexity of O(n). When combined with another algorithm, it can find a BFS of a planar graph in O(log3(n)) time using n1.5/log(n) processors. A variation of the algorithm can also construct a simple cycle separator of size O(d ċ √f), where d is the maximum face size.
411307	4113079	On the locality of distributed sparse spanner construction	The paper presents a deterministic distributed algorithm that can construct a (2k-1,0)-spanner, with an optimal stretch-size trade-off, for an unweighted graph with n nodes in k rounds. If n is not known to the nodes, the algorithm can still construct the spanner in 3k-2 rounds. This algorithm is then used to propose another algorithm that can construct a (1+ε,2)-spanner with O(ε-1 n3/2) edges in O(ε-1) rounds, without prior knowledge of the graph. Lower bounds are also presented, showing that any (randomized) distributed algorithm must take at least k rounds in expectation to construct a (2k-1,0)-spanner of o(n1+1/(k-1)) edges. Additionally, it is shown that any (randomized) distributed algorithm that constructs a spanner with fewer than n1+1/k + ε edges in at most nε expected rounds must stretch some distances by a factor of nΩ(ε). This means that while spanners with O(n1+1/k) edges may exist, they cannot be constructed distributively in a sub-polynomial number of rounds in expectation.
411308	41130828	Evolution of Spiking Neural Controllers for Autonomous Vision-Based Robots	This article presents a series of experiments on evolving spiking neural controllers for a mobile robot using vision-based inputs. The experiments were conducted on physical robots without human intervention, and the results showed that functional controllers were quickly evolved using a simple genetic encoding and fitness function. The study also included a neuroethological analysis of the network activity to understand the functioning of the controllers and the relative importance of individual neurons. Additionally, the controllers were found to be robust to synaptic strength decay, which is commonly seen in hardware implementations of spiking circuits. Overall, the research demonstrates the potential for using evolutionary algorithms to develop efficient and robust controllers for autonomous robots.
411309	41130976	Posi-modular Systems with Modulotone Requirements under Permutation Constraints	The transversal problem involves finding a minimum set of elements in a finite set V, denoted as R, such that a given function f is less than or equal to a modulotone function r for all subsets of V not containing R. This problem is a generalization of the source location problem and external network problem in undirected graphs and hypergraphs. A greedy algorithm can be used to solve the transversal problem if the modulotone function r is 驴-monotone, meaning it satisfies a certain condition. The minimal deficient sets for the transversal problem can also be characterized by a basic tree structure. A fractional version of the problem can also be solved using a similar algorithm.
411310	4113108	Outfix-Free Regular Languages and Prime Outfix-Free Decomposition	An outfix is a string that can be inserted between two parts of another string to create a longer string. A set of strings is outfix-free if none of the strings in the set can be an outfix of any other string. We have developed an efficient algorithm to determine if a regular language is outfix-free, which means it only contains a finite number of strings. This algorithm works for both finite sets of strings and languages represented by finite-state automata. We also explore the concept of prime outfix-free decomposition for these languages and have created a fast algorithm to compute this decomposition, which is unique for every outfix-free regular language.
411311	41131142	Integrating conflicting data: the role of source dependence	Data management applications often require integrating data from multiple sources, which can lead to conflicting values. To ensure high-quality data for users, it is important for data integration systems to resolve conflicts and find true values. This can be challenging, as false values can be spread through copying. In this paper, the authors propose a novel approach that takes into account dependence between data sources to improve truth discovery. They use Bayesian analysis to detect dependence and consider factors such as accuracy and similarity to further improve accuracy. Results from experiments on both synthetic and real-world data show the effectiveness and scalability of their algorithm.
411312	41131261	Joint Segmentation and Recognition of Categorized Objects from Noisy Web Image Collection.	The problem of segmenting categorized objects across a collection of images is addressed in this article. Existing methods assume that all images contain the target object, which may not be the case in noisy image collections gathered from image search engines. To overcome this, a co-training algorithm is proposed which combines automatic object segmentation and category recognition. The object segmentation algorithm is trained on a subset of images with high confidence, while the category recognition model is guided by its results. This approach is shown to be effective on four datasets, including a new dataset with hand-annotated category and segmentation labels. The method outperforms previous techniques and is capable of handling noisy image collections.
411313	41131333	A mathematical model for human flesh search engine	The Human Flesh Search Engine (HFSE) is a phenomenon where individuals use online platforms, such as blogs and forums, to expose personal information of people who have committed perceived wrongdoings. With the rise of the internet, the HFSE has become a powerful tool in uncovering information that would otherwise be difficult to obtain. While previous research has focused on the legal and privacy implications of this tool, this study aims to create a mathematical model to better understand the search process and evaluate the effectiveness of this collaborative intelligence. By viewing the initiator and target of a search as nodes in a social network, the HFSE is modeled as a probabilistic flooding routing algorithm. The model is validated through case studies and simulations, providing new insights into the workings of the HFSE. 
411314	41131418	An image rectification scheme and its applications in RST invariant digital image watermarking	This research paper proposes a method for image rectification that can be used with any image watermarking algorithm to improve its robustness against rotation, scaling, translation, and flipping transformations. The method involves using log-polar mapping (LPM) to detect these transformations and compute the cross-correlation between a template and the LPM of the transformed image to determine the parameters of the transformation. This method is also efficient and cost-effective, with templates that can be compressed. The paper also presents experimental results and comparisons with other filtering methods, demonstrating the effectiveness of the proposed method. Additionally, the paper discusses three potential applications for this rectification scheme.
411315	41131511	Semantics-based software techniques for maintainable multimodal input processing in real-time interactive systems	Maintainability is an important quality requirement for software frameworks, especially in the complex system designs of Real-time Interactive Systems (RISs) used in Augmented, Mixed, and Virtual Reality and computer games. This becomes even more challenging when incorporating AI methods for multimodal interfaces and smart environments. To address this issue, existing approaches focus on establishing technical solutions to support the close temporal and semantic coupling needed for multimodal processing while maintaining a general decoupling principle between software modules. The article presents two key solutions for addressing the semantic coupling issue: a semantics-based access scheme and the specification of effects through semantic function descriptions, both modeled in an OWL ontology. The concepts are demonstrated through a prototypical implementation and illustrated with an interaction example in two application areas.
411316	411316111	Eliminating High-Degree Biased Character Bigrams for Dimensionality Reduction in Chinese Text Categorization	The main challenge in Text Categorization (TC) is the high dimensionality of the feature space. The use of Chinese character bigrams as features often leads to the inclusion of biased bigrams that are not useful for categorizing documents. This paper proposes a criterion for identifying these high-degree biased bigrams and presents two schemes, sigma-BR1 and sigma-BR2, for dealing with them. The first scheme eliminates the biased bigrams from the feature set, while the second replaces them with significant characters. Experimental results show that eliminating these biased bigrams and using the sigma-BR1 scheme is effective in reducing dimensionality and improving TC performance. This approach should be used after performing feature selection using a Chi-CIG score function.
411317	41131759	Efficient Dissection of Bicomposite Problems with Cryptanalytic Applications	This paper introduces a new algorithm called dissection, which can solve a wide range of problems with a bicomposite structure. This algorithm has better time and memory tradeoffs compared to previous algorithms. An example of its use is in breaking multiple encryption schemes with independent keys, where it can find all possible keys in a shorter time and with less memory. The improvement ratio of this algorithm increases as the number of keys increases. It can also be combined with parallel collision search for even better tradeoffs. The dissection technique can also be used to improve attacks on hash functions and solve difficult combinatorial search problems.
411318	41131814	Simultaneous recognition and segmentation of cells: application in C.elegans.	The ability to automatically recognize cell identities is crucial for studying gene expression and regulation, cell lineages, and cell fates at a single-cell level in model animals. However, existing methods often rely on segmented cells and may encounter difficulties in new experimental settings. To address this issue, a new method called simultaneous recognition and segmentation (SRS) of cells has been developed and applied to 3D image stacks of the model organism Caenorhabditis elegans. SRS uses an atlas-guided voxel classification process to accurately recognize cells without relying on additional image features. The method achieved a 97.7% recognition accuracy for a key class of cells, and can be applied to other cell types as well.
411319	41131945	Rapid Construction Of Software Comprehension Tools	This paper discusses the concept of a software comprehension tool that utilizes an abstract software model, various views of the model, and analyses to create a mapping between the model and views. Instead of manually coding these components, the paper suggests using online configuration to set up the analyses, models, views, and their mappings. The paper introduces an architecture for implementing this approach and presents a framework as a proof of concept. Several examples are provided to illustrate the effectiveness and adaptability of this method.
411320	41132050	Application access control at network level	This paper presents a network-level access control mechanism that uses pre-computed encrypted counters called tickets to enforce access control decisions made at the application level. The mechanism works by verifying the presence of a valid ticket in each packet, and killing unauthorized packets. The tickets are not based on user data, which allows for efficient verification in shared media LANs. The mechanism is specifically designed for Internet protocols over Ethernet, and its properties for internetworking and multicasting are also discussed. 
411321	41132152	Empirical evaluation of clone detection using syntax suffix trees	Despite the known negative impact of reusing software through copying and pasting, it is still a common practice in software development. To address this issue, various techniques have been proposed to detect duplicated code, also known as software clones. A recent study compared these techniques and found that token-based clone detection using suffix trees is fast but often yields clone candidates that are not syntactic units. On the other hand, techniques based on abstract syntax trees are more accurate but less efficient. This paper presents a new approach that combines the use of suffix trees and abstract syntax trees to efficiently and accurately detect syntactic clones. The results of a large case study on eight software systems in different application domains support the effectiveness of this approach. The paper also includes additional analysis on Java programs and explores an alternative path using parse trees instead of abstract syntax trees. It also investigates the impact of consistent parameter renaming on recall and precision in clone analyses.
411322	41132247	On random hyper-class random forest for visual classification	Random forest is a popular method for classification tasks due to its effectiveness and generalization capabilities. However, for visual classification problems with high dimensions and a large number of classes, the traditional impurity measurement used in training tree nodes may not be sufficient to capture the strong conditional dependencies among visual attributes. To address this issue, a new method called random hyper-class random forest (RHC-RF) is proposed. This method partitions the entire class space into two hyper-classes and trains a weak learner to discriminate between them. This approach maintains the randomness of traditional random forest while also directly targeting the discrimination of classes. Experimental results on various visual classification tasks demonstrate the superior accuracy, compactness, and robustness of RHC-RF compared to traditional random forest.
411323	41132368	iCoseg: Interactive co-segmentation with intelligent scribble guidance	This paper presents an algorithm for Interactive Co-segmentation, a process of separating a foreground object from a group of related images. Unlike previous approaches which rely on unsupervised methods, this algorithm uses ideas from interactive object-cutout techniques. It allows users to guide the output by identifying the foreground object through scribbles. This leads to simpler and more parallelizable functions, allowing for more images to be processed. To address the issue of user fatigue in examining multiple cutouts, the algorithm includes an automatic recommendation system called iCoseg. This system intelligently suggests where the user should scribble next, resulting in faster and more efficient cutout generation. The paper also introduces a new co-segmentation dataset, the CMU-Cornell iCoseg Dataset, which includes a large number of groups, images, and hand-annotated groundtruth. Experimental results show that iCoseg achieves good quality cutouts with less time and effort compared to exhaustive examination of all cutouts. 
411324	41132412	Semantic interpretation of temporal information by abductive inference	The interpretation of temporal information in a text not only relies on explicit clues found in verbs and adjuncts, but also on general knowledge and assumptions about the world. This theory explains the relationship between verbs, their tenses and adjuncts, and the events and time periods they represent and their relative temporal positions. It is presented in a logical format and is based on concepts from Ness Schelkens et al. The theory can be applied practically using an abductive resolution procedure to extract temporal information from texts.
411325	4113259	Web Document Classification Based on Rough Set	The Vector Space Model is a traditional way of representing Web documents, but it often leads to zero-valued similarity between vectors, reducing the quality of classification. To address this issue, a new approach based on rough set theory is proposed in this paper. The approach utilizes TF*IDF weighting scheme to assign weight values for Web documents, with missing information being supplemented by rough set for incomplete information. By generating tolerance classes in both term space and Web document space, the missing information is complemented and the essential information is extended, resulting in improved classification performance. Experimental results demonstrate the effectiveness of this approach.
411326	41132636	Semi-Supervised Learning for Relation Extraction.	This paper presents a semi-supervised learning approach for relation extraction, which combines SVM bootstrapping and label propagation. It uses a co-training procedure with random feature projection to generate a set of weighted support vectors, and then applies a label propagation algorithm using these support vectors. The method was evaluated on the ACE RDC 2003 corpus and was found to outperform the normal LP algorithm, while also reducing computational burden. This suggests that the proposed method combines the strengths of both SVM bootstrapping and label propagation for more effective relation extraction.
411327	41132769	Instrumenting Network Simulators for Evaluating Energy Consumption in Power-Aware Ad-Hoc Network Protocols	This paper discusses the development of an energy consumption model for ad hoc network protocols in network simulators. By explicitly accounting for low-power radio modes and considering the energy costs associated with different radio states, the model allows for accurate measurement of energy consumption. It can be applied automatically to any layer of the protocol stack. The model is validated through simulations and analytical results, as well as comparison to testbed experiments. The paper also demonstrates the model's usefulness by comparing different protocols in terms of energy consumption. Overall, the model provides a valuable tool for evaluating energy consumption in ad hoc networks.
411328	41132861	Accessible protein interaction data for network modeling. structure of the information and available repositories	In recent years, there has been a significant increase in computational studies focused on understanding molecular biology systems. This has been made possible by the availability of new data derived from experimental proteomics, such as the yeast two-hybrid system and affinity purification methods. These studies have provided insights into the structure and organization of molecular networks and have helped in identifying protein interactions. Other fields like structural biology and computational structural biology have also contributed to this by providing detailed descriptions of protein complexes. However, the storage, manipulation, and visualization of the large amounts of data on protein interactions and networks pose challenges. As a result, several systems and standards have emerged to facilitate the analysis of this information. This review provides an overview of the experimental and computational methods used for studying protein interactions, as well as the databases and standards being developed to aid in the analysis of this data.
411329	41132934	A scalable sparse matrix-vector multiplication kernel for energy-efficient sparse-blas on FPGAs	Sparse Matrix-Vector Multiplication (SpMxV) is a commonly used operation in many scientific and engineering applications. However, traditional computing architectures struggle to efficiently handle the compression formats required for storing sparse matrices, resulting in lower computational throughput compared to dense matrices. To address this issue, a FPGA-based SpMxV kernel is proposed that is scalable and utilizes available memory bandwidth and computing resources efficiently. Benchmarking on a Virtex-5 SX95T FPGA shows an average computational efficiency of 91.85% and a peak efficiency of 99.8%, which is more than 50 times better than Intel Core i7 processors and over 300 times better than NVIDIA GPUs. Additionally, the FPGA kernel achieves higher performance while using only 64 processing elements, resulting in a 38-50 times improvement in energy efficiency compared to CPU and GPU counterparts.
411330	41133034	Improving AR using shadows arising from natural illumination distribution in video sequences	The paper proposes a method for generating realistic shadows of virtual objects in a real video sequence, building upon previous work by Sato, Sato, and Ikeuchi (1999) using a static camera. The method involves calibrating both the video and graphic cameras, addressing false shadows caused by the limitations of the static camera approach, and using camera self-calibration and a "match move" technique for flexible graphic world coordinate system embedding. The method also takes advantage of information from the video sequence to overcome previous limitations. The paper concludes with experimental results from a real video sequence.
411331	411331132	A large-alphabet three-party quantum key distribution protocol based on orbital and spin angular momenta hybrid entanglement.	The orbital angular momentum (OAM) of a single photon can carry multiple bits of information due to its orthogonality. This allows for a high-dimensional Hilbert space to be spanned, increasing information capacity and security. By using a hybrid entangled state of spin and OAM, the Shannon dimensionality can be increased even further. A three-party quantum key distribution (QKD) protocol is proposed, using spatial light modulators (SLMs) and specific phase holograms to modulate the OAM state of photons. This allows for the shared key to be generated among the parties without classical message exchanges. This protocol can also be extended to multiple-party QKD by repeating the same operation for each party.
411332	41133246	Sharing encountered information: digital libraries get a social life	The study examined the sharing of information encountered in everyday reading, along with the traditional focus on sharing intentionally retrieved materials in digital libraries. Through 20 interviews in home and work settings, the researchers found that sharing encountered materials, in the form of paper or electronic clippings, is a significant use for these materials. This practice is widespread among participants and serves a social role beyond simply informing the recipient. The study also discusses the broader spectrum of clipping practices, the function and value of shared information, and the social impact of sharing encountered materials. The researchers suggest moving beyond an email model for sharing and considering the social ties that are strengthened through this practice.
411333	411333121	Process-Centered Software Engineering Environments, A Brief History and Future Challenges	The history of software engineering environments spans about two decades, with early environments focusing on small parts of the software process and later evolving to support the entire process. The concept of process-centered software engineering environments emerged ten years ago, introducing the idea of using a model of the software process as a guide for the environment's behavior. While some aspects of this vision have been realized, others have proven impractical. This article discusses the evolution of software engineering environments, with a focus on process-centered environments. It also explores the trend of distributed software processes and the need for a software process middleware to manage processes across different sites. Other current trends in software process research are also discussed.
411334	4113347	Detecting and resolving unsound workflow views for correct provenance analysis	Workflow views are used to group tasks in a workflow into higher level tasks for easier reuse and provenance analysis. However, if these views are not carefully designed, they may not accurately preserve the dataflow between tasks, causing unsound views. This paper addresses the problem of identifying and correcting unsound views efficiently, with minimal changes. The authors prove that this is a difficult problem and propose two local optimality conditions, along with polynomial time algorithms, to correct the views. Experiments show that the proposed algorithms are effective and efficient, with the strong local optimality algorithm producing better solutions with minimal processing overhead.
411335	41133596	Querying big graphs within bounded resources	This paper discusses the issue of querying large graphs when resources are limited. The goal is to find a way to answer queries with a fraction of the graph, denoted as GQ, which has a size smaller than a given ratio α of the original graph G. The proposed solution is a dynamic scheme that reduces G to GQ. The paper investigates the accuracy of exact and approximate answers obtained from GQ, and develops resource-bounded algorithms for two types of queries: pattern queries with data locality and reachability queries without data locality. The experiments using real and synthetic data show that the algorithms perform well and provide accurate answers, even for small values of α. However, it is shown that obtaining 100% accuracy is not always possible, as it is NP-hard for pattern queries and impossible for reachability queries when α is not equal to 1.
411336	41133650	Experimental Analysis of Guess-and-Determine Attacks on Clock-Controlled Stream Ciphers	This paper discusses Guess-and-Determine (GD) attacks on clock-controlled stream ciphers, specifically focusing on the analysis of irregular clocking. The proposed GD attacks are applied to a typical clock-controlled stream cipher (AA5) and the process complexity is calculated. The attacks assume random clocking of linear feedback shift registers (LFSRs), but the practicality of these assumptions is questioned as clocking is typically determined by internal states. The effectiveness of the GD attacks is evaluated through the implementation of miniature ciphers and comparison with other clock-controlled stream ciphers. The results of this research can be used to inform the design of clock-controlled stream ciphers.
411337	411337125	On the Power of Multidoubling in Speeding Up Elliptic Scalar Multiplication	We discuss methods for efficient computation of elliptic scalar multiplication, specifically focusing on multidoubling techniques. These methods allow for the direct computation of 2kP from a randomly selected point P on an elliptic curve, without needing to calculate intermediate points. Our algorithms are designed for elliptic curves with Montgomery and Weierstrass forms over finite fields with a characteristic greater than 3, using affine coordinates. These techniques are faster than repeated doublings and have been applied to scalar multiplication on elliptic curves, with a 28% and 31% reduction in running time achieved for Montgomery and Weierstrass forms, respectively, for a 160-bit curve over fields with characteristic greater than 3.
411338	41133835	Desiring to be in touch in a changing communications landscape: attitudes of older adults	This paper explores the attitudes of older adults towards keeping in touch with important people. The findings, gathered from three focus groups with participants aged 55 to 81, reveal that older adults value keeping in touch but also see it as something that requires careful management in their daily lives. Communication is viewed as a way to showcase skills and express personality, and is seen differently than the casual interactions facilitated by new technologies. The paper suggests design implications and proposes design concepts for new communication devices based on the themes that emerged. 
411339	41133974	On the Rate of Structural Change in Scale Spaces	The article examines the impact of different regularization parameters on the suppression of image details. It compares the effectiveness of first order Tikhonov regularization, Linear Gaussian Scale Space, and Total Variation image decomposition. The study looks at the squared L 2-norm of the regularized solution and the residual as the parameters vary. It finds that for first order Tikhonov regularization, the regularized solution norm is convex, while the residual norm is not concave. A similar result is seen for Gaussian Scale Space, but may not hold for different parameter values. This suggests that the regularized solution norm is not sufficient for scale selection. Empirical studies confirm that the squared residual norms contain vital scale information.
411340	41134041	Analysis of Two-Dimensional Non-Rigid Shapes	The analysis of deformable two-dimensional shapes is an important issue in various fields such as pattern recognition, computer vision, and computer graphics. This paper focuses on three main problems related to non-rigid shapes: similarity, partial similarity, and correspondence. The authors propose an approach to determine deformation-invariant similarity criteria for shape comparison, using intrinsic geometric properties and the Gromov-Hausdorff distance. They also introduce a method to compute similarity for shapes with similar parts but different overall structures, based on Pareto optimality. Additionally, the paper discusses how the problem of correspondence between non-rigid shapes can be solved as a byproduct of the similarity problem. The authors use generalized multidimensional scaling as a numerical framework for these problems. 
411341	41134160	Self-stabilizing leader election in optimal space under an arbitrary scheduler	SSLE is a self-stabilizing algorithm designed for leader election in a connected unoriented network with unique IDs. It also creates a BFS tree with the leader as the root. The algorithm uses O(logn) space per process and stabilizes in O(n) rounds, even in the presence of an unfair daemon. This makes it efficient and reliable for networks with a large number of processes. 
411342	41134227	Piecewise quadratic reconstruction of non-rigid surfaces from monocular sequences	This paper presents a new method for reconstructing 3D surfaces that are highly deforming, such as a flag waving in the wind, using only a single orthographic camera. The approach is based on tracking a set of feature points on the surface throughout an image sequence. Unlike previous methods that assume a global deformation model with small deviations from a rigid mean shape, this method uses a quadratic deformation model and divides the surface into overlapping patches. These patches are individually reconstructed and then registered together, ensuring that shared points correspond to the same 3D points in space. The results on challenging sequences with strong deformations show that this method outperforms global approaches.
411343	411343116	Vibrotactile haptic feedback for intuitive control of robotic extra fingers	Wearable robots, mainly in the form of exoskeletons, have been designed to augment human capabilities or aid in rehabilitation. However, advancements in technology have led to the development of wearable extra robotic limbs. These extra limbs have been lacking in effective haptic feedback systems, until the development of a robotic extra finger coupled with a vibrotactile ring interface. Through perceptual experiments and a pick-and-place task with ten subjects, it was found that haptic feedback significantly improved performance and was preferred by all subjects. This highlights the importance of haptic feedback in wearable robots for enhanced control and task execution.
411344	41134452	An Authoring Tool for an Emergent Narrative Storytelling System	This paper presents the initial conceptual design of an authoring tool for the FAtiMA architecture, which powers the virtual bullying drama FearNot!. The process of authoring emergent narrative involves designing a planning domain for a virtual character planner, which can be challenging for non-technical authors. After reviewing existing authoring tools, the authors propose a new approach where the author plays through example story lines to gradually increase the knowledge and intelligence of a virtual character. This approach also includes a mixed initiative feature, allowing the author to cooperate with the character planners. The authors intend to implement this design within the FearNot! framework and believe it may provide useful ideas for other interactive storytelling systems. 
411345	41134522	Scalable Distributed Processing of K Nearest Neighbor Queries over Moving Objects	Many applications involving moving objects rely on processing k-nearest neighbor (k-NN) queries, but existing approaches are designed for a centralized setting and struggle to handle the large volume of data and concurrent queries in a distributed setting. To address this, the authors propose a suite of solutions including a new index structure called Dynamic Strip Index (DSI) and a distributed k-NN search (DKNN) algorithm based on DSI. These solutions are implemented on Apache S4 and are shown to outperform three baseline methods in extensive experiments. DKNN is particularly efficient and predictable due to its avoidance of potentially expensive iterations.
411346	4113460	GOLD: a parallel real-time stereo vision system for generic obstacle and lane detection	This paper discusses a system called GOLD that uses stereo vision technology to detect obstacles and lane position while driving, improving road safety. It is based on custom hardware and can detect obstacles of any shape and lane markings in structured environments at a rate of 10 Hz. The system removes perspective distortion from images and uses morphological filters to detect lane markings. The output is displayed on a monitor and control panel for the driver. The system was tested on a mobile laboratory vehicle and proved to be robust in various driving conditions. 
411347	41134749	Energy efficient resource allocation in mobile ad hoc computational grids	This paper discusses the issue of energy efficient resource allocation for interdependent tasks in mobile ad hoc computational grids. These tasks require a lot of data exchange, which consumes energy. Inefficient allocation can lead to increased energy consumption and communication costs, limiting the node's lifespan and possibly causing power failures. The proposal is a hybrid power-based resource allocation approach that considers task dependencies and types, and allocates them to nodes that can be accessed with minimum transmission power. The paper also presents a power-based algorithm to find the closest nodes for task allocation. This approach is less complex than traditional algorithms as it depends on the number of transmission power levels rather than the number of nodes in the grid.
411348	41134881	Agent-Based Approach to Dynamic Meeting Scheduling Problems	Multi-Agent systems are increasingly being utilized to solve complex real-world problems, such as meeting scheduling (MS). MS is challenging due to its distributed and dynamic nature, as well as conflicting user preferences. Existing methods for MS are limited in that they treat it as a static problem, often relaxing constraints and disregarding consistency. To address these shortcomings, a new distributed approach based on the DRAC model is proposed. This approach allows for the relaxation of user preferences while maintaining arc-consistency, and uses localized asynchronous communications to efficiently reach an optimal solution. This approach is scalable and effective in handling strong constraints, making it a valuable solution for dynamic MS problems. 
411349	41134970	Intelligent presentation skills trainer analyses body movement	Public speaking is a challenging task as it is influenced by nonverbal behaviors that are often expressed subconsciously. This paper discusses a study on the nonverbal behaviors of presenters, which was used to develop an intelligent tutoring system. The system uses a depth camera to capture the presenter's bodily characteristics, analyzes this information to evaluate the presentation's quality, and provides immediate feedback through a virtual conference room. The system also allows for control of simulated avatars' reactions based on the presenter's performance. This technology aims to help individuals practice and improve their public speaking skills by receiving real-time feedback.
411350	41135032	Data sparsity issues in the collaborative filtering framework	The need for efficient information filtering on the rapidly growing Web has led to the development of popular techniques in user profiling and Web personalization. This chapter focuses on one such technique - collaborative filtering - and presents an overview of its approaches. The k-Nearest Neighbor (kNN) algorithm, commonly used for collaborative filtering, and the Support Vector Machine (SVM), a state-of-the-art classification algorithm, are compared using different datasets. Results show that the quality of collaborative filtering recommendations is affected by the availability of data. The kNN algorithm is more effective for less sparse data, while SVM may perform better for highly sparse data. Experiments were conducted on standard and real-life datasets, demonstrating the applicability of supervised learning algorithms in collaborative filtering.
411351	411351105	Changes in the brain intrinsic organization in both on-task state and post-task resting state.	Intrinsic functional connectivity is crucial for maintaining the stability and flexibility of the brain. Studies have shown that this connectivity can be influenced by task performance, leading to changes in local spatial patterns and global organization. However, it is still unclear how this connectivity changes during and after a task. To better understand this, a functional MRI experiment was conducted with an active semantic-matching task and two rest periods before and after the task. Results showed that the brain's small-world topology remains robust, but with higher local efficiency and lower global efficiency during the task. The default mode network (DMN) is also found to be engaged during both task and post-task processes, with changes in spatial patterns and nodal graph properties. This study contributes to our understanding of the brain's intrinsic organization and its role in memory and learning.
411352	41135223	Modeling and Mining Domain Shared Knowledge for Sentiment Analysis.	Sentiment classification involves predicting the sentiment polarity (positive or negative) of user-generated content such as reviews and blogs. However, labeling training data for all domains is difficult since there are so many. In this article, we focus on sentiment classification adaptation, where a system is trained on one domain but used on another. One challenge is dealing with the significant differences in data distribution between the source and target domains. To address this, we propose a method that learns and mines domain shared knowledge from different sentiment review domains using a joint non-negative matrix factorization framework. This helps bridge the distribution gap between domains and our experiments show that it outperforms other methods for sentiment classification adaptation.
411353	41135374	A search algorithm for motion planning with six degrees of freedom	The motion planning problem is crucial in the fields of robotics, spatial planning, and automated design. It involves finding a continuous, collision-free path for a moving object in an environment with obstacles. This paper presents an algorithm for solving the three-dimensional Movers' problem, where a rigid polyhedral object with six degrees of freedom needs to move from an initial to a desired configuration. The algorithm transforms the problem into a six-dimensional configuration space and uses three operators to find collision-free paths. The paper discusses the theoretical properties and implementation of the algorithm, including heuristic strategies for evaluating local geometric information. This research was conducted at the Massachusetts Institute of Technology, with support from various organizations. 
411354	41135487	Generalised characteristic polynomials	Multipolynomial resultants are efficient tools for solving systems of polynomial equations or eliminating variables. They are defined as polynomials in m-n+1 variables and can determine if a system has a solution in the algebraic closure of a field. However, they are only exact for homogeneous systems, and can be zero even if there is no affine solution. To address this issue, a projection operator is introduced that is guaranteed to eliminate all components of the system with dimension m-n, including those that are not affine. This operator is based on a generalization of the characteristic polynomial for linear systems. As a result, a method is presented for finding all isolated solution points of a polynomial system in a single-exponential time, even when there are infinitely many solutions.
411355	41135528	Behavior-based search of human by an autonomous indoor mobile robot in simulation	The paper discusses the creation of a behavior-based strategy for an indoor autonomous mobile robot to locate an elderly person living alone in a cluttered home environment. The robot uses a Markov decision process to estimate the possible locations of the person based on a perception of their presence in different rooms at different times of the day. The strategy considers the distance to the destination and the probability of finding the person at that location, as well as the last known position of the person. The robot autonomously navigates to the desired location, avoiding obstacles along the way. A simulated environment with an animated human character has been created to test the effectiveness of the strategy, and the results are promising.
411356	41135633	Vertex-arboricity of planar graphs without intersecting triangles	The vertex-arboricity of a graph is the smallest number of subsets that can be created from its vertices, where each subset forms an acyclic graph. A conjecture by Raspaud and Wang states that for planar graphs without intersecting triangles, the vertex-arboricity is always 2. This paper provides a proof for this conjecture, confirming that the minimum number of subsets needed to partition the vertex set of such graphs is indeed 2. This result has implications for understanding the structure and complexity of planar graphs.
411357	41135736	The “mad cow disease”, Usenet Newsgroups and bibliometric laws	The paper explores the reactions of Usenet News users to "mad cow disease" and examines how the crisis was discussed on the internet. The study collected data from relevant news items for 100 days after the outbreak of the disease. The findings reveal similarities between the characteristics of news items on the internet and those found in scientific literature. This is one of the first attempts to use bibliometric methods to analyze information on the internet. With the vast number of newsgroups available, individuals are able to freely express their thoughts on various subjects, making the internet a valuable platform for studying societal reactions to events like "mad cow disease."
411358	41135834	DC Proximal Newton for Nonconvex Optimization Problems.	Our novel algorithm addresses learning problems with nonconvex loss and regularizer functions by using a proximal Newton approach. It obtains a descent direction from an approximation of the loss function and performs a line search for sufficient descent. Theoretical analysis shows that the algorithm's iterates converge to stationary points of the difference of convex objective function. Numerical experiments demonstrate its efficiency compared to current methods for problems with a convex loss function and nonconvex regularizer. It also performs well in high-dimensional transductive learning problems.
411359	41135991	Relational reinforcement learning with guided demonstrations	Model-based reinforcement learning is a popular approach for teaching robots new tasks, but it often requires extensive exploration and prior knowledge of actions. To address this issue, a new algorithm has been proposed that incorporates teacher demonstrations to learn new domains with minimal exploration and no previous knowledge. These demonstrations help the robot learn new actions and reduce the amount of exploration needed. However, the algorithm is designed to only request demonstrations when they are expected to significantly improve the learning process, as the teacher's time is considered more valuable. Additionally, the algorithm uses rule analysis to identify incomplete parts of the state and provide guidance to the teacher on which actions to demonstrate, minimizing the number of demonstrations needed. Through experiments, it has been shown that this approach can reduce exploration by up to 60% and improve success rates by 35% in various domains.
411360	41136016	The structure of the 3-separations of 3-connected matroids	In short, Tutte defined a k-separation of a matroid as a partition of its ground set with certain conditions. If a matroid has no m-separations for all m less than n, it is considered n-connected. Whitney's work showed that a 1-separation is equivalent to a union of 2-connected components. Cunningham and Edmonds provided a tree decomposition for 2-connected matroids that displays all 2-separations. This paper presents a tree decomposition for 3-connected matroids that displays all non-trivial 3-separations.
411361	41136196	Supervised feature selection via dependence estimation	This article presents a new method for selecting features in machine learning by using the Hilbert-Schmidt Independence Criterion (HSIC) to measure the dependence between features and labels. The approach aims to maximize this dependence in order to identify the most relevant features. This framework can be applied to various supervised learning problems, such as classification and regression, and can be approximated using a backward-elimination algorithm. Results from experiments on both artificial and real-world datasets show the effectiveness of this method in feature selection.
411362	41136266	A Linear-Time Kernel Goodness-of-Fit Test.	The proposed adaptive test of goodness-of-fit has a linear computational cost and uses Stein's method to construct features that distinguish observed samples from a reference model. The false negative rate is minimized and the test has higher efficiency compared to a previous linear-time kernel test under a mean-shift alternative. In experiments, the test performed better than the linear-time test and matched or exceeded the power of a quadratic-time kernel test. In high dimensions and with exploitable model structure, the test outperformed a quadratic-time two-sample test based on the Maximum Mean Discrepancy.
411363	4113636	A cubic kernel for feedback vertex set	This paper presents a polynomial time algorithm for the Feedback Vertex Set problem on unweighted, undirected graphs. The algorithm finds a smaller graph H with a feedback vertex set of size at most k′, if and only if the original graph G has a feedback vertex set of size at most k. The size of H is at most O(k3), which is an improvement over a previous result that had a size of O(k11). The algorithm can also be used as a preprocessing heuristic or as a first step in a fixed-parameter tractable algorithm. Additionally, the algorithm can be made constructive by transforming a minimum size feedback vertex set of H into a minimum size feedback vertex set of G. 
411364	41136410	Using Wikipedia to boost collaborative filtering techniques	The sparsity of data is a major challenge for recommender systems, as it limits their ability to accurately predict user ratings. However, this problem can be overcome by utilizing publicly available user-generated information from Wikipedia. By mapping items to Wikipedia pages and analyzing text and links, we can identify similarities between items and use them to improve the recommendation process and ranking predictions. This method has been shown to be especially useful in cases where ratings are scarce or non-existent. Initial experiments on the MovieLens dataset have yielded promising results.
411365	41136533	Forecasting Run-Times of Secure Two-Party Computation	Secure computation (SC) is a method used in cryptography to allow multiple parties to jointly perform computations while keeping their inputs private. Current methods for evaluating the performance of SC protocols rely on approximations of complexity, but these often fail to accurately predict performance due to differences in measures and constants. This study introduces a performance model (PM) that can forecast run-times of secure two-party computations. Through empirical testing on the problem of secure division, the correctness of the PM is demonstrated. The PM can also be used to choose the most optimal algorithm and cryptographic protocol combination, as well as determine security tradeoffs. This model can aid in designing more efficient and secure protocols.
411366	4113660	Hitting Selected (Odd) Cycles.	The Subset Odd Cycle Transversal (Subset OCT) problem involves finding a k-sized vertex subset that intersects every odd cycle containing a vertex from a given subset of vertices in a graph. This problem is a generalization of the classic Odd Cycle Transversal problem and also includes the well-known Multiway Cut and Odd Multiway Cut problems. Previous solutions to this problem have had a triple exponential dependence on the parameter k, but a new algorithm has been proposed that has a polynomial dependence on k. This algorithm uses a recursive application of generalized important separators to reduce the problem to its standard version.
411367	41136753	A Fast Parallel Algorithm for Selected Inversion of Structured Sparse Matrices with Application to 2D Electronic Structure Calculations	The article presents an efficient parallel algorithm for computing selected components of the inverse of a structured symmetric sparse matrix. These calculations are useful for various applications, such as electronic structure analysis of materials. The proposed algorithm is a direct method that utilizes a block $LDL^T$ factorization. The selected elements of the inverse are found in the nonzero positions of $L+L^T$. The algorithm is organized using the elimination tree associated with the factorization, and synchronization overhead is reduced by passing data level by level using local buffers and relative indices. The efficiency of the parallel implementation is demonstrated by applying it to a two-dimensional Hamiltonian matrix, and its performance is analyzed in terms of load balance and communication overhead. The algorithm shows excellent weak scaling on a large-scale high performance distributed-memory parallel machine.
411368	41136836	Parallel Lagrange--Newton--Krylov--Schur Methods for PDE-Constrained Optimization. Part II: The Lagrange--Newton Solver and Its Application to Optimal Control of Steady Viscous Flows	This paper discusses the use of algorithms to optimize systems governed by partial differential equations (PDEs). In the first part, a Lagrange-Newton-Krylov-Schur method (LNKS) is proposed that uses Krylov iterations to solve the Karush-Kuhn-Tucker system of optimality conditions, with a preconditioner inspired by reduced space quasi-Newton algorithms. The second part focuses on the outer iteration and details how to obtain a robust and globally convergent algorithm. This involves the use of line search methods, a mix of quasi-Newton and Newton algorithms, and continuation techniques. The LNKS algorithm is tested on an optimal flow control problem and is found to have good parallelism and scalability. It is also shown to be significantly faster than reduced quasi-Newton SQP methods, and able to solve previously intractable problems. Implementation details and notation conventions are also discussed.
411369	41136920	Activity analysis in crowded environments using social cues for group discovery and human interaction modeling	This paper presents a new approach for analyzing group activities in crowded scenes. It uses a graph-based algorithm with social cues to identify interacting groups. A novel descriptor is proposed to capture the motion and interaction of people within these groups. The advantage of using social cues is that it helps in better understanding of activities in crowded scenes. The proposed framework is evaluated on two public datasets and outperforms existing methods for group discovery and activity recognition. This approach effectively eliminates dataset contamination and achieves recognition rates comparable to state-of-the-art methods. Overall, the paper demonstrates the effectiveness of using social cues for group activity analysis.
411370	41137060	Network Topology Optimization for Data Aggregation	This paper explores the problem of optimizing data aggregation in data center networks by configuring the network topology. The authors prove that this problem is NP-hard even with only one aggregator. They also compare the approximation ratios of two different algorithms, finding that the Wang et al. algorithm may result in increased aggregation time with higher switch degrees. However, using the longest processing time (LPT) rule for topology configuration can improve this. By combining the LPT rule with the Wang et al. rule, the authors achieve better throughput and reduce total network traffic by up to 90%. Experimental results show that the LPT rule reduces aggregation time by up to 90% and when combined with the Wang et al. rule, total network traffic is reduced by up to 90%.
411371	41137120	Actuator selection for desired dynamic performance	The article discusses two methods for selecting actuators that result in a robot with desired dynamic performance. Dynamic performance is measured by the robot's acceleration and force capabilities, which are determined by the actuator torques. The dynamic capability equations are used to model this relationship in a consistent and physically meaningful manner. The article covers actuator selection for both single and multiple configurations.
411372	41137246	Statistical Debugging Using Latent Topic Models	Statistical debugging is a method that uses machine learning to identify the root causes of bugs in software programs. This is done by using a Delta-Latent-Dirichlet-Allocation model, which models execution traces from failed runs of a program as being generated by two types of latent topics: normal usage topics and bug topics. By also modeling execution traces from successful runs, the model is able to identify weak bug topics that would otherwise go undetected. The model is evaluated on four real programs and is found to produce bug topics that are highly correlated to the true bugs. Domain experts also suggest that this model outperforms existing methods for bug cause identification and may have additional uses in software tasks. 
411373	41137317	Fast Global Minimization of the Active Contour/Snake Model	The active contour/snake model is a popular method for segmenting images by evolving a contour towards object boundaries. However, it has a drawback of getting stuck in local minima, making the initial guess crucial for good results. To solve this issue, a new approach is proposed that unifies three well-known image models and establishes the existence of a global minimum for the active contour model. A practical and efficient method is also introduced for propagating the contour, avoiding the need for frequent re-initialization. This method is tested on different types of images and shows better performance compared to other segmentation models.
411374	41137422	Enforcing Architectural Styles In Presence Of Unexpected Distributed Reconfigurations	ADR is a formal framework used for modelling the development of distributed system architectures. It uses rules to refine ADR graphs and applies logic to these rules, adding pre- and post-conditions. These conditions limit when rules can be applied and specify the resulting graph properties. An algorithm is presented for computing the weakest pre-condition based on a rule and its post-condition. Using this algorithm, a methodology is created for selecting rules at the architectural level to reconfigure a system and restore its original architectural style when unexpected run-time changes occur. 
411375	41137528	The Complexity Of Recognizing Incomplete Single-Crossing Preferences	The complexity of determining if a partial preference profile can be extended to a complete single-crossing profile is studied. This scenario represents situations where we have limited information about voters' preferences and want to know if the given profile follows a single-crossing pattern. When the order of votes is fixed and the input profile is made up of top orders, a polynomial-time algorithm can be used. However, if vote permutation is allowed and the input profile includes weak orders or independent-pairs orders, the problem becomes NP-complete. Certain practical situations of both problems have been identified that can be solved in polynomial time.
411376	41137672	Exploiting query reformulations for web search result diversification	This paper introduces a new method for diversifying web search results when the user's initial query is not specific enough. This probabilistic framework takes into account the different aspects of an underspecified query and diversifies the document ranking accordingly. The effectiveness of this approach is evaluated in the context of the TREC 2009 Web track diversity task, using reformulations from three major search engines. The results show that this framework outperforms other existing methods. Furthermore, by simulating an ideal query reformulation mechanism, the study reveals insights on the effectiveness of query reformulations from different search engines in promoting diversity. 
411377	41137714	Delay compensation in Shared Haptic Virtual Environments	Shared Haptic Virtual Environments (SHVEs) use a client-server architecture, with a physics simulation engine calculating object states based on position information from clients. Clients receive and update their local copy of the environment, with haptic forces computed based on object interactions. However, communication delay can lead to delayed updates and increased interaction forces, causing users to perceive increased object weight. To address this, the paper proposes an adaptive force rendering scheme that reduces object stiffness at clients based on delay, device velocity, and motion constraints to achieve perceptual transparency. Simulations and subjective evaluations show successful compensation for delays up to 150 ms, while preserving the perception of interacting with a rigid object.
411378	41137839	Planar and infinite hypohamiltonian and hypotraceable graphs	Chvatal and Grunbaum have debated the existence of planar graphs with specific properties. Chvatal questioned the existence of planar hypohamiltonian graphs, while Grunbaum suggested they do not exist. However, an infinite class of planar hypohamiltonian graphs has been identified. In addition, an infinite class of planar hypotraceable graphs with connectivity two or three has been described. The study of these infinite graphs has also resulted in the discovery of a new infinite family of finite hypotraceable graphs. This research sheds light on the properties and potential existence of certain types of planar graphs. 
411379	41137966	Efficient Sub-/Inter-Group Key Distribution for ad hoc Networks.	The paper discusses the importance of efficient communication in emerging networks, specifically in ad hoc networks. To ensure secure group communication, group key management has been proposed as a fundamental cryptographic tool. However, existing proposals have limitations in efficiently facilitating subgroup and intergroup communication. The paper introduces two group key distribution schemes that address these issues and do not require user interaction. Storage and computation analyses show that the proposed schemes are secure and efficient compared to existing ones. Furthermore, extensions for multipartite groups are presented, greatly improving efficiency in this scenario. 
411380	41138037	Fast transmission to remote cooperative groups: A new key management paradigm	The problem of broadcasting securely to a remote cooperative group is a common issue in new networks. Overcoming challenges such as limited communication, lack of a trusted key generation center, and dynamic senders is crucial in designing effective systems. Existing key management methods are not effective in addressing these challenges. This paper proposes a new approach that combines traditional broadcast encryption and group key agreement. Each member has a unique public/secret key pair, allowing a sender to securely broadcast to a chosen subgroup. The system is proven secure and has minimal computation and communication costs, even if non-intended members collude. It also allows for easy member changes and rekeying strategies. This scheme is highly secure, efficient, and does not require a fully trusted authority, making it a promising solution for various applications.
411381	41138136	Simulating Non Stationary Operators in Search Algorithms.	This paper presents a model for simulating search operators that have continuously changing behavior during a search. These operators are often classified as either exploitation or exploration operators and their performance decreases when applied in these scenarios. The model is used to compare different operator selection policies and their ability to adapt to the changing behavior of these operators. An experimental study is conducted and reveals interesting results on the performance of operator selection policies in non-stationary search scenarios. This research provides valuable insights on how to effectively select operators in situations where their behavior is not constant.
411382	41138246	Making preferences more active	The paper introduces the concept of pseudo-texts, a way to incorporate richer semantic structures into the Preference Semantics system. These pseudo-texts are consistent with previous accounts of the system and are used to handle extended use, where semantic preferences are broken in sentences. The authors argue that extended use is common in normal language and a language understanding system must be equipped to handle it. They propose a notion of sense projection, which involves altering semantic formulas to adapt to unexpected context. This idea could be implemented in a new semantic parser for the Preference Semantics system, which could serve as a test-bed for the usefulness of frames in language understanding. Overall, the authors suggest that this approach could be more effective than traditional question-answering systems and provide a better understanding of language.
411383	41138312	Hierarchical-granularity holonic modelling	Designing distributed and pervasive intelligent systems, like Multi Agent Systems (MAS), is driven by breaking down the application-specific knowledge into functional components. Changes in problem semantics or granularity levels can greatly affect the system re-engineering process. To address these challenges, a new framework called Hierarchical-Granularity Holonic Model (HGHM) is proposed, which uses holons (agents with special features) for distributed intelligent systems modelling. Holons can be decomposed into other holons, allowing for complex systems to be modelled at different hierarchical-granularity levels. This approach has an advantage over traditional holonic systems and MAS as it directly derives the architecture from the problem ontology. The HGHM is applied in a case study for indoor air quality monitoring systems, showing improvements in system design compared to existing solutions. 
411384	41138449	A radix-8 wafer scale FFT processor	Wafer Scale Integration (WSI) technology offers the potential for significant enhancements in the performance of digital signal processing systems. This paper focuses on the development of a radix-8 systolic fast Fourier transform (FFT) processor, which is designed specifically for implementation with WSI. The use of a radix-8 FFT butterfly wafer, currently in the works, is expected to allow for continuous data rates of 160 million samples per second (MSPS) for FFTs of up to 4096 points using 16-bit fixed point data. This advancement in WSI technology could greatly improve the capabilities of digital signal processing systems.
411385	41138516	Brain-computer interfacing in discriminative and stationary subspaces.	The non-stationary nature of neurophysiological measurements, such as EEG, makes it difficult to accurately classify motion intentions in Brain Computer Interfacing (BCI). Changes in brain processes can lead to unexpected variations in feature distribution, resulting in decreased accuracy. To address this problem, methods have been developed to adapt to changes or extract invariant features. A recent method called Stationary Subspace Analysis (SSA) has been applied to BCI data, which diminishes the impact of non-stationary changes by learning and classifying in a stationary subspace of the data. This paper proposes two extensions to SSA: a variant that can extract stationary subspaces from labeled data without disregarding class-related variations, and a discriminant variant that balances stationarity and discriminativity. Results show that learning in a discriminative and stationary subspace is more effective for BCI than the standard SSA method.
411386	411386229	What is happening right now ... that interests me?: online topic discovery and recommendation in twitter	The Social Web is becoming increasingly reliant on Twitter for real-time information and knowledge about current events. However, with the overwhelming amount of tweets, it can be difficult for users to find relevant and ranked information. To address this issue, the authors propose a new approach that focuses on the individual user's interests rather than the general perspective of what is happening. They introduce a method called RMFO, which creates personalized rankings for tweets based on the user's past interactions. Experiments show that this method outperforms other techniques, demonstrating its effectiveness in improving information discovery on Twitter. 
411387	41138744	Semantic program repair using a reference implementation.	Automated program repair techniques often rely on tests as the criteria for correctness, leading to the problem of overfitting. While various approaches have been proposed to address this, they are not guaranteed to produce patches that work beyond the given tests. This study proposes using a reference implementation to infer a specification of the intended behavior, which is then used to synthesize a patch that enforces conditional equivalence with the reference program. This approach reduces overfitting and allows for the generation of more accurate patches. Additionally, the use of semantic analysis allows for the patch to have a different implementation from the reference program. Experiments on repairing embedded Linux Busybox with GNU Coreutils as the reference showed promising results.
411388	41138885	Dessy: Search and Synchronization on the Move	Smartphones today have the capability to store large amounts of information. As more and more data is stored on mobile devices, the need for effective information organization is becoming increasingly important. To tackle this challenge, people are turning to desktop search tools. With the rise of multiple devices, the synchronization of information between them has become crucial. Dessy is a framework that combines desktop search and synchronization for mobile devices. It allows users to sync search results, individual files, and entire directories. This paper also includes an evaluation of the system's energy usage. Dessy works closely with the Syxaw file synchronizer, which optimizes network usage for efficient file and metadata synchronization. 
411389	41138976	Degree condition and Z3-connectivity	The paper discusses a type of graph called a 2-edge-connected simple graph with 3 vertices, and an abelian group with 3 elements. It introduces the concept of A-reduction, where nontrivial A-connected subgraphs are repeatedly contracted until none are left, resulting in a simplified graph. The paper then proves that for a graph G with maximum degree of either n or 2 where the endpoints are not connected, G is not Z\"3-connected if and only if it is isomorphic to one of twenty two specific graphs or can be reduced to K\"3, K\"4, K\"4^- or G\"5. This result builds on previous work by Luo et al. and Fan and Zhou.
411390	41139071	Hierarchical discriminant analysis for image retrieval	The Self-Organizing Hierarchical Optimal Subspace Learning and Inference Framework (SHOSLIF) is a system for object recognition that uses a hierarchical database structure for image retrieval. It utilizes optimal linear projection for feature derivation and a Space-Tessellation Tree generated using the Most Expressive Features (MEFs) and Most Discriminating Features (MDFs) to achieve a logarithmic retrieval complexity. By recursively deriving a better set of features for each level of the tree, it avoids the limitations of global linear features and improves generalization capability. The system also allows for perturbations in object size and position through learning. This approach has been successfully applied to a large image database with varying real-world objects. The focus of this paper is on the hierarchical partitioning of feature spaces.
411391	411391206	Crowdsourcing label quality: a theoretical analysis	Crowdsourcing has become a widely used method for obtaining labels for large amounts of data. While various methods have been developed for extracting labels from the crowd, there has been little theoretical analysis of this popular human-machine interaction process. This paper presents a theoretical study on the quality of labels obtained through majority voting and shows that the label error rate decreases exponentially with the number of workers selected for each task. The paper also addresses the issue of identifying and removing low-quality workers from the crowd, providing both conservative and aggressive conditions for doing so without eliminating any high-quality workers. 
411392	41139219	Security testing of a secure cache design	Cache side channel attacks are a type of security breach that allows secret information to be leaked through the physical implementation of cryptographic operations. These attacks have gained attention in recent years, as software countermeasures have been found to be insufficient in protecting against them. Secure cache designs, such as Newcache, have been developed to address this issue and have shown to be more efficient in preventing cache side channels. However, these designs have not been extensively tested against various types of attacks. In this paper, the security of Newcache is evaluated using representative classes of cache side channel attacks and it is found to successfully defend against them. However, when specifically crafted attacks are targeted at Newcache, it may succumb to timing attacks due to a vulnerability in its replacement algorithm. This vulnerability is addressed by slightly modifying the replacement algorithm, resulting in an improved and simplified version of Newcache. 
411393	41139341	A New Memory Monitoring Scheme for Memory-Aware Scheduling and Partitioning	The proposed memory monitoring scheme utilizes novel hardware counters to track the gain in cache hits as the cache size is increased, allowing for an accurate estimation of the cache miss-rate for each process under the LRU replacement policy. This information can be used for job scheduling and cache partitioning to minimize overall miss-rates. The collected data can also be used to improve an analytical model of cache and memory behavior, resulting in a more accurate overall miss-rate for a group of processes sharing a cache. This can further enhance scheduling and partitioning strategies. 
411394	41139491	Three-dimensional face pose detection and tracking using monocular videos: tool and application.	The paper discusses a real-time tracker that can simultaneously track the 3-D head pose and facial actions in low quality monocular video sequences. The proposed system has two main contributions: an automatic 3-D face pose initialization scheme using a 2-D face detector and eigenface system, and enhancing the human-machine interaction functionality of an AIBO robot by controlling the orientation of its camera through head pose estimation. The proposed techniques can be applied in various applications such as telepresence, virtual reality, and video games. Experiments on real videos demonstrate the effectiveness and usefulness of the proposed methods.
411395	41139587	Undecidability of Operation Problems for T0L Languages and Subclasses	The article examines the solvability of the operation problem for T0L languages and their subclasses. This refers to determining if applying a specific operation to languages from these families will result in a language that still belongs to the same family. The Lindenmayer language families, which include 0L and T0L languages, are not closed under common operations such as homomorphisms, inverse homomorphisms, intersection with regular languages, union, concatenation, and Kleene closure. The study also looks at the intersection and substitution operations, and finds that, except for Kleene closure, the operation problems for 0L and T0L languages and their propagating variants are not even semidecidable.
411396	41139645	Computing Reputation for Collaborative Private Networks	As collaborative network services become more popular, protecting the shared resources and relationships among participants is crucial. One important aspect is evaluating participant reputation, as access to network resources may depend on it. Previous reputation models, such as Ebay and Sporas, were either too simplistic or too complex to be suitable for privacy concerns. In response, the authors propose a new reputation model based on OWA and WOWA operators. This model allows for private computation using elGamal crypto-system and incorporates user preferences in the reputation calculation. The feasibility of this model is demonstrated in a scenario of a Web-based Social Network.
411397	41139741	An alternative storage organization for ROLAP aggregate views based on cubetrees	ROLAP is a popular approach for data warehousing and decision support applications. It relies on creating summary tables to improve query performance. However, the traditional relational storage implementation of ROLAP views is inefficient and slow. This paper proposes the use of Cubetrees, a compressed and packed storage structure, for ROLAP views. An algorithm is also presented for mapping OLAP views to Cubetrees. Experiments using data from the TPC-D benchmark show that Cubetrees offer significant improvements in storage (2-1 reduction), query performance (10-1 faster), and update speed (100-1 faster) compared to the traditional relational storage approach. This highlights the superiority of Cubetrees as a storage and index organization for ROLAP views.
411398	4113983	Implicit interaction profiling for recommending spatial content	GIS applications often return default maps with standard content when users request area maps. These maps may contain irrelevant information, making it necessary for users to customize them repeatedly. One solution is to ask for explicit input from users before generating a map, but this can be costly and rely on user input. Another solution is to store basic profile information, but this only captures limited customization. A better approach is to create personalized maps that only display the most relevant spatial content based on users' implicit interactions with maps. This does not require any extra effort from the user and allows the system to constantly learn and update their preferences. 
411399	41139995	Morphology analysis of 3D scalar fields based on morse theory and discrete distortion	The study focuses on analyzing and understanding 3D scalar fields using a morphological approach. This involves creating a discrete model of the field by dividing it into a tetrahedral mesh and using Morse theory to identify important morphological features. The researchers measure the distortion of this transformation and develop an algorithm to segment the field and control the number of regions. Experimental results demonstrate the effectiveness of this approach.
411400	41140032	Real-valued evolutionary multi-modal optimization driven by hill-valley clustering.	Model-based evolutionary algorithms (EAs) use an underlying search model to adapt to the features of a problem, such as the connection between variables. However, the performance of these algorithms can suffer when trying to model multiple modes in the fitness landscape with a unimodal search model. The number of modes is often unknown, making it difficult to adapt the search model. This study introduces a simple approach called Hill-Valley Clustering, which adapts to the multi-modality of the fitness landscape by dividing it into niches, with one mode in each niche. This approach, combined with an EA and restart scheme, is compared to other niching methods on a benchmark for multi-modal optimization. Results show that this approach is competitive within the limited budget of the benchmark and outperforms other methods in the long run.
411401	411401100	On the possibility and reliability of predictions based on stochastic citation processes	The author analyzes a statistical model for citation processes, which is a version of a non-homogeneous birth process. The model has been previously studied and demonstrated to be applicable through several examples. However, the practical aspects of predicting future citation rates and the statistical reliability of these predictions have not been addressed. The current study focuses on demonstrating the possibility of accurate predictions and analyzing the statistical reliability using the mean value function of citation processes. Data from 1980 to 1995 for papers published in 1980 and 1991 were used to show that the parameters estimated for earlier time periods can also be applied to more recent years. This model can also be used to validate the choice of citation windows in evaluation studies.
411402	41140297	Navigation techniques for small-screen devices: An evaluation on maps and web pages	The paper discusses three techniques for navigating large information spaces on small-screen devices like PDAs and Smartphones. The first technique, DoubleScrollbar, uses two scrollbars and zoom buttons. The second, Grab&Drag, allows users to directly drag the displayed portion of the information space. The third, Zoom-Enhanced Navigator (ZEN), is an adaptation of the Overview&Detail approach, displaying an overview and detail view of the space. A user study was conducted to compare these techniques in terms of performance and satisfaction for tasks involving maps and web pages. Results showed that ZEN was the most preferred and efficient technique, with the least number of user interface actions and highest accuracy in gaining spatial knowledge. 
411403	41140318	LPCEL Editor: A Web-Based Visual Authoring Tool for Learning Design	Educational Modeling Languages (EMLs) are used to represent learning processes in a formal way. One popular EML is the IMS-LD, and there are many tools available to help users create courses using this specification. However, these tools are limited by the level of expressiveness allowed by IMS-LD. To address this limitation, the LPCEL Editor was created. This visual tool allows for the authoring of courses while maintaining a broad level of expressiveness. This means that users can create more complex and dynamic courses without being restricted by the limitations of IMS-LD compliance. Overall, the LPCEL Editor provides a more user-friendly and versatile option for creating courses using EMLs.
411404	41140422	Note on Upper Density of Quasi-Random Hypergraphs.	In 1964, Erdos showed that for any alpha > 0, a hypergraph with n vertices and alpha(n^l) edges contains a large complete l-equipartite subgraph. This means that a sufficiently large hypergraph with density alpha>0 contains a large subgraph with density at least l!/l^l. In this study, we examine a similar problem for hypergraphs with a weak quasi-random property, and prove that a sufficiently large quasi-random hypergraph with density alpha>0 contains a large subgraph with density at least (l-1)!/l^l-1 - 1. This result also implies that every number between 0 and (l-1)!/l^l-1 - 1 is a jump for quasi-random l-graphs. For l=3, this interval can be improved based on a recent result. Specifically, we prove that every number between 0 and 0.3192 is a jump for quasi-random 3-graphs.
411405	41140566	Edge-Unfolding Nearly Flat Convex Caps.	This paper presents a proof that a convex cap, defined as the intersection of a convex polyhedron and a halfspace, can be unfolded into a non-overlapping polygon in the plane. The cap must be nearly flat, with each face normal forming a small angle with the z-axis. The required angle depends on the acuteness gap of the cap, with a smaller angle needed for more acute triangles. Even if the cap is closed by adding a convex polygonal base, it can still be unfolded without overlap. The proof uses angle-monotone and radially monotone curves and results in a polynomial-time algorithm for finding the necessary edge cuts.
411406	41140663	Blind separation of audio signals using trigonometric transforms and Kalman filtering	This paper addresses the issue of separating audio signals from noisy mixtures. Instead of separating the signals in the time domain, the authors propose using a blind separation algorithm on the Discrete Cosine Transform (DCT) or the Discrete Sine Transform (DST) of the mixed signals. This is because both transforms have an energy compaction property, which concentrates the signal energy in a few coefficients and leaves the rest close to zero. Additionally, the authors recommend using Kalman Filtering as a post-processing step for noise reduction. The simulation results demonstrate the effectiveness of this approach and the feasibility of using Kalman Filtering for further noise reduction.
411407	41140716	Latent structured models for human pose estimation	This article discusses a new method for automatically reconstructing 3D human poses from single images using a discriminative approach with latent segmentation inputs. The approach utilizes a pool of figure-ground segment hypotheses and combines learning and inference to predict 3D human articular configurations. The authors also propose new augmented kernels to better encode dependencies between output variables and provide linear re-formulations based on Fourier kernel approximations to improve scalability. The effectiveness of the proposed models is demonstrated through experiments on the HumanEva benchmark and a clip from a Hollywood movie, showing their ability to infer human poses in complex environments.
411408	41140813	Nearest hyperdisk methods for high-dimensional classification	In high-dimensional classification problems, it is often impossible to have enough training samples to densely cover all class regions. This leads to irregular decision boundaries for local classifiers like Nearest Neighbors and kernel methods. To address this issue, a method has been proposed where a convex model is built for each class using the training samples and examples are classified based on their distances to these models. This approach has been studied using various methods such as affine hulls and bounding hyperspheres. The proposed method uses bounding hyperdisks, which are the intersection of affine hulls and the smallest bounding hypersphere, and is shown to be more effective in tightly bounding classes and avoiding overfitting and computational complexity. This method can also be extended to non-Euclidean distance metrics through kernelization and has shown promising results in experiments. 
411409	4114096	Reducing frame rate for object tracking	Object tracking is often used in video surveillance, but sending full frame rate videos is not necessary. This paper presents an analytical framework for determining the critical frame rate needed to successfully track objects using two popular algorithms: frame-differencing-based blob tracking and CAMSHIFT tracking. The paper also explores ways to modify these algorithms to further reduce the critical frame rate. Results show that the frame rate can be reduced by up to 7 times for blob tracking and 13 times for CAMSHIFT tracking, without losing the tracked object. This information can be used to optimize object tracking in video surveillance systems and potentially save on bandwidth and storage costs.
411410	4114106	Automatic Generation of Tailored Accessible User Interfaces for Ubiquitous Services	Egoki is a tool that automatically creates user interfaces for people with disabilities, allowing them to access various services. It uses a model-based approach to determine the best interaction resources and modalities for each user's capabilities. A prototype was tested with accessibility experts and participants with disabilities in two scenarios: one for blind people and one for people with cognitive impairments. The interfaces generated for each scenario were evaluated and found to be operable and accessible. A user evaluation was also conducted, with all participants successfully completing tasks using the tailored interfaces. This demonstrates the effectiveness of Egoki in providing accessible services for people with disabilities.
411411	4114114	Semi-supervised Gaussian process latent variable model with pairwise constraints	Gaussian process latent variable model (GP-LVM) is a popular technique used in unsupervised dimensionality reduction in machine learning. However, when some supervised information is available, the traditional GP-LVM cannot effectively use it to improve its performance. In order to address this issue, a new semi-supervised GP-LVM framework has been proposed, which utilizes pairwise constraints to incorporate supervised information. By transferring these constraints to the latent space, the algorithm can optimize the latent variables using maximum a posteriori (MAP) algorithm. Experiments on different data sets have shown the effectiveness of this approach.
411412	411412118	Teleoperation of Multiple Social Robots	The paper discusses the application of teleoperation, or remote control, of multiple social robots by a single operator. This concept has been extensively studied for search and navigation purposes, but has not been applied to social, conversational robots before. The paper outlines the challenges of remote operation of multiple social robots, including the need for the operator to multitask audibly. A system is proposed that allows a single operator to simultaneously control four robots in conversational interactions, using a control architecture, graphical interface, and a technique called "proactive timing control". Metrics for robot performance are also presented, and experimental results demonstrate the effectiveness of the system.
411413	41141327	Determining cylindrical shape from contour and shading	This paper presents an algorithm that can reconstruct the shape of a cylindrical object using only contour and shading information, without the need for surface albedo or lighting conditions. The algorithm segments the input image into different types of surfaces, such as spherical, cylindrical, or planar, by analyzing local shading. The direction of the generating lines on the cylindrical surface is determined using spatial derivatives in the image. By considering the brightest generating line, the equation representing the relationship between contour shape and shading can be simplified. While there is still one degree of freedom in the solution, the algorithm is able to accurately reconstruct the cylindrical shape (up to reflection). Experimental results using synthetic images are provided.
411414	41141438	Evaluating spreading activation for soft information fusion	A soft-information fusion process refines natural language messages to produce more accurate estimates of soft-information. This information can then be used to retrieve related information from background knowledge sources using contextual cues, known as Context-Based Information Retrieval (CBIR). The performance of the CBIR process relies on the selection of appropriate algorithms and parameters for the given problem domain. This paper evaluates the performance of two spreading activation algorithms and their parameters in a counterinsurgency domain using an f-measure evaluation. The first phase determines how different parameter values affect the algorithms' performance and sets the parameters for future use. The second phase compares the results of the algorithms using the parameter settings learned in the first phase. 
411415	411415121	Toward Socially Assistive Robotics for Augmenting Interventions for Children with Autism Spectrum Disorders	Children with Autism Spectrum Disorders (ASD) often struggle with communication and social interaction, which can make it difficult for them to benefit from therapy and learn social skills. In recent years, research has shown that robots can be effective tools in promoting social behavior in children with ASD. The use of robots in therapeutic settings has led to the development of systems that act as catalysts for social behavior. A pilot experiment was conducted with children with ASD interacting with a socially assistive robot, and the results were promising. This suggests that robots have the potential to improve social skills and enhance the effectiveness of therapy for children with ASD. 
411416	41141645	Steiner tree packing number and tree connectivity.	The paper discusses the concept of S-Steiner trees in a graph G, where S is a set of at least two vertices. These trees are considered edge-disjoint or internally vertex-disjoint if their edges or vertices do not intersect. The maximum number of such trees, denoted by λG(S) and κG(S), are studied for different values of S. Kriesell conjectured that if λG({x,y})≥2k for any x,y∈S, then λG(S)≥k. This conjecture was proven for S with 3 or 4 vertices. The paper provides a concise proof for this conjecture and also establishes a relation between λk(G) and κk(L(G)), where L(G) is the line graph of G.
411417	41141747	Chosen-ciphertext secure multi-hop identity-based conditional proxy re-encryption with constant-size ciphertexts.	Proxy Re-Encryption (PRE) is a method that allows one user to delegate the decryption rights of their encrypted data to another user. However, previous methods such as Multi-Hop Identity-Based PRE (MH-IBPRE) had the limitation of increasing ciphertext size and decryption complexity with the number of re-encryption "hops". In this paper, a new MH-IBPRE scheme is proposed that maintains a constant ciphertext size and computational complexity regardless of the number of hops. This scheme is also bidirectional and supports conditional re-encryption, and is proven secure against selective identity and chosen-ciphertext attacks. Additionally, the scheme can be extended to a set of conditions, making it of independent interest.
411418	41141833	Practice Sharing Paper: Motivating Computer Scientists to Engage with Professional Issues: A Technology-Led Approach	The authors of this paper discuss the challenges of incorporating professional issues modules into a computer science curriculum. These modules, which focus on discursive teaching, can be seen as different from the more typical knowledge and skills-based learning in computer science. The authors have experience teaching these modules and recently faced the challenge of consolidating two separate courses into one smaller course with less face-to-face contact time. This posed a challenge in motivating students who may have specialized in science, technology, and math to engage in a topic that may not directly relate to their chosen field. The paper discusses the motivations for designing the module, the approaches taken by the teachers and curriculum design team, and the impact of this change. The authors hope to encourage students to gain a deeper understanding of their own learning and technological preferences, and take ownership of their professional development.
411419	41141948	Enlarging learnable classes	The paper discusses the concept of Ex-learnable sets, which are sets of functions that can be learned through inductive inference. It is found that these sets are not closed under unions, leading to the question of which classes of functions can be combined with an Ex-learnable set to create another Ex-learnable set. The paper presents a criterion for this to be possible, as well as exploring the extension of learners to potentially learn infinitely more functions. It is shown that learners of non-dense sets of functions cannot be extended in this way, but a different split of learners into two sets allows for effective extension for all learners in each set. Similar concepts are also analyzed for other learning criteria.
411420	41142034	An Agent-Based Platform for Cloud Applications Performance Monitoring	Virtualization in Cloud environments presents challenges when it comes to monitoring resources. To ensure scalability and reliability, applications are distributed across various resources like Virtual Machines and storage. This means that users can only access information about the Cloud infrastructure through monitoring services provided by the Cloud provider, requiring them to trust the provider for performance data. In response, a comprehensive architecture is proposed to cover all monitoring activities in a Cloud application's lifecycle. An agent-based implementation of a specific module is also suggested, offering greater customization and tolerance for network and resource failures. This approach aims to address the challenges of resource monitoring in virtualized Cloud environments.
411421	411421101	Using the mOSAIC's semantic engine to design and develop civil engineering cloud applications	The development of applications for the Cloud requires knowledge of programming models, APIs, and underlying infrastructures provided by Cloud vendors. The European Project mOSAIC aims to simplify this process by creating an API, Platform, and tools that allow for application development and deployment on various Infrastructure as a Service platforms. Within the project, ontologies, a knowledge base, and a Semantic Engine have been developed to assist developers in discovering necessary functionalities and resources in a vendor-independent manner. This paper demonstrates the use of the Semantic Engine, ontologies, and knowledge base in the design and implementation of a Finite Element Method-based application for structural analysis under static loading.
411422	4114225	Quasi-symmetric 2-(31, 7, 7) designs and a revision of Hamada's conjecture	The article discusses the existence of nonisomorphic quasi-symmetric 2-(31, 7, 7) designs with 2-rank 16, in addition to the design formed by planes in PG(4, 2). This disproves a conjecture by Hamada about the relationship between the incidence matrices of designs and their parameters. The five identified designs are extendable to nonisomorphic 3-(32, 8, 7) designs with 2-rank 16, one of which is made up of 3-flats in AG(5, 2). This also shows that designs arising from finite affine geometries cannot be characterized solely by their ranks. Additionally, a quasi-symmetric 2-(45, 9, 8) design is constructed using an extremal doubly even (48, 24) code, resulting in a pseudo-geometric strongly regular graph with parameters (r, k, t) = (15, 10, 6).
411423	41142331	Admission Control for Wireless Networks with Heterogeneous Traffic using Event Based Resource Estimation	The paper introduces a new admission control algorithm for wireless cellular networks that can handle different types of traffic. It estimates the resources needed for handoff calls by considering the probability of visiting specific cells during the call lifetime. This information is shared with surrounding base stations, and the reserved resources are updated during handoffs and call termination. The algorithm also maintains tuning parameters to ensure that handoff dropping probabilities meet certain constraints. The paper also provides QoS bounds for homogeneous traffic. Overall, the proposed algorithm aims to improve resource utilization and maintain quality of service for different types of traffic in wireless cellular networks.
411424	41142483	Learning on Graph with Laplacian Regularization	This article discusses transductive learning on graphs and how it can be improved using Laplacian regularization. The authors derive margin-based generalization bounds by examining the geometric properties of the graph. They also explore the impact of graph Laplacian matrix normalization and dimension reduction on the performance of the learning algorithm. The results highlight a limitation of the commonly used degree-based normalization and propose a solution based on their analysis. Empirical evidence shows that this remedy leads to better classification performance. Overall, this study contributes to a better understanding of transductive learning on graphs and provides practical insights for improving its performance.
411425	41142528	Automatic creation of a reference corpus for political opinion mining in user-generated content	The proposed method aims to automatically create a reference corpus for training text classification procedures to analyze political opinions in user-generated content. This is achieved by compiling a collection of highly opinionated comments from an online newspaper and using manually-crafted rules and a sentiment-lexicon to identify sentences expressing opinions about political entities. The identified opinions are then propagated to other sentences in the comment mentioning the same entities, resulting in a larger and more diverse set of opinion-bearing sentences. The method shows high precision in identifying negative opinions, but lower precision in identifying positive opinions due to issues with irony. Overall, the approach proves effective in creating a reference corpus for training text classification procedures for political opinion mining.
411426	41142632	Dynamic Network DEA approach to basketball games efficiency.	Data Envelopment Analysis (DEA) has been widely used in sports, but there is limited research on its application in basketball. Two approaches have been developed to measure efficiency in basketball: player assessment and team performance. This paper introduces a new approach that focuses on measuring scoring efficiency of both teams in a game, as the number of points scored greatly impacts the appeal of a game. The proposed method, called Dynamic Network DEA, takes into account the performance of each team in each quarter and the carry-overs between quarters. This allows for a scoring efficiency to be calculated for each team in each quarter, overall, and for the entire game. The approach is applied to matches from the 2014-2015 NBA season. 
411427	41142734	Deterministic catalytic systems are not universal	A catalytic system (CS) is a language acceptor with evolution rules of the form Ca → Cv or a → v, where C is a catalyst, a is a noncatalyst symbol, and v is a string representing a multiset of noncatalyst symbols. A CS can be deterministic if there is a unique maximally parallel multiset of rules applicable at each step. It has been an open problem whether deterministic CSs are universal. This article shows that the membership problem for deterministic CSs is decidable, and the Parikh map of the language accepted by any deterministic CS can be effectively constructed. The results also generalize to multimembrane deterministic CSs.
411428	41142831	Network Inference From Co-Occurrences	The discovery of networks is a crucial issue in various fields such as communication, biology, sociology, and neuroscience. However, obtaining complete data to reveal network structure is often challenging or impossible. This paper focuses on inferring network structure from "co-occurrence" data, where the order of network components is not known. The number of possible networks that fit the data grows exponentially with network size, but certain principles suggest that some networks are more likely than others. The paper proposes a model and algorithm based on random walk and permutation to estimate network parameters. For networks with long paths, a polynomial-time Monte Carlo algorithm is suggested for efficient reconstruction. Experiments show promising results.
411429	41142953	Discriminative model selection for object motion recognition	This paper focuses on the challenge of determining the optimal number of components in mixture-type models for trajectory classification using motion vector fields. The authors propose a discriminative criterion that considers the classification accuracy on a separate dataset to choose the best model for each class. The approach takes into account the task of classification and experiments with both synthetic and real data demonstrate its effectiveness in pedestrian activity classification. By incorporating this knowledge into the model selection process, the proposed method improves the performance of mixture-type models for trajectory classification. 
411430	41143010	A Topological Characterisation Of Belief Revision Over Infinite Propositional Languages	Belief revision is the process of updating one's beliefs in light of new evidence. The AGM framework, proposed by Grove in 1988, models belief revision as revising theories by propositions. This framework has been represented using systems of spheres, which have been extended to characterize multiple belief revision operators. However, there are still unresolved issues with this 'spheres' model. This paper introduces a topology on the set of all worlds in a propositional language and uses it to characterize systems of spheres. It also shows that there is a minimal system of spheres contained in all others for each AGM operator, and provides a topological characterization of these minimal systems. Furthermore, the paper proposes a method for extending an AGM operator to a multiple revision operator, and shows that this extension is not unique, answering a previously unresolved problem.
411431	41143118	A note on near hexagons with lines of size 3.	We have determined a classification for all finite near hexagons that have certain properties, including three points on every line and a point at a specific distance from every other point. Additionally, we have found that every pair of points at a certain distance have a specific number of common neighbors. As a result, we have also identified all finite near hexagons that meet these criteria with a specific value for t(2). This information can be used to classify all finite near hexagons with t(2) equal to 4. 
411432	41143234	Enhanced Kinematic Model For Dexterous Manipulation With An Underactuated Hand	Recent studies have focused on underactuated manipulation and have used Kinematic Models (KM) to describe the system by adding external constraints to the standard manipulation analysis method. However, in real-world dexterous manipulation tasks, these external constraints are often violated, resulting in control errors. To address this issue, an Enhanced Kinematic Model (E-KM) has been proposed, which integrates the KM with the Sparse Online Gaussian Process (SOGP). The E-KM can compensate for the shortcomings of the KM by training the SOGP on the residual between the KM's prediction and the ground truth data in real-time. An optimal controller for underactuated manipulations has also been developed based on the E-KM and tested on the iCub humanoid robot. Real-world experiments showed that the E-KM controller achieved higher control accuracy than using the KM alone for a variety of objects.
411433	41143391	An improved multiple birth support vector machine for pattern classification.	Multiple birth support vector machine (MBSVM) is a new machine learning algorithm used for multi-class classification, based on the twin support vector machine (TSVM). It has a faster training speed compared to other multi-class classifiers based on TSVM, especially with a large number of classes. However, MBSVM may not perform well on certain datasets, such as the "Cross planes" dataset. To address this issue, an improved version of MBSVM is proposed, which includes a modified item to minimize the variance of distances between samples of a class and their hyperplanes. This improved MBSVM uses a smoothing technique and an interval-based approach for classifying new samples. Experimental results on artificial and UCI datasets demonstrate the efficiency and good performance of the proposed algorithm.
411434	411434126	An Investigation on LTE Mobility Management	In LTE networks, the Mobility Management Entity (MME) is responsible for managing the movement of user equipment (UE). The MME is connected to a group of evolved Node Bs (cells) that are grouped into Tracking Areas (TAs). These TAs are further organized into TA Lists (TALs). When a UE moves to a new location, it reports this to the MME. If the network needs to connect to the UE, the MME will ask the cells in the TAL to page the UE. This paper explores the effectiveness of LTE paging and offers recommendations for the best sequence to page cells in.
411435	41143515	A process for continuous validation of self-adapting component based systems	This paper proposes a method to incorporate time-related stochastic properties into a continuous design process using runtime models. Time-related specifications of services are crucial in component-based architectures, especially in distributed and volatile networks. The models at runtime approach simplifies managing these architectures by keeping abstract models of the architecture in sync with the physical execution platform. For self-adapting systems, predicting delays and throughput is essential for making adaptation decisions and accepting changes that meet time specifications. The approach includes a new metamodel extension using stochastic Petri nets for internal time modeling and a library of patterns to specify and predict common time properties. The prediction engine can perform in real-time and validate the models, making synchronization of behaviors and structural changes easier.
411436	41143629	Convex enclosures for the reachable sets of nonlinear parametric ordinary differential equations	This article discusses the use of convex enclosures to compute reachable sets for nonlinear parametric ordinary differential equations. These equations are commonly found in nonlinear control systems, and being able to accurately calculate reachable sets is important for control and verification purposes. The proposed method involves computing convex and concave approximations of the ODE solutions based on the parameters, which are then used to create a convex enclosure of the reachable set. This enclosure is expressed as an infinite intersection of halfspaces, allowing for the computation of a convex polyhedral enclosure by considering a finite number of these halfspaces. The approach involves solving convex dynamic optimization problems, ensuring accuracy even for nonconvex reachable sets. 
411437	411437135	Improved Tardiness Bounds for Global EDF	The Earliest Deadline First (EDF) scheduling algorithm is not optimal for global scheduling on multiprocessor platforms. Previous studies have found bounds for the maximum tardiness of implicit-deadline sporadic tasks under global EDF scheduling, but these bounds are not accurate. This paper presents a new algorithm that can calculate better tardiness bounds for each task individually, rather than a single bound for the entire system. This algorithm is especially useful for task systems with diverse parameters, as it yields significantly improved bounds compared to previous methods. Additionally, the algorithm includes a simple test for verifying if a task system can be scheduled by global EDF without violating the maximum tardiness constraints.
411438	41143843	Approximability and Fixed-Parameter Tractability for the Exemplar Genomic Distance Problems	This paper presents a survey of approximability and fixed-parameter tractability results for three Exemplar Genomic Distance problems: exemplar breakpoint distance, exemplar non-breaking similarity, and maximal strip recovery. The results discussed in the paper pertain to the simplest case of only two genomes, each with one sequence of genes. It was shown that the exemplar breakpoint distance problem is NP-hard and does not admit any approximation or FPT algorithm, even when a gene appears at most twice in a genome. The exemplar non-breaking similarity problem is also hard to approximate and is W[1]-complete. The maximal strip recovery problem was recently proven to be NP-complete, but also has a factor-4 approximation and a simple FPT algorithm with a running time of O(22.73k n + n^2).
411439	41143923	QNoC: QoS architecture and design process for network on chip	In this text, we learn about the concept of Quality of Service (QoS) and its application in Systems on Chip (SoC). The authors propose a cost model for communication in SoCs and use it to design a Network on Chip (NoC) architecture. The inter-module communication traffic in SoCs is divided into four classes: signaling, real-time, RD/WR, and block-transfer. By analyzing the communication traffic, the authors determine the QoS requirements for each service class. They then modify a generic network architecture to create a customized Quality-of-Service NoC (QNoC) for the target SoC. This customization process includes minimizing network cost, optimizing placement of modules, removing unnecessary links and switches, and balancing link utilization. The end result is a low-cost QNoC that meets the QoS requirements of the target SoC.
411440	41144039	Time Optimal Synchronous Self Stabilizing Spanning Tree	This research introduces a time-optimal self stabilizing algorithm for constructing a synchronous distributed spanning tree. Previous algorithms had a stabilization time complexity of O(diameter), assuming that a larger message could be sent through each link in one time unit. However, the current algorithm stabilizes in O(diameter) time even with standard message sizes and without prior knowledge of the network size or diameter. The algorithm also does not depend on a given upper bound on the network size. Additionally, a new self stabilizing silent phase clock algorithm is presented, which is optimal in time. This research has potential applications in various distributed global tasks, such as distributed reset.
411441	4114413	ON THE SUPPORTS OF THE WALSH TRANSFORMS OF BOOLEAN FUNCTIONS	 supports of Boolean functions This paper focuses on the structure of subsets of Fn2 that can be the Walsh supports of Boolean functions. These functions are important in cryptography for designing hash functions and ciphers. The Walsh transform is a key mathematical tool for studying these functions, as it measures the correlation between a Boolean function and all linear functions. The Walsh support of a Boolean function plays a crucial role in determining its cryptographic properties, such as resiliency and the existence of covering sequences. The paper also discusses the relationship between covering sequences and other important criteria for cryptographic Boolean functions, such as degree, non-linearity, and propagation criterion. While there are some known examples of subsets that can and cannot be Walsh supports, there is still limited knowledge on the overall structure of these supports. The paper presents new results in this area and provides an overview of what is currently known. 
411442	41144215	Bayesian fuzzy clustering of colored graphs	The growing availability of interaction data from various fields highlights the importance of mining and understanding underlying graph structures. This includes data with different node types, represented by node color. To cluster nodes into communities, an unsupervised approach is used, where nodes of the same color are highly connected within but sparsely connected to the rest of the graph. A fuzzy extension of this clustering concept has been proposed, allowing nodes to have membership in multiple clusters. Two unresolved issues were the determination of cluster numbers and the integration of prior information. These issues are addressed by reinterpreting the factorization in a Bayesian framework, allowing for the inclusion of priors and automatic estimation of group sizes using automatic relevance determination. This approach is illustrated on a toy and protein-complex hypergraph, showing significant enrichment of distinct gene ontology categories in the resulting fuzzy clusters.
411443	4114438	Compact Signed-Digit Adder Using Multiple-Valued Logic	The article discusses the challenges of interconnect complexity in future VLSI chips and how they can be solved using multivalued logic. The use of multiple-valued signals requires fewer interconnecting wires and can increase bandwidth. The paper introduces a new design for a signed-digit adder that utilizes multiple-valued logic and a combination of resonant-tunneling diodes and MOS transistors. This design allows for compact circuits and the use of current-mode logic for efficient addition of multiple signals. The proposed circuit eliminates ripple-carry effects and was verified through simulation and a prototype built using a standard 2-micron CMOS process. Although current technology does not integrate RTDs and MOS devices, there are efforts to develop such technology for their combined advantages.
411444	411444243	Rate-distortion hint tracks for adaptive video streaming	The article introduces a new technique for adaptive video streaming called rate-distortion hint track (RDHT). This method involves storing precomputed characteristics of a compressed media source that are important for online streaming but difficult to compute in real time. It allows for low-complexity adaptation to variations in transport conditions such as data rate and packet loss. The RDHT-based streaming system has three components: information summarizing R-D attributes, an algorithm for predicting distortion, and a method for determining the best packet schedule. The article also presents distortion chain models which accurately predict distortion for various packet loss patterns. Experimental results show that the proposed techniques outperform traditional low-complexity streaming systems for both data rate and packet loss adaptation.
411445	41144573	A Comparison of the LERS Classification System and Rule Management in PRSM	The LERS classification system and rule management in probabilistic rough set models (PRSM) are compared in terms of rule interpretation, quantitative measures, and conflict resolution when classifying new cases. The PRSM uses positive and boundary regions to interpret probabilistic rules, while LERS associates rules with different quantitative measures. These measures reflect different aspects of rules. Additionally, the rule conflict resolution method used in LERS can also be applied to PRSM. 
411446	41144641	Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach	Recent research has shown that domain adaptation techniques can be beneficial in computer vision tasks. In this study, a new method called Domain Selection Machine (DSM) is proposed for recognizing events in consumer videos. This method utilizes a large number of loosely labeled web images from various sources to train a set of SVM classifiers, referred to as source classifiers. A parametric target decision function is then used to combine static SIFT features from web images/video keyframes and spacetime (ST) features from consumer videos. To select the most relevant source domains, a new data-dependent regularizer is introduced in the objective of Support Vector Regression (SVR). An alternating optimization algorithm is also developed to solve the target decision function and select the most relevant source domains. The proposed DSM method outperforms existing methods by up to 46.41% in real-world experiments on three datasets.
411447	41144739	Reconstructing a 3D Line from a Single Catadioptric Image	The paper discusses a method for estimating the equation of a 3D line in axial non-central optical systems using only four points from a single image. This is due to the lack of vantage point and is supported by a result in enumerative geometry. The algorithm for reconstructing the equation is based on computing the Singular Value Decomposition (SVD) of the matrix of Plücker coordinates of the four corresponding rays. The paper also addresses the limitations of the method, such as when the four rays are nearly coplanar, and presents experimental results using a spherical catadioptric camera. The impact of calibration and numerical errors on the reconstruction algorithm is also discussed.
411448	41144823	Fast Individual Facial Animation	This article discusses a rapid method for creating personalized face models and generating facial animations. A frontal face image is captured and key points are automatically detected. These points are then used to deform a generic model into an individual model using RBF network. The texture is generated from the same image and texture coordinates are interpolated. The original motion vectors are then transferred to the individual model in real time. This process can be done with minimal or no manual adjustments and has been shown to be effective for various applications. Overall, this approach offers a fast and efficient way to generate personalized face models and animations.
411449	41144937	Relax, but don't be too lazy	This paper discusses different types of algorithms for expanding the product of two formal power series, f and g. The two main types of algorithms are zealous, which first expands f and g and then truncates at a certain order, and lazy, which gradually computes the coefficients of f, g, and h, only using the necessary computations at each stage. The paper introduces a new type of algorithm, called relaxed, which balances the advantages and disadvantages of zealous and lazy algorithms. The paper also surveys various algorithms for manipulating formal power series and provides theoretical complexity bounds and benchmarks for these algorithms. It is intended for both those interested in the latest algorithms and those looking to implement a power series library.
411450	41145026	Modelling and multi-objective optimization of quality attributes in variability-rich software	Variability-rich software, like software product lines, offers optional and alternative features to meet the diverse needs of users. However, designers face the challenge of understanding how selecting these features affects the quality of the resulting software variant. To address this, attributed feature models have been proposed, but current variability modelling languages and tools do not fully support them. This paper introduces ClaferMoo, a language and tool that addresses these limitations. ClaferMoo uses type inheritance to organize feature attributes and allows for multiple optimization goals. The tool was evaluated on various attributed feature models, demonstrating its ability to handle small-scale models quickly.
411451	4114510	Predictive blacklisting as an implicit recommendation system	The use of blacklists is a popular defense mechanism against malicious traffic on the Internet. These lists contain known attack sources and aim to predict and block future attacks. Current blacklisting techniques focus on highly active attack sources and collaborative efforts. This paper presents a new approach, called "predictive blacklisting," which uses shared attack logs as an implicit recommendation system. The authors compare existing methods to an upper bound for prediction and find room for improvement. They then propose a multi-level collaborative filtering model, inspired by the NetFlix competition, that considers various factors such as attacker-victim history and interactions. This model is evaluated on one month of logs from Dshield.org and shows significant improvement in prediction rate and resistance to poisoning attacks compared to other methods. 
411452	41145241	Discrete Gaussian Leftover Hash Lemma over Infinite Domains	The Leftover Hash Lemma (LHL) is a commonly used tool to show that certain distributions, particularly those related to modular subset-sums, are close to being uniform over a finite domain. However, its usefulness is limited in lattice-based cryptography for two reasons. First, the distributions of interest are often discrete Gaussians, not uniform. Second, these distributions are over an infinite domain, a lattice rather than a finite field. This work introduces a \"lattice-world\" version of LHL that applies to infinite domains, showing that certain generalized subset-sum distributions are statistically close to well-behaved discrete Gaussian distributions. This has applications in the construction of multilinear maps, where it is used to sample discrete Gaussians. The authors also suggest other potential applications for this new lemma.
411453	41145340	Perfect Structure on the Edge of Chaos - Trapdoor Permutations from Indistinguishability Obfuscation.	This article discusses the construction of trapdoor permutations using indistinguishability obfuscation and one-way functions. This is the first time such a candidate has been proposed that does not rely on the hardness of factoring. The construction also shows that even complex primitives like trapdoor permutations can potentially be based on noisy structures similar to those used in indistinguishability obfuscation constructions. Additionally, this approach may provide resistance against quantum attacks and could potentially be based on problems outside the complexity class SZK. This eliminates the need for trapdoor permutations and injective one-way functions in many recent constructions based on indistinguishability obfuscation.
411454	41145418	Continuous Amortization: A Non-Probabilistic Adaptive Analysis Technique	This paper discusses subdivision algorithms for isolating the roots of a univariate polynomial with real coefficients. These algorithms use algebraic techniques or numerical primitives, such as function evaluation. The paper introduces a form of continuous amortization for adaptive complexity analysis, which is a new challenge in computer science. The analysis is applied to an evaluation-based algorithm called EVAL, which is also a 1-dimensional analogue of other algorithms. The main result is an O(d3(log d + L)) bound on the subdivision-tree size of EVAL for isolating all real roots of a square-free integer polynomial. The proof uses several novel techniques, including an adaptive upper bound using integrals and two algebraic amortization techniques.
411455	4114557	Coreflective Concurrent Semantics for Single-Pushout Graph Grammars	The paper discusses the challenge of applying Winskel's unfolding semantics to graph grammars, which has been attempted multiple times in the past but only with limited success. The focus is on single-pushout grammars, and the authors propose a complete extension of Winskel's approach that provides a concurrent semantics expressed as a coreflection between the categories of graph grammars and prime algebraic domains.
411456	41145652	Cooperability In Train Control Systems: Specification Of Scenarios Using Open Nets	Train control systems, such as the European Train Control Systems (ETCS), require different software components to work together effectively in order to achieve the desired system behavior. High-level Petri net techniques have been identified as a suitable formal specification technique for specifying the operational behavior of ETCS. However, traditional Petri nets lack the ability to interact with the environment, making them inadequate for modeling scenarios and achieving cooperability between system components. The concept of open nets, developed by the research group "Petri Net Technology", is a promising solution for this issue. In this paper, a simplified railway level crossing control system is studied using open nets, called scenario nets. Integration and composition techniques for open nets ensure the cooperability of system components, making them a valuable tool for train control systems, particularly in the area of ETCS.
411457	41145773	Blind sequential detection for Rayleigh fading channels using hybrid Monte Carlo-recursive identification algorithms	The detection of data transmitted over a Rayleigh fading channel, without prior knowledge of the channel, has been a topic of study for researchers. A new algorithm is proposed in this paper for joint detection and channel estimation, using a combination of Monte Carlo sampling and recursive identification methods. The channel is modeled as an autoregressive process, allowing for a dynamic state space representation of the communication system. The algorithm also takes into account fast fading and time diversity in the received signal for a more accurate channel modeling. Simulation results demonstrate the effectiveness of this approach.
411458	41145819	Constructing Multiple Independent Spanning Trees On Recursive Circulant Graphs G(2(M), 2)	A recursive circulant graph G(N, d) is a graph with N vertices and a recursive structure, where two vertices are adjacent if they have a specific relation based on an integer k. These graphs are commonly used as interconnection networks for computing systems and have many attractive features. The design of multiple independent spanning trees (ISTs) on these graphs has various applications, such as fault-tolerant broadcasting and secure message distribution. Previous work has provided a scheme to construct k ISTs on G(cd(m), d) with d >= 3, but this cannot be applied to the case of d = 2. This paper addresses this issue by proposing new constructing rules for G(2(m), 2) and shows that they result in lower heights for ISTs compared to other known constructions. 
411459	41145928	The lower and upper forcing geodetic numbers of block-cactus graphs	The geodetic number of a graph is the smallest number of vertices that must be included in a set for all vertices to be on a shortest path between two other vertices. A subset of this set is called a forcing subset if it is the only possible geodetic set containing it. The forcing geodetic number is the smallest size of a forcing subset, and the lower and upper forcing geodetic numbers are the smallest and largest forcing geodetic numbers among all minimum geodetic sets in the graph. The paper explores the lower and upper forcing geodetic numbers of block-cactus graphs.
411460	41146074	A feasible intrusion detector for recognizing IIS attacks based on neural networks	Log files are used by website administrators to locate problems after a network intrusion. However, due to the large amount of data in these files, it is difficult for administrators to determine their hidden meanings. One solution is to use neural networks, which can effectively identify abnormal data and lighten the administrator's workload. This paper discusses the use of a hybrid intrusion detection system for IIS, validated through the Internet scanner system. The system was trained using different data sets and experimental designs, with the best results achieved with a training set size of 1400 or 2000. It is important to regularly retrain the detector to maintain accuracy, with the frequency and size of retraining dependent on the downgrade percentage of the detection rate.
411461	41146122	Extended Temporal Logic Revisited	Model-checking tools are important in verifying the correctness of software systems. One key issue in their design is choosing the formal language for specifying properties. A good language should be able to handle all ω-regular properties and be familiar to designers who use finite-state machines. Early extensions of linear temporal logic with automata used nondeterministic Büchi automata, but they have limitations. In this study, an extension called ETL2a is proposed, which uses two-way alternating automata as temporal connectives. These automata have the ability to refer to the past and are exponentially more succinct than one-way automata. ETL2a is shown to be powerful, convenient, and efficient, with the same space complexity as other extensions with automata. The use of alternating automata in the industry and the development of symbolic procedures make ETL2a a promising choice for practical use.
411462	41146213	Almost all k-cop-win graphs contain a dominating set of cardinality k.	In the binomial random graph G(n,1/2), we study k-cop-win graphs and find that the majority of them contain a universal vertex. This result is extended to show that for any given k, almost all k-cop-win graphs have a dominating set of size k. As a result, the number of labelled k-cop-win graphs of order n approaches (1+o(1))(1-2^-k)^-knk2n2/2-(1/2-log2(1-2^-k))n.
411463	41146332	Analysis of algorithms on the cores of random graphs	The k-core of a graph is the largest subgraph where every vertex has a degree of at least k. This subgraph can be found by repeatedly deleting vertices with a degree less than k. The threshold for when the k-core appears in a random graph was originally determined by Pittel, Spencer, and the speaker using differential equations. Recently, other papers have provided alternative derivations using a pseudograph model introduced by Bollobás and Frieze. This model is useful for analyzing algorithms on the k-core of sparse random graphs. The model was used to rederive the threshold for the k-core and has also been used to analyze algorithms for off-line load balancing and to determine lower bounds on the longest cycle lengths in random graphs. These analyses use differential equations to approximate random variables during the algorithm and have shown to be accurate in determining the algorithms' performance.
411464	41146454	A Cooperative Intrusion Detection Model Based on Granular Computing and Agent Technologies	This paper examines different attack types, such as Probing, DoS Denial of Service, R2L Remote to Local, and U2R User to Root. The attacks are then categorized based on the number of hosts involved and the resource and destination addresses of network packages. Using a granular computing methodology and agent technologies, a cooperative intrusion detection model is proposed. An intrusion detection agent is also constructed, and experiments show that the proposed method is effective in detecting slow scanning attacks that traditional scanning detectors may miss. 
411465	41146594	Bichromatic Quadrangulations with Steiner Points.	The paper discusses quadrangulations of k-colored point sets, where k is greater than or equal to 2. A quadrangulation is a family of quadrilaterals with disjoint interiors that covers the entire point set and has edges connecting points of different colors. Not all k-colored point sets can be quadrangulated, but those that can are called quadrangulatable. The paper explores the question of how many Steiner points (points added to the interior of the convex hull) are needed to make a point set quadrangulatable. It is shown that for a bichromatic point set with equal numbers of red and blue points, at most (n-1)/3 + n/2 + 1 Steiner points are needed, and for monochromatic point sets, the convex hull can be partitioned into a set of star-shaped polygons. However, there are some 3-colored point sets that cannot be made quadrangulatable.
411466	41146610	Image annotation via learning the image-label interrelations	Image annotation is the process of assigning relevant labels to digital images using machines. This is beneficial for image search and sharing on social networks. While several methods have been proposed in the past decade, most are not accurate or fast enough for practical use. In this paper, a new image annotation method is proposed, which uses the relationship between images and labels to predict labels quickly. The method involves a model that learns this relationship through regression and incorporates a label-biased regularization for better results. The model can be solved quickly, and experiments on three datasets show that it performs as well as other methods while having a faster learning time.
411467	41146742	Fast image upsampling via the displacement field.	The paper introduces a new method for fast image upsampling, which ensures sharpness in both large-scale edges and small-scale structures. This is achieved through a two-scale framework, where a low-frequency image is recovered using a displacement field and a sharpness preserving interpolation technique. The model preserves the distances of pixels on edges, resulting in sharp edges in the final high-resolution image. Additionally, a sharpness preserving reconstruction algorithm is used to reconstruct local high-frequency structures. The method outperforms existing approaches in terms of quantitative and qualitative evaluations, as well as user perception. It is also noted for its speed, making it suitable for practical applications. 
411468	41146837	Decorating surfaces with bidirectional texture functions.	The presented system allows for the decoration of arbitrary surfaces with bidirectional texture functions (BTF). The system generates BTFs in two steps: first, a BTF sample is synthesized over the target surface, and then the user can interactively paint BTF patches onto the surface to seamlessly integrate with the background patterns. The system utilizes a patch-based texture synthesis approach called quilting and includes a graphcut algorithm for BTF synthesis on surfaces. This algorithm works well with a wide variety of BTF samples, including those that present challenges for existing algorithms. Additionally, a graphcut texture painting algorithm is described for creating imperfections on surfaces from existing BTF samples. This system allows for the realistic decoration of surfaces with textures that have varying reflectance, detailed geometry, and imperfections. Examples are provided to demonstrate the effectiveness of the system.
411469	41146985	Automatic recognition of touch gestures in the corpus of social touch.	This paper discusses the importance of automatic detection and recognition of touch in human-robot interactions. The authors collected data for the Corpus of Social Touch (CoST), which includes 7805 captures of 14 different touch gestures in three variations (gentle, normal, and rough) using a pressure sensor grid. They found that gentle gestures are more difficult to classify than normal and rough gestures, and different classifiers and interpersonal differences also affect recognition accuracy. The paper concludes with suggestions for future research in this area to improve the touch modality in human-robot interactions.
411470	4114702	Multi-target ray searching problems.	This article discusses the problem of searching for multiple targets in a set of concurrent rays, with the exception of a common point. The goal is to design efficient search strategies for finding a specified number of targets. This problem is a generalization of the well-known ray search problem, and is applicable in various scenarios such as interleaved heuristic algorithms. The efficiency of the search strategies is evaluated using two different measures, and the article presents optimal strategies for both measures. Interestingly, the results show that the difficulty of finding multiple targets is comparable to finding a single target in a slightly smaller number of rays. 
411471	41147113	Visibility Constrained Surface Evolution	The paper discusses the problem of feature-based surface reconstruction and introduces a method that takes into account visibility constraints, such as points, curves, and silhouettes, in the surface fitting process. These constraints not only improve the initial surface estimate and speed up convergence, but also aid in determining surface topology. The method uses a variational approach within the level set framework to evolve the surface while satisfying the visibility constraints. The theory is applied to various geometric primitives, including points, curves, and visual hulls, and is tested on real image sequences, showing promising results.
411472	41147250	Set covering with ordered replacement: additive and multiplicative gaps	The article discusses set covering problems in which the set system follows a specific replacement property with respect to a given partial order. This means that if a set is in the system, a set obtained by replacing an element with a smaller one will also be in the system. Many variants of Bin Packing problems fall into this category. The article provides a detailed analysis of the integrality gap and approximability of set covering with replacement, including a polylogarithmic upper bound on the additive integrality gap. Additionally, the article lists several covering problems that fit this framework and have a polylogarithmic additive gap. 
411473	411473173	Ontology-Based Data Sharing in P2P Databases	The article discusses the use of schema mappings in peer-to-peer systems, where peers share structured data and express queries on their local schema. The existence of ontologies is assumed to describe the domain of interest. One major issue is the difficulty in determining the semantic relevance of interests and answers between peers with different schemas. To address this, the authors propose a semantic similarity measure that can evaluate the semantic relativeness between peer schemas and queries. It is first introduced for a shared ontology among peers and then extended to support comparison across multiple ontologies. The measure uses recall and precision from Information Retrieval to identify relevant peers and evaluate the quality of answers based on semantic annotations, mappings, and queries. 
411474	41147412	Two tricks to triangulate chordal probe graphs in polynomial time	A chordal probe graph is a type of graph where the vertices can be divided into two sets, one being a stable set and the other being probes. By adding edges between non-probe vertices, the graph can be extended to a chordal graph. These graphs were initially studied as a generalization of interval probe graphs used in DNA mapping. However, they also have applications in computational biology, specifically in constructing phylogenies for genetic mutations. This study provides characterizations for chordal probe graphs in cases with a fixed partition and without any partition. Recognition algorithms for these graphs have been developed with polynomial time complexity.
411475	41147540	Incomplete Directed Perfect Phylogeny	The perfect phylogeny model is an important tool for studying evolution. A variant of this model involves a species-character matrix where some species have unknown character states. The goal is to determine if these missing states can be filled in to create a perfect phylogeny. This problem arises in phylogenetic studies and in studies using DNA repeat elements. Current solutions have a time complexity of O(n2m). A new graph theoretic approach has been developed which has a near-optimal time complexity of $\tilde{O}(nm)$. Another problem addressed is finding a general solution tree, which can be used to obtain any other solution by node splitting. An algorithm has been developed for constructing this tree or determining if none exists.
411476	41147651	Predicting the performance of component-based software architectures with different usage profiles	Performance predictions are important during the design phase of software architecture in order to improve its overall quality. To make accurate predictions, specifications of the performance properties of individual components within the architecture are necessary. However, existing prediction approaches often overlook the impact of component configuration and data on response times. This paper proposes extensions to the Palladio Component Model, a performance specification language for components, to account for these factors. The extended model allows for predicting response times for different architectural options. A case study on a web portal architecture demonstrates the effectiveness of this approach in aiding design decisions.
411477	411477138	Beyond the Touchscreen: An Exploration of Extending Interactions on Commodity Smartphones.	Smartphones have a variety of sensors that are not fully utilized in terms of input methods. The BeyondTouch project aims to explore the potential of using built-in sensors to enhance the input experience on smartphones. By implementing a variety of machine learning and rule-based methods, users can now interact with their phones through additional inputs such as tapping on the case or surface adjacent to the device. This article is an extended version of a previous work, now including rule-based methods which do not require training data and work well with different users and devices. The effectiveness and usability of these interaction techniques are demonstrated, along with their practicality for various application scenarios. 
411478	41147815	A PSO approach for robust aircraft routing	In order to minimize the effects of unexpected disruptions, airlines are implementing strategies to create more resilient schedules. In this study, a new simulation-optimization method using a particle swarm optimization algorithm is proposed for solving the problem of ensuring robust aircraft routing and flight retiming. The approach involves adding buffer times between flight departures to improve the reliability of both aircraft and passenger connections. The process utilizes a combination of discrete-event simulation and a Particle Swarm Optimization routine to find optimal solutions. Results from experiments using real data demonstrate the effectiveness of this approach in improving schedule robustness.
411479	41147928	Group-Oriented Undeniable Signature Schemes without the Assistance of a Mutually Trusted Party	In a (t, n) undeniable signature scheme, a group of n members shares a single public key and at least t members work together to sign a message. All signers must cooperate to prove the signature's validity to an outsider. If there is a dispute, a judge can intervene. This scheme has four main properties: the signature is generated by at least t members, verification is simplified with one public key, all signers must agree for the signature to be valid, and signers are responsible for the messages. Two proposed schemes have a threshold value of either n or 1, meaning all or just one member must sign. No mutually trusted party is needed, and the group public key is determined by all members.
411480	4114802	Data Lakes, Clouds and Commons: A Review of Platforms for Analyzing and Sharing Genomic Data.	Data commons are a type of data sharing platform that combine cloud computing infrastructure, commonly used software services, tools, and applications to create resources for managing, analyzing, harmonizing, and sharing large-scale biomedical data. These platforms have been particularly useful for analyzing genomics datasets. Data ecosystems, which involve connecting multiple data commons, have also been used for this purpose. However, the curation and analysis process in data commons can be time-consuming. As an alternative, data lakes provide access to data without requiring immediate curation and analysis. This article reviews different software platforms for managing, analyzing, and sharing genomic data, with a focus on data commons, but also discussing data ecosystems and data lakes.
411481	411481163	An architecture for highly available wide-area service composition	Service composition allows for the quick addition of new application functionalities in next generation networks. This is achieved by "composing" the services of other providers through an overlay network of service clusters. The architecture includes algorithms for detecting and recovering from failures in client sessions, with a focus on quick recovery. To evaluate this architecture, the researchers developed an emulation platform that allows for a realistic and controlled design study. The experiments showed minimal control overhead and resource usage, with failure recovery within 1 second using alternate service replicas. Trace data also demonstrated fast failure detection within 1.8 seconds, a significant improvement over existing mechanisms. 
411482	41148210	General Model for Infrastructure Multi-channel Wireless LANs	This paper presents an integrated model for managing requests and data transmission in multi-channel wireless local area networks. The model considers noisy channel conditions and calculates performance parameters for both single and multi-channel networks. It is a general model that can be applied to various wireless networks including IEEE802.11x, IEEE802.16, CDMA, and Hiperlan\2. This model provides a comprehensive approach for optimizing network performance in the presence of noise.
411483	411483172	Personalizing product rankings using collaborative filtering on opinion-derived topic profiles	Product review sites like Trip Advisor, Yelp, and Amazon use a non-personalized ranking system to evaluate products. This makes it difficult to personalize recommendations based on sparse review data. Collaborative Filtering, which uses review texts to identify user profiles, can improve the accuracy of personalized recommendations by carefully using available data and separating users into classes. This method has shown significant improvements in metrics such as MAE, RMSE, and Kendall tau compared to previous methods. However, not all users benefit equally from personalization, and a new approach has been proposed to switch between personalized and non-personalized methods based on the user's opinion profile. The user's level of "opinionatedness" can be a good indicator of whether personalization will be effective or not.
411484	4114848	Experiments on user experiences with recommender interfaces	Recommender systems are popular tools used in e-commerce to personalize the browsing and purchasing experience for users. However, most research has focused on improving the algorithm's performance rather than analyzing user experience with the interface. This article discusses two studies comparing two recommender interfaces: the organization-based interface and the standard ranked list. The first study used an eye tracker and found that the organization interface attracted more user attention and led to more product choices. The second, larger-scale study showed that the organization interface was perceived as more useful and easier to use by users from different cultures. These findings suggest that the design of the recommender interface can impact both user attention and their attitudes towards the system.
411485	41148524	On Dillonʼs class H of bent functions, Niho bent functions and o-polynomials	Dillon's thesis introduced a class of bent Boolean functions called family H, which is a construction of bent functions in bivariate form. However, these functions were found to already belong to the well-known Maiorana-McFarland class. A larger class, denoted by H', was then discovered by extending H. It was also observed that the bent functions constructed using Niho power functions are the univariate form of those in class H. Further, the bent functions whose restrictions to certain vector spaces are linear were characterized. The question of whether the duals of Niho bent functions are affinely equivalent to them was answered by calculating the dual of one of these functions. It was also shown that this Niho function belongs to the Maiorana-McFarland class, raising the question of whether H (or H') is a subclass of this class. The condition for a function in bivariate form to belong to class H was found to be equivalent to a polynomial being an o-polynomial. By using existing classes of nonlinear o-polynomials, a significant number of new bent functions in H were discovered, potentially not equivalent to known bent functions.
411486	41148655	Data transmission and base-station placement for optimizing the lifetime of wireless sensor networks	This paper discusses the optimization problem of positioning base-stations in wireless sensor networks to enable energy-efficient data transmission. The focus is on a scenario where sensors transmit data directly to the base-station or through one other node, resulting in low duty-cycling and end-to-end delay. The objective is to maximize network lifetime considering the limited battery capacity of sensors. The paper presents efficient algorithms for computing transmission schemes in a distributed manner with a constant number of messages per sensor. It also shows that the problem becomes NP-Hard when sensors can transmit data through more than 2 hops. Furthermore, the paper proposes algorithms for simultaneously locating the base-station and finding a transmission scheme. Simulations show the effectiveness of these algorithms compared to linear-programming based approaches for more general settings.
411487	41148719	Correcting noisy exponentiation black-boxes modulo a prime.	This article discusses the problem of correcting a noisy exponentiation black-box modulo a prime number. The black-box takes in an integer and outputs another integer with a small error, representing the result of raising a fixed base to the input power. Howgrave-Graham, Nguyen, and Shparlinski developed an algorithm for this problem that works within a certain range of error and a multiplicative order of the base modulo the prime. This study expands upon their work by extending the range of error and order for which the algorithm can successfully recover the original result with high probability. This is achieved by utilizing a bound on character sums in finite fields and combinatorial techniques.
411488	411488129	Visualization techniques for mining large databases: a comparison	Visual data mining techniques have become highly valuable for exploratory data analysis and have great potential for use in mining large databases. This article introduces a new visualization-based approach to mining large databases, which involves representing as many data items as possible on the screen at once by mapping data values to pixels and arranging them appropriately. The main focus of the article is to evaluate and compare these visual data mining techniques with other commonly used techniques for multidimensional data, such as parallel coordinate and stick figure visualizations. The evaluation considers the perception of data properties as the primary factor, followed by CPU time and secondary storage accesses. A testing environment has also been developed to generate test data sets with predefined characteristics for comparing the perceptual abilities of different visual data mining techniques.
411489	4114891	Binary optimization using hybrid particle swarm optimization and gravitational search algorithm	The PSOGSA is a hybrid optimization algorithm that combines the strengths of PSO and GSA, resulting in improved exploration and exploitation. While the original version is suitable for continuous search spaces, this paper introduces a binary version called BPSOGSA to handle problems with binary parameters. Adaptive values are also integrated to balance exploration and exploitation. The efficiency of BPSOGSA is evaluated on 22 benchmark functions, divided into three groups. The results show that BPSOGSA outperforms other binary algorithms such as BGSA, BPSO, and genetic algorithm in avoiding local minima and achieving faster convergence.
411490	41149015	High-Power-Factor Single-Phase Diode Rectifier Driven by Repetitively Controlled IPM Motor	This paper introduces a new method for power factor correction using an inverter-driven interior permanent magnet (IPM) motor. This system utilizes a three-phase pulsewidth modulation inverter and an IPM motor to achieve a high power factor in a single-phase diode rectifier. The proposed converter includes a small film capacitor, a voltage-source inverter, and an IPM motor. The inverter serves two functions: controlling the motor's speed and regulating the source-side current waveform. To achieve a unity-power-factor operation, a new control method is implemented, where the inverter output power is regulated by a proportional-integral and repetitive controller. Experimental results show that the maximum power factor achieved by this method is 98.7% under rated load conditions.
411491	41149115	Estimation of Vector Fields in Unconstrained and Inequality Constrained Variational Problems for Segmentation and Registration	Vector fields are commonly used in computer vision, especially in non-rigid registration problems. This study introduces a method for estimating vector fields that describe the deformation between objects and the segmentation of these objects. The authors also investigate the use of inequality constraints in solving variational problems in computer vision, such as estimating deformation fields and tracking. Their approach utilizes the Kuhn-Tucker theorem from optimization theory to solve inequality constrained vector field estimation problems. This method differs from other joint segmentation and registration algorithms by using a set of coupled partial differential equations derived from the same energy terms for both registration and segmentation. The paper presents the theory and results of this approach.
411492	4114926	A rational reconstruction of nonmonotonic truth maintenance systems	This paper presents a detailed description of the inferences made by nonmonotonic truth maintenance systems (TMSs) using two commonly used formalisms: logic programming with stable set semantics and autoepistemic logic. It also examines the use of dependency-directed backtracking in handling contradictions and proves that implementing a nonmonotonic TMS is a computationally complex task. This paper provides a comprehensive understanding of the workings of nonmonotonic TMSs and their role in reasoning with incomplete and inconsistent information.
411493	41149358	On illuminating line segments in the plane	The concept of illumination in a family of convex sets in the plane is explored, where a set of light sources is defined as illuminating if they can make every point on the boundary of each set visible. It is proven that for a family of n line segments, at least 2n/3 light sources are needed to illuminate the sets if n is greater than or equal to 11. Additionally, if all the sets in the family are parallel to the x or y-axes, then the minimum number of light sources needed is (n+1)/2.
411494	41149412	A unified information-theoretic framework for viewpoint selection and mesh saliency	Viewpoint selection is a growing area in computer graphics that has various applications like scene exploration and image-based modeling. It involves finding the most optimal views of an object or scene with the least number of images. In this article, a unified framework for viewpoint selection and mesh saliency is presented, using mutual information as a tool to deal with various aspects like viewpoint stability, object exploration, and saliency. The framework can be applied to any set of viewpoints in a closed scene and has been tested through various experiments, showing its robustness and effectiveness. The incorporation of saliency as an importance factor further enhances the framework's capabilities.
411495	41149559	A New Adaptive Sampling Technique for Monte Carlo Global Illumination.	The paper discusses the use of Monte Carlo methods for solving the problem of global illumination in realistic image synthesis. It explains how adaptive sampling can be used to reduce noise in these algorithms and introduces the concept of entropy from information theory to measure pixel quality. The use of the nonextensive Tsallis entropy, which includes a parameter q to represent the degree of nonextensivity, is explored as a means of evaluating pixel quality and conducting adaptive sampling. The paper also describes a method for systematically determining the appropriate value of q using least-squares design. Implementation results demonstrate that this approach outperforms existing methods and may be the first attempt to systematically choose an appropriate entropic index for Tsallis entropy in engineering applications.
411496	41149618	Using internal sensors and embedded detectors for intrusion detection	The article discusses the use of internal sensors for intrusion detection in computer systems. It presents a classification of data collection mechanisms for intrusion detection, categorizing them as direct and indirect monitoring. Internal sensors are considered more advantageous in terms of reliability, completeness, timeliness, volume of data, efficiency, and resistance against attacks. The ESP architecture is introduced as a framework for building intrusion detection systems using internal sensors. A prototype implementation based on this architecture is described, along with the concept of embedded detectors for localized data reduction. Performance and detection testing of the implementation show promising results, demonstrating the potential of embedded detectors in detecting new attacks.
411497	41149730	Information flow security in Boundary Ambients	Boundary Ambients is a variation of the Mobile Ambient calculus that allows for the modeling of multi-level security policies. Boundaries, which are specific ambients, act as resource access managers for confidential data and ensure its protection. By satisfying certain syntactic conditions, direct information leakage can be prevented. A new notion of non-interference is also introduced to capture indirect flows of information. Additionally, a Control Flow Analysis is developed to identify ambients that may be affected by high-level data at runtime, and it is shown that this analysis can be used to enforce non-interference and detect any potential information leakage at runtime. 
411498	41149898	Relating Theories of Actions and Reactive Control	This paper focuses on the formalization of reactive control using action theories. The concept of a reactive control program being correct in relation to a specific goal, set of initial states, and action theories is defined. The paper also presents conditions that ensure correctness and offers an automated method for developing control modules that are proven to be correct. Further, the approach is extended to incorporate action theories about both the agent/robot and external actions. 
411499	41149928	Theoretical analysis of differential microphone array beamforming and an improved solution	Differential microphone arrays (DMAs) have gained popularity due to their frequency-invariant beampatterns, small size, and maximum directivity potential. Traditionally, DMAs are designed in a multistage way using time delays to achieve the desired beampattern. However, a recent study showed that DMAs can be designed using a linear system of equations based on the nulls of the desired beampattern. This paper focuses on beamforming with linear DMAs and makes four main contributions. Firstly, it presents theoretical analysis showing that traditional and null-constrained DMAs are equivalent. Secondly, it introduces a two-stage approach to robust DMA beamforming based on maximizing white noise gain. Thirdly, it addresses the issue of extra nulls in the beampattern at high frequencies in robust DMA beamforming. Lastly, it proposes a method to solve this problem while maintaining a frequency-invariant beampattern.
411500	41150023	Noise reduction algorithms in a generalized transform domain	Noise reduction for speech applications involves using a filter or transform to reduce noise in speech signals without distorting the speech. The goal is to design an optimal filter that can effectively suppress noise while maintaining speech quality. This can be done in either the time or transform domain, with the latter being advantageous as it allows for better separation of speech and noise signals. While the Fourier and Karhunen-Loève transforms are commonly used, there has been no formal study to determine which is more effective. This paper proposes a generalized transform domain for noise reduction, which allows for easy comparison of different transforms and the design of optimal and suboptimal filters. 
411501	41150128	Indexing Mobile Objects Using Duality Transforms	This article discusses techniques for efficiently indexing mobile objects in order to answer range queries about their future positions. This is a common problem in real-life applications, such as predicting traffic congestion or allocating bandwidth for areas with high concentrations of mobile phones. The authors propose dynamic solutions for both one-dimensional and two-dimensional cases, using external memory. Their approach involves transforming the problem into a dual space that is easier to index. They also compare the advantages and disadvantages of different indexing schemes proposed in literature for mobile objects. 
411502	41150248	Storing and querying multiversion XML documents using durable node numbers	Managing multiple versions of XML documents is a significant issue for traditional applications like software configuration control and newer ones such as maintaining web document links. Research is being conducted to develop effective methods for storing, retrieving, and querying these documents. This paper proposes a new method for version management using Durable Node Numbers and timestamps for XML document elements. The paper discusses efficient storage and retrieval techniques as well as indexing and clustering strategies to support complex queries on document content and evolution.
411503	41150353	Up-front interaction design in agile development	This paper discusses the relationship between interaction design and agile development, specifically addressing the issue of whether interaction design should be done before or during software development. The study involved conducting interviews with interaction designers and software developers on agile teams and using a qualitative approach to analyze the results. The findings suggest that there are benefits to both pre-development interaction design and incorporating it throughout the development process. The paper concludes that a balance between these two approaches is ideal for successful integration of interaction design and agile development.
411504	41150439	Interpolating vertical parallax for an autostereoscopic three-dimensional projector array	This article discusses a new technique for achieving vertical parallax for multiple users using different types of autostereoscopic projector setups, including front- and rear-projection and curved displays. This technique allows for both horizontal and vertical parallax, creating a more realistic 3-D experience for viewers. It uses a low-cost RGB-depth sensor to track multiple viewer head positions, updating the imagery sent to the array accordingly. Unlike previous methods, this technique does not assume a single vertical perspective for each projector, allowing for a hybrid parallax approach. The authors demonstrate this technique using a horizontal array of pico-projectors aimed at a vertical diffusion screen, resulting in a clear and seamless viewing experience for multiple users. 
411505	41150520	A novel remote user authentication scheme through dynamic login identity	Password authentication is a widely used method for verifying a user's identity and preventing malicious activity. In 1981, the first password-based remote user authentication scheme was introduced by Lamport. However, the problem with static login-ID based schemes is that they can be easily compromised if the login-ID is shared or intercepted. To address this issue, a dynamic login-ID based scheme using smart cards was proposed. This scheme assigns a unique login-ID for each login session, preventing unauthorized access. The use of smart cards also restricts the distribution of login-IDs. This scheme is particularly useful for applications like digital libraries and uses secure methods like RSA and one-way hash functions for login request verification. It also allows for easy password changes without assistance from the remote system.
411506	41150647	Hybrid Scheduling of Dynamic Task Graphs with Selective Duplication for Multiprocessors under Memory and Time Constraints	This paper introduces a hybrid scheduling method for multiprocessor embedded systems that can handle dynamic task graphs with conditional tasks and unpredictable execution times. The methodology involves three phases: static mapping of task nodes to processors, selective duplication of critical nodes for possible rescheduling at runtime, and an online scheduling algorithm based on the current schedule. The proposed technique has shown to improve schedule length by up to 20% compared to previous methods, with low overhead and complexity similar to other online techniques. The paper also explores the impact of different parameters on performance, such as the number of processors and memory.
411507	4115077	Multigrid Reinforcement Learning with Reward Shaping	Potential-based reward shaping is a technique used in reinforcement learning to improve the speed at which agents learn. It allows for the incorporation of background knowledge into the learning process in a systematic manner. However, the challenge lies in determining the potential function that shapes the agent's reward. This paper proposes a solution to this problem by learning the potential function in parallel with the reinforcement learning process. By using a grid-based approach, a lower-resolution V-function can be learned simultaneously with the Q-function, allowing for the approximation of the potential function. The algorithm is introduced and its effectiveness is demonstrated through experiments.
411508	41150851	Agent-Based Coalitions In Dynamic Supply Chains	Coalition formation is a significant issue in multi-agent systems, and recent research has focused on simplifying the process. This involves agents actively seeking out potential coalition members before engaging in negotiations. A framework has been proposed for coalition formation among agents in a dynamic supply chain setting, which includes a negotiation protocol and a decision mechanism. The negotiation protocol allows for effective communication between agents from different sectors (buyers, sellers, and logistics providers), while the decision mechanism involves two steps: forming loosely-coupled coalitions within sectors to reduce complexity, and forming cross-sector coalitions to deliver goods to end customers. An example is provided to demonstrate the success of this approach in helping agents form coalitions.
411509	411509129	Mixed Neighbourhood Local Search For Customer Order Scheduling Problem	COSP is a difficult problem with practical applications in industries like paper and pharmaceuticals. Current algorithms struggle with finding quality solutions for large problems. In this paper, a new heuristic called RBM is proposed, along with a mixed neighbourhood local search (MNLS) algorithm. MNLS includes various move operators to improve local exploitation and a greedy diversification method for plateau situations. Experiments on 960 instances show that MNLS outperforms existing algorithms and finds new best solutions for 721 instances. This highlights the potential of using MNLS for solving COSP.
411510	4115100	Relating weight constraint and aggregate programs: Semantics and representation	This paper discusses the relationship between weight constraint and aggregate programs, which are two commonly used types of logic programs with constraints. The stable model semantics for weight constraint programs and the answer set semantics for aggregate programs are compared and shown to be closely related. The paper also explores the possibility of representing aggregate programs using weight constraints, which can be more compact and efficient. Experimental results show that using weight constraints to handle aggregates can be a competitive approach for computing answer sets compared to more traditional methods. Overall, the paper provides insights into the connection between these two types of programs and offers a potential improvement for handling aggregates in logic programming.
411511	41151124	An Efficient Protocol to Study the Effect of Flooding on Energy Consumption in MANETS.	Mobile Ad-hoc network (MANET) is a network of mobile nodes that communicate with each other through wireless channels. Unlike fixed wired networks, MANET has nodes with limited transmission range and mobility, which introduces challenges such as energy consumption and finding the location of the nodes. Most MANET routing protocols use flooding to locate nodes, but this can be costly in terms of energy consumption. In this paper, the Ring routing protocol is introduced as an alternative to flooding, and its effectiveness is compared to other protocols (DSDV, AODV, DSR) that use flooding. The results of the comparison, which were obtained using the ns-2 simulator, show that the Ring protocol is a viable option for reducing energy consumption in MANETs.
411512	41151230	Chosen ciphertext secure authenticated group communication using identity-based signcryption	Secure group communication requires efficient access control and scalable rekeying to ensure the protection of dynamic and large groups. In addition, sender authentication is crucial in a many-to-many communication environment where each participant can send and receive messages. This study presents an authenticated group communication scheme that uses identity-based signcryption to prevent adaptive chosen ciphertext attacks. The scheme allows multiple senders to dynamically send messages to a chosen group of receivers, and group members can be stateless receivers. This ensures data confidentiality and sender authentication in the group communication.
411513	41151337	Entity Based Peer-to-Peer in a Data Grid Environment	The past decade has seen a growing interest in Grid technologies, resulting in various Grid projects being launched with different visions. While all these visions aim to facilitate resource sharing, they vary in terms of functionality, grid characterization, and programming environments. A new Grid system, called DGET, has been introduced to specifically address data-related issues. DGET utilizes a peer-to-peer communication system and an entity-based architecture, combining the key features of both P2P and Grid systems. Currently in development, DGET's main components are being tested in a prototype. This paper focuses on the system's architecture and highlights its uniqueness compared to other systems.
411514	41151441	Flattening single-vertex origami: The non-expansive case	 and closed origami.The Single-Vertex Origami Problem involves determining if a piece of paper with creases emanating from a fold vertex can be flattened without tearing or stretching the paper. Streinu and Whiteley showed that this problem can be reduced to the Carpenter's Rule Problem for spherical polygons. They were able to solve this problem by using spherical expansive motions and applied it to both open and closed origami cases.
411515	41151573	Fast, memory efficient flow rate estimation using runs	Per-flow network traffic measurements are crucial for effective network traffic management, performance assessment, and detection of anomalous events like DoS attacks. However, tracking the large number of flows in backbone networks requires high-speed memories, making explicit measurement difficult. To reduce overhead, random sampling has been proposed and used in commercial routers. The goal is to develop a new scheme with low memory requirements and quick convergence to a specified accuracy. This is achieved through a novel approach of sampling two-runs, which automatically biases the samples towards larger flows for more accurate estimation. This leads to smaller memory requirements compared to random sampling and is simple to implement and performs well. 
411516	41151668	Routing restorable bandwidth guaranteed connections using maximum 2-route flows	Restorable routing is crucial for maintaining reliable connections in MPLS and optical networks. It involves having both an active path and a backup path for each connection, allowing for quick restoration in case of failure. In order to optimize bandwidth usage, backups can be shared, but this requires knowledge of the backup bandwidth distribution among nodes. One approach for restorable routing in optical networks is simultaneous transmission on both paths, with the receiver choosing the stronger signal. However, this does not allow for backup sharing. This article focuses on efficient routing without sharing, using minimum-interference techniques to improve path selection. Two new algorithms are proposed, both of which outperform previous methods.
411517	41151731	Refuse to crash with Re-FUSE	Re-FUSE is a framework designed to support restartable user-level file systems. It continuously monitors the file system and in the event of a crash, it automatically restarts the file system and restores its state. This process is transparent to applications, ensuring smooth operation. Re-FUSE achieves this through various techniques such as request tagging, system-call logging, and non-interruptible system calls. It has been tested with three popular FUSE file systems and has shown minimal impact on performance. Additionally, Re-FUSE requires minimal changes to existing FUSE file systems, making it a non-intrusive solution for improving crash tolerance.
411518	41151840	ViewBox: integrating local file systems with cloud storage services	Cloud-based file synchronization services have gained popularity for their ability to sync files across devices and provide automatic cloud backups. However, the loose coupling between these services and local file systems can make data vulnerable. Local corruption can spread to the cloud, causing issues on other devices, and crashes can lead to inconsistency between local and cloud copies. ViewBox is a solution that integrates synchronization and local file systems to prevent data corruption and inconsistency. It uses ext4-cksum and a user-level daemon to detect and recover from problems, and employs a view manager to create consistent snapshots of the file system. Experiments show that ViewBox has minimal overhead and effectively addresses these issues.
411519	41151925	Exploitation of human shadow perception for fast shadow rendering	This paper discusses an experiment that was conducted to understand how the human visual system perceives shadows. Shadows are important for conveying the spatial structures of objects and enhancing the realism of rendered images. However, algorithms used in computer graphics to create realistic shadows are often computationally expensive. The experiment aimed to simplify the shadow casting process in order to improve performance, while still maintaining a high level of accuracy. Test subjects were asked to indicate the point at which they could notice a difference in the quality of the shadows. Initial results suggest that a simplified mesh with only 1% of its original complexity can produce soft shadows that are satisfactory to 90% of the participants.
411520	41152044	Synchronized extension systems	SE-systems are 4-tuples consisting of an alphabet, two languages, and a synchronizing set. They are used to generate new languages by combining the two languages and synchronizing on specific words. These systems are commonly found in various structures such as stacks, queues, and grammars, and are also used in coding systems. 
411521	411521176	Efficient Face Image Deblurring via Robust Face Salient Landmark Detection.	In recent years, there has been significant progress in image deblurring. However, the deblurring of face images has not been well studied. Existing methods for deblurring faces rely on constructing exemplar sets and matching candidates, which is time-consuming and can be affected by complex or exaggerated face variations. To address these issues, a new method is proposed that integrates a classical L-0 deblurring approach with face landmark detection. A specialized landmark detector is used to identify key facial features, which are then used to guide the deblurring process. Experimental results show that this method is effective in handling complex face poses and reduces computation time compared to other approaches.
411522	41152214	On the complexity of pure-strategy nash equilibria in congestion and local-effect games	Congestion games are a type of noncooperative game where players aim to route flow from their origin to their destination at minimal cost. This cost is determined by the total number of players using a certain path. Weighted congestion games allow for players to send varying amounts of flow, making it more complex. While it has been shown that pure-strategy Nash equilibria may not exist in these games, it is also proven that it is difficult to determine their existence. This holds true for both unsplittable flow (must be routed on one path) and splittable flow. Another related game is the local-effect game, where the cost of taking an action depends on the actions of other players. It is shown that determining whether a bidirectional local-effect game has a pure-strategy Nash equilibrium is a computationally difficult problem, and even finding a pure-strategy Nash equilibrium in a bidirectional local-effect game with linear local-effect functions is a complex task. This is due to the use of a tight PLS-reduction, which means that finding a pure-strategy Nash equilibrium can take exponential time.
411523	41152340	A proposal to detect errors in Enterprise Application Integration solutions	Enterprise Application Integration (EAI) solutions are used to keep different applications' data in sync and to create new functionality. However, these solutions can be prone to errors due to their distributed nature and integration challenges. To address this issue, many researchers have focused on adding fault-tolerance capabilities to EAI solutions. This article examines EAI solutions from two perspectives: orchestration versus choreography and process- versus task-based execution models. It is found that current proposals have limitations or are limited to a specific viewpoint or execution model. To overcome this, an error monitor has been developed that can be used to add fault-tolerance to EAI solutions. The monitor has been proven to have efficient algorithms and has shown promising results in high workload situations.
411524	4115242	Schema integration and transaction management for multidatabases	This paper proposes a technique for integrating multiple independent database systems using a unified model, which combines relational and object-oriented features. The advantages of this model are discussed, along with the methods for integrating databases with different models and resolving conflicts. The paper also addresses how queries on the integrated schema are handled in a heterogeneous environment and presents a transaction management scheme that allows for semantic recovery without restricting transactions or violating local autonomy. This paper was published in 1998 by Elsevier Science Inc.
411525	41152531	Incremental Design Debugging in a Logic Synthesis Environment	Today's VLSI design process is complicated and can lead to multiple logic errors due to human mistakes and bugs in CAD tools. This can be challenging for designers to correct, but a new study presents a simulation-based solution for debugging combinational circuits with multiple errors. Unlike other techniques that identify all errors at once, this method works incrementally by fixing one error at a time using linear time algorithms. The study also includes the use of theorems, heuristics, and data structures to guide the search for a solution in a large space. Experiments on benchmark circuits show that this incremental approach is effective for logic debugging.
411526	411526174	Resource description framework: metadata and its applications	The Web's universality, which allows for easy access to vast amounts of data and information, also hinders the development of a uniform organization scheme. To fully utilize the potential of Web data, a semantic web is needed, where applications and websites can exchange information. This requires detailed and structured metadata, which can be achieved through the Resource Description Framework (RDF). The success of RDF and the semantic web will depend on the development of applications, interfaces, and databases that can utilize it effectively. Practical issues such as security and compatibility will also play a crucial role. This survey provides an overview of the past, present, and future of RDF and the Semantic Web, emphasizing its potential for improved organization, accessibility, and usefulness. It is believed that knowledge discovery and data mining will benefit from this technology.
411527	4115279	Acquiring a radiance distribution to superimpose virtual objects onto a real scene	This paper presents a new method for superimposing virtual objects onto real scene images with accurate shading. Unlike previous methods, this approach automatically measures the radiance distribution of the scene and uses it for appropriate virtual object placement. The method begins by constructing a geometric model of the scene from omni-directional images using a stereo algorithm. Then, radiance is computed from a sequence of images with different shutter speeds and mapped onto the geometric model. This information is then used to render virtual objects onto the real scene image, resulting in convincing shading and shadows. The effectiveness of this method was tested using real images. Keywords include computer graphics, computer vision, augmented reality, and illumination distribution measurement. 
411528	4115286	Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain	Hyper spectral imaging has a wide range of applications, but current methods do not take into account the fluorescent effects present in everyday items such as paper, clothing, and food. This paper presents a method for efficiently separating and recovering reflective and fluorescent emission spectra using high frequency illumination in the spectral domain. This allows for the estimation of fluorescent absorption spectra from the emission spectra, which is the first of its kind. Unlike conventional methods, this approach only requires two hyper spectral images and has been evaluated through simulations and real experiments. An application of this method to synthetic relighting of real scenes is also demonstrated.
411529	411529148	Modeling sensors: toward automatic generation of object recognition program	This paper discusses the process of automatically generating object recognition programs from given geometric models, which is a key method in building model-based vision systems. It explains the necessary components for this process, including object and sensor models, strategy generation, and program generation. The focus is on sensor modeling, as it is seen as the main obstacle in automatic program generation. The paper specifically looks at two aspects of sensor characteristics: detectability and reliability. A configuration space is defined to represent these characteristics, and a representation method is proposed. The paper concludes by exploring how this sensor model can be used in the automatic generation of object recognition programs.
411530	41153013	Identification without randomization	The theory of identification via noisy channels involves using randomization in encoding to reduce the optimal code size, which experiences a double-exponential growth in blocklength. However, when considering more robust channels such as compound and arbitrarily varying channels, randomization is not necessary as the jammer already knows the input sequence. In these cases, the identification capacity is equal to the logarithm of the number of different row-vectors in the channel. This holds true for compound channels, but for arbitrarily varying channels, the identification capacity is different from the transmission capacity. A lower bound on the identification capacity is presented, and it is shown that randomization in decoding does not increase the capacity. This work is related to source coding.
411531	41153119	The Architect's Role in Software Ecosystems Health	Software ecosystems have become increasingly important for businesses to achieve success and maintain good health. The role of a software architect is crucial in this process, as they are responsible for implementing the organization's business strategy. Unlike traditional two-sided markets, software ecosystems operate as multi-sided markets, requiring different strategies. As a result, software architects must focus on flexibility and quick response to meet the demands of these markets. This shifts the understanding of their role and highlights the importance of collaboration and cooperation in fostering competition within the ecosystem. This paper examines how the actions of software architects impact the health of the ecosystem, as measured by productivity, niche creation, and robustness. 
411532	41153256	Interval Valued Versions of T-Conorms, Fuzzy Negations and Fuzzy Implications	The article discusses the various ways in which classical prepositional connectives can be extended to the set [0,1] while preserving their behavior in extreme cases. However, it is generally agreed upon that simply preserving classical properties is not enough, leading to the introduction of t-norms, t-conorms, fuzzy negations, and fuzzy implications. This paper focuses on generalizing these concepts to the set II = {[a, b] : 0 les a les b les 1}, known as interval t-norms, and provides constructions to obtain the best interval representation of these connectives. The paper also explores the relationships between these concepts, such as obtaining an interval fuzzy t-conorm from an interval t-norm and an interval fuzzy negation. Various properties of these constructions are also proven.
411533	41153341	Using gaze data in evaluating interactive visualizations	The importance of proper evaluations in publications presenting new visualization techniques is increasing. However, there have been many challenges and reluctance in conducting these evaluations, such as the difficulty and time-consuming nature of the task. To address this, a simple evaluation approach is proposed, which involves a set of tasks performed in a controlled environment with the use of eye tracking to estimate the user's focus. Additionally, three methods for visualizing gaze data are discussed, along with examples from a study evaluating a parallel coordinate browser. This approach offers valuable insights into the distribution of user attention, aiding in the understanding and improvement of visualization techniques.
411534	4115342	Emotions and heart rate while sitting on a chair	Researchers are seeking new ways to monitor computer users' emotions in human-computer interaction research. In this study, participants were presented with emotionally provocative audio, visual, and audiovisual stimuli while their heart rate was measured using a special office chair and traditional earlobe photoplethysmography. The results showed a strong correlation between the two methods and indicated that emotional stimulation leads to a deceleration of heart rate, with the most significant changes occurring in response to negative stimuli. The researchers suggest that the EMFi chair could be a useful tool for unobtrusive measurement of emotional reactions in human-computer interaction.
411535	41153535	Study on two privacy-oriented protocols for information communication systems	The privacy of users in information communication systems is crucial, especially for mobile systems. Many cryptographic tools have been developed to protect users' privacy, but some have been found to have security vulnerabilities. In this paper, two privacy-oriented cryptographic protocols are reviewed and their weaknesses are exposed. The first protocol, proposed by Hsu and Chuang, allows users to log into a system anonymously and establish a secret key, but it is not secure against session key attacks. A solution to this issue is proposed. The second protocol, proposed by Harn and Ren, aims to protect the privacy of message senders but has potential vulnerabilities. These vulnerabilities are discussed and potential solutions are suggested. 
411536	411536106	Comparative modeling and evaluation of CC-NUMA and COMA on hierarchical ring architectures	Parallel computing performance on scalable shared-memory architectures is influenced by the interconnection networks connecting processors to memory modules and the efficiency of the memory/cache management systems. CC-NUMA and COMA are two effective memory systems, and the hierarchical ring structure is a efficient interconnection network in hardware. This article compares the performance of CC-NUMA and COMA on a hierarchical ring shared-memory architecture, using analytical models and performance measurements on a shared-memory machine. The results show that COMA balances work load well, but the frequent data movement overhead may offset the gains. CC-NUMA allows for explicit data locality management, potentially improving performance. These findings can be applied to other hierarchical network architectures.
411537	41153721	A personalized search engine based on Web-snippet hierarchical clustering	SnakeT is a meta-search engine that queries over 18 search engines and presents the results in two views: a flat-ranked list and a hierarchical organization of folders with clear labels. Users can browse this hierarchy for various purposes, including knowledge extraction and personalization of results. The personalization is done by users themselves, making it adaptive, private, and scalable. SnakeT has been extensively tested and compared to other Web-snippet clustering engines, showing its efficiency and effectiveness. The quality of the ranking of the underlying search engines directly affects the relevance of the results in the folder hierarchy, and vice versa. This work was published at the 14th International World Wide Web Conference in 2005 and contains a detailed description and experiments of the SnakeT software system. 
411538	41153836	Compressed representations of sequences and full-text indexes	This article discusses a method for representing a sequence of integers using a small amount of storage space, based on the zero-order empirical entropy of the sequence. This allows for constant time operations on the sequence, such as finding specific integers or answering queries, even for large sequences. The method is also applied to designing compressed full-text indexes that can handle larger input alphabets, with improved query times compared to previous methods. The article also mentions the ability to handle even larger alphabets with a slight increase in storage space and query times. Overall, this method provides a more efficient and scalable approach for handling sequences and full-text indexes.
411539	4115398	Cross-Language Relevance Assessment and Task Context.	An experiment was conducted to explore how users evaluate relevance in a foreign language that they are proficient in. The findings revealed that this process takes longer and is prone to mistakes compared to assessing relevance in their native language. These results were found to be influenced by the task and context. The study also proposes a new methodology for conducting context-sensitive studies. It was noted that people are naturally multilingual and many have proficiency in multiple languages, especially those engaged in intellectual activities. These individuals are also skilled at making relevance assessments.
411540	41154048	Pattern Matching Algorithms with Don't Cares	This paper discusses algorithms for pattern matching with "don't care" characters in either the pattern or the text. If the pattern has don't care characters, the problem can be solved in O(n + m + ) time, where is the total number of occurrences of component subpatterns. Online queries can also be handled with O(n) preprocessing time and O(m + ) time per query. If the text has don't care characters, the problem can be solved in O(n + m + |occ(P)|) time, assuming the length of each component sub-text is longer than the pattern.
411541	41154116	Drawing Directed Graphs Using One-Dimensional Optimization	The presented algorithm efficiently draws directed graphs by solving optimization problems for each axis. This results in a clear representation of the graph's hierarchy structure, with nodes not confined to fixed horizontal layers. The layout naturally conveys the symmetries of the graph. The algorithm can be used for cyclic or acyclic digraphs, as well as those with a mix of directed and undirected edges. Additionally, a hierarchy index is derived from the input graph to measure its level of hierarchy. 
411542	41154256	Applying the PLEX framework in designing for playfulness	Interactive products now not only need to be functional and usable, but also provide enjoyable experiences for users. However, playfulness can manifest in various forms, making it challenging to design for. To address this, a framework called Playful Experiences (PLEX) has been developed. It serves as a conceptual tool to understand playful aspects of user experience (UX) and a practical tool to design for such experiences through user-centered design (UCD) methods. This paper presents an overview of the PLEX framework and its development between 2008-2010. It also discusses the creation and evaluation of PLEX Cards and their use in a design case, as well as the development of PLEX Design Patterns for practical design solutions. The PLEX framework is proposed as a powerful tool for understanding and designing for playful experiences in interactive products.
411543	4115435	Generative Adversarial Nets for Information Retrieval: Fundamentals and Advances.	Generative adversarial nets (GANs) are a popular method in deep learning and unsupervised learning for training a generative model to fit an unknown data distribution. This is achieved through an adversarial training mechanism, where a discriminative model guides the generative model to produce realistic data instances. While GANs were originally designed for continuous data like images, they have been adapted for discrete data in information retrieval scenarios, such as IDs, text, and graphs. This tutorial covers the fundamentals of GANs, their applications in discrete data generation, and their use in information retrieval tasks like text and graph generation. It also introduces relevant open-source platforms for conducting research experiments with GANs and discusses future research directions for GANs in information retrieval.
411544	41154446	Adaptive diversification of recommendation results via latent factor portfolio	This paper discusses the importance of result diversification in collaborative filtering for recommendation systems. The authors argue that the level of diversification in a recommended list should be tailored to the individual interests and needs of the target users. This is because users with different interests may have varying levels of uncertainty in their preference models. To address this, the authors propose a Latent Factor Portfolio (LFP) model that considers both the user's interest range and the uncertainty in their preference model. This model utilizes correlations between items and latent factors to reduce computation load and improve diversity in recommendations. Experimental results support the effectiveness of the LFP model in adapting to individual user needs and improving upon traditional latent factor models. 
411545	411545252	Explorable Volumetric Depth Images from Raycasting.	View-dependent image-based rendering techniques have become popular for their ability to combine high quality images with interactive exploration. However, previous approaches for volume rendering have limitations such as being limited to surfaces, expensive generation, and issues with depth perception. In this paper, the authors propose a solution called Volumetric Depth Images (VDI) which overcomes these issues by using a compact representation that is independent of the original data structure. This representation, generated with low overhead, allows for efficient rendering with arbitrary camera configurations. The results produced by VDIs are identical to the original ray casting, making it useful for preview rendering and analysis in various contexts. A prototype implementation and data from different fields demonstrate the effectiveness of this approach.
411546	41154649	Visualization of Documents and Concepts in Neuroinformatics with the 3D-SE Viewer.	A new interactive visualization tool has been proposed for mining text data from neuroscience research. This tool utilizes the spherical embedding (SE) algorithm to visually represent complex relationships between pairs of lexical entities such as terms, keywords, posters, or papers' abstracts. By embedding these entities on spheres in a 3-D space, the tool can convey more information than traditional planar or linear displays. It can also extract and visualize different types of information, such as keywords, indexing terms, or topics, allowing for interactive browsing of various fields of research. This tool operates as an applet on top of existing resources and provides direct access to document sources in databases.
411547	41154789	A safe, efficient regression test selection technique	Regression testing is a crucial but costly process used to ensure that modifications made to software do not negatively impact other parts of the program. A new technique for regression test selection has been developed, which involves creating control flow graphs for the original and modified versions of a procedure or program and using these graphs to select tests that execute the changed code. This technique has been proven to be safe and can potentially reduce the cost of regression testing. It also has the advantage of handling all types of program modifications and language constructs. Initial studies have shown promising results.
411548	411548112	Test Case Prioritization Based on Information Retrieval Concepts	Regression testing involves running all the test cases of a system, which can be time consuming and resource intensive. Test case prioritization (TCP) aims to schedule test cases in a way that achieves specific goals, such as higher coverage or faster fault detection. While most TCP methods focus on code coverage, recent research has explored using additional information to improve effectiveness. This study focuses on using Information Retrieval (IR) techniques to improve TCP, particularly for testing rarely used code. By considering both the frequency of testing and code coverage, our approach utilizes linear regression modeling to balance these factors. Our empirical study shows that this approach is more effective than random or traditional code coverage-based methods, with a potential increase in fault detection rate of up to 4.7%.
411549	41154945	Type-2 fuzzy logic-based classifier fusion for support vector machines	SVMs have become popular as a machine-learning tool due to their promising performance, but their generalization abilities depend on the suitability of selected kernel functions for real classification data. To address this issue, a fuzzy fusion model is proposed in this paper to combine multiple SVMs classifiers. This model uses interval type-2 fuzzy sets to handle uncertainties in real classification data and membership functions in traditional fuzzy logic systems. It considers both the classification results and accuracy information of individual SVMs to generate a more robust combined classification decision. Experiments show that this type-2 based SVM fusion model outperforms individual SVM classifiers and the type-1 based SVM fusion model in most cases. 
411550	411550135	More Efficient Topological Sort Using Reconfigurable Optical Buses	Topological sort is a method used to order the vertices of an acyclic graph in a linear sequence. This has various practical applications, including job scheduling and network analysis. Many models have been developed to solve this problem efficiently, such as the hypercube and shuffle-exchange networks, CRCW PRAM model, and LARPBS model. Dekel et al. proposed an O(log2 N) time algorithm on the hypercube or shuffle-exchange networks with O(N3) processors, while Chaudhuri gave an O(log N) algorithm on the CRCW PRAM model using O(N3) processors. Li et al. showed that the problem can be solved in O(log N) time on the LARPBS model with N3 processors. This paper proposes a more efficient topological sort algorithm on the LARPBS model, with a time complexity of O(log N) and a processor complexity of N3/log N, outperforming previous algorithms on the hypercube and CRCW PRAM models.
411551	41155150	Mining for Structural Anomalies in Graph-based Data	This paper discusses the use of graph-based methods for detecting anomalies in data where the anomalies involve unexpected changes in entities and relationships. The authors introduce three new algorithms that use the minimum description length principle to identify anomalous substructures in graphs, and evaluate their effectiveness using both synthetic and real-world data. They define anomalies in the context of graphs and present examples and results from their algorithms. This research highlights the potential of using graph-based representations for detecting fraud and suggests avenues for future work in this area. 
411552	41155244	Enabling Incremental Query Re-Optimization.	As declarative query processing expands to various platforms, there is a growing need for adaptive techniques to handle unanticipated performance changes. This requires innovation in both cost estimation algorithms and search algorithms. The authors propose a cost-based optimizer that can continuously update the optimal query plan based on new cost information, similar to a stream engine updating its outputs with new data. Their implementation is particularly effective for stream processing workloads and serves as the foundation for future adaptive optimization algorithms. They utilize a recursive datalog query approach and develop novel optimization methods for both static and incremental cases. These techniques can also be applied to traditional optimizer implementations.
411553	411553107	TinyDB: an acquisitional query processing system for sensor networks	We are discussing the design of a query processor that collects data in sensor networks. This processor addresses the "acquisitional" issues of where, when, and how often data is acquired and delivered to query processing operators. By considering the locations and costs of data acquisition, we can reduce power consumption compared to traditional passive systems. Simple extensions to SQL are proposed for controlling data acquisition, and the impact of acquisitional issues on query optimization, dissemination, and execution is discussed. The effectiveness of this design is evaluated through the example of TinyDB, a distributed query processor for smart sensor devices, which demonstrates significant power consumption reductions.
411554	41155431	Towards realistic benchmarks for virtual infrastructure resource allocators	The rise of virtual infrastructures has sparked interest in developing resource allocation systems to reserve and use different combinations of nodes, switches, and network links. However, there is a lack of realistic benchmarks in this relatively new area. This paper proposes a method for creating realistic benchmarks for virtual infrastructure and introduces a workload generator based on a 5-year trace of experiments from the Emulab testbed. The importance and potential uses of these benchmarks are demonstrated through various evaluation scenarios. This research suggests a way to improve resource allocation in virtual infrastructures through the use of realistic benchmarks.
411555	411555226	Plugging Taxonomic Similarity in First-Order Logic Horn Clauses Comparison	Horn clause logic is a representation language used in Logic Programming and Inductive Logic Programming. It is used to express relationships among objects in order to capture relevant information. While the predicates used in this language are defined by the knowledge engineer and handled by interpreters, they may not fully capture all underlying relationships. To address this, a taxonomic background knowledge can be used to better assess the similarity between two descriptions. This approach extends an existing distance framework by incorporating information from the taxonomic background knowledge. The effectiveness of this solution is demonstrated through sample problems.
411556	41155631	Simulating empathic behavior in a social assistive robot.	The paper discusses the importance of social robots in Ambient Assisted Living (AAL) and their role in establishing a social empathic relationship with the user. The NAO robot has been equipped with the ability to recognize the user's emotions through communicative signals from speech and facial expressions. This helps the robot to trigger appropriate empathic behaviors, which have been evaluated by experts and elderly users. The results are encouraging and suggest the potential for further development of the robot's social and affective capabilities.
411557	41155768	A General Similarity Framework for Horn Clause Logic	First-Order Logic formulæ, which use relations, can lead to computational problems due to indeterminacy. To address this issue, a framework for comparing and assessing similarity between two descriptions is needed. This could have many applications in Artificial Intelligence, such as guiding subsumption procedures and implementing flexible matching. However, there is limited research on this topic. This paper focuses on Horn clauses, the basis of Logic Programming, and proposes a new similarity formula and evaluation criteria for identifying similar components in descriptions. Experiments on real-world datasets demonstrate the effectiveness and efficiency of this approach.
411558	41155845	A Logic Programming Framework for Learning by Imitation	This paper discusses the use of imitation as a way for humans to acquire knowledge, by following instructions or demonstrations from others. The authors propose a logic programming framework for teaching agents to learn from relational demonstrations, where the demonstrations are received incrementally and used as training examples while the agent interacts in a stochastic environment. This framework allows for the representation of domain-specific knowledge and complex relational processes in a compact and declarative way. The effectiveness of this framework is demonstrated through experiments in simulated agent domains. 
411559	41155915	Translation and optimization for a core calculus with exceptions	Source languages need to have a balance of complexity and conciseness for programmer use. However, this complexity can make it difficult to analyze and some features may be overlooked. Exception handling is important for robust software, but is often not considered in program analysis and optimization. Traditional methods of converting high level languages to machine code can result in code that is too cryptic for analysis. To address this issue, an intermediate, minimal but expressive, core calculus has been designed to handle major language features. This allows for easier analysis and optimization without sacrificing the flexibility and richness of the source language. 
411560	41156029	Peering and provisioning of differentiated Internet services	The article discusses the importance of maintaining consistent and stable service level agreements (SLAs) for differentiated network services across multiple networks. A game theoretic model is used to investigate the feasibility of this, taking into account raw-capacity sellers, brokers, and users in a two-tier market. The model provides a necessary and sufficient condition for the stability of the game and determines whether a set of SLA configurations among peering ISPs is sustainable. Simulations using a distributed progressive second price auction as the spot market mechanism in a scenario with three interconnected networks and two services validate the analytical results. Overall, the study emphasizes the significance of considering network dynamics and market mechanisms in ensuring stable and consistent SLAs for differentiated network services.
411561	4115611	Deaccumulation techniques for improving provability	The paper discusses the limitations of induction theorem provers in verifying functional programs with accumulating arguments. To address this issue, the authors propose automatic transformations from accumulative to non-accumulative functional programs using concepts from tree transducers and previous research. This approach aims to reduce the need for generalizing induction hypotheses in (semi-)automatic provers, and also has potential applications in reducing the need for inventing loop invariants in the verification of imperative programs. The paper highlights the potential of this approach to improve the efficiency and accuracy of mechanized verification for functional and imperative programs.
411562	41156212	Certification of Termination Proofs Using CeTA	The paper discusses the use of automated tools to prove termination of term rewrite systems, which often involve complex criteria. However, the generated proofs can be too large for humans to easily check for accuracy. To address this issue, the authors propose using the theorem prover Isabelle/HOL to automatically certify termination proofs. They first formalize the necessary theory and develop executable checks for three major termination techniques. These checks not only ensure correct application of the techniques, but also provide readable error messages if a proof is not accepted. Additionally, a certified Haskell program, CeTA, is generated for efficient certification of termination proofs without requiring Isabelle installation. 
411563	41156323	Termination Analysis by Dependency Pairs and Inductive Theorem Proving	Current techniques and tools used for automated termination analysis of term rewrite systems (TRSs) are highly effective. However, they are not able to handle algorithms that rely on inductive reasoning for their termination. To address this issue, we propose a method that combines the dependency pair approach for TRS termination with inductive theorem proving. This approach has been implemented in the tool AProVE and has shown success in analyzing the termination of TRSs that rely on inductive arguments. With this new approach, TRS termination techniques can now handle a wider range of algorithms.
411564	41156433	A Rounds vs. Communication Tradeoff for Multi-Party Set Disjointness	The set disjointness problem involves k players with private inputs who communicate over a shared blackboard to determine if their sets have a common element. The trade-off between the number of rounds of interaction and the total number of bits sent is studied, with a lower bound of Ω̃(nk <sup>1/R</sup> /R <sup>4</sup> ) bits for R rounds. This is nearly tight. The same proof is used to show that wellfare maximization with unit demand bidders cannot be efficiently solved in a small number of rounds. The lower bound for this problem is improved to Ω(log k/log log k), which is known to be tight up to a log log k factor.
411565	41156595	Information Theoretical Limit of Media Forensics: The Forensicability.	This paper discusses the potential limitations of forensic techniques used to detect the processing history of multimedia content. While there have been advancements in this field, it is important to consider if there is a fundamental limit to the capability of forensics. The author suggests that it is important to not only focus on what investigators can do, but also understand the boundaries of their capabilities. The paper explores the fundamental principles of forensics and proposes ways to improve its effectiveness in detecting tampering and manipulation of multimedia content. 
411566	411566107	Energy-Efficient Base-Station Cooperative Operation with Guaranteed QoS	The development and deployment of energy-efficient communication technologies has become a key focus in the information and communications technology sector due to the increasing energy costs and environmental concerns. To improve the energy efficiency of mobile cellular networks, shutting off base stations during off-peak periods is proposed. This paper introduces a strategy for energy-efficient base station switching, using cooperative communication techniques among base stations to extend network coverage. Both path-loss and fading effects are considered in the system model, and closed-form expressions for call-blocking probability and channel outage probability are derived. By identifying user equipment at worst-case locations, the proposed scheme ensures quality of service. Results show that this strategy, with BS cooperation, can significantly reduce energy consumption while maintaining quality of service. 
411567	411567106	Monkey-in-the-browser: malware and vulnerabilities in augmented browsing script markets	As more applications move from desktop to web-based platforms, power users have found ways to customize and enhance them to better suit their needs. This has been made possible through tools like the Greasemonkey extension, which allows users to write scripts that can manipulate the content of any web page. However, the popularity of script-sharing markets has introduced new security risks, with potential for malicious scripts to be installed by unsuspecting users. A study of the most popular Greasemonkey script market reveals numerous vulnerabilities, including some that could be used to access sensitive user data. The practicality of these attacks is demonstrated through a proof-of-concept exploit against a vulnerable script with over 1 million users. 
411568	41156837	A Survey on Mobile Social Networks: Applications, Platforms, System Architectures, and Future Research Directions	Mobile social networks (MSNs) have gained popularity in recent years and offer a variety of applications and services that are of interest to service providers, developers, and users. This paper provides a comprehensive survey of MSNs, distinguishing them from conventional social networks and discussing their platform, solutions, and system architecture. The popular MSN platforms and experimental solutions for existing applications are reviewed, along with the dominant mobile operating systems on which MSNs are implemented. The overall architectural designs of current and future MSN systems are analyzed and proposed, considering both client-server communication and wireless data transmission. The unique features, services, and key technologies of two generations of MSN architectural designs are compared. The paper also introduces the concept of a vehicular social network and its challenges, and concludes with an overview of the major challenges and potential future research directions for MSNs.
411569	41156975	Context-aware regulation of context-aware mobile services in pervasive computing environments	The paper presents a context-aware policy system for controlling mobile services in pervasive computing environments. This system considers the various elements of a pervasive environment, including users, services, contexts, policies, and roles. Depending on the context of a user, such as their location, activity, and role, appropriate policies are selected to determine which services they can see and access. This approach offers a generic model for executing mobile services in different pervasive domains, such as a campus setting. The design and prototype implementation of this system are also discussed.
411570	41157031	A New Symmetry-Based Method For Mid-Sagittal Plane Extraction In Neuroimages	The estimation of the mid-sagittal plane (MSP) in neuroimage analysis has several applications but has been a challenging problem. In this study, a new method was developed using bilateral symmetry maximization and a more suitable error metric to compare different MSP extraction methods. The method was evaluated on a dataset of 164 clinical images and was found to outperform three other state-of-the-art approaches in terms of accuracy and precision. It also does not have limitations regarding imaging protocol or initial head position and is one of the fastest methods, taking only 30 seconds on a regular workstation. This new method shows promise for accurate and efficient MSP estimation in neuroimage analysis.
411571	41157155	Learning Ground CP-Logic Theories by Leveraging Bayesian Network Learning Techniques	Causal Probabilistic Logic (CP-logic) is a language used to express causal relations in different domains. This paper presents an algorithm called SEM-CP-logic that learns CP-theories using Bayesian network (BN) learning techniques. The algorithm transforms the CP-theories into BNs and modifies the BN parameter learning and structure search to ensure that the constructed networks represent valid CP-theories. The paper also compares the learning of CP-theories and BNs, showing that simple CP-theories can be represented with noisy-OR nodes while more complex theories require fully connected networks. In experiments, SEM-CP-logic shows better performance in learning CP-theories with fewer training data compared to BN learning. It is also applied in a medical domain for HIV research and shows competitive results with other methods. 
411572	411572102	Estimating Size of Search Engines in an Uncooperative Environment	The size of a search engine is determined by the number of documents it indexes. This information is crucial for metasearch engines to select and merge results from multiple search engines. Accurately estimating the size of search engines is important for efficient functioning of metasearch engines. A new algorithm has been proposed in this paper that has better accuracy and efficiency compared to existing methods. It also shows better tolerance to unfavorable environments compared to the Sample-Resample approach, which is the most widely used method. This new algorithm can help improve the performance of metasearch engines that use multiple autonomous search engines. 
411573	41157326	Design of a remote procedure call system for object-oriented distributed programming	This paper discusses the development of an RPC system specifically designed for building object-oriented distributed software systems. The main goals of this system are to support inheritance, polymorphism, dynamic binding, and modular development in the implementation of distributed software. The paper outlines the features of this RPC system that meet these requirements and briefly discusses the lessons learned from previous versions that influenced the design of the final system. The final system was implemented as part of the Nexus distributed operating system, and this paper was published in 1998 by John Wiley & Sons, Ltd. 
411574	4115740	Dimensional Affect Recognition from HRV: an Approach Based on Supervised SOM and ELM	Dimensional affect recognition is a complex area that has yet to achieve the necessary accuracy for effective use in human-computer interaction (HCI) applications. In this study, two new methods are proposed to address this issue. The first method is a self-organizing model that learns from the similarity between features and affects and produces a graphical representation of the multidimensional data. The second method uses extreme learning machines, which only require heart rate variability data recorded by a small set of sensors to minimize intrusiveness. These methods were evaluated using two datasets and outperformed existing models on a classification task and dimensional affect estimation. The results demonstrate the potential for these models to improve affect analysis and inform future HCI applications.
411575	41157511	Three-Receiver Broadcast Channels With Common and Confidential Messages	This paper presents a study on the secrecy capacity regions of a three-receiver broadcast channel with one common and one confidential message. Two scenarios are considered: sending the confidential message to two receivers while keeping it secret from the third, and sending it to one receiver while keeping it secret from the other two. The authors propose achieving the secrecy capacity using indirect decoding, Wyner wiretap channel coding, and a novel method of generating secrecy from a publicly available codebook. The inner bounds are shown to be tight for specific types of channels and noise levels. The study provides insights and potential solutions for secure communication in complex broadcast channels.
411576	41157683	Design of an efficient VLSI architecture for non-linear spatial warping of wide-angle camera images	Endoscopic images can be distorted due to the wide-angle camera lenses, making it difficult to use them for diagnostics. This paper presents a digital architecture that can correct the distortion in real-time. The technique uses a polynomial mapping model based on least-squares estimation to map the distorted images onto a corrected image space. Experiments were conducted to test the algorithm, and a hardware implementation was developed using linear processing modules. The architecture was simulated and showed a computation time of 1.8 ms for a 256x192 pixel image, making it suitable for real-time applications in endoscopy systems.
411577	41157793	Retrieval effectiveness of an ontology-based model for information selection	Technology in digital media produces a vast amount of information including audio, video, and images, in addition to traditional text. Retrieving this information efficiently and accurately is challenging, and the traditional method of using keywords often results in either irrelevant information being included or relevant information being overlooked. To overcome this issue, a concept-based model using domain-specific ontologies was developed. This model indexes documents based on their meaning and context rather than just keywords, and a scalable disambiguation algorithm helps to filter out irrelevant concepts. Additionally, an automatic query expansion mechanism is proposed, which utilizes knowledge from the ontology to generate relevant queries. This model has been successfully tested on audio data and has shown to significantly improve both precision and recall compared to keyword-based search. These techniques can be applied to all types of media for efficient information retrieval.
411578	41157848	Update Conscious Bitmap Indices	Bitmap indices are widely used in data warehousing and scientific applications because they are efficient in answering certain types of queries on large data sets. However, their use is limited to read-only data or static snapshots due to the cost of updating or adding new data. Each attribute in a table has multiple bitmaps associated with it, making the insertion of a single record very expensive. To address this issue, a new type of bitmap index called "update conscious" has been proposed. This index only updates the relevant bitmaps for a new record insertion, reducing the cost and improving performance. A cost model is provided to compare it with traditional bitmaps in terms of storage, update time, and query execution.
411579	41157952	A survey on ear biometrics	Ear recognition has become a popular topic in recent literature due to its advantages over other non-contact biometric methods, such as face recognition. It is particularly useful for multi-pose face recognition and can be used in surveillance videos where the face may be obstructed. The ear also shows minimal signs of aging, making it a reliable method for human recognition. However, current ear detection and recognition systems are limited to controlled indoor conditions. Challenges such as variations in lighting, hair obstruction, and ear individuality still need to be addressed. This article provides a comprehensive overview of ear detection and recognition research, discussing current advancements and potential areas for further development. It also highlights available databases for researchers to use in their studies.
411580	41158053	Examining the robustness of sensor-based statistical models of human interruptibility	The paper discusses the limitations and potential solutions for current systems that often cause socially awkward interruptions or demand attention when a person is busy. Previous studies have explored using sensors and statistical models to estimate human interruptibility in an office setting, but there are concerns about the robustness of this approach. The paper presents findings that show sensors can be constructed with enough accuracy to drive predictive models, and that these models can be applied to a wider range of people. The effects of training data quantity on accuracy are also examined, along with the tradeoffs of using different sensor combinations. Overall, the paper suggests that incorporating these models into systems can improve human-computer interaction and communication.
411581	41158146	Configuration analysis and recommendation: Case studies in IPv6 networks.	This paper focuses on studying the configurations of IPv6 networks and compares them to the more well-researched configurations of IPv4 networks. The study was conducted on two networks - a pure IPv6 network and a dual-stack network. The authors found that IPv6 configurations are more complex due to the complexity of IPv6 addresses, but have fewer command lines and less reliance on references compared to IPv4. The rapid development of IPv6 has led to a higher growth rate in configurations. The authors also propose a framework for network configuration recommendation based on their analysis. Understanding and managing configurations is crucial for the development of IPv6 networks.
411582	41158230	A Channel-based Coordination Model for Components	This paper presents a coordination model for component-based software systems using mobile channels, which allow for anonymous and dynamic communication between components. The model supports mobile components and promotes efficient interaction between them. It also separates the computational and coordination aspects of a system, allowing for transparent and external development of the coordination structure. The Java implementation of this model is self-contained and suitable for developing component-based systems in object-oriented languages. However, it can also be extended to incorporate other aspects of components beyond composition and coordination.
411583	4115839	Load balancing in MapReduce environments for data intensive applications.	Distributed computations are a common method for processing large scale tasks in modern technology. The Hadoop framework, based on the Google MapReduce model, has gained popularity for its high processing power and user-friendly nature. However, in a heterogeneous computing environment, the lack of load management can lead to a decline in performance. To address this issue, a load balancing algorithm has been developed and tested using the Hadoop simulator HSim. The results show a significant improvement in cluster performance with the implementation of this algorithm. This paper discusses the details of the algorithm and its successful impact on enhancing the performance of the Hadoop framework.
411584	41158417	Self-similarity in World Wide Web traffic: evidence and possible causes	The concept of self-similarity has been applied to both wide-area and local-area network traffic. In this paper, the authors examine the reasons behind the self-similarity of network traffic. They propose a possible explanation for this phenomenon by studying a subset of wide area traffic - traffic from the World Wide Web (WWW). Using data from over half a million requests for WWW documents, they analyze the dependence structure of WWW traffic and find evidence that it exhibits characteristics of self-similarity. This is attributed to factors such as the distribution of document sizes, caching and user preferences, user "think time", and the overlapping of multiple transfers in a local network. The authors support their findings with empirical data from their own traces and from other sources.
411585	4115857	Optimizing Queries with Universal Quantification in Object-Oriented and Object-Relational Databases	This study focuses on optimizing and evaluating queries with universal quantification in object-oriented and object-relational data models. The queries are classified into 16 categories based on referenced variables. The authors propose new query evaluation plans, utilizing anti-semijoin, division, generalized grouping with count aggregation, and set difference. Performance analysis was conducted on various database configurations, showing that anti-semijoin-based plans were the most effective, even when using advanced division algorithms. Additionally, the study found that anti-semijoin plans can be derived in object-oriented models where they are not possible in relational models. This highlights the benefits of leveraging object-oriented features for query optimization.
411586	41158678	The wave kernel signature: A quantum mechanical approach to shape analysis	The Wave Kernel Signature (WKS) is a method for characterizing points on non-rigid three-dimensional shapes. It measures the average probability of finding a quantum mechanical particle at a specific location, with varying energy levels. This allows for separation of information from different Laplace eigenfrequencies, making the WKS useful in various applications. The WKS is shown to be more effective in feature matching than the commonly used Heat Kernel Signature (HKS), both theoretically and in experiments. As an example, the WKS is used in shape analysis for shape matching.
411587	411587129	An open software architecture for virtual reality interaction	OpenTracker is an open software architecture that simplifies the development and maintenance of hardware setups for virtual reality and augmented reality applications. It uses an object-oriented design based on XML and features a data flow concept for multi-modal events. The engine is multi-threaded for optimal performance and allows for transparent network access. The developer's interface includes both time-based and event-based models for versatile use. OpenTracker aims to be a "write once, input anywhere" solution for virtual reality development and has already been integrated into an augmented reality system. It also demonstrates how consumer input devices can be used for mobile augmented reality with the help of OpenTracker. The software will be made available to the public under an open source license once it is fully developed.
411588	41158845	Statistical sampling of microarchitecture simulation	Current microarchitecture simulators are significantly slower than the hardware they simulate, leading to inaccurate and misleading conclusions in microarchitecture design studies. This article introduces the Sampling Microarchitecture Simulation (SMARTS) framework, which allows for fast and accurate performance measurements of full-length benchmarks by selectively measuring a subset of instructions. Through analysis of the SPEC CPU2000 benchmark suite, it is shown that SMARTS can estimate CPI and energy per instruction (EPI) with 99.7&percnt; confidence by measuring fewer than 50 million instructions per benchmark. Despite some uncertainty in microarchitectural state initialization, SMARTS achieves an average error of only 0.64&percnt; on CPI and 0.59&percnt; on EPI, with significant speedups compared to detailed simulation.
411589	41158943	Exploiting reference idempotency to reduce speculative storage overflow	Recent proposals for multithreaded architectures use speculative execution to allow threads to run in parallel, even if their dependencies are unknown. This is achieved through hardware speculative storage that tracks data dependencies and corrects incorrect executions through roll-backs. However, the use of small memory structures for speculative storage results in performance loss due to overflow when a thread's speculative state exceeds the storage capacity. To address this issue, the article introduces the concept of memory reference idempotency, which identifies references that can be eventually corrected and do not need to be tracked in speculative storage. The article presents a formal framework, a compiler-assisted model and an algorithm to label idempotent memory references for the hardware. Experimental results show that a large number of references in nonparallelizable program sections are idempotent, reducing the demand for speculative storage space in larger threads.
411590	41159027	Race detection for event-driven mobile applications	Mobile systems, like Android, use an event-based model of concurrent programming, which is supported by the devices' sensors and user input options. However, most existing tools for detecting concurrency errors focus on a thread-based model, leading to inaccuracies when applied to event-based programs. In this paper, the authors introduce CAFA, a race detection tool specifically designed for event-driven mobile systems. This tool uses a causality model that accounts for the causal order of events in the Android system, which is often ignored by other data race detectors. By checking for races between high-level operations instead of low-level memory accesses, CAFA reduces the number of false positives and has successfully identified harmful races in open-source Android applications.
411591	41159144	The future is cloudy: Reflecting prediction error in mobile applications	Mobile applications rely on predicting the future to make decisions in the present, but these predictions are often uncertain. However, applications typically assume they are completely accurate, which can lead to incorrect decisions and negative consequences. To address this issue, prediction error should be considered as a fundamental aspect in mobile systems. This can be achieved by taking into account uncertainty when evaluating alternatives and using redundant strategies when there is no clear superior option. A system has been developed to quantify uncertainty and allow applications to balance the tradeoff between redundancy and resource usage. This approach has been shown to significantly improve application performance compared to other strategies.
411592	411592144	A formal requirements engineering method for specification, synthesis, and verification	This paper introduces a formal requirements engineering method that combines multiple established formal methods to capture specification, synthesis, and verification. The approach uses temporal logics to express abstract specifications and Statecharts to support the development of more detailed specifications. The two layers are connected through a semi-automatic synthesis process and automatic formal verification through model checking. The method is demonstrated through a detailed user session.
411593	4115938	Encoding Asynchronous Interactions Using Open Petri Nets	The article introduces an encoding method for representing asynchronous CCS processes with replication using open Petri nets. In this method, ordinary Petri nets are used with a designated set of open places to model the interactions between processes and their environment. The encoding is able to maintain the strong and weak bisimilarities of CCS, allowing for a precise correspondence between the two formalisms. The goal of this work is to promote the transfer of technology between these two methods, and it is shown how results on expressiveness can be transferred back and forth between the calculus and the nets.
411594	41159431	The effects of asymmetry on TCP performance	This paper examines the impact of network asymmetry on TCP performance and suggests methods to enhance it. The study focuses on two types of networks commonly found in mobile ad hoc networks - a wireless cable modem network and a packet radio network. Previous research has primarily looked at bandwidth asymmetry, but this study expands the concept to include other forms of asymmetry such as latency, media-access, and packet error rate. By conducting experiments and simulations, the researchers analyze TCP performance in these networks where the reverse direction (from receiver to sender) plays a significant role in achieving desired throughput. They propose and evaluate various techniques, including ack congestion control, ack filtering, TCP sender adaptation, and acks-first scheduling at the reverse bottleneck router, to improve end-to-end performance. 
411595	41159543	Chimpp: a click-based programming and simulation environment for reconfigurable networking hardware	Reconfigurable network hardware offers a convenient way to experiment with and prototype high-speed networking systems, but it can be difficult to program and requires co-development with software. To address this issue, Chimpp is a development environment modeled after the Click modular router system. It uses a modular approach and a simple configuration language, similar to Click, to design hardware-based packet-processing systems. Chimpp can be combined with Click at the software layer and allows for integrated simulation with the OMNeT++ network simulator. The goal of Chimpp is to make experimentation easy by providing a toolbox of reusable, modular elements and allowing for customization and incorporation of existing hardware modules. Initial evaluations show that it simplifies the implementation, simulation, and modification of packet-processing systems on the NetFPGA platform.
411596	41159657	Improved Scene Reconstruction From Range Images	Modeling real scenes is difficult and using laser rangefinders is a popular method. However, it is often not feasible to capture all surfaces in a scene due to occlusions and accessibility limitations. This can result in incomplete or inaccurate models. The paper proposes a pipeline and system that can enhance model reconstruction using incomplete range images. This approach aims to improve the overall quality of the reconstructed models by utilizing available information effectively. The system implementation provides a practical solution for dealing with incomplete data, making it a valuable tool for modeling real scenes.
411597	41159732	Turing Trade: A Hybrid of a Turing Test and a Prediction Market	Turing Trade is a web-based game that combines elements of a Turing test and a prediction market. The game involves a mystery conversation partner, known as the "target," who is either a human or a bot. Multiple judges participate in the game by interrogating the target and placing bets on whether it is a human or a bot. The game collects detailed data on the judges' changing beliefs throughout the conversation, providing insights into specific moments where the target's response influenced their belief. This game offers advantages over traditional Turing tests, such as more precise data and a more enjoyable experience for participants. This paper describes how Turing Trade works, provides example logs, and analyzes its effectiveness through real user data. 
411598	41159897	A new approach to countering ambiguity attacks	Watermarking schemes for resolving ownership disputes are vulnerable to ambiguity attacks, which exploit the high false-positive rate of the scheme. To address this issue, a new scheme is proposed that embeds multiple watermarks and detects a randomly selected subset during ownership proof. This is achieved through the use of one-way functions for watermark generation and selective detection to inject uncertainty. The false-positive probability is reduced compared to single watermark embedding, as shown through numerical analysis. The security level of the proposed scheme is also examined with different parameters, using a modified notion of security that is more practical. This allows for a better understanding of the achievable security with typical parameters. 
411599	41159983	Evaluating Human-Robot Interaction Focusing On The Holistic Interaction Experience	The paper discusses the challenges in designing and evaluating human-robot interaction (HRI) due to the unique experience of interacting with a robot. Unlike other technologies and artifacts, robots have a strong social and emotional component, which is influenced by their dynamic presence in the real world and their ability to invoke a sense of agency. This complexity of interaction requires careful consideration when applying HCI evaluation methods to HRI. The paper proposes a holistic view of social interaction with robots and introduces three perspectives for exploring this interaction: visceral factors, social mechanics, and social structures. The authors also present a heuristic for brainstorming different types of interaction experiences, called the interaction experience map. These perspectives and heuristic can be used to guide the evaluation process of HRI.
411600	41160042	The complexity of the free space for a robot moving amidst fat obstacles	The article introduces a new definition of fatness for geometric objects and compares it to other definitions. It demonstrates that, based on certain assumptions, the complexity of free space for a robot with a set number of degrees of freedom moving in a d-dimensional Euclidean workspace with fat obstacles is directly related to the number of obstacles present. This has implications for the complexity of motion planning algorithms, which can be very high in theory. The findings suggest the potential for developing efficient motion planning algorithms in realistic situations.
411601	41160129	Competitive on-line coverage of grid environments by a mobile robot	The paper discusses two algorithms, called Spanning Tree Covering (STC), for covering planar areas using a square-shaped tool attached to a mobile robot. The algorithms divide the area into a grid of cells and use a spanning tree of a grid graph to guide the robot's movements. The first algorithm, Spiral-STC, creates spiral-like patterns while the second, Scan-STC, creates scan-like patterns along a specific direction. Both algorithms can cover any planar grid using a path no longer than (n + m)D, where n is the total number of cells and m is the number of boundary cells. The paper also shows that any on-line coverage algorithm will have a path length of at least (2 - ε)lopt, where lopt is the length of the optimal off-line path. The STC algorithms are worst-case optimal and generate close-to-optimal paths in practical environments.
411602	41160220	An adaptive method for image registration	The paper discusses a new technique for registering images with non-linear local geometric distortions. This involves dividing the image into smaller subregions based on the type of distortion and applying a simple local transformation to each subregion. This method allows for registration with varying levels of accuracy and has been compared to other existing methods in terms of accuracy and computational complexity. A practical application of this method is also demonstrated.
411603	41160313	Fair Division under Ordinal Preferences: Computing Envy-Free Allocations of Indivisible Goods	This article discusses the issue of dividing a set of goods fairly among a group of individuals. The problem arises when the individuals have preferences for different bundles of goods, but this information is incomplete and expressed in a compact language. The article explores the algorithmic problem of determining if an allocation can be made that satisfies envy-freeness and economic efficiency, given the limited preference information. The authors present simple characterizations and algorithms for some cases, while also showing the complexity of the problem in other instances. 
411604	41160447	Automatic Verification of Safety and Liveness for XScale-Like Processor Models Using WEB Refinements	This article discusses the use of Well-founded Equivalence Bisimulation (WEB) refinement to automatically verify that complex pipelined machine models, similar to the XScale architecture, adhere to the same safety and liveness properties as their corresponding instruction set architecture models. This is achieved by converting the WEB-refinement proof obligation into a formula in the logic of Counter arithmetic with Lambda expressions and Uninterpreted functions (CLU), which is then checked using a SAT solver. The models being verified include advanced features such as out of order completion, precise exceptions, branch prediction, and interrupts. Two types of refinement maps are used for this verification process: flushing and commitment. Experimental results and verification times are also presented, with liveness being found to account for approximately 5% of the overall verification time.
411605	4116054	A distributed index for efficient parallel top-k keyword search on massive graphs	In this paper, the authors propose a new distributed disk-based index for optimizing keyword search on graphs. This index can be constructed in a MapReduce manner and utilizes heuristics to track and prune irrelevant vertices before search. Additionally, a parallel search algorithm is developed, which runs multiple asynchronous search instances to incrementally find the top-k answers. Experiments on synthetic and real graphs demonstrate that this approach significantly improves search efficiency on massive graphs while keeping indexing overheads affordable.
411606	411606121	Word sense disambiguation with pictures	This article discusses the use of images for word sense disambiguation, either alone or in combination with traditional text-based methods. The approach is based on a statistical model that automatically annotates images using a joint probability for image regions and words. The model is learned from a database of images with associated text. The predicted words are constrained to be possible senses for the word being considered, making the word prediction more reliable. The article reports on experiments using this method, both on its own and in conjunction with a text-based disambiguation algorithm. A new corpus, ImCor, was created to evaluate the effectiveness of this approach, using a portion of the Corel image dataset with disambiguated text from the SemCor corpus. The results suggest that incorporating visual information can greatly aid in disambiguating word senses and help to ground language meaning.
411607	41160714	Binary and Multi-Class Learning Based Low Complexity Optimization for HEVC Encoding.	HEVC is a video coding technology that improves compression efficiency but requires high computational complexity. To address this issue, a fast HEVC encoding algorithm is proposed in this paper. It uses binary and multi-class support vector machine (SVM) to optimize the processes of recursive CU decision and PU selection, reducing the need for intensive rate distortion (RD) cost calculation. The algorithm also incorporates a learning method and optimal parameters determination scheme for better prediction performance. Experimental results show that it outperforms previous fast coding algorithms in terms of time-saving and RD performance. 
411608	41160813	Multi-modal learning for affective content analysis in movies	Affective content analysis is an important topic in video content analysis with various applications. However, designing a computational model for predicting emotions induced by videos is challenging due to the subjectivity of emotions. To address this, a multi-modal learning framework is proposed that uses motion keypoint trajectory and convolutional neural networks to depict visual emotions, and a global audio feature from the openSMILE toolkit for audio emotions. Linear support vector machines and support vector regression are used to learn affective models. Comparing these features to five baseline features, it is found that they are significant in describing affective content and complement each other. The proposed framework also achieves state-of-the-art results on two challenging datasets.
411609	411609100	Low-Complexity Adaptive Block-Size Transform Based on Extended Transforms.	This paper proposes a low-complexity Adaptive Block-size Transform (ABT) scheme for the Chinese Audio and Video coding Standard (AVS)1. The scheme uses an integer 8x8 transform derived from the 4x4 transform used in AVS, with a focus on high energy compacted property and efficient implementation. The 8x8 transform shares the same scale matrix with the 4x4 transform, leading to efficient use of hardware and storage resources for both encoder and decoder. Experimental results on various sequences demonstrate the significant performance improvement achieved by the proposed ABT scheme for AVS. 
411610	411610330	How many users should be turned on in a multi-antenna broadcast channel?	This paper discusses broadcast channels with a base station having L antennas and m single-antenna users. Partial channel state information is available through finite rate feedback. The optimal number of on-users (s) is found to be dependent on the signal-to-noise ratio (SNR) and feedback rate. An asymptotic analysis is used for large values of L, m, and feedback rate, yielding an optimal feedback strategy and a criterion for turning on users. The resulting spatial efficiency, defined as asymptotic throughput per antenna, is also dependent on s. A scheme is proposed for systems with finite antennas and users, achieving a significant gain compared to previous studies with a constant s. The analysis and scheme are applicable to heterogeneous systems with varying path loss coefficients and feedback rates.
411611	41161152	Supremum preserving upper probabilities	This article discusses the relationship between possibility measures and the theory of imprecise probabilities. It argues that possibility measures play a significant role in this theory and shows that a possibility measure is a coherent upper probability if and only if it is normal. The differences between the possibilistic and natural extension of an upper probability are examined, and it is proven that a possibility measure can be seen as the restriction of the natural extension of a special type of upper probability. The concept of possibilistic extension is also explained and related to natural extension. The article then moves on to discuss the connection between upper probabilities and upper previsions, showing that a coherent upper prevision must take the form of a Shilkret integral associated with a possibility measure if it is supremum preserving. However, it is also shown that such a supremum preserving upper prevision is never coherent unless it is the vacuous upper prevision with respect to a non-empty subset of the universe of discourse.
411612	41161239	A New Definition of the Subtype Relation	Hierarchy plays a crucial role in object-oriented design as it enables the use of type families, where higher level supertypes encompass the shared behavior of their subtypes. It is essential to have a thorough understanding of the relationship between subtypes and supertypes for this approach to be successful. A new definition of the subtype relation is proposed in this paper, ensuring that any property proven for supertype objects also applies to subtype objects. The implications of this definition on the design of type families are also discussed. 
411613	41161390	Combining OWL ontologies using E-Connections	In this paper, the authors address two unresolved issues in the standardization of the Web Ontology Language (OWL): representing and reasoning with multiple linked ontologies, and promoting knowledge sharing and reuse on the Semantic Web. They propose using E-Connections as a solution to these problems, providing a modular approach for developing web ontologies and an alternative to the owl:imports construct. This involves a syntactic and semantic extension of OWL-DL, which can be applied in different modeling situations and optimized using reasoning algorithms for logics such as SHIN(D), SHON(D), and SHIO(D). The authors also present support for E-Connections in two tools: an ontology editor, SWOOP, and an OWL reasoner, Pellet.
411614	41161423	Annotating and searching web tables using entities, types and relationships	Tables are a commonly used method for displaying relational data, and billions of tables on the web contain information about entities, attributes, and relationships. These structured representations are often more useful than unstructured text. However, since web tables are not manually created, they lack a formal, uniform schema, making it difficult for search engines to utilize them effectively. In this paper, the authors propose new machine learning techniques to automatically label table cells with entities, column types, and relationships, using a graphical model. Experiments with various datasets show the effectiveness of this approach, and the authors also demonstrate the benefits of these annotations for a prototype relational web search tool. 
411615	41161591	Resource-Efficient Routing and Scheduling of Time-Constrained Network-on-Chip Communication	Network-on-chip-based multiprocessor systems-on-chip (NoC MPSoCs) are becoming increasingly popular as embedded systems platforms. A key aspect of mapping applications onto these parallel platforms is scheduling communication on the NoC. This paper introduces various scheduling strategies that take advantage of the flexibility of NoCs to minimize resource usage. Results from experiments demonstrate that these strategies lead to improved resource utilization compared to existing techniques. Overall, this research contributes to the development of more efficient and optimized NoC MPSoCs, which are expected to play a significant role in future embedded systems.
411616	41161620	Representation Learning via Semi-Supervised Autoencoder for Multi-task Learning	Multi-task learning is a method that aims to learn multiple related tasks simultaneously. There are two approaches to multi-task learning: one focuses on shared feature space for knowledge sharing, while the other focuses on sharing model parameters among tasks. However, these approaches have limitations. To address this, the paper proposes a feature representation learning framework, called Semi-Supervised Autoencoder for Multi-Task Learning (SAML), which combines the benefits of both approaches. SAML uses autoencoders to learn feature representations from unlabeled data and a regularized multi-task softmax regression method to learn distinct prediction models for each task. Experiments on real-world data sets show the effectiveness of SAML, and its feature representations can also be used by other methods for improved results. 
411617	41161716	On Subjective Trust for Privacy Policy Enforcement in Cloud Computing	Cloud computing has become a crucial part of the technology ecosystem, forcing clients to interact with cloud service providers, even if they do not fully trust them. To address this issue, a policy-based approach is proposed to implement subjective trust in privacy policy enforcement within a cloud computing environment. This approach involves acknowledging each client's individual trust assessment and providing services accordingly. An abstract model is presented, consisting of computational, storage, and monitoring units with configurable elements. Algorithms are described to show how changes in trust affect the configuration of these elements. A prototype storage middleware is also developed based on this model and its performance is benchmarked.
411618	41161814	Policy-driven autonomic management of multi-component systems	Policies are proposed to guide the behavior of systems and applications and manage violations. In complex systems like e-commerce, different policies are used to manage individual components. The autonomic manager uses these policies to make decisions when a violation occurs. However, this can result in multiple directives, so heuristics are used to determine the most appropriate action. This study focuses on the design and implementation of an autonomic manager using these heuristics, which was tested on a dynamic web server. Results showed the effectiveness of the heuristics in managing violations.
411619	411619106	Hypermedia presentation generation in Hera	Hera is a methodology used to design Semantic Web Information Systems (SWIS) by separating different aspects into models represented using RDF. It has two phases: data collection and presentation generation. The focus is on the presentation generation phase, which has two variants: a static one that creates a full Web presentation at once and a dynamic one that allows the user to influence the next page. The dynamic variant includes new models to capture user interactions. The implementation involves applying data transformations to the models to produce a hypermedia presentation. Overall, Hera aims to create efficient and user-friendly SWIS designs.
411620	41162093	Interweaving Trend and User Modeling for Personalized News Recommendation	This paper explores user modeling on Twitter and how personal interests and public trends interact. The authors propose a framework that enriches the semantics of Twitter messages to create meaningful user profiles, which can be used for personalized applications. They analyze user and trend profiles using a large Twitter dataset and evaluate their effectiveness in a personalized news recommendation system. The results show that personal interests are more significant than public trends in the recommendation process, but combining both types of profiles can lead to even better recommendations. This study highlights the importance of considering both personal interests and public trends in user modeling on Twitter.
411621	41162153	Compiler/Interpreter Generator System LISA	The paper discusses the LISA system, a versatile tool for developing programming languages. LISA uses formal language specifications to create a language-specific environment including an editor, compiler/interpreter, and graphic tools. The paper highlights the design choices, implementation challenges, and integration of various tools in LISA. The main motivations for developing LISA were to support incremental language development, enable visual language design, and ensure portability. LISA comprises scanner, parser, and compiler generators, as well as graphic tools, an editor, and conversion tools, all connected through well-designed interfaces. It is written in Java, making it highly portable across different platforms. Overall, LISA offers the benefits of both a single system and a federated environment.
411622	4116221	Preference Between Allocentric and Egocentric 3D Manipulation in a Locally Coupled Configuration.	This study explores user preference for manipulating 3D objects on mobile devices using allocentric and egocentric methods. The experiment evaluates preference for translation, rotation, and full 6-DOF manipulation, and also considers the impact of context by testing in different 3D scenes. The influence of each manipulation axis is also examined. The results offer guidance for interface designers on selecting a default mapping in this type of setup.
411623	41162311	Towards Discourse Meaning	The article discusses issues related to dependencies at the discourse and sentence level. It describes the Penn Discourse Treebank (PDTB) corpus, which annotates discourse connectives and their arguments, as well as attributions and their relationship to dependencies. The PDTB is a valuable resource for natural language processing tasks and follows a theory-neutral approach to annotation. It also provides sense labels for each relation and separately annotates attributions. The PDTB has been released in two versions, with the latest one being available through the Linguistic Data Consortium (LDC).
411624	41162415	Detection of Significant Sets of Episodes in Event Sequences	The authors propose a method for detecting "unusual" sets of event sequences within a large event stream. This includes investigating permutations of the same sequence, where the order of events does not matter. To ensure reliability, the method compares measurements to a probabilistic reference model. The authors provide a precise analysis and determine a threshold for detecting unusual episodes, taking into account the potential occurrence of multiple patterns within the same window. The method is tested on a real-world dataset and shows good agreement with the theoretical analysis. This paper builds upon previous work and extends it to the detection of multiple episodes simultaneously.
411625	41162532	Modeling ill-structured optimization tasks through cases	CABINS is a framework used for modeling optimization tasks in ill-structured domains where exact models are not available and the user's idea of optimality is subjective and context-dependent. It uses case-based reasoning to iteratively revise a solution. Task structure analysis is used to create an initial model and generic vocabularies are specialized into case feature descriptions for specific problems. Through experimentation on job shop scheduling problems, CABINS has been shown to effectively operationalize and enhance the model by accumulating cases. 
411626	411626166	Large-scale malware indexing using function-call graphs	The AV industry faces a challenge in processing the large number of malware samples received daily. One solution is to quickly determine if a new sample is similar to known malware. The paper presents SMIT, a malware database management system that uses function-call graphs to efficiently make this determination. Each malware program is represented as a graph, allowing for a nearest-neighbor search in a graph database. To speed up the search, a method to compute graph similarity and a multi-resolution indexing scheme are developed. Results from testing on a database of over 100,000 malware show the effectiveness and scalability of SMIT's search mechanisms.
411627	41162733	Supporting revisitation with contextual suggestions	Web browsers often lack features that allow users to easily revisit pages they have not recently visited. To address this issue, a browser toolbar has been developed that reminds users of previously visited pages related to the one they are currently viewing. This toolbar uses a combination of ranking and propagation methods to make recommendations. In a user evaluation, it was found that on average, 22.7% of revisits were triggered by the toolbar, significantly changing the participants' revisitation habits. This paper discusses the effectiveness of the recommendations and the implications of the evaluation results. 
411628	41162812	xTagger: a new approach to authoring document-centric XML	The process of creating document-centric XML documents in humanities disciplines differs from the typical approach used by standard XML editing software, which focuses on data-centric views of XML. Document-centric encoding involves starting with the content and then adding markup, while data-centric XML involves first creating a tree structure and then adding content. This paper discusses the development of a tool called xTagger, originally designed for the Electronic Boethius project and later enhanced for the ARCHway project. Both projects aim to create methods and software for preparing electronic editions of historic manuscripts using image-based techniques.
411629	41162915	Informing the Requirements Process with Patterns of Cooperative Interaction	The RE community recognizes the importance of understanding the social context in which computer-based systems will be used. Ethnographic studies have been used to inform the requirements process from a social perspective. However, there has been little focus on how these findings can be generalized and reused in new settings. This paper introduces a resource called Patterns of Cooperative Interaction, which compares and contrasts ethnographic findings, discusses their relevance to design, and introduces the analytic capabilities of such studies. The paper explains how these interaction patterns were developed from a collection of ethnographic studies and provides examples of how they can be used by requirements engineers to identify potential social issues and generate requirements that support social interaction.
411630	41163026	Context sharing in a 'real world' ubicomp deployment	The article discusses the use of ubicomp systems for context sharing, which has gained interest but has not been extensively studied outside of lab settings. It focuses on the Hermes interactive office door display system, which allows for asynchronous messaging between users in an office setting. The article presents an analysis of the context sharing behaviors that emerged during the deployment of the Hermes system. It identifies a range of issues related to context sharing, including privacy concerns and ease of use, based on qualitative data from user interviews and questionnaires.
411631	41163151	Computing machine-efficient polynomial approximations	Polynomial approximations are commonly used in computing systems, but often the coefficients of the polynomial cannot be represented exactly with a finite number of bits. This is due to the limitations of floating-point representations in software and the need for fast and inexpensive multipliers in hardware. To address this, polynomial approximations with coefficients that have a maximum number of fractional bits are considered. This means that the coefficients are rational numbers with denominators that are powers of 2. A method is proposed for finding the best polynomial approximation under this constraint, and it can also be applied to other constraints such as minimizing relative error or having some coefficients equal to predefined constants.
411632	41163238	Health education in rural communities with locally produced and locally relevant multimedia content	Health education is crucial in addressing common health issues in developing countries. In rural communities, Community Health Workers (CHWs) play a vital role in spreading health education information. This paper discusses important principles for designing solutions to create and distribute digital health content in rural areas, based on previous work in health education and CHW training. The model presented involves providing tools for rural health professionals to independently create health content within their communities. Lessons learned from implementing this model in Lesotho highlight the benefits of using locally produced and relevant digital content in health education. This approach has the potential to greatly improve health outcomes in rural communities.
411633	4116333	On a New Method for Dataflow Analysis of Java Virtual Machine Subroutines	The Java Virtual Machine's bytecode verifier is responsible for checking the type safety of Java bytecode, making it a key component of Java's security model. However, there are technical issues with the type system for bytecode, especially when it comes to handling subroutines. This paper proposes a new type system for Java Virtual Machine subroutines, building on the work of Stata and Abadi and Qian. The new type system includes types like last(x) and return(n), which allow for purely type-based analysis of instructions and simplify the bytecode verification process. Additionally, this method is more powerful and has no limitations on the number of subroutine entries and exits.
411634	41163410	Mechanising Partiality With Re-implementation	Partial functions are often overlooked, but they are important in practical deduction systems. Kleene's three-valued logic allows for rejecting faulty formulae that the simpler two-valued logic would accept. However, existing theorem provers cannot easily be adapted to this approach. Recently implemented calculi for many-valued logics are also not suitable, as they do not exclude undefined elements. In this work, the authors show that a two-valued theorem prover can be enhanced with a simple strategy to generate proofs for three-valued logic theorems. This allows for the use of an existing theorem prover for a large part of the language. For more information, see [Far90].
411635	41163554	Events and Controversies: Influences of a Shocking News Event on Information Seeking	The internet has been criticized for creating a "filter bubble" where users are only exposed to information that aligns with their existing beliefs, leading to intellectual isolation and limited exposure to alternative viewpoints. This phenomenon is particularly harmful when it comes to discussions on emotionally charged topics, such as gun control, where people tend to become even more entrenched in their beliefs. Researchers seek to study the impact of large-scale news events, like mass shootings, on people's information-seeking behavior and access to diverse perspectives. Using measures of information diversity, they analyze the browsing patterns of users before and after such events to understand the influence of the news on their search and browsing habits. 
411636	411636117	Using latent semantic analysis to improve access to textual information	This paper presents a new method for addressing the vocabulary problem in human-computer interaction. Traditional approaches rely on exact word matching, but this is limited due to the vast range of words people may use to describe the same thing. The proposed solution is to use latent semantic indexing, which organizes text objects into a semantic structure to better match user requests. This is achieved by breaking down a large word by text-object matrix into smaller, orthogonal factors, resulting in a 50 to 150 dimensional representation for terms and objects. Initial tests have shown this approach to be effective in improving access to various types of textual materials and descriptions.
411637	41163781	Two-Party Privacy Games: How Users Perturb When Learners Preempt.	Internet tracking technologies and wearable electronics generate a large amount of data that is used by machine learning algorithms. This amount of data is expected to increase with the emergence of the internet of things and cyber-physical systems. While these technologies offer many benefits, they also pose a risk to privacy. To address this, machine learning algorithms can add noise to outputs using the concept of differential privacy. Users can also protect their privacy by injecting noise into the data they report. This paper examines the relationship between privacy and accuracy, and user and learner perturbation in machine learning. It proposes a solution for the case of an averaging query, where either users or learners perturb the data, but not both. The threshold for learner perturbation increases as incentives become more misaligned. The paper also suggests future directions for this topic and hopes to encourage further research in the area of differential privacy games. 
411638	41163842	Using Attribute-Based Access Control to Enable Attribute-Based Messaging	Attribute Based Messaging (ABM) allows message senders to create a targeted list of recipients based on their attributes from an enterprise database. This can reduce unnecessary communications and enhance privacy, but there are challenges with access control. In this paper, the authors propose an approach to ABM that uses the same attribute database for addressing and access control. They demonstrate a manageable access control system, show how it can be integrated with existing messaging systems, and prove its efficiency for mid-size enterprises. Their implementation can dispatch ABM messages for a large number of users with minimal latency.
411639	41163943	Not so greedy: Randomly Selected Naive Bayes	The attribute selection approach is one way to improve the performance of Naive Bayes, and it has been particularly successful. There are two main types of algorithms for attribute selection: filters and wrappers. Filters use general data information to select attributes before the learning algorithm is applied, while wrappers use the learning algorithm itself to evaluate the selected attributes. This paper focuses on the wrapper approach and introduces a new algorithm called Randomly Selected Naive Bayes (RSNB). Three versions of RSNB are designed for classification, ranking, and probability estimation tasks. Experiments on various datasets show that RSNB is effective in terms of classification accuracy, area under the ROC curve, and conditional log likelihood.
411640	41164049	MFQE 2.0: A New Approach for Multi-Frame Quality Enhancement on Compressed Video	The past few years have seen significant progress in using deep learning to improve the quality of compressed images and videos. However, most existing methods only focus on enhancing individual frames and do not consider the similarities between consecutive frames. This paper introduces a new task called Multi-Frame Quality Enhancement (MFQE) which utilizes the similarities between frames to enhance the quality of low-quality frames using neighboring high-quality frames. The proposed approach includes a BiLSTM-based detector to locate Peak Quality Frames (PQFs) and a Multi-Frame Convolutional Neural Network (MF-CNN) to compensate for motion and reduce compression artifacts. Experiments show that this approach is effective and can improve the quality of compressed videos. 
411641	41164166	A Morphological Approach to the Generalised 2-stage Stock-Cutting Problem	This paper discusses the two-stage stock-cutting problem, where rectangular pieces need to be cut from a general shape object with holes or defects. Mathematical morphological operators are used to find the best shifting for the cutting pattern, and simulated annealing is used to determine the optimal cutting pattern. The paper also presents experimental results.
411642	41164213	Abstractive Document Summarization With A Graph-Based Attentional Neural Model	Abstractive summarization, the goal of document summarization research, has received less attention due to limitations in text generation techniques. However, advancements in neural models have led to progress in abstractive sentence summarization. Despite this, abstractive document summarization is still in its early stages and has shown worse evaluation results compared to extractive methods. To address this issue, a new graph-based attention mechanism is proposed in this paper to consider the saliency factor of summarization, which has been overlooked in previous works. Experimental results show a considerable improvement in the proposed model compared to previous neural abstractive models, making it competitive with state-of-the-art extractive methods. The data-driven approach of this method further strengthens its effectiveness.
411643	41164343	Manifold learning for biomarker discovery in MR imaging	The authors propose a framework for extracting biomarkers from low-dimensional manifolds of MR image data. This method captures information about the brain's structural shape and appearance, as well as the subject's clinical state. A key contribution is the incorporation of longitudinal image information into the learned manifold. This is achieved by simultaneously embedding baseline and follow-up scans into a single manifold, rather than using separate manifolds for inter-subject and intra-subject variation. The proposed method is applied to data from 362 subjects enrolled in the Alzheimer's Disease Neuroimaging Initiative, and shows promising results in classification of healthy controls, subjects with Alzheimer's disease, and subjects with mild cognitive impairment. The biomarkers identified using this method are data-driven and may provide an alternative to traditional biomarkers derived from manual or automated segmentations.
411644	4116445	Automatic anatomical brain MRI segmentation combining label propagation and decision fusion	Automated classification of brain regions in three-dimensional magnetic resonance (MR) images is challenging due to the time and expertise required for manual segmentation and labeling. To address this, individual segmentations can be propagated to other individuals using an anatomical correspondence estimate. However, the accuracy of these propagated segmentations is limited. To improve accuracy, multiple segmentations can be combined using decision fusion. In a study using 30 normal brain MR images, the average similarity index (SI) for individual propagations was 0.754±0.016, while decision fusion with 29 input segmentations increased SI to 0.836±0.009. Additionally, a model was developed to predict the improvement in accuracy based on the number of individual propagated segmentations combined. This automated method shows promise in accurately classifying brain regions and can compete with manual delineations.
411645	411645164	Intervention in gene regulatory networks via phenotypically constrained control policies based on long-run behavior.	The main purpose of studying gene regulatory networks is to find strategies for intervening and designing gene-based therapies. Previous studies have focused on using transition probability matrices to develop intervention strategies for probabilistic Boolean networks. However, it is important to also consider minimizing collateral damage when designing these strategies. This paper proposes two new control policies that take into account the long-term effects on the network and aim to reduce the risk of entering undesirable states while limiting collateral damage. Experiments on random networks and a melanoma network demonstrate the effectiveness of these policies in reducing the risk of entering undesirable states and their potential use in designing genetic therapies.
411646	4116467	Learning-Based Approach to Real Time Tracking and Analysis of Faces	This paper presents a trainable system that can track faces and facial features in real time. The system is able to accurately estimate features such as eye and nostril locations, as well as mouth parameters like openness and smile. The development of this system focused on addressing the challenges of image representation and learning algorithms. The system uses Haar wavelets for image representation, which allows for robust detection of facial features. Unlike previous methods, this system is entirely trained using examples and does not rely on pre-existing models. The system works in stages, using supervised learning techniques such as support vector machines for classification. The estimation of mouth parameters is based on regression from a subset of Haar wavelet coefficients. 
411647	4116471	A System for Automatic Notification and Severity Estimation of Automotive Accidents	New communication technologies in modern vehicles can greatly improve assistance for those injured in traffic accidents. Studies have found that integrating artificial intelligence into these technologies can automate decision making for emergency services, reducing response time and adapting resources to the severity of the accident. To further enhance the rescue process, accurately estimating the severity of the accident is crucial. A new intelligent system has been proposed that uses data mining and knowledge inference to automatically detect accidents, notify them through vehicular networks, and estimate their severity based on relevant variables such as vehicle speed, type of vehicles involved, impact speed, and airbag status. Testing at a research facility has shown promising results in reducing response time for emergency services.
411648	411648132	Evaluating Bluetooth performance as the support for context-aware applications.	This article discusses an experiment that explores the use of Bluetooth wireless technology in supporting context-aware applications. By combining wireless and wired network technologies, the authors propose a method for network interconnection. They also outline the process of creating a context-aware application using Bluetooth. The technology's performance is then evaluated through a test-bed and simulation. The results suggest that Bluetooth can effectively support context-aware applications in this area.
411649	41164963	An Adaptive Anycasting Solution for Crowd Sensing in Vehicular Environments	Vehicular networks have the potential to greatly enhance our society by acting as mobile sensors and collecting various types of information while vehicles travel. This can enable new services such as environment monitoring, traffic management, and urban surveillance. In this paper, the authors introduce AVE, a message delivery protocol that utilizes both geographical and topological information to dynamically adapt to network conditions. This protocol is specifically designed for V2I connectivity for cloud services, where vehicles transmit individual messages to a cloud service through any available nearby Road Side Unit (RSU). Simulations show that AVE outperforms other protocols in terms of delivery ratio, with a 10% improvement over DYMO in sparse scenarios and DTN techniques in dense scenarios. 
411650	41165068	Studying the feasibility of IEEE 802.15.4-Based WSNs for gas and fire tracking applications through simulation	Wireless Sensor Networks (WSNs) have become increasingly popular in various fields such as military, environmental, and industrial applications due to their reliability and low latency. For time-critical WSN applications, it is important to quickly respond to changes in the environment and ensure accurate data collection. This paper proposes a near real-time monitoring system using binary detection sensors for both indoor and outdoor environments. The system utilizes a routing protocol that minimizes control traffic to reduce end-to-end data delivery delay. The performance of gas and fire tracking applications is evaluated using IEEE 802.15.4 technology and a routing scheme that relies on sink announcements for route discovery. The accuracy of the monitoring process is determined by simulating emergency events and developing gas and fire propagation models. 
411651	41165160	A Practical Framework for Privacy-Preserving Data Analytics	The availability of large amounts of user-generated data has transformed our society. This data is used for various purposes, such as disease outbreak detection and traffic control, as well as for commercial interests like smart grid and product recommendation. However, this data can also be used to identify individuals, as seen in the AOL search log release incident. To address this issue, the authors propose a framework that provides differential privacy guarantees to individual data contributors. This framework allows for accurate data analytics while protecting user privacy, through the use of differentially private aggregates and sampling techniques. Empirical studies show that this solution effectively balances privacy and data analysis needs.
411652	411652123	Parallel Maximum Weight Bipartite Matching Algorithms for Scheduling in Input-Queued Switches	An input-queued switch with virtual output queuing can achieve a maximum throughput of 100% by using advanced scheduling strategies. This can be seen as a maximum flow problem and a maximum weight bipartite matching (MWBM) algorithm has been proposed for input-queued switches. The goal is to maintain fairness and stability while achieving 100% throughput. The algorithm has a sublinear parallel run time complexity and can obtain the MWBM in sublinear time by using the previous time slot's matching. This algorithm outperforms all previously proposed MWBM algorithms for input-queued switches. A linear time complexity MWBM algorithm has also been developed for a general bipartite graph that outperforms the best known sublinear algorithm for graphs with less than 1015 nodes. 
411653	41165329	A Context-Based Approach to Reconciling Data Interpretation Conflicts in Web Services Composition	The article discusses a method for detecting and resolving data interpretation conflicts in Web services composition. This is done through the use of a lightweight ontology with added modifiers, contexts, and atomic conversions. By annotating the WSDL descriptions of the Web services, correspondences to the ontology are established. The proposed approach can automatically identify conflicts in the Business Process Execution Language (BPEL) specification and generate a mediated BPEL. A prototype is also developed to test the effectiveness of the approach.
411654	41165490	The PowerPC Backend Molen Compiler	This paper discusses a C compiler that was created to work with the Virtex II Pro PowerPC processor and incorporate the Molen architecture programming paradigm. To test the efficiency of the compiler, a multimedia video frame M-JPEG encoder was used with the Discrete Cosine Transform (DCT*) function mapped onto an FPGA. The results showed a speedup of 2.5, which is 84% efficient compared to a maximal theoretical speedup of 2.96. This was achieved using a non-optimized DCT* hardware implementation that was automatically generated. Overall, the backend C compiler proves to be effective in targeting the Virtex II Pro PowerPC processor and incorporating the Molen architecture programming paradigm.
411655	4116556	Types for safe locking: Static race detection for Java	This article discusses a static race-detection analysis for multithreaded shared-memory programs in Java. The analysis is based on a type system that can identify common synchronization patterns and supports different types of classes. The authors have implemented the type system in a checker and applied it to over 40,000 lines of hand-annotated Java code, finding multiple race conditions. They also present two improvements that allow for checking larger programs, including an annotation inference algorithm and a user interface for clarifying warnings. These extensions have allowed the checker to be used on software systems with up to 500,000 lines of code. The overall effectiveness of the type system is demonstrated by the relatively low number of additional annotations required per 1,000 lines of code.
411656	41165621	Exploiting fact verbalisation in conceptual information modelling	Many information modelling approaches now use verbalisation techniques to create a model for a specific problem domain. This involves using verbal descriptions of facts from the domain to understand the relevant concepts and their relationships. These verbalisations also help validate the resulting model for user familiarity. This approach is present in various modelling techniques such as ER, Object-Role Modelling, and Object-Oriented Modelling. However, after the modelling process, the verbalisations are not utilized further. This article discusses the potential of using these verbalisations in different ways, including a conceptual query language, describing database contents, and formulating queries in a computer-supported environment. An example is also provided to demonstrate how verbalisations can be used in an end-user query tool.
411657	411657145	Efficient Lattice Boltzmann Solver for Patient-Specific Radiofrequency Ablation of Hepatic Tumors	Radiofrequency ablation (RFA) is a well-established treatment for liver cancer when surgery is not an option. However, its effectiveness can be limited by the presence of large blood vessels and the varying thermal properties of tissue. This can lead to incomplete treatment and a higher risk of recurrence. To address this, a new method has been developed that uses patient-specific images and a computational model to accurately plan the extent of ablation needed. This method was verified and evaluated on 10 patients, showing promising results. It also highlights the importance of considering liver perfusion in the simulation. The method can be implemented in real-time, making it a valuable tool for RFA treatment planning.
411658	41165812	Practical Techniques for Searches on Encrypted Data	Data storage servers, such as mail and file servers, often store data in encrypted form to mitigate security and privacy risks. However, this can limit functionality, such as performing searches on the data without compromising confidentiality. In this paper, the authors propose cryptographic schemes that allow for searching on encrypted data while maintaining provable security. These schemes offer several advantages, including secrecy of the plaintext, isolation of search queries, controlled searching, and support for hidden queries. Additionally, the proposed algorithms are efficient, with minimal space and communication overhead, making them practical for use today.
411659	41165978	A new fuzzy interpolative reasoning method based on the areas of fuzzy sets	In sparse fuzzy rule-based systems, fuzzy interpolative reasoning is an important inference technique. This paper introduces a new method for fuzzy interpolative reasoning based on the areas of fuzzy sets. The method ensures the normality and convexity of the conclusion and can handle complex membership functions such as hexagons, polygons, and Gaussians. It is also capable of dealing with fuzzy rules that have different types of membership functions for the antecedents and consequents. The proposed method is compared to existing methods using various examples, and the results show that it is more effective for fuzzy interpolative reasoning. This method provides a useful approach for handling fuzzy interpolative reasoning in sparse fuzzy rule-based systems.
411660	41166059	Characterization of Efficiently Solvable Problems on Distance-Hereditary Graphs	In the literature, various algorithms have been developed to solve problems in distance-hereditary graphs using techniques based on the properties of the graph. By examining the structural properties of these graphs, a general problem-solving approach has been identified. This approach involves using a decomposition tree representation of the graph to construct sequential dynamic-programming algorithms for fundamental graph-theoretical problems. These algorithms can also be efficiently parallelized using the tree contraction technique. This unified approach allows for systematic solving of problems on distance-hereditary graphs and has been shown to be effective in solving multiple problems in this type of graph.
411661	41166172	Invited talk: towards declarative programming for web services	The World Wide Web (WWW) is seeing two emerging trends - the rise of Web Services and the development of the Semantic Web. Web Services are self-contained, accessible software applications that are used for distributed systems. The Semantic Web is the next generation of the WWW, designed to be easily understood by computers. To achieve reliable, large-scale automation of Web Services, a declarative language for describing their properties and capabilities is needed. This enables tasks such as Web service discovery, invocation, composition, and monitoring. To address the issue of automated Web service composition, researchers have proposed reasoning techniques and a logic programming language that allows for generic and customizable programs. A middle-ground interpreter is also suggested for information gathering and search. This work has been done in collaboration with others, and related research can be found in various sources.
411662	41166228	Designing Learning by Teaching Agents: The Betty's Brain System	Teaching others is an effective way to learn, backed by research. Teachable Agents, computer-based tools, allow students to teach using visuals and monitor their own learning and problem-solving. This motivates students to learn more and improve their agent's performance. Betty's Brain is a Teachable Agent that combines teaching with self-regulated learning feedback to enhance science understanding. A study in a 5th grade classroom compared three versions of the system: one where students were taught by an agent, a baseline learning by teaching version, and a version where students received feedback on self-regulated learning and domain content. Results showed all groups made learning gains, but the two learning by teaching groups performed better. The self-regulated learning group showed better preparation for learning in new domains even without access to the self-regulation environment. 
411663	41166319	An adaptive meta-heuristic approach using partial optimization to non-convex polygons allocation problem	The two-dimensional packing problem involves placing non-convex polygons of various shapes on a sheet in a way that minimizes waste. To solve this problem, a new method is proposed using an adaptive meta-heuristics approach with partial optimization. This algorithm divides objects into subgroups and uses meta-heuristic techniques to allocate them, resulting in improved search efficiency. The method was tested through simulation experiments, which showed superior performance compared to using adaptive meta-heuristics alone. 
411664	41166431	From Cluster Monitoring to Grid Monitoring Based on GRM	GRM, or Grid Resource Management, was initially created as a component of the P-GRADE program development environment for supercomputers and clusters. However, in the DataGrid project, the potential for using GRM as a grid application monitoring tool was explored. This led to the redesign of GRM to function independently as a grid monitoring tool. This paper discusses the changes made to GRM's architecture to enable its standalone use in monitoring grid applications.
411665	41166517	Database systems: achievements and opportunities	Database system research in the U.S. has had a significant impact on the economy, with a $10 billion per year information services industry that has been growing at an average rate of 20 percent since 1965. This success is due to the support of federal funding in universities and industrial research labs. The advancements in database research have not only contributed to the growth of the information services industry, but also to other areas such as communications, transportation, finance, and science. These achievements also serve as a basis for progress in various fields, including computing and biology. Despite being a relatively young field, the history of database system research in the U.S. has been marked by exceptional productivity and significant economic impact.
411666	41166690	Perception of Springs With Visual and Proprioceptive Motion Cues: Implications for Prosthetics.	A study has found that controlling objects with an upper limb prosthesis requires more visual attention than using an intact limb. To address this issue, researchers investigated the difference between visually seeing a virtual prosthetic limb move and feeling one's real limb move. Fifteen participants controlled a virtual prosthetic finger using a haptic device and were tested on their ability to discriminate between different levels of stiffness. The study found no significant performance differences between using visual or proprioceptive information, but participants perceived proprioceptive information to be more useful. This suggests that conveying proprioceptive information through a nonvisual channel could improve the experience of using an upper limb prosthesis by reducing the need for visual attention.
411667	41166714	Slip classification for dynamic tactile array sensors.	The article discusses the ability of humans to distinguish between different events that occur during the manipulation of objects with their hands, such as contact and slippage. This is because humans have specialized mechanoreceptors in their hands that respond differently to these events. The article proposes two features that can be extracted from dynamic tactile array data to help robots discriminate between hand/object and object/world slips. These features are effective for a wide range of frequencies and grasp conditions and do not require extensive training. Testing on different sensors and object textures shows that these features have a high accuracy in identifying the location of slip.
411668	41166848	Power Optimization in Fault-Tolerant Mobile Ad Hoc Networks	This paper discusses the problem of optimizing the lifetime of k-connected Mobile Ad hoc NETworks (MANETs) by minimizing power consumption. The proposed solution is a fully distributed model-based transmission power adaptation strategy that uses model-predictive control. This involves using a stochastic model and state estimator to predict the network's future connectivity and energy levels, and then using an optimizer to derive an optimal transmission power assignment sequence. Results from experiments on a simulated wireless sensor network with 100 nodes show that this localized topology control algorithm is nearly as effective as a globalized approach, but with less communication and energy requirements, making it more scalable.
411669	411669211	Relay Selection for OFDM Wireless Systems under Asymmetric Information: A Contract-Theory Based Approach	User cooperation is beneficial for wireless systems, but it requires incentives for nodes to act as relays. The problem is that potential relays have more information about their transmission costs than the source, causing asymmetry. This paper uses contract theory to address relay selection in OFDM-based systems with decode-and-forward relaying. Incentive-compatible contracts are designed and broadcasted to nearby nodes. These nodes can then accept contracts for specific subcarriers. The problem of relay selection is a nonlinear non-separable knapsack problem, but a heuristic solution is proposed. Numerical results show that this solution performs well compared to a simple relay selection scheme. The overall mechanism is simple to implement and has minimal signalling overhead.
411670	4116700	FlowNet: A Deep Learning Framework for Clustering and Selection of Streamlines and Stream Surfaces.	The paper discusses the problem of identifying representative flow lines and surfaces in flow visualization. While this has been studied before, no existing method can effectively solve the problem for both lines and surfaces. The authors propose a single deep learning framework called FlowNet, which uses an autoencoder to learn latent feature descriptors for streamlines and stream surfaces. These descriptors are used for error estimation and network training. Through clustering and dimensionality reduction, the framework allows for easy exploration and selection of representative flow lines and surfaces. The effectiveness of FlowNet is demonstrated through various flow field data sets and comparison with other selection algorithms. The paper also provides intuitive user interactions for visual reasoning of the flow lines and surfaces.
411671	41167148	Effects of short-time plasticity on the associative memory	This study examines the impact of short-term synaptic plasticity on associative memory networks. Through analysis and simulations, it is determined that synaptic dynamics do not affect the retrieval of information, but do affect its stability. This can lead to a lower storage capacity in cases of depression. Additionally, the network operates in an oscillatory manner, which is not seen in networks with static connections. The study also suggests that the dynamics and diversity of synapses can aid in switching between memory states and storing patterns in sequence.
411672	41167211	Approximate Boolean Reasoning: Foundations and Applications in Data Mining	Boolean algebra, introduced by George Boole in the mid-1800s, has become an important tool in mathematics, science, engineering, and artificial intelligence research. It has proven to be effective in decision-making and approximation optimization. In recent years, it has also been recognized as a useful technique in rough set theory for developing concept approximation methods. This paper presents a general framework, called Rough Sets and Approximate Boolean Reasoning (RSABR), which combines rough set theory, Boolean reasoning, and data mining. It addresses the need for a general framework in machine learning and data mining and provides solutions for various data mining problems such as feature selection, feature extraction, data preprocessing, and classification. The paper discusses the theoretical foundation of RSABR and its practical applications in knowledge discovery in databases.
411673	41167354	On the spanning fan-connectivity of graphs	The connectivity of a graph G, @k(G), is the maximum number of paths that can be drawn between two distinct vertices without overlapping. A spanning container is a set of paths that covers all the vertices in G. A graph is k^*-connected if there is a spanning k-container between any two vertices. The spanning connectivity of G, @k^*(G), is the maximum k for which G is connected. A spanningk-(x,U)-fan is a set of paths connecting a vertex x to a subset of vertices U. A graph is k^*-fan-connected if there is a spanning F"k(x,U)-fan for every choice of x and U. The spanning fan-connectivity, @k"f^*(G), is the maximum k for which G is k^*-fan-connected. The paper discusses the relationship between @k(G), @k^*(G), and @k"f^*(G), and presents conditions for a graph to be k"f^*-connected and k^*-pipeline-connected.
411674	41167468	The globally Bi-3*-connected property of the honeycomb rectangular torus	The honeycomb rectangular torus, represented by HReT(m,n), is a 3-regular bipartite graph that has gained attention as a cost-effective and well-connected network for parallel and distributed applications. In this network, with n=4 and m being a positive even integer, it has been proven that there are always three internally-disjoint paths connecting any two vertices belonging to different parts. Additionally, for vertices in the same part, there is always a third vertex in the opposite part that allows for three internally-disjoint paths to exist after removing it from the network. However, this property is only true for all three vertices being in the same part if n=6 or m=2. 
411675	41167529	Steering symbolic execution to less traveled paths	Symbolic execution is a powerful method for testing and analyzing programs by systematically exploring their execution space and generating high-coverage test cases. However, the huge number of possible program paths in real-world programs presents a challenge for symbolic execution. Existing heuristics for guiding symbolic execution are often inefficient and ad-hoc. In this paper, the authors propose a new approach that utilizes a specific type of path spectra, called length-n subpath program spectra, to prioritize less explored parts of a program. By using frequency distributions of explored length-n subpaths, their strategy improves test coverage and error detection. This approach has been implemented in the KLEE engine and evaluation results on GNU Coreutils programs show its effectiveness compared to traditional strategies.
411676	41167627	Enabling multi-core based monitoring and fault tolerance in C++/Java	Monitoring and fault tolerance are important for ensuring the reliability of long-running online software systems. However, these approaches can be costly in terms of efficiency. Multi-core platforms can help mitigate this cost by taking advantage of parallel performance. To make it easier for ordinary software developers without knowledge of multi-core platforms to efficiently handle these tasks, the authors propose an approach for enabling multi-core based monitoring and fault tolerance in C++/Java. This involves presenting the principles of multi-core based monitoring and fault tolerance, designing and implementing tasks through multi-process/thread programming, and using special annotations for developers to specify a multi-core based design. A prototype tool, McC++/Java, is then used to automatically map these annotations to the runtime platform, making it an accessible and efficient approach for a wide range of software developers.
411677	41167747	Probabilistic analysis of onion routing in a black-box model	The study analyzed the security of onion routing, a method of anonymous communication, using a black-box model in the Universally Composable framework. The model considered an active adversary who controls part of the network and knows user preferences for destinations. Results showed that the adversary can gain information about users by exploiting their probabilistic behavior. In larger networks, a user's anonymity is most at risk when other users always choose either the least likely or the same destination as the user. The study also found that the worst-case anonymity against an adversary controlling a fraction b of routers is similar to the best-case anonymity against an adversary controlling the square root of b.
411678	411678144	Reliability maps for probabilistic guarantees of task motion for robotic manipulators.	The use of reliable and safe robots is becoming increasingly important in various applications, such as assisting disabled or elderly individuals and performing surgical procedures. However, different tasks require varying levels of safety and reliability, making it crucial to understand the reliability of robots. Currently, fault-tolerant workspaces are used to ensure task completion, but they do not consider the reliability of different joints in the robotic manipulator. This paper seeks to address this issue by introducing the concept of reliable fault-tolerant workspaces, which takes into account the reliability of each joint. This is achieved through the use of conditional reliability maps, which can improve the performance of robots while ensuring their reliability for completing tasks. 
411679	41167918	Circularly polarized spherical illumination reflectometry	We have developed a new technique for accurately measuring the surface reflectance of objects using a single uniform spherical light source. This method analyzes the Stokes reflectance field of circularly polarized light and can estimate the diffuse albedo, specular albedo, index of refraction, and specular roughness of isotropic BRDFs for each pixel in the scene. To gather this information, we take four photographs of the object with different polarizers and measure the Stokes parameters of the reflected light. Our method only requires knowledge of the surface orientation, which can be obtained with a few additional photometric measurements. We have tested our method with various lighting setups and it offers several advantages such as a more detailed model, minimal measurements, applicability to different materials, and independence from the viewpoint. 
411680	41168012	Providing QoS Guarantees in Large-Scale Operator Networks	This paper discusses the challenges of ensuring quality of service (QoS) guarantees in application areas such as global sensor networks and data stream processing, where large amounts of data are processed in an overlay network on top of the Internet. The paper proposes a formalized constrained optimization problem for placing operators in the network to meet user QoS constraints and minimize network load. To solve this NP-hard problem, the paper presents a two-step approach: first solving in a continuous latency space and then mapping to a discrete solution. Evaluations show that this algorithm effectively balances user requirements and network resource usage.
411681	41168149	Common due-window problem: Polynomial algorithms for a given processing sequence	The paper discusses the Common Due-Window (CDW) problem, where a single machine processes multiple jobs with different processing times and asymmetric penalties for early and late completion. The goal is to minimize the total penalty by determining the job sequence, completion times, and due-window position. The paper presents polynomial algorithms with a runtime complexity of O(n2) for optimizing a given job sequence and an O(n) algorithm for unit processing times. These algorithms take a sequence of jobs as input and return the optimal completion times. Additionally, the authors use Simulated Annealing to improve the results and compare them with previous research. 
411682	41168243	Empirical analysis of evolutionary algorithms with immigrants schemes for dynamic optimization	The study of evolutionary algorithms (EAs) for dynamic optimization problems (DOPs) has become increasingly popular in recent years. One approach that has shown to be beneficial is the use of immigrants schemes, which aim to maintain diversity in the population by introducing new individuals. This study examines the mechanism of generating immigrants, dividing existing schemes into two types: direct and indirect. Experiments are conducted to compare their performance in dynamic environments, and a new scheme that combines the merits of both types is proposed. Results show that this combination improves the performance of EAs in dynamic environments. Overall, immigrants schemes have proven to be effective in addressing DOPs in EAs.
411683	41168326	An algorithm for shifted continued fraction expansions in parallel linear time	The Berlekamp-Massey algorithm (BMA) can efficiently determine the linear complexity profile of a sequence of length n in O(n2) steps. However, to ensure that the profile is acceptable for all shifted sequences, Piper requires the BMA to be repeated, resulting in O(n3) steps. This paper introduces a transducer that can compute the continued fraction expansions of all n shifted sequences in only Cq·n+12Fq operations, where C2=7 and Cq=6.5 for q⩾3. This eliminates the need for additional computational effort to check Piper's demand. When n transducers are used in parallel, the output can be obtained in parallel linear time, with an overall time-space product of O(n2·log n). Keywords include continued fraction expansion, linear complexity profile, Berlekamp-Massey algorithm, and transducer.
411684	41168431	Optimal rate list decoding of folded algebraic-geometric codes over constant-sized alphabets	We have developed a new type of algebraic-geometric (AG) codes that can be list-decoded up to an error fraction approaching 1-R, where R is the rate. These codes are constructed using class field theory and Drinfeld modules of rank 1, which allows us to use a large-order automorphism to "fold" the AG code. This is a generalization of previous work on folded AG codes based on cyclotomic function fields. The Chebotarev density theorem is used to establish a polynomial upper bound on the list-size for decoding, and this can be done in polynomial time with pre-processed information about the function field. Our codes can be list-decoded up to the Singleton bound, with an alphabet size of (1/ε)O(1/ε2) and close-by messages confined to a subspace with NO(1/ε2) elements. This is an improvement compared to previous methods that either used concatenation or involved subcodes of AG codes. Our approach also has potential for independent algebraic applications.
411685	41168531	Can We Interpret the Depth?: Evaluating Variation in Stereoscopic Depth for Encoding Aspects of Non-Spatial Data.	The article discusses the use of a 2D node-link representation on a stereoscopic platform to enhance focus and context interactions. This approach utilizes stereoscopic depth to highlight structural relations and allows for interactive operations such as expanding or contracting nodes in compound graphs. Different visual cues, such as color and shape, can also be used to encode data aspects. A user study with 30 participants was conducted to evaluate the effectiveness of this approach, considering factors such as graph size and transparency. The results show that stereoscopic depth can be used to encode data in compound graphs, but there are limitations based on certain circumstances. Overall, the study found that participants were generally accepting of using stereoscopic depth as a cue for compound graphs.
411686	41168653	Spring-based manipulation tools for virtual environments	This paper introduces new tools for user interaction with virtual worlds, aiming to make the manipulation of objects in these environments more natural. The authors propose principles for realistic behavior of virtual objects and present a set of input techniques suitable for semi-immersive virtual reality devices. These tools, including springs, spring-forks, and spring-probes, aid in direct manipulation of objects in virtual environments, with a focus on controlling rotational motions. These tools are designed to enhance the user's experience and make interactions more intuitive and realistic in virtual worlds. 
411687	41168727	Geometric Spanners for Weighted Point Sets	We are studying spanners for a finite metric space (S,d) where each element p∈S has a non-negative weight w(p). The weighted distance function dω is defined as 0 if p=q and w(p)+d(p,q)+w(q) if p≠q. We present a method for turning spanners with respect to the d-metric into spanners with respect to the dω-metric. This method can be applied to obtain (5+ε)-spanners with a linear number of edges for points in Euclidean space, points in spaces of bounded doubling dimension, and points on the boundary of a convex body in ℝd. We also describe an alternative method for obtaining (2+ε)-spanners for weighted points in ℝd and points on the boundary of a convex body in ℝd with O(nlog n) edges. This bound on the stretch factor is almost optimal, as it is possible to assign weights that result in a stretch factor larger than 2−ε for any non-complete graph in any finite metric space.
411688	41168814	Implementation of the ANSYS® commercial suite on the EGI grid platform	This paper discusses the implementation of the ANSYS® commercial software suite in a high-throughput computing environment. ANSYS® has been designed to efficiently utilize parallel architectures, making it a good fit for use on the Grid. To make ANSYS® accessible to a wider community, the User Support Unit of the Italian Grid Initiative (IGI) and the INFN-Legnaro National Laboratories (INFN-LNL) collaborated to create a Grid-enabled version of the code using the IGI Portal. This collaboration focused on adapting the code to run on the EGI Grid, which allows for easier access to distributed computing and storage resources. The goal of this project was to make ANSYS® more accessible and beneficial for both the involved community and other groups interested in using production Grid infrastructures.
411689	41168931	Analysis of link failures in an IP backbone	This paper examines the impact of failures in Sprint's IP backbone on emerging services like Voice-over-IP (VoIP). The study analyzes the frequency and duration of failure events in the backbone and finds that link failures occur frequently but are typically short-lived. The authors also discuss various statistics related to link failure, such as inter-failure time and duration, which are important for creating a realistic failure model. They then look at the reconvergence time for routing and services during a controlled link failure and find that it is influenced by both routing protocol dynamics and router architecture. The paper concludes by suggesting that network-wide availability is a more suitable metric for service-level agreements to support emerging applications. 
411690	41169040	Improved myocardial scar characterization by super-resolution reconstruction in late gadolinium enhanced MRI.	Image resolution plays a crucial role in accurately assessing myocardial scar using late gadolinium enhanced (LGE) MRI. However, the commonly used short-axis (SA) acquisition with anisotropic resolution can lead to overestimation of scar size due to partial volume effect, affecting the reliability of LGE MRI in critical clinical applications. To address this issue, a new method is proposed that combines three anisotropic orthogonal LGE sequences into a single isotropic volume using super-resolution reconstruction and motion compensation techniques. This approach was validated on 15 post-infarction patients using electroanatomical voltage mapping (EAVM) as the gold standard, and results showed improved agreement with EAVM in the clinically significant gray zone region compared to the conventional SA image.
411691	4116915	On fixed-parameter algorithms for Split Vertex Deletion	The Split Vertex Deletion problem involves deleting k vertices from a graph G to create a split graph, which has two sets of vertices: one that forms a clique and one that forms an independent set. This paper explores fixed-parameter algorithms for this problem, showing that it can be solved in the same time as the Vertex Cover problem, with a quasipolynomial factor in k and polynomial factor in n. By using an existing algorithm for Vertex Cover, the problem can be solved in O(1.2738^k * k^O(log k) + n^O(1)) time. The authors also prove a structural result that can be useful for other problems: for any graph G, a family of partitions can be computed in size n^O(log n) that contains all possible partitions of G into two sets, one forming a clique and the other an independent set. 
411692	41169235	Kernelization hardness of connectivity problems in d-degenerate graphs	A d-degenerate graph is one in which every subgraph has at least one vertex with a degree of at most d. This property is seen in planar graphs, which are 5-degenerate. Recent research has shown that problems in this class of graphs, such as DOMINATING SET, have polynomial kernels. However, CONNECTED DOMINATING SET does not have a polynomial kernel in d-degenerate graphs for d ≥ 2 unless the polynomial hierarchy collapses up to the third level. This is proven using a problem from bioinformatics, called COLOURFUL GRAPH MOTIF, which encapsulates the difficulty of connectivity in kernelization. It is also shown that other problems, including STEINER TREE, CONNECTED FEEDBACK VERTEX SET, and CONNECTED ODD CYCLE TRANSVERSAL, do not have polynomial kernels in d-degenerate graphs for d ≥ 2 unless PH = Σp3. However, a polynomial kernel is found for CONNECTED VERTEX COVER in graphs that do not contain the biclique Ki,j as a subgraph.
411693	41169378	Active exploration for robot parameter selection in episodic reinforcement learning	As robots and autonomous systems become more complex, it is crucial for them to actively adapt and optimize their settings. However, this optimization process is not always straightforward and can be costly in terms of time and resources. The parameter space is often continuous and multi-dimensional, which presents a challenge for effective optimization. To address this, a reinforcement learning framework is proposed, where the policy is defined by the system's parameters and rewards are based on system performance. This framework employs a method based on Gaussian process regression, which can handle non-linear mappings. The proposed method was tested on a real robot, where it successfully adapted its grasping parameters for different objects. Overall, the method shows promise for efficiently optimizing autonomous systems in continuous multidimensional spaces.
411694	41169459	Individual-based stability in hedonic games depending on the best or worst players	The article discusses different types of hedonic games where players' preferences are based on the best or worst player in their coalition. The focus is on analyzing the computational complexity of finding stable coalition partitions in these games, which can be represented concisely. The key difficulty is identified as extending preferences over players to preferences over coalitions. The article looks at various stability concepts, such as individually stable and Nash stable, and examines the difficulty of determining the existence of and computing these types of stable coalition partitions. 
411695	4116959	How to Protect DES Against Exhaustive Key Search	The DESX block cipher was developed as a way to protect the DES cipher from exhaustive key-search attacks. It was suggested by Ron Rivest and has been proven to be sound in a formal model. This means that it is a reliable and secure method of encryption. The construction of DESX, which involves bitwise exclusive-or operations, is more resistant to key search attacks than the original DES cipher. This is due to the use of an idealized block cipher, which has a higher effective key length. The analysis of DESX shows that it has an effective key length of ϰ+n - 1 - lg m bits, where ϰ is the key length of the idealized cipher, n is the block length, and m is the maximum number of (x, FXk(x)) pairs an adversary can obtain. Overall, DESX is a cost-effective and secure solution for protecting data using the DES cipher.
411696	41169646	A bounded search tree algorithm for parameterized face cover	The face cover problem asks whether a given graph can be covered by a set of faces, with a specified limit on the number of faces. Previous approaches used a reduction to a special form of dominating set, resulting in run times of O^*(12^k) or worse. This paper presents a direct approach using a surgical operation on the graph at each branching decision. By building on previous work, the authors were able to develop an algorithm with a run time of O(4.6056^k+n^2), significantly improving upon previous results. This algorithm also has implications for solving red-blue dominating set on planar graphs and producing a linear kernel for annotated face cover.
411697	4116979	Fast Shape from Shading for Phong-Type Surfaces	Shape from Shading (SfS) is a longstanding problem in image analysis that aims to reconstruct a 3-D scene from a 2-D image using brightness variations and illumination conditions. While the quality of models for SfS has improved, there is still a need for efficient numerical methods for large images. In this paper, the authors address this issue by proposing the use of a Fast Marching (FM) scheme, which is known to be one of the most efficient approaches. However, the FM scheme is not easily applicable to modern non-linear SfS models. The authors demonstrate how this can be achieved for a recent SfS model that incorporates the non-Lambertian reflectance model of Phong. Results from numerical experiments show that this FM scheme is significantly faster without compromising quality, making it two orders of magnitude faster than standard methods. 
411698	411698139	Energy-efficient physically tagged caches for embedded processors with virtual memory	This paper presents a low-power tag organization for physically tagged caches in embedded processors with virtual memory support. By identifying a small subset of tag bits for each application hot-spot, the cache can access data without sacrificing performance. The minimal subset of physical tag bits, known as the compressed tag, is dynamically updated as the application's physical address space changes. The operating system plays a role in maintaining the compressed tag and updates it to match the current set of physical memory pages allocated to the application. The paper also proposes efficient algorithms for dynamic updates of compressed tags in cases such as memory allocation and swapping of physical pages. The only hardware support required is the ability to disable tag arrays' bitlines. Extensive experiments demonstrate the effectiveness of this approach.
411699	41169914	Dual regularized multi-view non-negative matrix factorization for clustering.	Multi-view datasets are becoming increasingly prevalent in real-world applications, as they provide complementary information about the data. To more effectively utilize this information, a novel algorithm called DMvNMF has been developed for multi-view data clustering. The algorithm incorporates a parameter-free strategy for constructing a data graph that preserves the geometric structures of the data in both the data space and feature space. This is achieved by first learning an affinity graph for each view using self-expressiveness and sparsity principles, and then combining them to generate a global data graph. Similarly, feature graphs are constructed for each view. The algorithm also includes an iterative updating scheme for model optimization and has been shown to outperform other baseline methods in experiments on real-world datasets.
411700	41170051	Relational Similarity-Based Model Of Data Part 1: Foundations And Query Systems	The article presents a general rank-aware model of data for relational databases, where equalities on values are replaced by similarity relations. This model introduces a scale of truth degrees and intermediate truth degrees to represent similarity and partial match of queries. The model also includes truth functions for many-valued logical connectives to aggregate degrees of similarity. This approach is conceptually clean, logically sound, and retains most properties of the classical relational model while allowing for new types of queries and data dependencies. The article discusses the fundamentals of the formal model and presents two equivalent query systems. In future work, the authors plan to address similarity-based dependencies.
411701	41170141	Interpolation in Hierarchical Rule-Bases with Normal Conclusions	The combination of fuzzy rule interpolation and hierarchical structures in fuzzy rule bases, as suggested by Sugeno, can simplify the complexity of fuzzy algorithms. This paper focuses on using the KH method and its variations for interpolation, but notes that it can sometimes produce incorrect results, making it unsuitable for hierarchical structures. To address this issue, the paper introduces a modified version of the KH method called the MACI algorithm, which avoids these abnormal conclusions and allows for the use of hierarchical structures.
411702	4117023	Predicting Locations Using Map Similarity(PLUMS): A Framework for Spatial Data Mining	Spatial data mining is a method used to uncover valuable patterns within spatial databases. This is particularly useful for organizations with large spatial datasets. Currently, the most common approach for spatial data mining is to first establish spatial relationships and then use traditional data mining tools. However, the unique property of spatial data is that it is inherently linked to its location, making it crucial to consider spatial relationships during the data mining process. Efficient tools for extracting information from spatial datasets are essential for organizations that handle these types of data. 
411703	41170334	CUR decomposition for compression and compressed sensing of large-scale traffic data	Intelligent Transportation Systems (ITS) often deal with large amounts of data from traffic networks and need efficient methods to handle it. Subspace methods like Principal Component Analysis (PCA) can create accurate low-dimensional models, but they are not easily interpretable. The CUR matrix decomposition, on the other hand, creates low-dimensional models where each component corresponds to an individual link in the network, making it more interpretable. This approach can be used for both data compression and compressed sensing in traffic networks. Results from a case study in Singapore show that the CUR matrix decomposition is a feasible and effective method for handling massive traffic data.
411704	41170480	Provably good performance-driven global routing	The authors propose a performance-driven global routing algorithm for cell-based and building-block design. It uses a bounded-radius minimum routing tree formulation and is based on heuristics similar to Prim's minimum spanning tree construction. The algorithm aims to minimize both routing cost and the longest interconnection path, with the bounds being only slightly away from optimal. Geometry is shown to play a role in routing, with improved wire length for Steiner routing in the Manhattan and Euclidean planes. The algorithm can also handle varying wire length bounds for different source-sink paths. Extensive simulations demonstrate the effectiveness of this approach.
411705	41170538	Logged virtual memory	Logged virtual memory (LVM) is a feature that allows for logging of writes to specific regions of the virtual address space. It is useful for applications that require rollback and persistence, such as parallel simulations and memory-mapped databases. LVM can also be used for output, debugging, and maintaining distributed consistency. This paper discusses the benefits of LVM and its implementation as an extension of the standard virtual memory system. The prototype implementation includes extensions to the virtual memory system software, hardware support, and a user-level library. Results and experience with the prototype show that LVM can be efficiently implemented and provides significant benefits for applications and servers. It also reduces the risk of error compared to manually generating log updates. LVM can be used for various purposes, such as supporting atomic transactions in object-oriented databases, debugging programs, and obtaining detailed address traces for performance analysis. Overall, incorporating LVM into virtual memory systems can greatly enhance the functionality and efficiency of computer systems.
411706	41170661	Thirty years is long enough: getting beyond C	After 30 years, C remains a popular systems programming language, but its power has become a hindrance for large projects focused on security and reliability. To address this issue, a new language called Ivy is proposed, which has an evolutionary path from C. This path involves using extensions and refactorings to add new features and assist programmers in updating their code. These tools have potential applications such as enforcing memory safety and detecting user/kernel pointer errors. A tool called Macroscope has also been developed to enable refactoring of existing C code. These developments aim to improve the quality of systems software and facilitate the adoption of modern languages and static analyses.
411707	4117070	Using the multi-living agent concept to investigate complex information systems.	The paper introduces the concept of the multi-living agent (MLA) in order to analyze complex information systems that operate under strict countermeasure environments. The authors first examine the system's livelihood under these conditions and then propose the concept of living self-organization based on the system's profile. They also present a Two-Set model for this self-organization mechanism. Using this framework, they develop a construction model for MLA-based information systems in the field of information security and countermeasures, including a three-level negotiation-coordination mechanism. Two practical examples are provided to demonstrate the application of the MLA concept in analyzing real complex information systems. This proposal aims to bridge the gap between research and practical application in the field, providing a theoretical basis for constructing and analyzing complex information systems in the context of information security and countermeasures.
411708	41170871	An Orientation Model for Hierarchical Phrase-Based Translation	The hierarchical phrase-based (HPB) translation method utilizes grammar to perform long distance reordering without specifying nonterminal orientations or considering lexical information. This paper introduces an orientation model, borrowed from phrase-based systems, to improve the reordering ability of HPB translation. The model identifies three orientations (monotone, swap, discontinuous) for a nonterminal based on grammar alignment and selects the appropriate one using lexical information. By incorporating this model, the approach outperforms standard HPB systems by up to 1.02 Bleu on NIST Chinese-English and 0.51 Bleu on WMT German-English translation tasks. 
411709	41170973	Interpolation Techniques for MIMO OFDM with Interference Cancellation	.This paper discusses the challenges of implementing a MIMO OFDM receiver with interference cancellation due to its high computational requirements. To address this issue, the authors propose new interpolation algorithms that reduce the need for matrix inversion on each OFDM sub-carrier. They also introduce algorithms to determine the optimal order for interference cancellation. The proposed receiver shows a significant reduction in computation requirements while maintaining acceptable performance. For an interpolation factor of 6.25%, the performance degradation compared to the optimal solution is only 2dB. The results also demonstrate that the sub-optimal interference cancellation detection order has a minimal impact on performance compared to the coefficient matrix interpolation error. Overall, using interference cancellation in a MIMO OFDM system with interpolation greatly improves performance.
411710	41171022	A theoretical framework for interaction measure and sensitivity analysis in cross-layer design	Cross-layer design is a popular method for achieving Quality of Service (QoS) in communication networks, particularly in wireless multimedia networks. However, current research on cross-layer design lacks a comprehensive approach and understanding of complex cross-layer behavior. This article proposes a theoretical framework for measuring interactions between different design variables, which can be further used for sensitivity analysis to determine the impact of each variable on the overall design objective. This framework can greatly improve our understanding of cross-layer behavior and provide valuable insights for future designs. The article includes a case study on cross-layer optimized wireless multimedia communications to demonstrate the effectiveness of the proposed framework. Both analytical and experimental results support the validity of the framework.
411711	4117115	Type checking annotation-based product lines	Software product line engineering involves creating a range of program variations for a specific domain from a single code base. However, testing all of these variants can be challenging, especially when it comes to ensuring type safety for the entire product line. To address this issue, a product-line-aware type system has been developed that can check the types of all program variants without generating each one separately. This type system is based on the Featherweight Java calculus and uses feature annotations for product-line development. It has been formally proven that all variants generated from a well-typed product line will also be well-typed. Additionally, a solution has been presented for handling mutually exclusive features. This approach has been implemented in the product-line tool CIDE for full Java, and has successfully detected type errors in existing product line implementations.
411712	41171261	Domain Types: Selecting Abstractions Based on Variable Usage	Software model checking is a crucial process in verifying the correctness of a program. However, its success depends greatly on finding the right abstraction of the program. Currently, the choice of abstract domain and analysis configuration is left to the user, who may not have enough knowledge on the available options. To address this issue, the concept of domain types is introduced, which classifies program variables into more specific types to guide the selection of an appropriate abstract domain. This is done through a pre-processing step that assigns each variable to a domain type and then an abstract domain. By specifying a separate analysis precision for each domain, the impact of the choice of abstract domain per variable is shown in experiments. It is found that a combination of different abstract domains based on domain types can greatly improve the performance of software model checking. 
411713	41171350	Efficient revised simplex method for SVM training.	Existing active set methods for training support vector machines (SVM) often encounter singularities when finding the search direction. While an infinite descent direction can be chosen to avoid cycling, it can make the algorithm more complex and less efficient. The revised simplex method introduced by Rusin avoids this issue by providing a guarantee of nonsingularity. This allows for a simpler and more efficient implementation, without the need to test for rank degeneracies or modify factorizations. By utilizing this guarantee, our algorithm is shown to be competitive with SVM-QP and effective when dealing with a large number of nonbound support vectors. It also performs competitively against popular SVM training algorithms, SVMLight and LIBSVM.
411714	41171419	A Metalanguage for the Formal Requirement Specification of Reactive Systems	The presented approach offers a new way of specifying both the static structure and dynamic activity of a system using a single formalism. Unlike other formalisms that use temporal logics, this approach allows for dealing with different entities and their subcomponents without sacrificing abstraction. It also includes specifications for abstract data types and integrates static and dynamic features of a system. The formalism is supported mathematically through the definition of appropriate institutions, with models being defined as entity algebras over extended signatures. The relationship between abstract requirement specifications and design level specifications can be explored using an algebraic approach. This approach is currently being used in industrial case studies to link requirements to concrete design specifications.
411715	4117157	Analysing UML Active Classes and Associated State Machines - A Lightweight Formal Approach	The authors propose a precise definition of UML active classes using labelled transition systems in the formal specification language Casl. They believe that a formal model is necessary to fully understand UML and improve its practical use. They plan to use animation and verification tools for algebraic specifications with UML in the future. To simplify the application of their semantics, they only consider a subset of state machine constructs, but this does not limit the overall UML subset as the restricted constructs can be replaced by equivalent combinations. Due to some ambiguities in the official UML semantics, the authors discuss different options and choose the most logical or simplified approach for each case.
411716	41171618	Effective use of prosody in parsing conversational speech	In this study, researchers have identified a set of prosodic cues that can be used to improve the accuracy of parsing conversational speech. These cues were incorporated into a statistical parsing model and tested on the Switchboard corpus, resulting in improved accuracy compared to a state-of-the-art system that only used lexical and syntactic features. Additionally, the researchers looked into alternative methods for detecting and removing edit regions, which are known to improve parse accuracy. They found that specialized techniques were more effective than traditional methods such as PCFGs. This study highlights the importance of considering prosodic cues and specialized techniques for improved parsing of conversational speech.
411717	4117179	P2p Multi-Agent Data Transfer And Aggregation In Wireless Sensor Networks	Wireless Sensor Networks (WSNs) allow for seamless communication with the physical world. This paper presents an architecture for a P2P multi-agent data transfer and aggregation system in WSNs. The architecture includes four types of agents: interface, query, routing, and data acquisition agents. The interface agent interacts with users, while the routing agent ensures energy-efficient data transfer. The query agent facilitates collaboration between the interface and routing agents and creates optimized plans. These agents are placed at the resource-enriched base station due to their computation-intensive operations. Proxy agents are created for the query and routing agents to conserve energy and computing resources. The paper outlines the design and architecture of these agents, enabling them to coordinate and communicate efficiently in WSNs. 
411718	41171884	A Divide-and-conquer Strategy in Shape from Shading Problem	The proposed strategy for information recovery of book surfaces under perspective conditions involves three steps: preprocessing, apparent shape recovery, and ortho-image generation. The preprocessing step uses a phenomenological model to extract pure shade images and remove pigment parts. Using existing invariances, equations for slope and ortho-image are derived from the equation of shading and observation. A recurrence relation is also derived to determine the mean slope in discrete images. Theoretically, unique shape can be recovered without iteration for Lambertian cylinders, but a practical algorithm is implemented to overcome self-shadows. Simulations and real experiments demonstrate the effectiveness of the strategy and algorithms. 
411719	41171915	A visual language compiler for information retrieval by visual reasoning	The article discusses how expanding databases can create challenges in retrieving data, and introduces a solution using data visualization and visual reasoning. The proposed approach involves transforming data objects and representing them in a visual space, allowing users to use a visual language to formulate information retrieval requests. A prototype system is described, which includes features such as customization for different applications, a fuzzy icon grammar for visual sentences, and a semantic model for translating fuzzy visual queries. Additionally, the system has a VisualNet component that stores learned knowledge for the VisualReasoner to adapt its behavior. 
411720	4117207	Distributed Welfare Games	Game-theoretic tools are being increasingly used in the design of distributed resource allocation algorithms. The main aspect of this design approach is assigning utility functions to individual agents. The aim is to assign each agent a suitable utility function that ensures desirable properties such as scalability, tractability, and the existence and efficiency of pure Nash equilibria in the resulting game. This paper focuses on studying the design of utility functions for a specific type of game called distributed welfare games. The paper presents various methodologies for utility design that guarantee desirable game properties regardless of the application domain. The results are illustrated in two commonly studied resource allocation problems: "coverage" and "coloring" problems. 
411721	41172165	An exploratory study of the impact of antipatterns on class change- and fault-proneness	Antipatterns are design choices that make object-oriented systems difficult to maintain. A study was conducted to examine the impact of antipatterns on classes in these systems, specifically looking at how the presence of antipatterns affects the likelihood of a class to experience changes or faults. The study analyzed 13 antipatterns in 54 releases of four popular systems and found that classes with antipatterns were more likely to experience changes and faults than other classes. This was not solely due to the size of the class, as even smaller classes with antipatterns had a higher likelihood of changes and faults. The study also found that structural changes were more likely to affect classes with antipatterns. These findings support previous research on antipatterns and can help inform quality assurance and testing efforts in object-oriented systems.
411722	41172211	Fuzzy efficiency measures in data envelopment analysis	This paper presents a new approach for measuring the efficiencies of decision making units (DMUs) using data envelopment analysis (DEA). The existing DEA models are limited to crisp data, but this new approach incorporates fuzzy data. The method involves transforming a fuzzy DEA model into a family of crisp DEA models using the a-cut approach. The result is a pair of parametric programs that describe the efficiency measures, which are expressed as membership functions rather than crisp values. This allows for more detailed information to be provided to management. By incorporating fuzzy data, the DEA approach becomes more powerful and applicable in a wider range of situations.
411723	41172323	Towards large scale argumentation support on the semantic web	This paper introduces the concept of a World Wide Argument Web (WWAW), which is a large-scale Web of interconnected arguments posted by individuals to express their opinions in a structured manner. The authors propose an extension of the Argument Interchange Format (AIF) to express arguments with a structure based on Walton's argumentation schemes. They also describe an implementation of this ontology using the RDF Schema language, and showcase how it can be used to represent networks of arguments on the Semantic Web. Additionally, they present a pilot system, ArgDF, which allows users to create and query arguments using different argumentation schemes. This tool is intended to serve as a foundation for future applications related to authoring, linking, navigating, searching, and evaluating arguments on the Web.
411724	41172447	Extracting predicates from mining models for efficient query evaluation	This article discusses how modern relational databases are now able to handle ad hoc queries on mining models. The authors propose techniques for optimizing these types of queries by using the internal structure of the mining model to automatically derive traditional database predicates. They present algorithms for deriving these predicates for popular discrete mining models such as decision trees, naive Bayes, clustering, and linear support vector machines. The experiments conducted on Microsoft SQL Server show that using these derived predicates can greatly reduce the cost of evaluating these types of queries.
411725	41172542	Untraceable mobility or how to travel incognito	User mobility is becoming increasingly important in networks, particularly in wireless and cellular networks. However, this also raises concerns about security, specifically the ability to track a user's movements and location. Ideally, only the user and a responsible authority should know this information. Currently, many networks do not address this issue or use solutions that are specific to cellular networks. This paper discusses the problem of anonymity and location privacy in mobile networks and reviews existing approaches. It also proposes several low-cost solutions with varying levels of protection and assumptions about the network environment.
411726	41172613	Defeasible Inheritance Through Specialization	The article discusses how typed substitution can capture inheritance in logic deduction systems, but this becomes problematic when dealing with method overriding and multiple inheritance. To address this issue, the article proposes a framework based on Dun's argumentation theory for developing a natural semantics for programs with dynamic nonmonotonic inheritance. The relationship between this proposed semantics and the perfect-model semantics proposed by Dobbie and Topor is explored, and it is found that they coincide for inheritance-stratified programs. The proposed semantics also provides accurate interpretations for programs that are not inheritance-stratified. 
411727	41172724	Prioritised fuzzy constraint satisfaction problems: axioms, instantiation and validation	This paper presents a framework for solving prioritized fuzzy constraint satisfaction problems (PFCSPs) and proposes methods for constructing specific schemes that follow this framework. The framework includes five methods for creating priority operators to calculate the local satisfaction degree of a prioritized fuzzy constraint, as well as identifying priority T-norm operators to calculate the global satisfaction degree. Numerical and real-life examples are used to demonstrate the effectiveness of the system and provide insights. The paper also discusses the relationship between weight schemes and prioritized FCSP schemes, showing that weighted FCSP schemes are the dual of prioritized FCSP schemes, also known as posterioritised FCSP schemes.
411728	41172844	Feature based robust watermarking using image normalization	A new watermarking scheme using feature point detection and image normalization is proposed in this paper. The scheme first detects stable feature points in the original image using a multiresolution filter. These points are then used to normalize the image and embed the watermark in the subband coefficients of each disk. The watermark detection process relies on the correlation between the embedding coefficients and the original watermark, eliminating the need for the original image. The combination of feature point detection and image normalization results in a robust scheme that can withstand signal processing and geometrical distortions. The experimental results show the effectiveness of this approach.
411729	41172917	Strict sensitivity analysis in fuzzy quadratic programming	Quadratic programming is a mathematical approach that can be applied to both linear and non-linear programming problems. It is commonly used in practical fields such as regression, production efficiency, and portfolio selection. In real-life situations, where uncertainty and imprecision are common, fuzzy quadratic programming can be used to handle these complexities. This paper focuses on studying the sensitivity analysis of fuzzy quadratic programming when there are simultaneous and independent variations in the constraints and objective function coefficients. The authors propose a method that uses auxiliary problems to identify the intervals of invariance and provide a fuzzy representation of the optimal value function. Numerical examples are also provided to demonstrate the effectiveness of this approach.
411730	411730196	Information acquisition through an integrated paradigm: agent + peer-to-peer	Agent computing and P2P development are two distinct paradigms that have their own strengths. While agent computing focuses on defining abstract problem-solving strategies, P2P development excels in resource gathering and efficient resource discovery. However, in order to create self-evolving and scalable systems, the integration of these two paradigms is necessary. This paper delves into the potential benefits of incorporating P2P facilities into agent development and explores different ways of integration. A prototype system, BestPeer, is introduced as an example of this integration. Additionally, the challenge of managing data in P2P environments where the schema is not predetermined is addressed with the introduction of PeerDB, a system that enables data sharing without a global schema.
411731	41173130	Using a pilot study to derive a GUI model for automated testing	Graphical user interfaces (GUIs) are widely used in software but testing them for functional correctness is an area that has not been extensively studied. GUIs have many different input options for users, making it challenging to test all possible interactions. Previous research has used informal examination and personal intuition to create an event-interaction graph (EIG) to help generate and execute test cases. This article describes a pilot study that empirically derives the EIG model and validates the intuition used in previous work. The EIG helps testers understand how events interact and why certain events detect faults, allowing them to better traverse the event space. New test adequacy criteria and algorithms are proposed and shown to be effective on four open-source applications.
411732	411732284	Graph indexing based on discriminative frequent structure analysis	This article discusses the importance of graphs in modelling complex structures and schemaless data. It explores the challenges of indexing graphs and proposes a new indexing model based on discriminative frequent structures, which are identified through graph mining. This compact index improves performance in retrieving graphs from a large database and is stable to database updates. The approach also shows how database indexing and query processing can benefit from frequent pattern mining. It can be applied to other structures such as sequences and trees.
411733	41173338	Adaptable Mobile Applications: Exploiting Logical Mobility in Mobile Computing	The article discusses the challenges that come with the increasing use of mobile devices for applications. These applications are often limited in interoperability and context-awareness, making deployment and updates difficult. The dynamic environments that these applications are used in require a shift from design-time to run-time effort in software development. To address these challenges, the authors have created SATIN, a middleware system that allows for the use of logical mobility techniques on mobile devices connected to diverse networks. Through SATIN, applications can be deployed and updated easily and efficiently. The article outlines the approach and benefits of using SATIN for building adaptable applications on mobile devices.
411734	4117345	Supplier selection using fuzzy AHP and fuzzy multi-objective linear programming for developing low carbon supply chain	The sustainability of a supply chain is heavily influenced by the purchasing strategy of its members. However, traditional models have focused on factors such as cost, quality, and lead time, neglecting the importance of carbon emissions in supplier evaluation. With increasing pressure to reduce carbon emissions, this study proposes an integrated approach that utilizes fuzzy-AHP and fuzzy multi-objective linear programming to select the most suitable supplier in a supply chain. The method involves analyzing the weights of various factors, including cost, quality, delivery performance, and carbon emissions, using fuzzy AHP. These weights are then used in a fuzzy multi-objective linear programming model for supplier selection and quota allocation. An example is provided to demonstrate the effectiveness of the approach in handling uncertain information.
411735	41173531	Development of a module based service family design for mass customization of airline sector using the coalition game.	This paper aims to address the challenges faced by the airline industry through the introduction of new and simplified methodologies. The use of strategic sharing of modules in service family design, modeled through coalition and Cournot game theories, is proposed as a cost-effective way to achieve mass customization and improve individual products. Through the analysis of empirical evidence, it is found that convenience, safety, and service quality play a significant role in passengers' choice of airline. The Cournot model is used to predict the impact of new services on an airline's profits and its competitors' response. This research can assist airline companies in selecting services and understanding the importance of service quality in the industry, giving them a competitive advantage.
411736	4117364	The Propositional Mu-Calculus is Elementary	The propositional mu-calculus is a type of logic used to reason about computer programs. It includes a least fixpoint operator and is more powerful than other logics such as Propositional Dynamic Logic, Streett's infinite looping construct, and Parikh's Game Logic. A decision procedure for the mu-calculus based on automata on infinite trees is presented, and a small model theorem is also derived.
411737	41173755	Some Results on Fixed and Best Proximity Points of Precyclic Self-Mappings.	This paper discusses the limit properties of distances and the existence and uniqueness of fixed points, best proximity points, and limit cycles for single-valued and contractive precyclic self-mappings on a finite set of subsets of uniformly convex Banach spaces. These self-mappings are defined with a contractive condition that allows for the mapping of points within a subset or to an adjacent subset. The contractive condition is point dependent and is applied to the complete set rather than each individual subset. The distances between adjacent subsets can also vary, including the possibility of intersection between some pairs of subsets.
411738	41173847	Experimental evaluation of query processing on encrypted Telemedicine data	Telemedicine technology has greatly improved the quality of medical care while reducing healthcare costs. However, it also brings challenges to data security and privacy. To address these concerns, various security strategies can be used, such as encrypting user-related data. With the increasing use of XML in data interchange, efficient querying over encrypted XML data is important. In a previous study, a hash-based approach was developed to enable efficient querying over encrypted XML data by augmenting it with encodings and filtering out candidate data for decryption. This paper presents an experimental evaluation of this approach for querying over encrypted XML-formatted Telemedicine data. The proposed search strategy consists of three phases: query preparation, pre-processing, and execution. 
411739	41173977	Spatial and temporal locality of content in BitTorrent: A measurement study	The study examines content locality on The Pirate Bay, one of the largest BitTorrent portals. The researchers analyze factors such as content category, publisher, and popularity to understand how and why content is consumed in different locations and times. They find that content consumption is heavily skewed in both spatial and temporal domains, with cultural factors and publishing purpose playing a role. The study also reveals that while spatial locality remains consistent on a daily basis, there is a noticeable spread of content consumption over the years. The researchers suggest that bundling and caching can be effective strategies to take advantage of content locality.
411740	41174024	StubDroid: automatic inference of precise data-flow summaries for the android framework.	Smartphone users often lack information about how commercial and malicious apps handle their sensitive data. Automated taint analyses offer a solution by allowing users to detect and investigate how apps access and handle this data. However, most of these analyses rely on explicit models of the Android runtime library, which are difficult to obtain due to the size and rapid evolution of smartphone operating systems. In response, StubDroid was developed as the first fully automated approach for inferring accurate and efficient library models for taint analysis. These models significantly improve the efficiency and precision of analyses, as demonstrated in experiments where StubDroid-inferred models allowed for app analysis in seconds rather than the usual 30 minutes. Additionally, StubDroid's models outperformed manually crafted ones in terms of precision, time, and memory usage. 
411741	41174135	Model Transformation Reuse Across Metamodels - A Classification and Comparison of Approaches.	Model transformations (MTs) are crucial in model-driven engineering (MDE) solutions. However, MDE often relies on creating new domain-specific metamodels, which can result in the need to create new MTs for each one. This paper examines different approaches to reusing MTs across different metamodels, including model types, concepts, a-posteriori typing, multilevel modeling, and design patterns. A feature model was developed to compare these approaches, and a common example was used to demonstrate their strengths and weaknesses. The paper also identifies gaps in current reuse approaches and provides a reading grid for comparing their features. This research highlights the importance of reuse mechanisms in MDE and offers insight into the most effective approaches for reusing MTs.
411742	41174213	A Framework for Model Transformation By-Example: Concepts and Tool Support	Model-Driven Engineering (MDE) is a new approach to software development that uses model transformations to bridge the gap between specifications and implementations. Many different model transformation methods and languages have been developed, but most of them require knowledge of the implementation details of modeling languages. This paper presents a new approach called Model Transformation By-Example (MTBE) that allows users to define mappings between example models without needing to know the implementation details. The paper describes the MTBE concepts, including a model mapping language and a metamodel mapping language, as well as algorithms for inferring metamodel mappings from model mappings. The authors also discuss how these concepts have been integrated into existing graphical modeling and model transformation frameworks, and provide a case study to demonstrate their effectiveness. 
411743	41174356	Rsa-Oaep Is Rka Secure	This paper presents a proof that the RSA-OAEP encryption scheme is secure against related key attacks (RKA) in the random oracle model, assuming the strong RSA assumption. The related key functions used can be affine functions. Unlike previous chosen ciphertext security proofs for OAEP, this proof overcomes two major obstacles: answering decryption queries under related keys, and preventing the adversary from promoting queries corresponding to the same message as the challenge ciphertext. These obstacles also exist in the RSA-OAEP+ and RSA-SAEP(+) RKA security proofs, but can be overcome by combining this technique with chosen ciphertext security proofs. The proof relies heavily on the algebraic property of the sRSA function.
411744	4117443	Symbolic Trajectories.	This article discusses the management of mobility data, specifically trajectories recorded by GPS-enabled devices. The focus is on discovering meaningful patterns and representing them in a systematic way. The authors propose a simple model called symbolic trajectory, which captures a wide range of meanings derived from a geometric trajectory. This model is integrated into a data types and operations framework for moving objects. Additionally, the article presents a language for pattern matching and rewriting of symbolic trajectories, which can be used in query operations within a DBMS environment. The implementation of the model using finite state machines is described and an experimental evaluation demonstrates its efficiency in terms of storage space and response time. 
411745	41174555	An Ultra-Fast User-Steered Image Segmentation Paradigm: Live Wire On The Fly	The authors have developed a new, faster method for user-guided image segmentation called "live wire on the fly." This method builds on previous segmentation paradigms and uses known properties of graphs to avoid unnecessary computations, resulting in faster segmentation even on low-powered computers. The user simply selects a point on the boundary and the live-wire segment is displayed in real time as they move the cursor, making the process more efficient. This method has been tested on medical images and found to be 1.3-31 times faster than previous methods, with the computational part alone being 120 times faster.
411746	41174660	Supporting multimodality in service-oriented model-based development environments	This article discusses the challenges of developing multimodal interfaces and the lack of authoring tools available for this purpose. The author proposes using model-based languages to specify multimodality and demonstrates how this can be applied to graphical and vocal interactions. The article also introduces the use of CARE properties and a model-based language to support service-based applications and Web 2.0 interactions. This approach is supported by an authoring environment and can generate implementations of multimodal interfaces for Web environments. An example of this process is provided, highlighting the potential for designers to modify the solutions to meet their specific needs.
411747	41174746	End-user visualizations	In recent years, computer visualization has greatly improved, largely due to the popularity of video games. Technology has advanced to the point where even handheld devices can efficiently render and animate complex 3D graphics. However, there is a lack of accessible tools and frameworks for end-user developers to utilize these advancements and create their own interactive 2D and 3D applications. The Agent Warp Engine (AWE) is a shape-warping framework that aims to address this issue and allow for the creation of rich visualizations, animations, and simulations by end-users.
411748	41174881	Experience with user-centred requirements engineering	This paper outlines a case study where human-computer interaction (HCI) principles and methods were used to develop a visualization tool, ADVISES, for epidemiological research. The development process involved scenario-based design, prototyping, and storyboarding to understand user tasks and mental models. HCI functional allocation heuristics were also applied to guide system requirements decisions. The prototype was evaluated to determine its effectiveness in supporting public health professionals. The study results and implications of using scenario-based design, functional allocation, and software architecture are discussed. 
411749	41174944	Simulating a Shared Register in a System that Never Stops Changing.	Emulating a shared register is a way to simplify the design of algorithms for asynchronous message-passing systems with crash failures. It involves replicating the register's value in multiple servers and requiring communication with a majority of servers. While this approach has been successful for static systems, there are challenges when it comes to dynamic systems where nodes can enter and leave. Existing emulations have limitations such as assuming the system will eventually stop changing or imposing a bound on the system size. This paper presents a new emulation that can support any number of readers and writers in a crash-prone system with continually changing nodes and without a bound on system size. The algorithm has certain requirements for the number of entering and leaving nodes and crashed nodes, and the paper also includes a lower bound for the necessary number of correct nodes.
411750	41175046	Effective Lossy Queue Languages	The paper discusses the computation of reachable states in a lossy channel system (LCS). While the set of reachable states is known to be regular, it cannot be effectively constructed. The paper characterizes certain classes of LCS for which the set of reachable states can be computed. It also introduces rewriting systems that capture the behavior of LCS and shows that for context-free rewriting systems, the corresponding language can be computed. The paper also presents a characterization of classes of meta-transitions with nested loops in the control graph of the system, rather than just single loops. The same technique is also used to establish a result for 0L-systems, where the downward closure of the language generated by any system is effectively regular.
411751	41175123	ADELFE: a methodology for adaptive multi-agent systems engineering	Adaptive software is necessary when the environment is unpredictable or the system is open. The ADELFE methodology, guided by the Rational Unified Process (RUP), is specifically designed for software engineering of adaptive multi-agent systems. By following ADELFE, the software is developed in accordance with the AMAS theory. This paper focuses on the additions that ADELFE brings to the first three core workflows of RUP. During the requirements phase, the system's environment is defined and characterized. In the analysis phase, the engineer is guided to use adaptive multi-agent technology and identify the agents through system and environment models. The design workflow provides a cooperative agent model and helps define the behavior of local agents. The methodology is demonstrated through a case study of timetable design.
411752	41175210	Monitoring the monitor: an approach towards trustworthiness in service oriented architecture	Service-oriented architecture focuses on separating clients and providers of a service through an abstract service description. This allows a service broker to match clients with suitable service implementations, and clients can then directly send requests to the implementation. However, the current architecture lacks a reliable way for clients to specify and verify non-functional properties, such as data access and persistence restrictions. In response, the authors propose an extended architecture that addresses this issue. A prototype implementation and preliminary results show the potential usefulness of this architecture in real-world software applications. 
411753	41175394	Power-aware multi-objective evolutionary optimization for application mapping on NoC platforms	Network-on-chip (NoC) is the next generation communication infrastructure that will be present in various environments. In platform-based design, an application is implemented using a set of collaborating intellectual property (IP) blocks. However, selecting the most appropriate IPs and mapping them onto the NoC efficiently is a challenging task. To address this, the paper proposes a power-aware multi-objective evolutionary algorithm for the assignment and mapping stages of NoC design synthesis. This algorithm uses either NSGA-II or microGA as a kernel and optimizes for area and execution time while also considering the decision maker's power consumption preferences. 
411754	41175479	Efficient concise deterministic pattern-matching automata for ambiguous patterns	Pattern-matching is a crucial aspect of various applications like functional programming, logic programming, and theorem proving. In this process, patterns are converted into a deterministic finite automaton for efficient matching. However, when dealing with ambiguous patterns, it is possible for a subject term to match multiple patterns, resulting in the need for a priority rule to select the appropriate one. To optimize this process, a new pre-processing technique has been developed that identifies and eliminates irrelevant instances of ambiguous patterns. This helps to reduce the space and time requirements of the matching automaton, making the process more efficient.
411755	411755139	A learning based training and skill assessment platform with haptic guidance for endovascular catheterization.	The demand for endovascular intervention has increased and there is a need for technical skill training and performance measures. However, there are no established online metrics for assessing technical skills and limited research on operator behavior during procedures. This paper introduces a platform for online training and objective assessment of endovascular skills, using multiple demonstrations to learn optimal catheter motions. An ungrounded hand-held haptic device is also proposed to provide haptic guidance to inexperienced users, based on the learned information. Statistical models are used to extract catheter motion patterns and improve performance and haptic guidance. Results show improved navigation and finer catheter motions with the proposed platform, suggesting its potential for integration into clinical training and further improvement of endovascular training with more realistic features.
411756	411756143	Bag Of Forests For Modelling Of Tissue Energy Interaction In Optical Coherence Tomography For Atherosclerotic Plaque Susceptibility Assessment	The use of intravascular optical coherence tomography (IV-OCT) allows for high resolution imaging of tissue pools that are at risk of rupturing in atherosclerosis. However, the interpretation can be subjective due to the stochastic nature of OCT speckles. To address this issue, a framework is proposed that utilizes OCT tissue energy interaction to identify susceptible tissues. This is achieved by combining local speckle statistics, optical attenuation estimates, and signal confidence measures in a multiscale approach. A bag of random forests is then trained on a bank of 22 OCT cross-sections and evaluated on 22 other diverse cross-sections to demonstrate the robustness of the framework. This framework has the potential to provide more objective assessment and aid in decision making during interventions for atherosclerosis.
411757	41175790	EDAS: energy and distance aware protocol based on SPIN for wireless sensor networks	The challenge of designing energy-efficient dissemination protocols for Wireless Sensor Networks (WSNs) has been addressed by multiple recent studies. One notable protocol, SPMS, has been shown to outperform the well-known SPIN protocol due to its use of the shortest path to minimize energy consumption. However, this approach also leads to a reduction in network lifetime as the same shortest path is repeatedly used. To address this issue, a new protocol called EDAS (Energy and Distance Aware protocol based on SPIN) is proposed. EDAS takes into account both residual energy and the most efficient distance between nodes to determine a path for data dissemination, resulting in both energy efficiency and a 69% increase in network lifetime compared to SPMS. 
411758	4117582	Junction detection for linear structures based on Hessian, correlation and shape information	This paper introduces a method for detecting junctions in 2D images with linear structures and determining the number of branches and branch orientations. The method combines Hessian information and correlation matrix to select candidate junction points, which are then refined and the branches are identified using intensity information from a stick-shaped window and a Gaussian-shaped multi-scale stick template. The proposed approach is compared to three other methods and results show that it is more accurate in detecting junctions. The multi-scale template is effective in detecting structures with varying widths. The algorithm was tested on various types of images and showed promising results.
411759	41175966	A scalable and highly available system for serving dynamic data at frequently accessed web sites	This paper discusses the system and techniques used to achieve high performance and availability for the official website of the 1998 Olympic Winter Games. The website utilized thirteen SP2 systems with a total of 143 processors, and a key feature was the constant updating of data presented to users. To efficiently serve dynamic data, the website cached pages and implemented a new algorithm called Data Update Propagation (DUP) which updated stale pages directly in the cache. This resulted in a cache hit rate of close to 100%. The website was able to serve pages quickly and was available 100% of the time. The paper also describes the key features used for high availability and the website's structure to provide useful information with minimal page visits.
411760	41176040	A DEVS component library for simulation-based design of automated container terminals	Modeling and simulation are commonly used during the analysis phase of design processes, but simulation-based design allows for continuous use throughout the synthesis phase. This allows for quick comparison of alternative designs in areas such as layouts, terminal operating systems, and equipment. Container terminals are complex systems with many interacting entities, so a component-based approach simplifies the model construction process by allowing designers to focus on relevant constructs rather than low-level details. However, achieving compatibility and modularity between components requires significant effort. DEVS offers higher level constructs to conceptualize complex systems independently from their implementation. A DEVS component library has been developed for container terminal design, with a focus on distinguishing between control and mechanics. This library supports the design process for container terminals.
411761	41176143	Detection and description of moving objects by stochastic modelling and analysis of complex scenes	This paper discusses a novel approach for detecting and describing moving objects in natural scenes using statistical analysis of video sequences. Traditional methods rely on change detection algorithms, but these can lead to false alarms or missed events due to factors like camera noise or varying illumination. To address this, the proposed technique incorporates additional features such as texture and motion to better distinguish between true moving objects and other sources of temporal changes in the image signal. The algorithm also adapts to fluctuations in the scene by estimating the probability distributions of the different features over time. Significant deviations from these distributions are identified as potential moving objects.
411762	41176214	A formal model of user-defined resources in resource-restricted deployment scenarios	The paper discusses the importance of considering deployment choices in software development and proposes a method for modeling and analyzing these choices using a combination of a generic cost model and deployment components in the Timed ABS language. The cost model can be customized by the user to capture specific resource requirements, while architectural variations are represented through resource-restricted deployment scenarios. The approach is demonstrated using an example of multimedia processing servers and a simulation tool is used to analyze deadline misses for different usage and deployment scenarios. This integration of low-level deployment concerns into high-level models allows for early evaluation of the consequences of deployment choices on software performance.
411763	4117636	A large user pool for accessibility research with representative users	Accessibility research is crucial for ensuring that technology is usable and accessible for all individuals. One key aspect of this research is involving representative users in the exploration and evaluation of ideas. However, it can be challenging to recruit these users, especially in a timely manner. This paper discusses the creation of a large user pool specifically for older adults who are interested in participating in technology research studies. The process of establishing and maintaining this pool is also discussed, along with lessons learned. This user pool will help facilitate accessibility research by providing a large and diverse group of users for studies.
411764	4117643	CaptainTeach: multi-stage, in-flow peer review for programming assignments	In this paper, the authors discuss the use of peer review in computing education, specifically in programming courses. They introduce a new tool called CaptainTeach, which allows for peer review at multiple stages within ongoing assignments. This is different from traditional peer review methods that only focus on completed assignments. The authors describe their experience using CaptainTeach in two undergraduate courses and highlight the logistical and pedagogical challenges that arise from this type of multi-stage, in-flow peer review. They focus on the unique issues that come with combining multiple stages and in-flow reviewing, rather than peer review in general.
411765	41176547	Coding partitions: regularity, maximality and global ambiguity	The canonical coding partition is a way of dividing a set of words into classes based on their factorizations in a sequence. It ensures that words with multiple factorizations are grouped together in the same class. If the set is not uniquely decipherable, the partition separates a unique unambiguous class from the remaining parts that reveal the ambiguities in finite sequences. The partition for a regular set has a finite number of regular classes, and an algorithm is provided for its calculation. The article also explores maximality conditions in coding partitions, proving the equivalence between two notions in the regular case. This has applications in deriving new properties of maximal uniquely decipherable codes.
411766	41176678	Adapting the Fitness Function in GP for Data Mining	The Stepwise Adaptation of Weights (saw) technique, initially designed for constraint satisfaction problems, can also be effectively utilized in genetic programming. This mechanism is suitable for data mining tasks where the fitness of a solution is determined by 'local scores' on data records. By incorporating saw-ing into genetic programming, it was found to improve performance on various benchmark classification data sets. However, the results also revealed that saw-ing may have a negative impact if different types of misclassifications are weighted differently. Overall, saw-ing shows promise in enhancing the capabilities of genetic programming for certain types of problems.
411767	411767101	Intelligent Software Agent Design Tool Using Goal Net Methodology	Intelligent agent technology is rapidly expanding and has many practical uses. However, there are limited tools available to assist in converting paper-based designs into effective representations that can be used by agent management systems. The Goal Net Designer, an Integrated Development Environment (IDE), is proposed as a solution. It is based on the Goal Net model, a goal-oriented approach, and simplifies the design process while automatically generating design data for the Multi-Agent Development Environment (MADE). This reduces the required level of expertise, allowing even those with little knowledge of intelligent agent technology to easily incorporate intelligent agents into their applications and save time and resources during development.
411768	411768154	Implementation of fuzzy cognitive maps based on fuzzy neural network and application in prediction of time series	The fuzzy cognitive map (FCM) is a popular tool for representing knowledge and simulating different systems. However, it is limited by the lack of efficient methods for determining system states and quantifying causal relationships. This means that FCMs are often constructed using expert knowledge, leading to subjective models with reliability issues. To address this, a fuzzy neural network approach was proposed that combines the inference mechanism of FCMs with the determination of membership functions and quantification of causalities. This allows for automatic construction of FCM models from data, reducing the need for human intervention. The approach was tested through numerical simulations and shown to be effective in predicting time series.
411769	41176923	Shoot & copy: phonecam-based information transfer from public displays onto mobile phones	Public displays are a common sight in our daily lives, but they usually only provide information without any interaction options. As a result, people often forget what they saw after leaving the display. This paper introduces a new interaction technique that allows users to transfer information from a public display onto their personal mobile phone by taking a picture with the phone's camera. This technique, called Shoot & Copy, eliminates the need for visual codes and allows users to capture a specific region of the screen, such as icons representing data. The captured image is analyzed and a reference to the corresponding data is sent to the phone. This allows users to retrieve the data at a later time, making it easier to remember and use. The paper also includes an evaluation of the prototype and its potential applications.
411770	4117706	Developing a Dataset for Technology Structure Mining	The paper discusses the creation of a development dataset for the task of Technology Structure Mining, which involves mapping a scientific corpus into a labeled graph that expresses interdependencies between technologies. The dataset consists of sentences from the ACL Anthology Corpus, with each sentence annotated with at least two technologies and their interdependence. The annotations are done at two layers: lexical and termino-conceptual. The lexical layer represents varying lexicalizations of a technology, while the termino-conceptual layer groups these variations under a single concept. The dataset also includes five groups of contexts, classified based on linguistic and syntactic criteria. The initial dataset contains 482 sentences and is intended to serve as a benchmark for the technology structure mining task.
411771	41177128	Can fault-exposure-potential estimates improve the fault detection abilities of test suites?	Code-coverage-based test data adequacy criteria treat all coverable code elements equally, but in practice, some faults are more easily revealed than others. Researchers have suggested estimating the probability that a fault in a code element will cause a failure and using this to determine the required number of executions for a certain level of confidence. This could improve fault-detection effectiveness and help distribute testing resources. However, the empirical evidence did not support this conjecture and the results showed only small increases in fault-detection effectiveness. This suggests that this approach may not be worth the cost of implementation. Further research is needed to explore methodologies for obtaining fault-exposure-potential estimates and incorporating them into test data adequacy criteria. 
411772	411772143	The u-SIG System; A Connectionist Driven Simulation of Socially Interactive Agents	The popularity of games such as The Sims and Black & White has highlighted the demand for Non Player Characters (NPCs) to have well-developed personalities, moods, and relationships. To achieve this, agent architectures used to create NPCs must be enhanced with models of these aspects of a character's persona. This paper introduces the μ-SIC system, which utilizes psychology-based models and Artificial Neural Networks (ANNs) to drive character behavior in social interactions. The system considers a character's personality, mood, and relationships with other characters in a virtual environment to determine which interaction is most suitable. The paper provides an overview of the project, describes the psychological models used, and explains the implementation and simulation of the μ-SIC system. It also discusses the strengths and weaknesses of the system and suggests potential improvements.
411773	41177356	An approach to complex agent-based negotiations via effectively modeling unknown opponents.	This article proposes a novel approach to complex agent-based negotiations. The approach focuses on learning the strategy of an unknown opponent and suggests making concessions in an adaptive manner. Through extensive experiments, the approach, called OMAC¿, has shown to improve negotiation performance compared to top agents from the International Automated Negotiating Agents Competition. OMAC¿ uses discrete wavelet transformation and non-linear regression with Gaussian processes to model opponents in real-time and adaptively adjust its utility expectations and negotiation moves. This approach has potential applications in various areas, such as e-commerce and e-business, and addresses limitations in current negotiation algorithms and techniques.
411774	41177423	The Dimensional Fact Model: A Conceptual Model For Data Warehouses	Data warehousing systems allow enterprise managers to gather and combine data from various sources and effectively query large databases. Building a data warehouse requires a different approach from operational information systems, with a strong emphasis on conceptual design. This paper presents a formal graphical model, the Dimensional Fact model, and a semi-automated methodology for building it from existing schemes. The model includes fact schemes with elements such as facts, measures, attributes, dimensions, and hierarchies, as well as features like additivity and optionality. Compatible fact schemes can be overlapped to support drill-across queries. The proposed language for denoting data warehouse queries, based on fact instances, can assist in the logical and physical design phases when coupled with information about expected workloads. 
411775	41177518	Data Warehouse Testing	Testing plays a crucial role in the development of software products, including data warehouses. While various aspects of data warehouse design have been extensively studied, there is a lack of research on data warehouse testing. In this paper, the authors propose a set of testing activities specifically for data marts and categorize them based on what is being tested and how. They also demonstrate how these activities can be incorporated into a reference design method to create a comprehensive and adaptable approach. The authors further discuss the practical application of their proposed approach through a real-life case study.
411776	41177612	Time-Frequency Image Descriptors-Based Features For Eeg Epileptic Seizure Activities Detection And Classification	This paper introduces a new set of features for automatically detecting and classifying epileptic seizure activity in EEG signals. Previous methods focused on analyzing signal features, but this new approach uses image descriptors extracted from the time-frequency representation of the signals. These descriptors include shape and texture-based features and are processed using time-frequency image processing techniques. The results from real EEG data showed an overall classification accuracy of up to 98% using a one-against-one SVM classifier. This outperforms other methods that only use signal features or a combination of signal and image features by 3%. These findings suggest that the proposed method is effective for detecting and classifying epileptic seizures in EEG signals.
411777	41177781	Problem-Based Learning Supported By Semantic Techniques	Problem-based learning has been utilized for the past thirty years in various learning settings. This approach involves presenting learners with different problems in a specific domain, allowing them to develop solutions while gaining knowledge about the subject. In conceptual modeling, specifically in Qualitative Reasoning, these solutions take the form of models that depict the behavior of a dynamic system. The learner's objective is to connect their initial model, which is their first attempt at representing the system, with the target models that provide solutions to the problem. To aid in this process, we suggest the use of semantic technologies and resources that can provide links to relevant terminology, formal definitions, and matching techniques, making use of existing models to enhance the learning experience.
411778	41177810	A scheduling policy with maximal stability region for ring networks with spatial reuse	A ring network with spatial reuse allows multiple users to transmit messages simultaneously, resulting in higher throughput compared to standard token rings. However, this also raises fairness concerns as some nodes may be blocked from accessing the ring for extended periods. To address this, policies have been proposed that guarantee a certain number of packets will be transmitted by each node in a cycle. This paper focuses on designing a stable policy for a ring with spatial reuse, even when the arrival rates are within the stability region. The proposed policy is adaptive and does not require knowledge of arrival rates, and can be implemented using a distributed mechanism. The main results are derived using the Lyapunov test function technique and methods from regenerative processes. 
411779	4117792	A Simple PromiseBQP-complete Matrix Problem	The article discusses the problem of estimating a diagonal entry of a real symmetric matrix using an efficiently computable function. The goal is to approximate the entry with an error of e bm, where b is an upper bound on the norm of the matrix and m and e are polylogarithmic and inverse polylogarithmic in N, respectively. It is shown that this problem is PromiseBQP-complete, meaning it can be efficiently solved on a quantum computer by repeatedly applying measurements to the jth basis vector and raising the outcome to the mth power. The article also discusses how every uniform quantum circuit can be encoded into a sparse matrix, and how estimating off-diagonal entries is also PromiseBQP-complete.
411780	41178063	Reachability by paths of bounded curvature in a convex polygon	B is a point robot that can only move forward with a maximum curvature of 1 in a two-dimensional plane. P is a convex polygon with n vertices. The starting configuration of B is inside P, and we want to determine the region of P that B can reach. This region has a complexity of O(n), and we have an algorithm that can compute it in O(n^2) time. We also prove that a point in P is only reachable if it can be reached by a specific path pattern, which consists of unit circle arcs and line segments.
411781	41178132	The Limits of Composable Crypto with Transferable Setup Devices	The paper discusses the use of setup devices in realizing secure communication in the UC framework. It points out that in stronger versions of the UC framework, such as EUC or JUC, multiple instances of these setups are allowed. The concept of "transferable setups" is introduced, which are setup devices that do not publicly disclose if they have been maliciously passed on. The paper proves that using transferable setups, one cannot realize certain protocols, such as oblivious transfer, in the EUC model. It also shows that physically unclonable functions (PUFs) can be considered as transferable devices, making them unsuitable for use as global setups. The paper further shows that if setups are transferable, they cannot intrinsically leak in the case of a relay attack. The concept of authenticated channels and their importance in the UC model is also discussed, and the paper presents ways to strengthen existing protocols using PUFs to make them not only UC-secure but also JUC-secure. 
411782	41178239	Uncovering the Small Community Structure in Large Networks: A Local Spectral Approach	Large graphs are commonly encountered in various contexts, and studying their structure and extracting information from them is an important area of research. Previous algorithms for mining communities have focused on the global structure of the graph and have been time-consuming. However, with the increasing size of networks, it is crucial to shift the focus to the microscopic structure when dealing with large graphs. A new approach called LEMON (Local Expansion via Minimum One Norm) has been proposed, which identifies communities by finding a sparse vector that includes the seed members. LEMON has shown to be highly efficient and effective in finding communities, with a running time dependent on the community size rather than the entire graph. This approach has been evaluated on various datasets and has shown promising results. Further, the impact of different seed sets on the performance of the algorithm has also been analyzed. 
411783	41178369	A trusted subject architecture for multilevel secure object-oriented databases	This paper discusses security concerns in object-oriented database systems for multilevel secure environments, where users have varying levels of security clearance and access information with different classifications. The authors propose a solution that incorporates security into the object model and can be implemented using a cost-effective and popular security architecture. They also provide a formal proof to demonstrate the confidentiality of their approach and highlight the support for secure synchronous write-up operations, which can be achieved through write-up messages sent by low level users. The authors also address the issue of covert channels and present a concurrent computation model to prevent information leaks while maintaining confidentiality, integrity, and performance. A confidentiality proof for a trusted subject architecture and implementation is also provided. 
411784	411784103	Encryption policies for regulating access to outsourced data	Access control models currently assume that a trusted party manages access requests to ensure compliance with the access control policy. However, with the rise of Web technology and the need for third party services, this approach is no longer sufficient. To address this issue, data owners can encrypt their sensitive information before outsourcing it, allowing only authorized users with the key to decrypt it. This article presents a solution that combines cryptography with authorizations to enforce selective access on outsourced data without involving the owner. The approach includes a formal model for access control management and a two-layer encryption method for efficient policy updates. Experimental results show the effectiveness of this approach.
411785	4117850	Relational Dependency Networks	Graphical models for relational data have shown promise in improving classification and inference by representing dependencies among instances. However, the assumption of instance independence, common in conventional statistical models, does not hold in most relational data sets. For instance, citation data has dependencies among paper references and genomic data has dependencies among protein functions. To address this, relational dependency networks (RDNs) are proposed as graphical models capable of expressing and reasoning with such dependencies. RDNs have strengths in representing cyclic dependencies, parameter estimation, and structure learning using pseudolikelihood techniques. Real-world data sets and synthetic data demonstrate the effectiveness of RDNs in prediction and accuracy under various relational data characteristics.
411786	41178636	Improving the Discovery and Characterization of Hidden Variables by Regularizing the LO-net	The paper introduces a new neural network architecture, called LO-net, which is able to infer hidden variables from streaming multivariate time series data. It consists of two networks, O-net and L-net, where O-net makes initial predictions based on a time delay embedding and L-net learns to approximate the value of a single hidden variable. To prevent L-net from learning the target output of O-net, a penalty term is introduced. The paper proposes a new penalty, called the decor relation penalty, which has shown to improve prediction performance for data with periodic patterns. This extension of the regularized neural network architecture has potential applications in various domains.
411787	41178715	Pride: A Public Repository Of Protein And Peptide Identifications For The Proteomics Community	PRIDE is a database that contains protein and peptide identifications from scientific literature, often related to specific species, tissues, and disease conditions. It also includes information about post-translational modifications and supporting mass spectra. Currently, it contains data from the HUPO Plasma Proteome Project and the human platelet proteome submitted by the University of Ghent. The HUPO Brain Proteome Project's data is expected to be added by late 2005. PRIDE encourages proteomics laboratories to submit their identifications and spectra to support their manuscript submissions. Data can be submitted in PRIDE XML format or mzData format. The database can be accessed through a web application and searched by experiment or protein accession number, literature reference, and sample parameters. Data can be retrieved in PRIDE or mzData XML format, or as HTML.
411788	411788175	Task scheduling for heterogeneous reconfigurable computers	This article discusses the problem of executing a constantly changing set of tasks on a system consisting of a processor and a reconfigurable device. A scheduler is responsible for allocating tasks to either the processor or the device. Each task has a corresponding implementation that is stored as a rectangular block in a database and must be placed on the device at run-time. A placer module is responsible for this task placement, but if there is not enough space on the device, the scheduler may choose to preempt a running task or execute the task on the processor. The article presents an implementation of the placer module and investigates task preemption, both of which are part of an operating system for reconfigurable systems currently in development.
411789	41178915	An Appealing Computational Mechanism Drawn from Bacterial Quorum Sensing	Quorum Sensing is a major discovery in microbiology that has had a significant impact in the last decade. It involves bacterial colonies using synchronized gene expression and phenotype changes to protect their environment, coordinate host invasion, and form biofilms. This process has been modeled in various ways and has been shown to be computationally complete. This means that it has the potential to impact other areas of computer science. This article provides a brief overview of the microbiological background and discusses some of the modeling approaches used. It also suggests potential applications for this computational method.
411790	41179057	Generating Test Sets from Non-Deterministic Stream X-Machines	Holcombe proposed X-machines as a specification language, and subsequent investigations have shown that it is intuitive, easy to use, and versatile for various applications. Stream X-machines, in particular, have been found to be highly effective for specification and most of the theory developed has focused on this type. A testing method has also been developed for systems specified by stream X-machines, which can detect all implementation faults if certain initial requirements are met. However, this method only works for deterministic X-machines. This paper presents a theoretical basis for a method to generate test sets from non-deterministic generalised stream X-machines.
411791	41179166	The Complexity of the Hajos Calculus	The Hajos calculus is a method for creating non-3-colorable graphs. The question of whether there exist graphs that require exponential-sized constructions has been posed, but little progress has been made despite efforts. This paper proves that the Hajos calculus can generate polynomial-sized constructions for non-3-colorable graphs if extended Frege systems are polynomially bounded. These systems are used to prove tautologies and finding lower bounds for them is a difficult problem in logic and complexity theory. The paper also establishes a relationship between the Hajos calculus and bounded-depth Frege systems, allowing for exponential lower bounds to be proven for a subsystem of the Hajos calculus.
411792	41179219	Shvil: collaborative augmented reality land navigation	Shvil is an Augmented Reality (AR) system designed for collaborative land navigation. It enables an overseer (indoor user) and an explorer (outdoor user) to work together by using AR and 3D printing techniques. The overseer is provided with a physical representation of the mission's topography, while the explorer's actions and physical presence are merged with dynamic AR visualization. The system allows for collaboration by overlaying relevant visual information for both users. The prototype and initial results of Shvil are discussed, along with the future vision for the system.
411793	41179336	Evaluating system integrity	The traditional approach to ensuring system integrity focuses on implementing specific controls, such as separation of duties and well-formed transactions. However, this paper suggests a new definition of integrity based on dependability that is not tied to a specific implementation. The authors provide examples to demonstrate how techniques like separation of duties, assured pipelines, fault-tolerance, and cryptography can be seen as ways to achieve integrity, rather than being the definition of integrity itself. This new perspective allows for a more flexible and adaptable approach to maintaining system integrity.
411794	41179445	Autonomic management of multimodal interaction: DynaMo in action	Multimodal interaction is essential in pervasive environments because it offers a natural and flexible way to interact with various digitally controlled equipment. The DynaMo framework is specifically designed to support the development and management of multimodal interaction in these environments. The autonomic approach of DynaMo uses partial interaction models to dynamically create and manage multimodal interaction according to the current conditions and predicted models. This allows for efficient and adaptive interaction with the equipment. Several examples and partial interaction models are used to demonstrate the effectiveness of this autonomic solution.
411795	41179573	Meta-Argumentation Modelling I: Methodology and Techniques.	This paper discusses the use of meta-argumentation in modeling argumentation. It combines Dung's abstract argumentation theory with an extended argumentation theory, using both instantiation and extension methods. The technique of meta-argumentation applies Dung's theory to itself through a process called flattening. The domain of instantiation is represented using a technique based on soundness and completeness. The use of specification languages helps to differentiate between different instantiations. Overall, this approach allows for a more comprehensive and flexible modeling of argumentation.
411796	41179645	A construction of peer-to-peer streaming system based on flexible locality-aware overlay networks	The peer-to-peer multicast system uses participants as peers to create an overlay topology over physical infrastructures. This allows for easy data dissemination and gathering through a multicast application. However, the topology of the overlay can negatively impact the system's performance, such as by affecting delivery efficiency and quality due to factors like heterogeneous and dynamic peers. To address this, the authors propose a flexible locality-aware overlay system that allows peers to establish streaming sessions and act as sources without dedicated servers. This overlay has a 2-layered structure that matches the underlying topology and reduces delivery paths. Simulation results show that this system has better transmission efficiency, shorter delivery delays, and higher reliability compared to other systems.
411797	41179710	Optimal location-multi-allocation-routing in capacitated transportation networks under population-dependent travel times	The article presents a model for optimizing transportation in a network with varying travel times between nodes. The model accounts for the possibility of allocating population in demand nodes to multiple server nodes. It also considers the impact of population flow on travel times and limitations on server node and arc capacities. The goal is to minimize total transportation time by determining optimal server node locations, allocation of population to servers, and allocation of population to routes. The problem is formulated as a mixed-integer non-linear programming model and then transformed into a mixed-integer linear programming problem. A genetic algorithm (GA) and a heuristic algorithm (GALS) are proposed to solve the problem. Numerical experiments show that GALS outperforms the standard GA and the CPLEX solver.
411798	41179852	Cognitive Adaptive Computer Help (Coach): A Case Study	COACH is an intelligent agent that aims to improve the user experience with computer interfaces. It uses a cognitive interface to anticipate the user's needs and provide proactive help. By analyzing user actions, COACH adapts its responses and provides useful information before the user even requests it. The system uses dynamic models to teach and guide the user, and has been shown to improve productivity and comfort in interface usage. COACH has been successfully implemented in various problem domains, including writing Lisp programs, teaching UNIX commands, and formatting text in GML. It has also been adapted for use in modern graphical user interfaces, using techniques such as masking and animation to guide users through procedures. A tool is available to easily create adaptive help systems for different problem domains. Future development is focused on incorporating 3D animation and adaptive tutoring into the system. 
411799	41179918	Synthesis of Mapping Logic for Generating Transformed Pseudo-Random Patterns for BIST	The paper discusses a new technique for improving the effectiveness of built-in self-test (BIST) by synthesizing combinational mapping logic. This logic transforms the set of patterns generated by a pseudo-random pattern generator in order to achieve better fault coverage while minimizing area overhead. By formulating the problem as finding a minimum rectangle cover in a binate matrix, the paper presents a procedure for selecting a mapping function and synthesizing the corresponding mapping logic. Experimental results show that this method requires less hardware overhead compared to other techniques. This is achieved through iterative global operations, resulting in improved fault coverage for a given test length. 
411800	41180030	Socioscope: Human Relationship and Behavior Analysis in Social Networks	This paper proposes a new socioscope model that uses mobile-phone call-detail records for social-network and human-behavior analysis. The model utilizes various probability and statistical methods to quantify social groups, relationships, and communication patterns and detect changes in human behavior. It also introduces a reciprocity index that can be applied in areas such as homeland security, spam detection, and product marketing. The effectiveness of the model is validated using real-life call logs of 81 users from MIT and 20 users from UNT, containing approximately 5000 callers. The results show that the proposed model is effective in analyzing social networks and human behavior.
411801	41180125	Iteratively Extending Time Horizon Reinforcement Learning	Reinforcement learning is a method used to find the best control policy for a system by interacting with it over an infinite time horizon. This is achieved by approximating the Q-function, which represents the optimal control, using a sample of four-tuples (system state, control action, reward, successor state). Classical algorithms use a stochastic approximation to iterate over these four-tuples. In this paper, the problem is reformulated as a sequence of batch mode supervised learning problems, which converges to an approximation of the Q-function. This approach allows for the use of standard supervised learning algorithms and has been tested successfully on the "Car on the Hill" control problem using regression trees. This technique is also efficient for solving larger reinforcement learning problems.
411802	4118028	Marketing of Vice Goods: A Strategic Analysis of the Package Size Decision	Consumers often struggle to resist the temptation of overconsuming products like cookies, crackers, and soft drinks. Some try to control their consumption by buying small packages or avoiding the product altogether, while others continue to buy large packages and overconsume. From a strategic perspective, companies can choose to offer small or large packages. Research on hyperbolic discounting shows that small packages can help consumers with self-control issues, but may also affect prices, profits, and welfare. Results suggest that small packages can increase profits if only a small portion of consumers have self-control issues or if they attract new customers. Competition can reduce incentives for small packages, especially when many consumers are attracted to them. Producing healthier alternatives may also decrease profits for companies. Despite this, small packages are found to enhance consumer and social welfare, even if they increase consumption of unhealthy goods.
411803	41180330	Unknown solution length problems with no asymptotically optimal run time.	This study focuses on optimizing a fitness function with unknown dimensions. The function is defined over bit-strings of length N, but only a small subset of n bits influences the fitness. The position and number of these relevant bits are also unknown. Previous research has developed variants of the (1+1) evolutionary algorithm (EA) that can solve instances of OneMax and LeadingOnes simultaneously for all n values in expected time. This study proves lower bounds for this problem, showing that the (1+1) EA with certain mutation operators has an expected run time of ω(n2 log(n) log log(n) ... log(s)(n)). Additionally, it is shown that there is no algorithm with the best asymptotic performance for this problem. Similar results are also shown for OneMax.
411804	41180418	Asynchronous Veri.able Information Dispersal	Information dispersal is a method of storing files by distributing them among multiple servers in an efficient manner. The problem of verifiable information dispersal in an asynchronous network, where some servers and clients may have faults, is introduced. Verifiability ensures data consistency despite these faults. A scheme for efficient verifiable information dispersal in an asynchronous setting is presented, achieving optimal storage blow-up. The method also guarantees confidentiality against adaptive attacks. This approach also improves the communication complexity of asynchronous reliable broadcast for large inputs.
411805	41180532	Parameter tuning of evolutionary reactions systems	Reaction systems are a formalism based on chemical reactions, introduced by Rozenberg and Ehrenfeucht. A new evolutionary algorithm, called Evolutionary Reaction Systems, has been developed and shown to have comparable performance to other machine learning methods. However, further analysis is needed to fully establish this algorithm, specifically regarding its unique parameters. This study focuses on the effect of one specific parameter, the number of symbols used to represent reactions, on the algorithm's performance. Results show that this parameter greatly impacts the algorithm's success and suggests the need for a set of default parameter values to aid beginners or inexperienced practitioners.
411806	4118062	A quantitative study of learning and generalization in genetic programming	This paper explores the relationship between generalization and functional complexity in genetic programming (GP). It introduces a new measure of functional complexity, called Graph Based Complexity (GBC), which has a strong correlation with GP performance on out-of-sample data. Another measure, called Graph Based Learning Ability (GBLA), is also introduced to quantify the ability of GP to learn difficult training points. The results show that GBLA is negatively correlated with out-of-sample performance. The paper also proposes a new fitness function based on GBC and GBLA, which is empirically shown to be effective. The experiments were conducted on three real-life multidimensional regression problems.
411807	411807114	A Purely Evolutionary Memetic Algorithm As A First Step Towards Symbiotic Coevolution	The paper introduces a new memetic algorithm that combines a local search step with a standard evolutionary algorithm. This approach was tested on symbolic regression problems and compared to other methods that incorporated non-evolutionary local search optimization. The results show that the memetic algorithm outperforms these other methods. The paper also presents preliminary results from a more complex coevolutionary scheme, where the fitness of individuals in the evolutionary algorithm is based on their contribution to the performance of individuals in the genetic programming component. This coevolutionary approach allows for the evolution of heterogeneous populations with a fitness function that reflects their contribution to solving the main problem. 
411808	41180844	Higher-Dimensional Box Integrals	This article discusses the use of symbolic computation to solve previously unsolved problems in the theory of n-dimensional box integrals. The authors were able to find solutions for n = 1, 2, 3, 4, and 5, including an elusive integral called kappa(5). They also discovered a general residue at the pole at s = n, leading to new relations and definite integrals. Additionally, the role of the Clausen function and its generalizations in these higher-dimensional evaluations is highlighted. These results serve as a benchmark for testing symbolic-algebra simplification methods.
411809	4118097	Evolutionary Support Vector Regression Machines	Evolutionary support vector machines (ESVMs) are a new approach that combines the learning engine of support vector machines (SVMs) with evolutionary algorithms (EAs) to evolve the coefficients of the decision function. This simplifies the optimization process in training and makes ESVMs a viable alternative to traditional SVMs. While SVMs are primarily used for classification, this paper introduces \in-evolutionary support regression (\in-ESVR) which applies the same approach to regression problems. The hybridization with the classical \in-support vector regression (\in-SVR) and subsequent evolution of the regression hyperplane coefficients is demonstrated on the Boston housing benchmark problem, showing the potential of ESVMs for regression tasks.
411810	41181076	Shape from Texture: Homogeneity Revisited	This paper aims to estimate the orientation of a scene plane using an uncalibrated perspective image with an unknown texture. The main contributions of the paper are: 1) showing that the problem is equivalent to estimating the vanishing line of the plane, 2) decomposing the estimation into two searches for each parameter of the line, and 3) providing an algorithm that works for regular and irregular textures without explicitly identifying texels. Once the vanishing line is determined, the locations of texels can be found and the geometry of the scene plane can be computed up to an affine transformation. Examples of this approach on real images are also demonstrated.
411811	41181135	An information-theoretic framework to aggregate a Markov chain	This paper proposes a framework for aggregating a large-scale Markov chain to obtain a reduced order Markov model using the Kullback-Leibler divergence rate as a metric for measuring the distance between two stationary Markov chains. The process involves solving an optimization problem to find the optimal aggregated model, which is shown to be the solution to an eigenvalue problem. A recursive algorithm is also presented for constructing a reduced order model with a specified number of super-states. This approach is demonstrated with examples, showing its effectiveness in reducing the complexity of large Markov chains.
411812	41181251	Domain-specific sentiment analysis using contextual feature generation	This paper introduces a new framework for sentiment analysis that utilizes sentiment topic information to generate context-driven features. The authors observed the difficulty of sentiment classification in specific domains and propose considering more contextual information, such as topic or domain. Their system automatically extracts sentiment clues in different domains and identifies their relationship with sentiment topics. Through a bootstrapping process, they generate new clues and train domain-specific sentiment classifiers. Experiments showed that these newly aggregated clues were more effective than commonly used features in sentiment analysis. 
411813	41181338	Dynamic Partitioned Sampling For Tracking With Discriminative Features	The article presents a new method for tracking objects using particle filters. This method, called Dynamic Partitioned Sampling (DPS), divides the state space into partitions based on different cues and samples from them in a hierarchical manner. Unlike previous approaches, the order of partitions changes dynamically based on the reliability of each cue. The reliability is measured by the cue's ability to distinguish the object from the background, which is represented by a set of informative "background particles". The effectiveness of this method is demonstrated in head tracking experiments using three different cues. Results show the robustness of the algorithm in various challenging video sequences. 
411814	411814142	ESearch: Incorporating Text Corpus and Structured Knowledge for Open Domain Entity Search.	The paper presents ESearch, an open domain entity search system that uses a combination of a Wikipedia text corpus and DBPedia knowledge base to find relevant entities to a natural language query. The system ranks entities based on context matching and category matching, with a re-ranking component that takes into account user or blind feedback. The paper highlights the importance of category matching for search performance and addresses the challenge of users not being familiar with entity types in the knowledge base. ESearch has an effective ranking model for entity types, allowing users to perform entity search without explicitly specifying query entity types.
411815	41181567	Evaluating the Influence of User Searches on Neighbors	The rise of Big Data has rendered traditional data management techniques insufficient in many real-life situations. The abundance of user data, particularly in the form of suggestions and searches, has created the need for advanced analysis methods to effectively utilize this information. Additionally, the heterogeneity and fast pace of this data require new data storage and management tools to be developed. This paper presents a solution for analyzing the impact of user searches and suggestions on their social network, with the goal of identifying influential users. This information is valuable in various scenarios such as viral marketing, tourism promotion, and food education. Understanding user preferences is crucial in these contexts.
411816	41181673	A general language for evolution and reactivity in the semantic web	This paper presents a UML model for a language that can handle evolution and reactivity in the Semantic Web. The language is based on Event-Condition-Action rules and allows for the combination of different languages for events, conditions, and actions. This accommodates language heterogeneity, which is important for dealing with the complexity of evolution and reactivity in the Semantic Web. The model specifies an ontology for the language, providing a foundation for its use and development. 
411817	411817112	Behaviour-Aware Discovery Of Web Service Compositions	Service-oriented computing faces the challenge of efficiently discovering and combining (Web) services to create complex applications. To address this issue, a matchmaking system has been developed that utilizes both semantics and behavioral data to identify suitable service compositions that can fulfill a client's request. This system aims to improve the discovery process and make it more efficient by taking into account the specific requirements and capabilities of individual services. By leveraging both semantic and behavioral information, this system enables the creation of more effective and tailored service compositions to meet the needs of clients.
411818	4118188	On Semantic Update Operators for Answer-Set Programs	Logic programs under the stable models semantics, also known as answer-set programs, are a popular and well-understood knowledge representation framework. However, managing changes to these rule bases is still a challenge. The traditional framework for belief change, AGM, does not work well with nonmonotonic semantics like stable models. Various approaches have been proposed to address this issue, but they mostly focus on syntactic rule rejection. Recently, AGM revision has been applied to a more expressive semantic characterization of logic programs based on SE models. In this paper, the authors use results from belief update to tackle the problem of updating logic programs. They prove a representation theorem and define a specific update operator, which they compare to existing syntactic operators. Surprisingly, they uncover a major drawback in many semantic operators.
411819	411819162	Exact formulations and algorithm for the train timetabling problem with dynamic demand	This paper examines the optimization of train timetabling in a dynamic demand environment, specifically for rapid train services in major cities. Three formulations are presented, with the goal of minimizing passenger average waiting time. While a binary variable model is most intuitive, it results in a non-linear objective function. To address this, flow variables are introduced to allow for a linear representation. Incremental improvements are made to these formulations and a branch-and-cut algorithm is developed. Computational experiments using real data from the Madrid Metropolitan Railway demonstrate the benefits of adapting timetables to demand patterns. Comparison of the linear formulations also reveals their differences in size, quality of solution, and running time.
411820	41182043	Verifying Programs with Unreliable Channels	This article discusses the verification of a specific type of infinite-state systems, composed of finite-state processes communicating through unlimited lossy FIFO channels. This class can model link protocols such as Alternating Bit Protocol and HDLC. The authors present algorithms for solving various verification problems for this class, including reachability, safety properties, and eventuality properties. These algorithms have been successfully applied to verify idealized sliding-window protocols. This is significant because these problems are typically undecidable for systems with perfect FIFO channels. 
411821	41182134	Dual Laplacian Editing for Meshes	Recent developments in mesh editing have utilized differential information as a local intrinsic feature descriptor. By minimizing changes in this information, a deformed mesh can be reconstructed based on user input constraints. However, since this information is in a global coordinate system, it must be transformed to fit the orientations of details in the deformed surface, or distortion will occur. To address this, a new method has been proposed that encodes both local parameterization and geometry details in the dual mesh domain. This approach allows for simpler neighborhood structures and better preservation of information. With this framework, both primal vertex positions and dual Laplacian coordinates are iteratively updated, resulting in visually pleasing deformations with minimal user input.
411822	41182219	Generating, Maintaining, and Exploiting Diversity in a Memetic Algorithm for Protein Structure Prediction.	Computational methods for predicting the three-dimensional structure of proteins have been limited in their ability to accurately predict the structures of larger proteins. This is due to several factors, such as a lack of exploration of diverse protein folds and an energy function that can be deceiving. To address these issues, a multistage memetic algorithm was developed, incorporating a successful local search method called Rosetta. By incorporating specialized genetic operators and a stochastic ranking procedure for selection, this algorithm was able to increase the diversity of protein folds and traverse deep energy wells that were previously considered deceptive. These improvements have the potential to significantly enhance the performance of protein structure prediction algorithms in blind experiments and advance the field in predicting the structures of larger proteins.
411823	4118235	Manifold Regularized Multitask Learning for Semi-Supervised Multilabel Image Classification	Classifying images with multiple labels using a small number of labeled samples is a significant challenge. One approach is to use manifold regularization, which explores the geometric structure of the data distribution, to improve classification performance. However, this method is not effective when dealing with high-dimensional visual features. To address this issue, a new algorithm called manifold regularized multitask learning (MRMTL) is proposed. MRMTL learns a shared discriminative subspace for multiple classification tasks, effectively controlling model complexity. The algorithm also ensures smoothness along the data manifold through manifold regularization. Experiments on two datasets show that MRMTL outperforms other image classification algorithms.
411824	41182444	Ranking Model Adaptation for Domain-Specific Search	The emergence of vertical search domains has made it difficult to apply a broad-based ranking model to different domains. Building a unique ranking model for each domain is time-consuming and labor-intensive. To address this issue, a new algorithm called ranking adaptation SVM (RA-SVM) is proposed. This algorithm adapts an existing ranking model to a new domain, reducing the need for labeled data and training time while maintaining performance. The algorithm only requires predictions from existing models, and assumes that documents with similar features should have consistent rankings. Constraints are added to control the margin and slack variables of RA-SVM, and a ranking adaptability measurement is introduced to quantify the adaptability of an existing ranking model. Experiments show the effectiveness of this approach on two large datasets. 
411825	41182514	Principles of Constructing a Performance Evaluation Protocol for Graphics Recognition Algorithms	Graphics recognition is a process that identifies and groups graphic objects, such as text and lines, in digital images. However, there is a lack of objective evaluation protocols and methods for comparing the performance of these recognition algorithms. To address this, a methodology for performance evaluation has been proposed, which includes a matching degree and comprehensive performance metrics. While some research has been done on evaluating specific types of graphic objects, there is a need for a more general methodology that can be applied to different graphics recognition algorithms. This methodology aims to improve the understanding and comparison of graphics recognition algorithms and inform the design of new systems. 
411826	4118262	Goal Satisfaction in Large Scale Agent-Systems: A Transportation Example	This study showcases the effectiveness of using a simple physics-based approach to solve a complex transportation problem on a large scale. The approach involves modeling a cooperative Multi-Agent System (MAS) using physical principles, where the agents inherit physical properties and behave similarly to physical systems. The researchers provide a detailed algorithm for a single agent and use simulations to demonstrate its successful use in allocating and executing tasks in a dynamic MAS with thousands of agents and tasks. This approach proves to be efficient and effective in managing such a complex system.
411827	41182740	Topographical maps of orientation specificity	The article presents a new framework for understanding how orientation is encoded in the primate striate visual cortex. This framework is based on the representation of straight lines and can explain the organization of orientation specificity in the cortex, as well as other observed phenomena such as the lack of orientation specificity in singularities, increased neural activity in these areas, and the distribution of singularities along ocular dominance columns. The framework can also be applied to other cortical regions with topographical maps of sensory spaces. A neural model of the proposed framework is presented, along with simulation results.
411828	41182818	Exact Max-SAT solvers for over-constrained problems	This article introduces a new method for solving over-constrained problems using Max-SAT. A formalism called soft CNF formulas is defined, allowing for the declaration of blocks of clauses as either hard or soft. Two Max-SAT solvers are presented, utilizing branch and bound algorithms, lazy data structures, and variable selection heuristics. An experimental study on various problem instances shows that this approach is competitive with existing methods from the CSP and SAT communities.
411829	41182912	Auction Equilibrium Strategies for Task Allocation in Uncertain Environments	This paper presents a model of self-interested information agents competing for tasks in an uncertain environment. These agents have different capabilities and are motivated to deviate from cooperative strategies in order to increase their own benefits. The paper focuses on a protocol where a central manager conducts a reverse auction among the agents to allocate tasks. The concept of a stable solution, where no agent can benefit from changing its strategy, is introduced. The paper suggests an efficient algorithm for calculating equilibrium strategies and compares this protocol to a central allocation mechanism. The effect of environmental settings on the equilibrium is also explored through sample environments.
411830	41183054	The metabolic algorithm for P systems: Principles and applications	Metabolic P systems, or MP systems, are a type of P systems used to model biological metabolism. Their evolution is governed by a metabolic algorithm, which replaces the traditional approach of using ordinary differential equations with chemical principles. The basic principles of MP systems are outlined and demonstrated through examples of biological modeling. A novel regulation mechanism is also discussed, which could potentially improve the construction of computational models from experimental data of specific metabolic processes. Overall, MP systems offer a unique and potentially more efficient way to express and understand biological metabolism. 
411831	41183114	Enterprise Modelling for Value Based Service Analysis	Service oriented architectures (SOAs) are essential for facilitating communication and collaboration between organizations and individuals. However, managing and designing services poses challenges due to their abstract nature. In this paper, the authors analyze the concept of services and propose a conceptual model based on the REA ontology. This model connects services to resources and demonstrates how services can be represented using encapsulation. An example of using this model is provided in an application for oil marketing, showcasing how it can aid in the representation and design of services. The proposed model offers a framework for understanding and managing services in a more structured and effective manner.
411832	411832101	Economic Viability of Paris Metro Pricing for Digital Services	Digital services, such as cloud computing and network access, are becoming increasingly popular, allowing for dynamic resource allocation and virtual resource isolation. This has led to the development of flexible pricing schemes, such as Paris Metro Pricing (PMP), which involves allocating multiple isolated service classes with different prices. PMP is advantageous due to its simplicity and applicability to a wide range of digital services. However, the viability of PMP in terms of profit for the service provider and social welfare has been a central issue, with previous studies providing conflicting conclusions. In this article, the authors identify general principles and conditions that ensure the success of PMP, and apply them to various digital service examples.
411833	41183353	The Impact of Network Structure on the Stability of Greedy Protocols	This article discusses the stability of packet-switching networks, focusing on how network structure affects this property. The authors use Adversarial Queueing Theory, which assumes that packets are adversarially injected into the network. They explore the impact of various structural parameters, such as size, diameter, maximum vertex degree, and minimum number of edge-disjoint paths, on the stability and instability bounds of different greedy protocols. They show that increasing the size of a network can lead to a drop in its instability bound, and that maintaining a smaller network size can already result in a low instability bound. They also demonstrate how network subgraphs can affect stability and contribute to a better understanding of the relationship between network structure and stability properties.
411834	41183473	Congestion games with player-specific constants	This article discusses a specific type of congestion game where players have individual delay functions and constants for each resource. The delay functions and constants are combined using a group operation, resulting in a player-specific latency function. The group used is a totally ordered abelian group. This class of games is a subset of a new intuitive class called dominance weighted congestion games. Results for games on parallel links, network congestion games, and arbitrary congestion games are presented, including the existence of a weighted potential for games with linear delay functions and player-specific additive constants.
411835	411835100	Porting social media contributions with SIOC	Social media sites have become extremely popular, with millions of users and billions of dollars invested in them. To make it easier for users to access multiple sites, there is a need for portability of personal profiles, friend networks, and content objects. The Semantic Web provides the necessary representation mechanisms for this portability by linking people and objects through agreed-upon formats like FOAF and SIOC. These formats help describe people, content objects, and their connections in a way that allows for interoperability between social media sites. This paper discusses how the SIOC project can address the issue of portability between sites by using Semantic Web technology. It also explores how SIOC data can be used to represent and transfer user contributions on various social media sites.
411836	411836105	A decentralised public key infrastructure for customer-to-customer e-commerce.	The success of eBay has highlighted the demand for customer-to-customer (C2C) e-commerce. However, eBay's centralised infrastructure has scalability issues that can lead to network bandwidth, server load, and availability problems. This paper suggests that C2C e-commerce is a natural fit for peer-to-peer (P2P) systems, which can expand beyond file sharing into new application areas. The ultimate goal is to create a decentralised P2P system that functions similarly to eBay without relying on its centralised infrastructure. Due to the importance of security in e-commerce, the paper proposes a decentralised P2P public key infrastructure (PKI) as a first step and presents an analytical model to quantify its effectiveness and resilience against potential threats and attacks.
411837	41183768	Maintaining Consistency Of Ontologies Registry With Description Logic	The article discusses the use of ontology registry in Semantic Web Services (SWS) to achieve semantic interoperability. Ontology models in the registry must meet certain representation and consistency requirements for use by both software agents and humans. The article proposes an approach for maintaining consistency in ontology models using Description Logic representation. This is done through an evolution algorithm that transforms and combines ontology models to improve the ontology registry's metamodel framework. The effectiveness of this approach is demonstrated through testing with the knowledge representation system PowerLoom. The study of ontology registry consistency has both theoretical and practical importance in achieving semantic interoperability in SWS.
411838	41183889	Towards Optimal and Scalable Non-functional Service Matchmaking Techniques	Service-orientation lays the foundation for the Internet of Services (IoS), which will offer millions of services for creating innovative applications. Therefore, the non-functional aspect of services must be taken into account when selecting from the vast number of functionally-equivalent options for a specific user task. Current approaches to non-functional service discovery use constraint solving techniques to optimize the matchmaking time between a service offer and demand. However, this method becomes less efficient as the number of service offers increases, making it unsuitable for the IoS. To address this issue, two alternative techniques have been proposed and evaluated. These techniques were found to improve matchmaking time without compromising accuracy, with the second technique proving to be highly scalable.
411839	41183921	Visual Modeling Of Defeasible Logic Rules With Dr-Vismo	Standardization of the Semantic Web has focused on ontologies and ontology languages, but reasoning over available information is crucial for its success. Defeasible reasoning, which handles incomplete and conflicting data, is one approach, but it can be confusing for users. To address this issue, a new representation schema based on directed graphs has been proposed. This paper introduces DR-VisMo, a system that implements this schema for editing and visualizing defeasible logic rule bases. It also includes a stratification algorithm for organizing the elements in the graph. DR-VisMo is part of VDR-DEVICE, a system for modeling and deploying defeasible logic rule bases on RDF ontologies.
411840	4118406	Taming Displayed Tense Logics Using Nested Sequents with Deep Inference	This article discusses two sequent calculi for tense logic, where the syntactic judgements are nested sequents. The first calculus, SKt, is a variant of Kashima's calculus and uses "shallow" inference, where rules are applied only to the top-level nodes. It includes "display postulates" to bring a node to the top level and allow for inference at any point. The second calculus, DKt, is more natural and uses deep-inference, allowing for inference rules at any node without structural rules. The two calculi are shown to be equivalent, and this equivalence also holds for extensions of tense logic. The deep-sequent system has easier proof search compared to the shallow system and a procedure is outlined for its use. 
411841	4118417	Faster and more robust point symmetry-based K-means algorithm	This paper introduces a new point symmetry distance (PSD) measure called symmetry similarity level (SSL) operator for K-means algorithm. The authors' modified point symmetry-based K-means (MPSK) algorithm is more robust than the previous PSK algorithm. The MPSK algorithm is suitable for both intra-clusters and inter-clusters that exhibit symmetry. Additionally, the authors present two speedup strategies to reduce the execution time of the MPSK algorithm. Experimental results show that the MPSK algorithm outperforms the previous PSK algorithm in terms of execution time and its ability to handle symmetrical inter-clusters.
411842	41184242	More efficient periodic traversal in anonymous undirected graphs	The article discusses the problem of periodic graph exploration, where a mobile agent with limited memory needs to visit all nodes of a connected graph in a periodic manner. The graphs are anonymous, meaning the nodes are unlabeled, but the agent can distinguish between edges through unique port numbers. Previous research has shown that arbitrary port number assignments make the problem unsolvable, but recent algorithms have achieved short traversal periods by carefully assigning port numbers. The article presents new algorithms that improve upon previous bounds, with a period length of no more than (4+13)n-4 for oblivious agents and 3.5n-2 for agents with constant memory. The article also introduces a new graph decomposition technique and presents a lower bound of 2.8n-2 for the oblivious case. 
411843	41184368	Sparse polynomial interpolation and Berlekamp/Massey algorithms that correct outlier errors in input values	This article proposes algorithms for sparse interpolation with errors, using a combination of Prony's--Ben-Or's & Tiwari's algorithm and a Berlekamp/Massey algorithm with early termination. The first algorithm can recover a t-sparse polynomial from a sequence of values, even if some values are incorrect due to random or misleading errors. It requires bounds on the number of terms and errors, and interpolates values at specific field elements. The article also addresses the problem of recovering a minimal linear generator from a sequence of linearly generated values with errors. The proposed Majority Rule Berlekamp/Massey algorithm can recover the generator and correct the errors. These algorithms have potential applications in sparse interpolation with numerical noise and outlier errors.
411844	41184442	Uncovering the unarchived web	Web archiving is an important practice for preserving cultural heritage, and is typically done by either harvesting a national domain or crawling a predetermined list of websites. However, this often results in more information being collected than just the intended websites, which can provide valuable insights into the past. To capture this additional information, a method called "web collection aura" has been developed, which uses crawl dates, anchor text, and link structure to reconstruct pages that were not included in the archived collection but are known to have existed. This method has been shown to uncover a substantial number of additional pages, highlighting the importance of considering the "aura" in web archiving.
411845	411845114	Providing consistent and exhaustive relevance assessments for XML retrieval evaluation	Retrieval approaches are methods used to find relevant information within a set of documents. In order to accurately compare these approaches, test collections are necessary. These collections contain documents, queries, and relevance assessments. It is important for these assessments to be consistent and comprehensive in order to properly compare retrieval methods. While the evaluation methodology for traditional text retrieval is well-established, evaluating XML retrieval is still a research topic. This is because XML documents are made up of nested components that cannot be considered independently in terms of relevance. The INEX initiative has developed a methodology to ensure consistent and thorough relevance assessments for XML retrieval.
411846	41184629	Probabilistic construction and manipulation of free Boolean diagrams	The content is not available in abstract form.
411847	41184714	Optimising long-latency-load-aware fetch policies for SMT processors	Simultaneous multithreading (SMT) processors are designed to increase instruction level parallelism by fetching instructions from multiple threads. The fetch engine plays a crucial role in determining which threads have priority and access to shared resources. However, when a thread experiences a cache miss, it can monopolize critical resources and slow down the execution of other threads. To address this problem, several approaches have been proposed. This paper evaluates and compares the three best published policies for handling long latency loads. Additionally, the paper proposes improved versions of these policies, which have been shown to significantly enhance both throughput and fairness in SMT processors. 
411848	41184820	Pedestrian detection via classification on Riemannian manifolds.	This paper introduces a new algorithm for detecting pedestrians in still images. It utilizes covariance matrices as object descriptors, which cannot be represented as a vector space. Therefore, traditional machine learning techniques are not effective for learning classifiers. To address this, the paper proposes a novel approach for classifying points on a connected Riemannian manifold, which is used to represent the space of d-dimensional nonsingular covariance matrices. The algorithm is tested on two popular pedestrian datasets and shows better detection rates compared to previous approaches. 
411849	4118494	A Group Theoretical Toolbox For Color Image Operators	This paper discusses the use of the direct product of two mathematical groups, the dihedral group D(4) and the symmetric group S(3), to automatically derive low-level image processing filters for RGB images. These filters lead to a block-diagonalization of the correlation matrix for certain stochastic processes. The group theoretical derivation allows for a fast implementation using mainly additions and subtractions, making them suitable for applications such as video processing and content-based image database search. The authors demonstrate the effectiveness of these filters in an experiment using a large image database. They also discuss the generalizability of this approach for other types of image data, such as multiband images and images with non-rectangular sampling grids. 
411850	4118505	Enhancing IPTV personalization	This paper presents an improved IPTV personalization system that utilizes viewer attributes instead of solely relying on set-top box identifiers. It eliminates the need for user logins, which may not accurately identify all viewers present. The system includes a lighting normalization module to improve attribute recognition in situations where lighting may vary, such as when watching TV at home. The proposed architecture aims to enhance user experience and provide more accurate personalization for multiple viewers.
411851	41185155	Adaptive Multiple Resources Consumption Control for an Autonomous Rover	In the context of autonomous rovers, controlling resources consumption is crucial. This consumption is often unpredictable and the rover must adapt during execution to prioritize important tasks and avoid failure. The Progressive processing model allows for tasks to be performed in multiple ways, giving the agent the ability to control resource consumption during the mission. This is achieved through an off-line solved Markov decision process. Initially, Progressive processing could only control time as a resource, but an extension has been developed for multiple resources. The main contribution is a compact and efficient state space representation for multiple consumable resources.
411852	41185246	Homomorphic Computation of Edit Distance.	Genomic sequence analysis is a crucial tool for understanding an organism's biology, but it can also pose a risk to privacy due to the sensitive information contained in the sequences. To address this issue, a method has been developed to perform the edit distance algorithm on encrypted data, allowing for sequence analysis to be done in a public setting without revealing the raw data. This approach involves the data owner providing only the encrypted sequence, which can be decrypted by designated individuals with the decryption key. The efficiency of this method has been analyzed and shown to be feasible, with the potential for further optimizations. A proof of concept using short DNA sequences has also been presented.
411853	4118534	Model-based analysis of configuration vulnerabilities	Vulnerability analysis is the process of identifying weaknesses in computer systems that could be exploited to compromise their security. This paper presents a new approach to vulnerability analysis using model checking. This involves formal specification of security properties, creating an abstract model of the system, and using a verification procedure to check if the model satisfies the properties. If not, it can identify exploit scenarios. This approach is beneficial as it can detect known and unknown vulnerabilities. The paper demonstrates this approach on a simplified UNIX system and shows that it is feasible despite the complexity of the system. The authors believe that with continued advancements in model checking techniques, automated vulnerability analysis of realistic systems will be possible in the future.
411854	41185457	Extensions of the Queueing Relations L = λW and H = λG	This paper discusses the extension of fundamental queueing relations, L = λW and H = λG, which connect customer averages (waiting time and cost) to time averages (queue length and cost) for an arrival process with rate λ. The relations can be established using a two-dimensional cumulative input process and can include continuous versions and higher dimensions. The paper also establishes inequalities when conditions for equality are not met and introduces central limit theorem versions of H = λG, expanding on previous results.
411855	41185566	Special Issue: Learning and creativity Part 1	The special issues of AIEDAM (Artificial Intelligence in Engineering Design, Analysis and Manufacturing) in Issue 3 and 4 were the result of a workshop on Learning and Creativity at the 2002 AID conference on Artificial Intelligence in Design. This workshop was the sixth in a series, with the previous five focusing on Machine Learning in Design and being held at AIDs '92, '94, '96, '98, and '00. The first three workshops also resulted in special issues of AIEDAM. The workshop aimed to explore the intersection of learning and creativity in design and its implications for artificial intelligence.
411856	41185668	Improved Distributed Algorithms for SCC Decomposition	The OBF technique, developed by Barnat and Moravec, is used in distributed algorithms to break down a partitioned graph into its strongly connected components. In this study, a recursive version of OBF is introduced and different implementations of it are experimentally evaluated, varying in their level of parallelism. Synthetic graphs with a few large components and graphs with many small components were used for the evaluation, as well as real model checking applications. The results were compared to other successful SCC decomposition techniques, such as those developed by Orzan and Fleischer, Hendrickson, and Pinar. 
411857	41185761	The Degree Of Squares Is An Atom	The article discusses the theory of degrees of infinite sequences and their transducibility by finite-state transducers. Previous studies have left many basic questions unanswered, including the existence of atom degrees. An atom degree is one that is only below the bottom degree consisting of ultimately periodic sequences. The article focuses on the 'squares sequence' and shows that its degree is an atom. The main tool used to prove this is the characterization of transducts of 'spiralling' sequences and their degrees. It is also shown that every transduct of a 'polynomial sequence' can either be transduced back to a polynomial sequence or is in the bottom degree.
411858	41185848	Amigo: proximity-based authentication of mobile devices	Amigo is a method for authenticating devices that are physically close to each other, without needing any prior knowledge or additional hardware. It uses the shared radio environment to verify the devices' proximity and is effective against various types of attacks. Its main benefits include not requiring extra hardware or user involvement, and being resistant to eavesdropping.
411859	41185967	Managing very large distributed data sets on a data grid	This work focuses on managing large data sets that need to be stored and processed across multiple computing sites. The motivation for this research comes from the ATLAS experiment for the Large Hadron Collider (LHC), where the authors have been involved in developing the data management middleware called DQ2. This middleware has been successfully used for several years to transport petabytes of data to research centers and universities around the world. The authors share their experience in deploying DQ2 on the Worldwide LHC computing Grid, a production Grid infrastructure with hundreds of computing sites. Through this experience, they have identified uncertainty in the behavior of large Grid infrastructures and have developed novel modelling and simulation techniques for Data Grids. They also discuss the practical limits of data distribution algorithms for Data Grids and suggest future research directions.
411860	411860121	Mobile Image Search via Local Crowd: A User Study	This paper examines the effectiveness of a mobile social search application in Japan that utilizes crowd sourcing to assist foreign visitors with image-based questions. A controlled field experiment was conducted over 6 weeks with 55 participants, and it was found that the application had a reliable response speed and quantity. On average, half of the requests were answered within 10 minutes, and 75% were answered within 30 minutes, with an average of 4.2 answers per request. The top active crowd workers were primarily motivated by intrinsic factors rather than extrinsic incentives. The application had the most success in the afternoon, evening, and night, with an average response time of 10 minutes and over 4 answers per request.
411861	41186131	Action Recognition from One Example	The authors propose a new method for recognizing actions in videos using space-time locally adaptive regression kernels and the matrix cosine similarity measure. This method only requires a single example of an action as a query and does not rely on prior knowledge or complex motion estimation. The proposed method computes space-time descriptors from the query video and compares them to features from the target video using a matrix generalization of the cosine similarity measure. The resulting scalar resemblance volume indicates the likelihood of similarity between the two videos. The method is shown to have high performance on challenging datasets and outperforms other methods in action categorization. 
411862	41186252	A web-based novel term similarity framework for ontology learning	The article discusses the importance of pairwise similarity computations in ontology learning and data mining. The authors propose a similarity framework that uses a conventional Web search engine to obtain the most up-to-date knowledge on terms. This is particularly useful for dynamic ontology management as ontologies must evolve over time. The method is also less sensitive to data sparseness compared to other approaches that use a fixed amount of web documents. The proposed methodology utilizes two measures for similarity computation and can be used for modifying existing ontologies. Experimental results show that the method can extract topical relations between terms that are not present in traditional concept-based ontologies.
411863	41186355	Multi-modal pharmacokinetic modelling for DCE-MRI: using diffusion weighted imaging to constrain the local arterial input function	In oncology, magnetic resonance imaging (MRI) is routinely used to gather data on tissue structure and function. It is now possible to combine different models, such as diffusion weighted imaging and dynamic contrast enhanced (DCE) MRI, to gain a more comprehensive understanding of tissue characteristics. In this study, the researchers use data from 8 subjects, including 4 with head and neck tumors, to investigate the potential benefits of incorporating parameters from the intra-voxel incoherent motion (IVIM) model in the DCE modeling process. While the results were not significant in this study, the researchers believe this technique could be useful in other applications.
411864	41186471	An evolutionary algorithm to model structural excursions of a protein.	Proteins need to be able to move between different structures in order to perform their biological functions effectively. This movement is determined by the energy landscape, which categorizes available structures based on their energies. Computational research aims to understand and simulate these structural changes, in addition to traditional laboratory studies. One popular method is to use robot motion planning or construct structured representations of the energy landscape. However, a new approach using evolutionary computation has been proposed. This method uses an evolutionary algorithm to evolve paths of structural excursions without prior knowledge of the energy landscape. Initial results on different protein variants show potential for this method and warrant further exploration.
411865	41186516	Partial and Constrained Level Planarity.	A level planar drawing of a directed graph G is where each vertex is mapped to a unique point on a horizontal line based on its assigned level, and edges are drawn without crossing in a y-monotone curve. In the problem CLP, we are given a partial ordering of vertices on each level and seek a level planar drawing where the order follows this ordering. A simpler version, PLP, involves extending a given level-planar drawing of a subgraph to a complete drawing without changing it. A polynomial-time algorithm with running time O(n5) has been developed for CLP, using a modified data structure that can handle the constraints efficiently. However, PLP remains NP-complete, even in restricted cases.
411866	41186698	Online Dynamic Power Management with Hard Real-Time Guarantees.	This paper discusses the problem of online dynamic power management in multi-processor systems. The goal is to minimize energy consumption while meeting all job deadlines. The paper examines the complexity of the problem and presents efficient online strategies for real-time systems. It is shown that even when the set of jobs can be feasibly scheduled on a single processor, the competitive ratio of any online algorithm is at least 2.06. A 4-competitive algorithm is also presented for using at most two processors. The paper also considers multiple streams of jobs and presents a trade-off between energy-efficiency and the number of processors used. Finally, it is shown that even for arbitrary input sets of jobs, the competitive ratio is at least 2.28, with a 1-competitive algorithm for jobs with unit execution times.
411867	41186762	Information processing in complex networks: Graph entropy and information functionals	This paper presents a new framework for defining the entropy of a graph. The definition is based on a local information graph and information functionals that measure the topological structure of the graph. These functionals are used to calculate a probability distribution, which is then used to determine the entropy of the graph. The paper also explores the relationship between different graph entropies and provides numerical results to demonstrate the usefulness and practicality of the method. The proposed method has polynomial time complexity and can be applied to understanding information processing in complex networks. 
411868	41186845	Assessing the health information needs of the emergency preparedness and management community	 A group of 34 professionals working in health-related emergency response management shared their sources of information, unmet information needs, and desired information systems tools. They heavily rely on the internet, but struggle with information overload and difficulty finding specific information among the vast amount available. They believe a system with social tagging and recommender features would be beneficial for accessing relevant documents in the "gray literature." This highlights the importance of such services for professional communities in general.
411869	41186963	D-AHP method with different credibility of information.	Multi-criteria decision making (MCDM) is a widely used method in various practical applications. The D-AHP method, which extends the traditional AHP method by incorporating D numbers preference relation, was previously proposed for MCDM problems. This method provides a solution for obtaining weights and rankings of alternatives based on decision data. However, the credibility of information used in the D-AHP method has not been thoroughly studied, leading to unsolved issues. This paper investigates the impact of information credibility on the results of MCDM problems using the D-AHP method. The study shows that while the ranking of alternatives is slightly affected by information credibility, the weights assigned to alternatives are significantly influenced. 
411870	4118706	Unifying the proper cores and dominance cores of cooperative fuzzy games	This paper aims to examine the equalities between various types of proper cores and dominance cores for a general worth function, without assuming it to be nonnegative. It presents different sufficient conditions that ensure the equalities between these cores, thus providing a comprehensive understanding of the relationships between them in this particular scenario. By investigating these equalities, the paper contributes to the understanding of core solutions for worth functions that may not satisfy the nonnegative property.
411871	41187167	Triangle factors of graphs without large independent sets and of weighted graphs.	The classical Corradi-Hajnal theorem states that a graph with at least 2/3 of its vertices having a degree of at least n will contain a triangle factor if the number of vertices is divisible by 3. In this paper, the authors present two related results that use the absorbing technique of Rodl, Rucinski, and Szemeredi. The main result determines the minimum degree condition needed to guarantee a triangle factor in graphs with a sublinear independence number. They also propose several open problems related to this result. Additionally, a fractional variant of the Corradi-Hajnal theorem is considered and a conjecture by Balogh, Kemkes, Lee, and Young is settled. Specifically, if the sum of weights on edges incident to a vertex is at least (1+2t/3 + o(1))n for every vertex, there will be n/3 vertex disjoint heavy triangles in the graph. 
411872	41187242	Continuous matrix approximation on distributed data	This paper addresses the challenge of tracking and approximating data matrices in a streaming fashion, particularly in the case of multiple distributed sites receiving a stream of data. The goal is to track an ε-approximation to the norm of the matrix along any direction. The paper presents algorithms that maintain a smaller matrix B as an approximation to the distributed streaming matrix A, with the property that for any unit vector x, the difference between the norms of Ax and Bx is within the limit of ε||A||2F. These algorithms work in a streaming fashion and have low communication requirements, making them suitable for distributed computation. The best method presented in the paper is deterministic and uses only O((m/ε) log(βN)) communication, where N is the size of the stream and β is an upper bound on the squared norm of any row of the matrix. Real-world experiments with large datasets demonstrate the effectiveness of these algorithms.
411873	411873110	A Fast Reduced Kernel Extreme Learning Machine.	This paper introduces a new algorithm, RKELM, which is a fast and accurate kernel-based supervised method. Unlike traditional approaches, which use iterative methods to identify support or weight vectors, RKELM randomly selects a subset of data samples as support vectors. This leads to significant cost savings in the training process, particularly for large datasets. The algorithm is based on rigorous proof of universal learning using a reduced kernel-based SLFN. The authors demonstrate through experiments on various real-world applications that RKELM performs competitively with traditional methods, such as SVM and LS-SVM, while requiring much less computational effort. 
411874	41187496	DynamicMR: A Dynamic Slot Allocation Optimization Framework for MapReduce Clusters	MapReduce is a popular computing method used for large-scale data processing in cloud computing. However, the traditional slot-based MapReduce system, such as Hadoop MRv1, can have performance issues due to its inefficient allocation of resources. To address this, a paper proposes a new technique called Dynamic Hadoop SlotAllocation, which allows for reallocation of slots between map and reduce tasks. Additionally, the paper suggests Speculative Execution Performance Balancing to balance performance and efficiency, and Slot PreScheduling to improve data locality without affecting fairness. Combining these techniques forms a new slot allocation system called DynamicMR, which has been shown to improve performance by up to 46-115% for single jobs and 49-112% for multiple jobs. Experimental results also show that DynamicMR outperforms YARN by 2-9% for multiple jobs.
411875	41187556	Boosting Face Retrieval by using Relevant Set Correlation Clustering	The article presents a method for improving the performance of face retrieval in news videos by using a relevant-set correlation (RSC) clustering model. This involves retrieving faces of a specific person by searching for their name in associated transcripts, organizing the faces into clusters using RSC, and only presenting representative faces to the user. This approach results in a significant increase in retrieval performance, as only a small number of relevant faces are returned. The RSC model has two main contributions: it can automatically determine the number of clusters and discard irrelevant faces, and high precision clusters can be used for feature selection through linear discriminant analysis (LDA). Experiments on a dataset showed that this method outperforms other existing methods for face retrieval.
411876	41187645	Unsupervised change analysis using supervised learning	The authors propose a new problem called change analysis and a method for solving it. Unlike existing methods, which only detect changes, the goal of change analysis is to find the explanation for the changes. The proposed approach uses supervised learning, where a classifier is trained using a hypothetical label to interpret the changes. This approach has shown promising results in both change analysis and concept drift analysis when tested on real data. 
411877	4118770	Characterizing causal action theories and their implementations in answer set programming.	In this article, the authors introduce a simple language for writing causal action theories and outline certain properties that the state transition models of these theories should possess. They then explore how these theories can be embedded into other action formalisms and implemented using logic programs with answer set semantics. The authors propose the concept of "permissible translations" from the causal action theories to logic programs, and identify two sets of properties that these translations must satisfy. They prove that these sets of conditions are minimal and that removing any condition would result in multiple possible mappings. The authors also show that these two sets of conditions correspond to two well-known action languages, B and C, and provide a new way of comparing and evaluating different action languages. They suggest that this approach could also be used to define new action formalisms based on different sets of properties. 
411878	41187834	Interactive cross and multimodal biomedical image retrieval based on automatic region-of-interest (ROI) identification and classification.	Biomedical articles often use annotation markers, such as arrows, letters, or symbols, to highlight important regions within figures and illustrations. These annotations are then correlated with concepts in the caption text or figure citations, creating a bridge between visual characteristics and their semantic interpretation. This allows for the extraction of relevant regions and aids in semantic search without knowing specific keywords or visual patterns. To improve biomedical image retrieval, a method using a combination of rule-based and statistical techniques localizes and recognizes annotations, which are then annotated for classification using biomedical concepts. This allows for both perceptual and conceptual search, validated through experiments on a dataset of thoracic CT scans.
411879	41187947	BitDew: A programmable environment for large-scale data management and distribution	Desktop Grids are used to efficiently run resource-demanding distributed applications by utilizing idle desktop PC's over multiple networks. However, data management in such large-scale, dynamic, and distributed Grids has been overlooked, resulting in ad-hoc solutions. The BitDew framework addresses this problem by providing a programmable environment for automatic and transparent data management on computational Desktop Grids. BitDew uses meta-data to drive key data management operations and its runtime environment integrates P2P components for data catalog and distribution. Through examples, the paper explains how BitDew can be utilized by application programmers and users. The performance evaluation shows that BitDew offers scalability, performance, and fault tolerance with minimal programming effort.
411880	41188077	Fault Tolerance for PetaScale Systems: Current Knowledge, Challenges and Opportunities	The emergence of PetaScale systems has sparked renewed interest in effectively managing failures and ensuring the successful completion of large applications. This talk will present existing results for key mechanisms of fault tolerance in HPC platforms, which are largely derived from distributed system theory and have received significant attention in the past decade. However, these approaches may not be suitable for the evolving challenges of large scale systems. Therefore, there is a need for new approaches, such as incorporating dedicated fault tolerance hardware or relaxing constraints from traditional distributed system theory. The talk will discuss these opportunities and their limitations.
411881	41188133	An Evaluation of Dynamic Mobility Anchoring	The article discusses the limitations of current centralized mobility support approaches in IP based mobility schemes and cellular networks. To address these issues, the authors propose a new distributed and dynamic approach called Dynamic Mobility Anchoring (DMA) for managing mobility at the access node level. The effectiveness of DMA is evaluated through simulations and compared to traditional centralized schemes such as Mobile IP. The results show that DMA has promising benefits in terms of handover and traffic delays for real-time traffic delivery. Future work will focus on adapting DMA to different networking environments.
411882	41188215	Fast nonlinear autocorrelation algorithm for source separation	ICA and BSS have been commonly used for pattern recognition, but they rely on the statistical properties of the original sources or components. In this paper, a new algorithm is proposed that maximizes the nonlinear autocorrelation of source signals, resulting in a faster and more efficient fixed-point algorithm for BSS. The convergence property of this algorithm is studied and it is shown to have a quadratic convergence speed. Simulations using artificial signals and real-world applications demonstrate the effectiveness of this approach. Overall, the proposed method offers an efficient solution for BSS in pattern recognition problems.
411883	41188346	Requirements and compliance in legal systems: a logic approach	The article discusses the similarities between the concepts of requirements and implementation in normative systems, such as law, and those in software engineering. It also explores the similarities between the concepts of compliance and conformance in these two areas. The use of a logic analyzer, specifically Alloy, is proposed as a tool for verifying legal compliance by checking consistency between legal and enterprise requirements. This approach is demonstrated through examples from privacy law and financial reporting law. Overall, the article highlights the potential for applying techniques from software engineering to improve the understanding and implementation of legal requirements.
411884	41188423	On the value of coordination in network design	The article discusses network design games where self-interested agents must purchase links to form a network. Shapley cost sharing mechanisms are used to fairly split the cost among agents. The price of anarchy, which measures the worst-case performance in terms of cost, is high in these games. Research has shifted to evaluating the price of stability, which measures the cost of the best Nash equilibrium compared to the optimal solution. The concept of strong Nash equilibria, introduced by Aumann, is studied and shown to improve the quality of solutions through coordination among agents. Bounds for unweighted and weighted games in both directed and undirected graphs are developed, with tight or nearly tight results for many scenarios. The first super-constant lower bound for the price of stability is also proven for standard equilibria without coordination. This shows that coordinated equilibria have similar performance to the best equilibria without coordination in most settings.
411885	41188572	Delay estimation of a user-preferred content distribution scheme in disruption tolerant networks	This article discusses disruption tolerant networks (DTN) that involve mobile nodes being carried by humans. These nodes subscribe to a content distribution service and share their content with others who have similar interests. The probability of a node subscribing to a content and the distribution of inter-contact times are known. The article examines both closed-form expressions and asymptotic approximations to determine the expected message delay and number of message copies at the time of delivery. These results are validated through simulations using various mobility models.
411886	41188625	Lightness, heaviness and gravity	The gravity g(H,H) of a graph H is the largest number of vertices that can be found in every subgraph of a larger graph G, where each subgraph is isomorphic to H and has at least a given degree m. This concept is studied in relation to different families of plane graphs, and new types of graphs called almost-light and absolutely heavy are introduced and examined. The paper ends with some unresolved questions for further research.
411887	41188730	Poster session: ASETS: A self-managing transaction scheduler	The success of Web-database applications relies heavily on user satisfaction, which can be measured through expected response time or delay. Due to the unpredictable nature of web user behavior, user requests are modeled as transactions with soft-deadlines. In such scenarios, the average tardiness is a more effective metric than the hit ratio. To address this, a new adaptive self-managing algorithm called ASETS is proposed and evaluated. ASETS prioritizes resources based on varying workloads to maintain user satisfaction. Results show that ASETS outperforms other optimal algorithms for under and over utilization system conditions. 
411888	41188870	Active Database Rules with Transaction-Conscious Stable-Model Semantics	Active databases face a major challenge in defining the semantics of their rules due to the lack of a formal framework and the use of ad-hoc operational semantics in different systems. This makes it difficult to predict the behavior of rule sets and ensure their termination, hindering the application of this technology in critical areas. This paper presents a durable change semantics for active database rules, combining elements from Starburst, Postgres, and Heraclitus to create a formal logic-based model that can be efficiently implemented and solves the non-termination problem. This approach aims to improve the applicability of active databases in important application areas.
411889	41188949	Quantifying the Impact of Input Data Sets on Program Behavior and its Applications	Workload composition is crucial in the design of a microprocessor. Selecting appropriate benchmarks and input data sets is a challenge due to limitations on simulation time and instruction counts. To efficiently explore the workload space, statistical data analysis techniques like principal components analysis (PCA) and cluster analysis are used. This allows for the identification of different input data sets for a given benchmark, measuring the distance between program-input pairs to understand their behavioral differences, and selecting representative input data sets. This approach is validated by showing that program-input pairs with a close distance exhibit similar behavior. The ultimate goal is to select a limited set of representative benchmark-input pairs that cover the entire workload space. This methodology can also be used to analyze the impact of input data sets on program behavior and evaluate the representativeness of sampled traces.
411890	41189070	Compilation and virtualization in the HiPEAC vision	This paper presents the HiPEAC vision of embedded virtualization, which has been developed over two years of discussions among HiPEAC cluster members. The vision integrates system and process virtualization into a unified layer for embedded systems, providing solutions for consolidation, performance optimization, software engineering, and legacy hardware components. Four adoption requirements are identified: real-time execution support, low performance overhead, virtualization of accelerator cores, and trustworthiness. Four research challenges are also defined: full virtualization of heterogeneous multi-core platforms, portable performance for heterogeneous multi-cores, virtual machine management interfaces, and standards for embedded virtualization. Overall, this vision aims to improve the efficiency and flexibility of virtualization in embedded systems.
411891	41189138	LMIRA: Large Margin Instance Reduction Algorithm.	Instance-based learning involves using a training set to classify new instances. However, not all information in the training set is useful and irrelevant instances can be discarded through instance reduction. This reduces the time for classification and training. The proposed Large Margin Instance Reduction Algorithm (LMIRA) removes non-border instances and keeps border ones. The instance reduction process is formulated as a constrained binary optimization problem and solved using a filled function algorithm. Storing too many instances can be problematic and LMIRA aims to select the most representative instances while maintaining high accuracy and reduction rates. The performance of LMIRA has been evaluated on real world data sets and compared to other methods, showing its superiority in terms of classification accuracy and reduction percentage.
411892	41189210	TapList: testing the effectiveness of tap access to the n-best list in pen-based text entry	Handwriting recognition is prone to errors, so managing these errors is important for user interfaces. In an experiment, a technique called TapList was tested, where a stylus tap can switch a recognized character to the next item on a list provided by a character recognizer. The goal was to see if this technique affects text entry performance under time constraints. The results showed that TapList did not significantly decrease performance compared to traditional error correction methods. This suggests that TapList could improve performance if the cost of re-entering misrecognized text is higher than with the fast unistroke alphabet used in the experiment. More research is needed to compare TapList to other techniques. 
411893	4118935	Truthful Online Auction For Cloud Instance Subletting	This paper discusses the issue of instance underutilization in the Infrastructure as a Service (IaaS) market, where users are constantly scaling up their cloud instances to meet growing demands. The limited billing options offered by major cloud providers and the constantly changing demands have led to instances being underutilized in both time and space. To address this issue, the paper proposes an instance subletting service where users can lease their underutilized instances to others for a specific period of time. The paper highlights the challenges of implementing such a service and presents an online auction mechanism that ensures truthful and rational decision-making while maximizing cost and social welfare. Simulations show that this mechanism can significantly improve performance in terms of cost and social welfare.
411894	41189417	An analysis of reduced error pruning	Top-down decision tree learning can be hindered by the pruning phase, resulting in larger trees without improved accuracy. Reduced Error Pruning is a commonly used algorithm to address this issue, but its effectiveness in different settings has not been fully analyzed. This paper examines the properties of Reduced Error Pruning in different scenarios, including situations where the pruning examples are independent of the decision tree. The analysis shows that the probability of pruning a node due to pure noise decreases exponentially as the tree grows. The algorithm is also analyzed under the assumption of uniform distribution of examples, providing insights into the number of subtrees that are pruned. This paper offers a better understanding of Reduced Error Pruning and includes previously overlooked empty subtrees in the analysis.
411895	41189538	On the well-behavedness of important attribute evaluation functions	 Well-behaved evaluation functions simplify and improve the handling of numerical attributes by focusing on boundary points when searching for the optimal partition. This applies to both binary and multisplits, as long as the function is cumulative and well-behaved. This class includes convex evaluation functions, making it a widely applicable superclass. However, some commonly used functions, such as C4.5's default gain ratio, have limitations when it comes to numerical attributes. While not convex, gain ratio is still well-behaved for binary partitioning but struggles with higher arity partitioning. Through experiments, a simple cumulative rectification method is shown to outperform gain ratio in handling numerical attributes.
411896	41189658	Anomalous loss performance for mixed real-time and TCP traffic in routers with very small buffers	Recent discussions have focused on the appropriate size of buffers at core Internet routers. Studies have shown that smaller buffer sizes can still achieve acceptable TCP throughputs, which is a step towards implementing optical packet switched networks. However, these studies have not taken into account the increasing use of real-time traffic, which is a significant source of revenue for Internet service providers. In this paper, the authors investigate the interaction between real-time and TCP traffic in small buffers and discover that in a certain range, larger buffer sizes actually result in increased losses for real-time traffic. They demonstrate this through simulations and models, and also highlight factors that affect this anomaly. This is the first study to consider the effects of real-time traffic in small buffers, providing important insights for router manufacturers and network operators.
411897	41189762	Unifying Byzantine Consensus Algorithms with Weak Interactive Consistency	The paper discusses the problem of achieving consensus in a partially synchronous system with Byzantine processes. There are two types of Byzantine faults - authenticated (where messages can be signed) and non-authenticated (where the sender's identity is known). The paper introduces a new abstraction called weak interactive consistency (WIC) that can be implemented with or without signatures. This abstraction is applied to two well-known Byzantine consensus algorithms, Castro-Liskov PBFT (without signatures) and Martin-Alvisi FaB Paxos (with signatures), showcasing the power of WIC in providing a concise representation of these algorithms.
411898	411898302	A Decentralized and Cooperative Workflow Scheduling Algorithm	In current workflow scheduling methods, there is no collaboration among distributed brokers, causing conflicts in schedules. To address this issue, a decentralized and cooperative algorithm is proposed in this paper. It uses a Peer-to-Peer (P2P) coordination space to coordinate schedules among brokers in the Grid. This approach is completely decentralized, with no central point of contact. Key functionalities such as resource discovery and scheduling coordination are delegated to the P2P coordination space. This approach aims to improve performance, scalability, and autonomy for users. Extensive simulations show the feasibility of this approach.
411899	41189925	Solving a real-world train-unit assignment problem	The problem at hand is assigning train units to timetabled train trips in a regional area while fulfilling the required number of passenger seats. This is made difficult by the large number of distinct train-unit types and the high number of trips that need to be covered. The existing solution used by practitioners can be improved upon, but even finding a solution of the same value is challenging. To address this, the authors propose a heuristic approach based on an ILP formulation with strong seat requirement constraints. Real-world instances show the effectiveness of this approach.
411900	411900299	Lamarckian clonal selection algorithm with application	The paper introduces a new algorithm called Lamarckian Clonal Selection Algorithm (LCSA), which integrates Lamarckism and Immune Clonal Selection Theory. This algorithm incorporates the idea of Lamarckian evolution, which describes how organisms can evolve through learning, into the Standard Clonal Selection Algorithm (SCSA). Ten benchmark functions are used to test the performance of LCSA, and the impact of parameters is carefully studied. The results show that LCSA outperforms SCSA and other evolutionary algorithms in terms of robustness and convergence. This suggests that the incorporation of Lamarckian principles into the immune-inspired algorithm can improve its performance.
411901	41190112	Online Transfer Of Human Motion To Humanoids	Transferring motion from a human to a humanoid robot is crucial for creating robots that can easily be programmed and mimic human actions. This is known as the motion retargeting problem, which has been extensively studied and has existing solutions that use optimization methods and motion capture data. However, there is a growing interest in online motion transfer without using markers, which presents challenges for existing methods. To address this, a new approach using task space control theory is proposed, taking into account joint limits, velocity, and self-collision constraints. This method uses low dimensional human motion descriptors obtained through a vision-based key-point detection and tracking algorithm, without the need for markers or special equipment. The effectiveness of this approach is demonstrated through experiments on the Honda humanoid robot ASIMO.
411902	4119022	Incorporating Weather Updates for Public Transportation Users of Recommendation Systems	This work introduces a system that enhances the capabilities of Yelp-like recommendation sites by allowing users to search for places based on travel-time via public transportation. It also takes into account weather conditions and adjusts recommendations accordingly. The use of public transport may require walking to and from specific locations, and the preferences for certain types of services may change based on weather. The system uses a model to predict a user's preferred mode of transport and incorporates weather data into the recommendation process. It also utilizes event-based modeling and existing tools like Google Maps Direction API and Open Weather Map API. A web application has been developed for both desktop and mobile platforms.
411903	41190331	Motion Trends Detection in Wireless Sensor Networks	The problem of detecting motion trends in Wireless Sensor Networks (WSN) is addressed, where tracking is done collaboratively among sensor nodes. In addition to determining single locations, applications may also need to detect properties along entire trajectories. However, sending location and time data to a central sink for property detection creates high communication overhead. To address this, an in-network distributed algorithm is proposed for efficiently detecting the Continuously Moving Towards predicate for a given destination. This destination can be a point or a region with a polygonal boundary. Experiments show that this approach is more efficient compared to the traditional brute-force method.
411904	41190429	Fast two-party secure computation with minimal assumptions	This content discusses the limitations of existing protocols for secure two-party computation and proposes a new protocol that overcomes these limitations. The proposed protocol is secure against malicious adversaries, requires minimal hardness assumption (only oblivious transfers), has minimal overhead complexity, and can be parallelized for efficient processing. To achieve these properties, the protocol uses novel solutions to address obstacles such as establishing authenticity of the generator's output, ensuring input consistency, and handling selective failure attacks. The implementation of this protocol also shows significant improvement over previous work that relied on specific number-theoretic assumptions, demonstrating that strong performance does not necessarily require specific algebraic assumptions. 
411905	41190522	An Experimental Study of Phase Transitions in Matching	The difficulty of finding models for a predicate logic formula is a well-known problem, with complexity increasing exponentially as the number of variables increases. However, this complexity can vary greatly across different instances of the problem, as seen in recent research. Many hard problems have a specific point, known as a phase transition, where the complexity suddenly becomes much higher. In this study, the focus is on the problem of matching a Horn clause to a universe, in search of a model or proof of its non-existence. The emergence of a phase transition in this problem is found to be influenced by two factors: the structure of the clause and the structure of the universe. These findings have important implications for understanding and solving hard problems in predicate logic.
411906	41190633	Phase Transitions in Relational Learning	Relational learning faces a major limitation in verifying hypotheses on examples due to the complexity of the task. Recent research has shown that many difficult problems have a "phase transition," where there is a sudden increase in computational complexity with respect to some order parameter. This paper examines this issue by investigating artificial and real-world learning problems. The authors find that learners tend to generate hypotheses that lie exactly on the phase transition, but not all problems within this region are intractable. To address this problem, a Monte Carlo algorithm is proposed to estimate the likelihood of a matching problem exceeding a certain amount of computational resources. The implications of these findings for relational learning are also discussed.
411907	41190719	Analysing interactive voice services	IVR services are becoming more common in automated telephone systems and VoiceXML is a popular language for creating these services. However, VoiceXML descriptions are complex and difficult to understand, making it challenging to check for errors. CRESS is a graphical notation that allows for a more abstract and language-independent way of describing IVR services. These descriptions can then be translated into LOTOS for automated analysis. While it is not practical to formally verify these specifications due to the infinite state space, CRESS and MUSTARD (a language for defining tests) can help identify potential issues and interactions among IVR features. By using these tools, confidence in IVR applications can be increased.
411908	41190826	Multifaceted Exploitation of Metadata for Attribute Match Discovery in Information Integration	Automating the process of matching attributes for information integration is a difficult task, especially given the constantly changing nature of the Web. To address this challenge, a framework for using multiple facets of metadata is proposed. By gathering information from different aspects of metadata and using machine learning, confidence values are assigned to potential attribute matches. Experiments have shown promising results, indicating that when the different facets align, the matches are highly reliable. This framework offers a solution for the complex task of attribute matching in a dynamic web environment.
411909	41190949	Bundling three convex polygons to minimize area or perimeter	The problem of finding optimal translations for a set of k convex polygons in the plane is considered. The translations must result in disjoint copies of the polygons and minimize the area or perimeter of the convex hull of their union. For k=2, this problem can be solved in linear time, but for larger k, it has been proven to be NP-hard. However, for k=3, a decomposition of the translation space into O(n2) cells has been discovered, allowing for an O(n2)-time algorithm to find the optimal translations and minimize the area or perimeter of the convex hull. This is the first algorithm of its kind for k=3. 
411910	41191046	Face Recognition For Homeland Security: A Computational Intelligence Approach	Morphological Shared-Weight Neural Networks (MSNN) can be used to improve common access restriction points by identifying specific individuals of interest. These networks are trained for face recognition and have the ability to learn and encode the necessary feature extraction and classification abilities for identifying a specific face. MSNN has been proven effective in analyzing images with the target face in a group of faces, even when the target face is at different angles, lighting, or partially covered. Recent experiments have demonstrated the potential use of MSNN for watch-list scanning in access screening areas, allowing for quick and accurate identification of individuals passing through.
411911	41191147	Hand Posture Classification and Recognition using the Modified Census Transform	Developing new techniques for human-computer interaction is a difficult task, and vision-based techniques have become increasingly popular due to their non-intrusive nature. Hands, being a natural device, have great potential for creating more intuitive interfaces. However, in order to use hands for interaction, they must be accurately recognized in images. In this paper, the authors propose using a successful approach from face detection for hand posture classification and recognition. The approach uses illumination-invariant features based on Modified Census Transform, and a simple linear classifier is trained using a set of feature lookup-tables. The experiments are conducted on a benchmark database for posture recognition and show promising results. Two protocols are defined for the experiments, and the authors provide results for both classification and recognition tasks. 
411912	41191228	Recognition of Genuine Smiles	This paper discusses the importance of automatic distinction between genuine and posed expressions for visual analysis of social signals. The authors propose a system that uses facial landmarking and tracking to extract features describing the dynamics of eyelid, cheek, and lip corner movements. These features are then fused to create a highly accurate smile classifier. The system is evaluated on a large database and compared to previous methods, showing significant improvement in smile classification. The authors also investigate the effects of age and gender on the system's performance and demonstrate the usefulness of their system in smile psychophysics. 
411913	4119136	Model-driven sketch reconstruction with structure-oriented retrieval.	Our proposed system allows users to convert a 2D sketch into a 3D sketch by utilizing existing shape models. The system utilizes a sketch-based shape retrieval method and a novel matching algorithm that takes into account both shape features and structure. After selecting a model from the retrieved options, users can use it as a 3D proxy to provide abstract 3D information. The sketch is then transformed into 3D geometry through back-projection and optimized using the Laplacian mesh deformation framework. Preliminary evaluations have shown that our system outperforms a state-of-the-art method and allows users to create impressive 3D forms from simple sketches, without requiring advanced drawing skills. 
411914	4119147	Computer vision for music identification	This paper discusses the use of computer vision techniques to address tasks in the audio domain, specifically the problem of music identification. The approach involves treating the spectrogram of a music clip as a 2D image and using pairwise boosting on Viola-Jones features to learn compact and discriminative local descriptors. During the query phase, a set of song snippets that match the noisy sample are retrieved and geometric verification is used to identify the most consistent song. The system has been implemented and tested, showing significant improvements in identifying music from short and distorted audio samples.
411915	41191563	Score-informed syllable segmentation for a cappella singing voice with convolutional neural networks.	The 18th International Society for Music Information Retrieval Conference, known as ISMIR 2017, was held in Suzhou, China from October 23-27, 2017. This conference focused on the intersection of music and technology, bringing together experts and researchers from around the world to discuss the latest advancements and innovations in the field. Presentations and discussions covered topics such as music analysis, music recommendation systems, and music perception. The conference provided a platform for collaboration and exchange of ideas between professionals in the music and technology industries.
411916	41191630	Vision-based finger detection, tracking, and event identification techniques for multi-touch sensing and display systems.	This study introduces a cost-effective and efficient approach for finger detection, tracking, and event recognition in multi-touch applications. It utilizes a fast bright-blob segmentation technique and connected-component analysis to extract touch blobs from infrared lights captured by a video camera. A blob tracking and event recognition process is then applied to determine touch events and actions performed by users. This system is robust and adaptable to different lighting conditions and infrared noise. The blob tracking and event recognition process includes two phases: blob tracking and touch event identification. This system has been tested and proven successful in various environments and conditions. Overall, the study shows that this vision-based approach is effective for multi-touch sensing applications.
411917	41191775	Training a reciprocal-sigmoid classifier by feature scaling-space	The paper introduces a new classifier, the reciprocal-sigmoid model, for pattern classification. It combines the theoretical advantages of linear machines and logistic regression, while addressing the issues of local minima and recursive search. To prevent overfitting, the classifier is trained using multiple samples of uniformly scaled pattern features. Empirical evaluation on synthetic data and 10-fold cross validations on 40 datasets show comparable performance to top classifiers in the literature, thanks to its use of reciprocal sigmoid for nonlinearity and bundled feature sets for smoothing training error. This makes the reciprocal-sigmoid model an effective and efficient classifier for pattern classification.
411918	41191839	A privacy-preserving smart parking system using an IoT elliptic curve based security platform.	Wireless Sensor Networks (WSNs) have evolved significantly since the initial concept of SmartDust was introduced 15 years ago, to the point where the Internet of Things (IoT) is now a reality. There are now various hardware and software options available for easy setup and use, and real-world applications utilizing large deployments of IoT devices are increasing. However, the wireless nature of communication and low-end capabilities of the devices pose security and privacy concerns that have not been adequately addressed. This paper proposes using Elliptic Curve Cryptography (ECC) as a more efficient and secure alternative to traditional public key cryptography, especially for constrained devices. A generic implementation of ECC is presented, which can run on various operating systems and devices. The paper also presents a solution for protecting user privacy in the Smart Parking application domain, using zero knowledge proofs (ZKP) in conjunction with ECC. The performance of the system is evaluated in a real-world IoT testbed, and the code is available as open source for developers looking to enhance security and privacy in their applications.
411919	41191965	Power to the peers: authority of source effects for a voice-based agricultural information service in rural India	Online communities provide a platform for people to easily connect and share knowledge regardless of their geographical location. With the use of mobile phones, billions of new users in emerging countries are now able to participate in these online communities. However, in India where social hierarchy is important, users may give more value to information from institutionally-recognized authorities rather than peer-sourced content. To test this hypothesis, a controlled experiment was conducted on a voice-based agricultural information service for farmers in Gujarat, India. The results showed that farmers were more likely to follow up on tips recorded by peers rather than scientists. However, in interviews, most farmers stated that they preferred information from scientists, which could be due to social pressure. This highlights the demand for peer-based information services in rural communities in India and suggests design implications for such services.
411920	41192021	A Completion-Based Method for Mixed Universal and Rigid E-Unification	Our method addresses a new version of E-unification called "mixed" E-unification, which combines universal and rigid E-unification. Rigid E-unification is useful for handling equality in first-order calculi. By using mixed E-unification instead of just rigid, we can improve the performance of provers using E-unification. We provide soundness and completeness results and share results from experiments with our method's implementation.
411921	41192144	Pattern-based retrieval in a fuzzy object oriented data base	This paper introduces a fuzzy object database, called FOOD, which can handle both crisp and fuzzy information. The database uses a graph-based data model and provides pattern-based operations for selecting and browsing data. This allows for direct manipulation of both the database schema and instances. The defined operations make it easy to navigate through the database and retrieve desired information. The FOOD model is an efficient and effective way to manage fuzzy data and provides a user-friendly interface for interacting with the database.
411922	41192259	Evaluation of Task Assignment Policies for Supercomputing Servers: The Case for Load Unbalancing and Fairness	The MPP architecture is still the most commonly used in supercomputer centers, but a cheaper and simpler alternative has emerged. This alternative involves a collection of multiprocessors or a distributed server system, where jobs are sent to one of the machines for processing. The main challenge in this setup is determining a good rule for assigning jobs to host machines. Many policies have been proposed but have not been systematically evaluated under supercomputing workloads. In this paper, the performance of existing policies is compared using simulations and analytical proofs. A new policy is proposed that intentionally unbalances load among hosts but is fair and outperforms the other policies.
411923	41192315	3d Morphable Face Models Revisited	This paper discusses the creation of a high resolution 3D morphable model of face shape variation using statistical tools such as thin-plate splines and Procrustes analysis. The new model is more efficient and accurate than previous ones, and has a reformulated probabilistic prior that can be used as a regularisation constraint in fitting the model to data. The authors also demonstrate the model's application in estimating 3D face shape from a sparse set of 2D feature points, with an average error of less than 3.6mm. Experimental results using ground truth data are presented, providing absolute reconstruction errors.
411924	41192430	CENTAUR: realizing the full potential of centralized wlans through a hybrid data path	In recent years, enterprise WLANs have shifted towards centralized architectures due to the benefits of easier management and improved control and security functions. However, the data path of WLANs still uses a distributed, random-access model as defined by the 802.11 standard. While a centrally scheduled data path has been shown to be more efficient in theory, its complexity has hindered practical consideration. This paper presents a new perspective on data path choices in enterprise WLANs and conducts extensive measurements to evaluate the impact of design choices. The results show that centralization can be beneficial in mitigating hidden terminals and exploiting exposed terminals. To address these issues, the authors propose a hybrid data path, called CENTAUR, that combines the simplicity of DCF with limited centralized scheduling. This solution does not require client cooperation and can support legacy 802.11 clients.
411925	41192527	Engineering Wireless Mesh Networks: Joint Scheduling, Routing, Power Control, and Rate Adaptation	Researchers have identified key engineering insights for optimizing the configuration of medium- to large-size wireless mesh networks (WMNs) to maximize the minimum throughput of all flows. They have developed efficient computational tools using column generation with greedy pricing, allowing for exact solutions to be computed for larger networks than previously possible. Additionally, fast approximations have been created to find nearly optimal solutions for even larger networks. These insights also apply to the case of proportional fairness, showing their applicability in different scenarios. Overall, this research provides valuable information for improving the design of WMNs and maximizing their performance.
411926	4119260	A proposal for news recommendation based on clustering techniques	The article discusses the use of clustering techniques in recommendation systems for electronic journals, specifically in a journalistic context where users have access to categorized news. The paper aims to present an approach for recommending news to readers based on their profile and previously accessed news. The Aspect Model and K-Means clustering algorithm are used and compared in this context.
411927	41192783	A Bayesian Mixed Effects Model Of Literary Character	This study focuses on automatically determining the hidden character types in a large collection of English novels published between 1700 and 1899. Previous research assumed that character types were solely responsible for generating text, but this study introduces a model that takes into account other factors, such as the author. The results of this model were compared to the judgments of a literary scholar and were found to be more accurate than alternative models.
411928	4119288	BML and Related Tools	The Bytecode Modeling Language (BML) is a language used for specifying properties in Java bytecode. It allows for a high level of abstraction without restricting the format of the bytecode. BML specifications can be stored in class files, making it useful for proof-carrying code frameworks. BML is also similar to the source code level language JML, allowing for easy translation of specifications and proofs. This paper explains the BML language and its binary representation, as well as the available tools for working with BML. These tools include BMLLib, Umbra, JML2BML, BML2BPL, and CCT, which allow for inspecting, editing, and generating proofs for BML specifications. 
411929	41192963	Inapproximability Results for Guarding Polygons and Terrains	Past research on art gallery problems has focused on finding the minimum number of guards needed in various situations. However, there is limited research on the complexity of these problems. This paper examines the approximability of three types of art gallery problems - VERTEX GUARD, EDGE GUARD, and POINT GUARD - motivated by a practical problem. The authors prove that for polygons without holes, these problems cannot be approximated by a polynomial time algorithm with a ratio of 1 + δ. For polygons with holes, the problems cannot be approximated by a polynomial time algorithm with a ratio of ((1 − ε)/12) ln n, unless NP ⊆ TIME(n O(log log n)). The inapproximability of the POINT GUARD problem for polygons with holes also applies to the problem of covering a 2.5-dimensional terrain with guards at a fixed height above the terrain or on the terrain itself.
411930	411930106	Inapproximability Results for Guarding Polygons and Terrains	Past research on art gallery problems has focused primarily on determining the minimum number of guards needed for worst-case scenarios in different settings. However, there is limited research on the complexity of these problems. This paper aims to fill this gap by studying the approximability of three types of art gallery problems: VERTEX GUARD, EDGE GUARD, and POINT GUARD. The authors show that for input polygons without holes, no polynomial time algorithm can achieve an approximation ratio of 1 + δ for these problems. They also prove that for polygons with holes, these problems cannot be approximated with a ratio of ((1-ε)/12) ln n, unless NP is a subset of TIME(n O(log log n)). These results are obtained through gap-preserving reductions from 5-OCCURRENCE-MAX-3-SAT and SET COVER problems. Furthermore, the inapproximability of the POINT GUARD problem for polygons with holes extends to the problem of covering a 2.5-dimensional terrain with a minimum number of guards, even when the guards can be placed on the terrain itself.
411931	41193186	Towards an integration of Golog and planning	Golog, an action language, has been successfully used for controlling robots. Its main advantage is the flexibility in writing programs that can constrain the search for an executable plan. However, when it comes to general planning, Golog falls short compared to modern planners. To address this issue, this paper proposes an integration of Golog and planning, where planning problems are solved by a modern planner during program execution. The focus is on the ADL subset of the plan language PDDL, which is shown to be correctly embedded within Golog. A comparison of the performance of Golog with an integrated ADL planner is also presented.
411932	41193246	A Formal Framework for On-line Software Version Change	Traditional methods of updating software involve shutting down the current version and installing the new one, causing a delay in service for users. However, an on-line software replacement system allows for updates to be made while the program is running, avoiding the need for a shutdown. While there have been various examples of this system in practice, there has been little research on its theoretical aspects. This paper presents a formal framework for studying on-line software version change, including a definition of validity and conditions for ensuring it in a procedural language. It is found that determining validity is generally impossible, but certain conditions can guarantee it.
411933	4119330	Topological map: an efficient tool to compute incrementally topological features on 3d images	This paper discusses the use of a three dimensional topological map to efficiently compute topological features on objects in a 3D image. These features have practical applications in image processing and computer vision. The topological map is a combinatorial model that represents both topological and geometrical information of a labeled 3D image. It can be constructed incrementally using only two operations, and in this study, the authors demonstrate how the Euler characteristic can also be computed incrementally during this process. This opens up possibilities for efficiently computing other features using this model.
411934	41193437	Implementation of Elliptic Curve Cryptosystems on a reconfigurable computer	In recent years, there has been a focus on developing reconfigurable computers that combine traditional microprocessors with Field Programmable Gate Arrays (FPGAs). These machines have shown promising speed improvements for computationally intensive tasks like codebreaking. However, effectively using and programming these machines remains a challenge. In this study, the researchers demonstrate an efficient implementation of an Elliptic Curve scalar multiplication over GF(2m) using SRC-6E, a leading reconfigurable computer. They discuss how the hardware architecture and programming model influenced their algorithm partitioning strategy. The paper also includes a detailed analysis of control, data transfer, and reconfiguration overheads, as well as a performance comparison with a microprocessor implementation. Overall, this study highlights the potential and challenges of using reconfigurable computers for complex computations.
411935	41193543	An n! lower bound on formula size	We have developed a new game, based on the Ehrenfeucht--Fraïssé method, for proving lower bounds on the size of first-order formulas. This game has been used to show that the CTL+ formula Occurn ≡ E[Fp1 ∧ Fp2 ∧ … ∧ Fpn], which states that all n predicates must occur on a path, requires a formula size of n! in CTL. This lower bound is optimal, meaning that no smaller formula can express this property. This result shows that the succinctness of CTL+ compared to CTL is exactly Θ(n)!. We have also used our game to prove an optimal Ω(n) lower bound on the number of boolean variables needed for RLf to polynomially embed CTL+. The number of booleans needed for RL and FO2(TC) is still unknown.
411936	4119368	Framework for Analyzing Garbage Collection	The development of garbage collection algorithms has advanced significantly, but the analysis of these algorithms is still in its early stages. Current analyses only measure the costs of individual executions, and a theoretical foundation is needed to make conclusive statements about the efficiency of garbage collectors across entire programs. This theoretical framework also allows for abstract examination and new designs without concern for implementation details. The proposed framework can accurately compute the time cost of garbage collection over the entire program execution, and has resulted in a new and significantly faster trace generation algorithm called Merlin. The paper also presents a novel result, proving the optimality of Merlin's asymptotic running time for trace generation.
411937	41193760	Scalable logo recognition based on compact sparse dictionary for mobile devices	This paper introduces a new logo recognition system that can be used on mobile devices. The system is unsupervised and does not require any supervised training, making it time efficient and low in memory usage. It is also robust against challenging conditions such as noise, different image scales, and rotation. The system uses a segmental quantization approach to generate a compact vocabulary of over one million visual words. This vocabulary is then used to efficiently retrieve logo instances in the dataset through a K-nearest neighbors (K-NN) algorithm. The system's vocabulary is significantly smaller than traditional methods, making it suitable for mobile devices. Additionally, a verification method is proposed to filter out false positives. Experiments on a dataset with 400 logo classes demonstrate the system's efficiency and effectiveness.
411938	41193851	Causal Behavioural Profiles - Efficient Computation, Applications, and Evaluation	Analysis of behavioural consistency is crucial in software engineering, particularly in process and service management. This involves verifying that behavioural models are consistent with each other, such as a business process model and a workflow model. Existing notions of behaviour equivalence, such as bisimulation and trace equivalence, can be computationally intensive and only provide a Boolean result. To address this, the article proposes the use of causal behavioural profiles, which capture important information about the behaviour of a process model. These profiles are weaker than trace equivalence but can be efficiently computed for a wide range of models. The article also introduces techniques for computing these profiles using structural decomposition techniques and presents findings from applying the technique to three industry model collections.
411939	41193926	Arity bounds in first-order incremental evaluation and definition of polynomial time database queries	The paper discusses a first-order incremental evaluation system (foies) for database queries that uses a first-order query on the new and old databases, as well as stored auxiliary relations, to derive a new query answer after inserting or deleting a tuple. The paper measures the space needed for foies in terms of the maximal arity of the auxiliary relations and presents results on the existence and nonexistence of space-restricted foies for graph queries. The paper also constructs space efficient foies for these queries and shows that the arity bounds are tight using Ehrenfeucht–Fraı̈ssé games. The paper also considers a variation of foies where the cost of storing the answer is also taken into account and shows that the arity hierarchy remains strict.
411940	411940149	A workflow net similarity measure based on transition adjacency relations	In business process management, determining the similarity or distance between two processes is essential for activities like process retrieval, process mining, and process integration. While various approaches have been proposed for measuring this similarity, there is no widely accepted definition or method. This paper defines similarity and distance based on firing sequences in workflow nets (WF-nets) and introduces transition adjacency relations (TARs) as a practical way to compare processes. It is shown that this distance measure is a metric and can be efficiently calculated using model reduction techniques. Experiments on artificial and real-life processes confirm the effectiveness of this approach.
411941	41194151	Monitoring Influenza Trends through Mining Social Media	This paper analyzes the correlation between Google search queries for influenza-like-illness (ILI) and Center for Disease Control and Prevention (CDC) seasonal ILI reporting data. It also evaluates trends in blog posts discussing influenza and identifies influential bloggers and communities that could play a role in disseminating information during a potential outbreak or epidemic. The study suggests that using web and social media (WSM) can aid in detecting increases in ILI and proposes a response strategy that utilizes WSM to reduce the impact of an infectious disease outbreak. The paper concludes with recommendations for expanding ILI-trend identification and creating an integrated Public Health and WSM community intervention campaign. 
411942	41194225	Two-way coupling of rigid and deformable bodies	The proposed framework allows for the full two-way coupling of both rigid and deformable bodies through a unified time integration scheme and individual two-way coupled algorithms. This eliminates the need for ad hoc methods to address stability issues and allows for the treatment of key aspects of both types of bodies, such as contact, collision, stacking, and friction for rigid bodies, and arbitrary constitutive models, thin shells, and self-collisions for deformable bodies. Additionally, the framework supports advanced features such as proportional derivative controlled articulation between rigid bodies, which can lead to the simulation of new phenomena and the design of deformable creatures that interact realistically with their environment.
411943	41194362	Robust Gaze Estimation Via Normalized Iris Center-Eye Corner Vector	Gaze estimation is important in various practical scenarios, but accurately estimating gaze with just a single web-cam is still difficult. This paper presents a method called the normalized iris center-eye corner (NIC-EC) vector, which uses facial features and pupil centers to improve the accuracy of single web-cam based gaze estimation. An interpolation method is used to map the NIC-EC vector to points of regard. Experimental results showed that this method has significantly improved accuracy compared to previous methods, with an average accuracy of 1.66 degrees even with slight head movements.
411944	41194419	An algebraic semantics of event-based architectures	This article proposes a mathematical framework for event-based architectures, with the goal of characterizing the modularization properties and supporting the categorical approach to architectural modeling. The use of this formalization allows for the integration of both synchronous and asynchronous interactions in the same modeling approach. The proposed framework is based on transition systems with events and utilizes a family of logics to support different levels of abstraction. This is seen as a first step towards engineering architectural styles.
411945	41194512	Tool-supported compression of UML class diagrams	Tool-supported compression techniques have been developed for UML class diagrams. These techniques involve hiding less important parts of the diagram, which can be revealed again at selected points. This allows users to gradually refine the diagram from an abstract view to more detailed ones, making it easier to manage large diagrams and understand the overall architecture of an object-oriented legacy system. The compressed form of the diagram can be created automatically or with human control. A management algorithm and prototype implementation are also presented.
411946	41194665	Maximizing Barrier Coverage Lifetime with Mobile Sensors	Sensor networks play a crucial role in detection and tracking tasks, and one of their main purposes is to provide coverage. The goal of this study is to maximize the coverage lifetime of a barrier using mobile sensors with limited battery power. Coverage lifetime is measured as the time until a sensor dies, leading to a breakdown in coverage. Energy is consumed both for mobility and coverage, with the latter being dependent on the radius of the sensor. The study focuses on two variants: fixed radii and variable radii, and proposes parametric search algorithms for both cases. The study also discusses the challenges and limitations of the variable radii problem, showing its complexity and providing approximation results for the fixed radii problem in certain scenarios.
411947	41194745	Mobile multi-layered IPsec	To improve the efficiency of wireless networks, smart forwarding and processing of packets in access routers is crucial. However, when end-to-end encryption is used, access routers cannot provide these services as the necessary information is encrypted. The Multi-layered IPsec (ML-IPsec) protocol was developed to expose certain portions of the packet to intermediate network elements, allowing them to enhance performance. In this paper, ML-IPsec is extended to support mobility in wireless networks. A key distribution protocol and two mobility protocols compatible with Mobile IP are defined and implemented. Measurements show that integrated Mobile IP/ML-IPsec handoffs result in a pause of 53-100 milliseconds, with only 28-75 milliseconds due to ML-IPsec. The implementation of ML-IPsec, when combined with SNOOP, significantly increases throughput compared to standard TCP over IPsec. A dynamic version of ML-IPsec is also proposed, allowing for a balance between performance and security. 
411948	41194858	Exploring the structure of rule based systems	The article discusses the importance of understanding the internal structure of rule-based expert systems in order to effectively measure and assess their performance. While previous attempts have been made to formalize this structure through the concept of a rule base execution path, these definitions have been found to be inadequate. The paper presents a new formal definition for rule base execution paths, which has been integrated into a tool called Path Hunter. This tool was used to analyze a rule base of 442 CLIPS rules, and successfully controlled the issue of combinatorial explosion that can occur during path enumeration. The analysis revealed important considerations for the development of rule-based systems.
411949	41194928	On Clock-Based Fault Analysis Attack For An Aes Hardware Using Rsl	Random Switching Logic (RSL) is a technique proposed in 2004 as a countermeasure against Differential Power Analysis (DPA) attacks. It was applied to an AES hardware prototype and tested with a 0.13-mu m standard CMOS library. However, despite its purpose to resist DPA attacks, Clock-based Fault Analysis (CFA) was able to reveal secret information from the prototype chip. This paper explains the mechanism of the CFA attack and discusses its success against the RSL-AES implementation. It also proposes an ideal RSL-AES implementation that can resist CFA attacks.
411950	41195085	Practical Password Recovery Attacks On Md4 Based Prefix And Hybrid Authentication Protocols	This paper discusses practical password recovery attacks on two challenge and response authentication protocols that use MD4. The number of queries required for these attacks is a crucial factor, as the opportunity for an attacker to ask queries is limited in real protocols. Previous methods required 2(37) queries, which is almost impossible in real protocols. However, the proposed attack only needs 17 queries and 2(34) MD4 computations to recover up to 8-octet passwords, and 2(10) queries and 2(41) MD4 computations for 12-octet passwords. When using the hybrid approach, previous methods required 2(63) queries, while the proposed attack only needs 28 queries and 2(39) MD4 computations to recover up to 8-octet passwords. This is achieved by guessing part of the password and simulating intermediate chaining variables from observed hash values.
411951	41195148	All-pairs bottleneck paths for general graphs in truly sub-cubic time	The all-pairs bottleneck paths (APBP) problem involves finding the maximum capacity path between every pair of vertices in a directed graph with non-negative edge capacities. This problem was first studied in operations research and is now being addressed with a new sub-cubic algorithm for general dense graphs. The algorithm uses a procedure for computing the (max, min)-product of two matrices in O(n2+Ω/3) time, where n is the number of vertices and Ω is the exponent for matrix multiplication. This allows for the extraction of an explicit maximum bottleneck path in linear time.
411952	41195252	Bounding Performance Loss in Approximate MDP Homomorphisms	The article discusses a metric for measuring behavior similarity between states in a Markov decision process (MDP) that takes action similarity into account. This metric corresponds to the classes of states defined by MDP homomorphisms and can be used to upper-bound the difference in optimal value function between different states. It is shown to be tighter than previous bounds provided by bisimulation metrics. The metric is applicable for both discrete and continuous actions and an algorithm is provided for constructing approximate homomorphisms using this metric. Previous research on this topic has primarily relied on heuristics. 
411953	41195337	The TELAR mobile mashup platform for Nokia internet tablets	The rise of Web 2.0 has led to an increase in online data and information services such as websites, Wikis, and web services. This has also given rise to the development of Mashups, which are web applications that combine data from multiple sources into one service. This is made possible by the use of script languages and web development platforms. Additionally, mobile devices are becoming more powerful and have ubiquitous access to the web, allowing for integration of local sensors such as GPS. This enables mobile applications to adapt to the user's current situation, particularly their location. The Telar Mashup platform is a client-server solution that facilitates the creation of adaptive Mashups for mobile devices, using wrappers to integrate data from web-based services and the DCCI specification to incorporate context information from local sensors. The platform is demonstrated on the Nokia N810 Internet Tablet. 
411954	41195437	Customization and provisioning of complex event processing using TOSCA.	In the world of Internet of Things, a large amount of sensor data is constantly being produced and shared among devices in smart environments. This poses a challenge as the data needs to be processed quickly and efficiently to conserve network resources. Complex event processing (CEP) is a commonly used method to handle this data. However, automatically provisioning CEP systems can be difficult as they require customization to be used in IoT scenarios. This can be a tedious task if done manually. To address this issue, a new approach based on the Topology and Orchestration Specification for Cloud Applications standard has been developed, allowing for the customization and provisioning of CEP systems with all necessary data sources, sinks, and queries.
411955	4119554	New algorithms for finding approximate frequent item sets	Standard frequent item set mining requires all items in a set to be present in a transaction, which can make it difficult to find relevant groups of items. By relaxing the support definition, allowing for some missing items, approximate, fault-tolerant, or fuzzy item sets can be found. This paper presents two new algorithms for finding such item sets: one based on cover similarities and subset size occurrence distribution, and the other using a clustering approach with item cover distances and a one-dimensional Sammon projection. These algorithms are demonstrated to be effective in a concept detection task and neuron ensemble detection in simulated parallel spike trains.
411956	411956175	Playing Fast and Loose with Music Recognition.	In this study, the authors discuss their experience developing a music recognition system that allows musicians to incorporate digital interactions into their performances. They worked with 23 musicians of varying skill levels and instruments, and the system evolved quickly to meet their needs. The system allows musicians to compose music fragments that can trigger digital interactions with varying levels of embellishment, disguise, and looseness. The authors highlight the challenges of control, feedback, and attunement in incorporating technology into performance practice and suggest the use of written notations in other recognition-based systems. Overall, the study supports the idea of intentionally introducing looseness into interactive systems to enhance musical expression.
411957	4119574	Simple Plans Or Sophisticated Habits? State, Transition And Learning Interactions In The Two-Step Task	The 'two-step' behavioural task has become popular for differentiating between model-based and model-free reinforcement learning and producing neurophysiologically-friendly decision data. However, the task's structure can affect the strategies used and the accuracy of conclusions drawn from behavioural performance. A balance must be struck between the need for stochasticity to distinguish strategies and the need for determinism to incentivize subjects to optimize their choices. Changes to the task structure can create correlations that make model-free strategies appear as model-based. A suggested correction to analysis can mitigate this issue, but understanding these complexities is crucial for fully utilizing the two-step task in behavioural neuroscience.
411958	41195856	Honey Encryption Beyond Message Recovery Security.	Juels and Ristenpart developed honey encryption (HE), which ensures message recovery security despite attacks that try all possible keys. This is particularly useful for password-based encryption, where keys are low entropy. Other HE schemes based on the Juels-Ristenpart construction were later proposed for password management and long-term protection of genetic data. However, in this setting, message recovery security is a weak property and does not prevent an attacker from obtaining partial information about plaintexts or altering ciphertexts. To address this, the authors introduce target-distribution semantic-security and target-distribution non-malleability notions and prove that a variation of the JR HE construction can meet them. Their analysis is different from previous work and they also provide a formal proof that an unbounded attacker can always succeed in message recovery with a limited number of encryptions of known plaintexts.
411959	41195924	Delay-Insensitive On-Chip Communication Link using Low-Swing Simultaneous Bidirectional Signaling	This paper introduces a new asynchronous delay-insensitive on-chip link structure that allows for simultaneous data exchange between two modules. Unlike traditional dual-rail links, this design only requires N+1 interconnects instead of 2N+1, making it more cost-effective for future systems. The transceiver circuits are designed using multiplevalued current-mode logic and have a low power consumption of 8.32mW with a 689ps propagation delay and 5mm interconnect length. Potential applications for this link include use in GALS systems, NoC routers, and adaptive systems. The circuit was simulated using Cadence Analog Spectre with 0.13um CMOS technology.
411960	41196024	Task oriented weighting in multi-criteria analysis	This paper introduces a new method for determining criteria weights in decision making processes involving multiple conflicting factors. A fuzzy knowledge base is used to address imprecise decision making and criteria weights are determined by specifying the state of each task requirement. This approach is incorporated into a fuzzy multi-criteria analysis model and tested on a dredger dispatching problem in China. The results demonstrate the effectiveness of the model and its potential for general application in decision making. This task oriented weighting procedure and algorithm have the potential to improve the consistency and accuracy of decision making in various situations.
411961	41196116	Resolution Limits of Sparse Coding in High Dimensions	 Grants DMS-1007062 and DMS- 1209017, and a grant from the Simons FoundationThis paper discusses the issue of detecting sparsity patterns in unknown k-sparse n-dimensional signals observed through m noisy, random linear measurements. This problem arises in various situations, such as statistical model selection, pattern detection, and image acquisition. The main focus of this paper is to determine necessary and sufficient conditions for achieving reliable sparsity pattern recovery, taking into account the dimensions m, n, and k, as well as the signal-to-noise ratio (SNR) and the minimum-to-average ratio (MAR) of the nonzero entries of the signal. The paper presents a necessary condition of m > 2klog(n − k)/(SNRMAR) for any algorithm to succeed, and a sufficient condition for a computationally-trivial thresholding algorithm. This analysis can provide insight into the limitations of convex programming-based algorithms and can be applied in the context of visual cortex modeling to understand the resolvability of visual features from visual data. The abstract estimation problem considered in this paper involves an unknown sparse signal x, modeled as an n-dimensional real vector with k nonzero components, and the problem of detecting the sparsity pattern of x from an m-dimensional measurement vector y = Ax + d, where A is a known measurement matrix and d is an additive noise vector with a known distribution. 
411962	41196230	Capacity of Systems with Queue-Length Dependent Service Quality.	This study focuses on the maximum amount of information that can be reliably processed by a server with a varying level of service quality based on queue length. The capacity of the system is measured in terms of bits processed per unit time, and is influenced by arrival and service process distributions. For systems with one arrival per time slot, a memoryless distribution is optimal, while multiple arrivals can negatively impact the system. This research is driven by the need to incorporate reliability into queuing systems, and has practical applications in crowdsourcing, multimedia communication, and stream computing.
411963	41196329	Generic Incremental Algorithms for Local Search	Introducing a new constraint in local search requires defining measures for penalty and variable conflicts and implementing incremental algorithms to maintain these measures. This can be time-consuming and reduces productivity. A generic scheme is proposed that automatically generates these measures and algorithms for a constraint described in monadic existential second-order logic with counting. The variable-conflict measure is proven to be bounded by the maximum penalty decrease that can be achieved by changing the value of a variable, ensuring good local search performance. This approach is demonstrated by replacing a built-in global constraint with a modelled version, resulting in competitive runtime and robustness.
411964	41196433	Properties of Almost All Graphs and Generalized Quantifiers	The study examines 0-1 laws for first-order logic expanded with Lindström quantifiers, which express graph properties. The logic FO[Q] has a 0-1 law if certain conditions on the quantifier Q are met. Using these conditions, it is shown that FO[Rig], with the quantifier for rigidity, has a 0-1 law. However, extensions with quantifiers for Hamiltonicity, regularity, and self-complementarity do not have a 0-1 law. The question of whether there exists a logic that can express Hamiltonicity or rigidity and has a 0-1 law is considered. It is concluded that there is no such logic for Hamiltonicity, but there is one for rigidity. Additionally, the study examines sequences of vectorized quantifiers and shows that adding quantifiers closed under substructures results in 0-1 laws for extensions of first-order logic. These results also apply to infinitary logic with finitely many variables.
411965	41196511	Fast $q$-gram Mining on SLP Compressed Strings	This article introduces efficient algorithms for calculating the frequency of $q$-grams in compressed strings represented by a straight line program (SLP). The proposed algorithm has a time and space complexity of $O(qn)$, where $n$ is the size of the SLP. Experiments demonstrate that the algorithm performs well on real string data and outperforms other algorithms designed for uncompressed text. The article also discusses potential applications in data mining and classification of string data. 
411966	4119669	Connected Identifying Codes	The problem of generating a connected identifying code for a graph is discussed. It is shown that this problem is NP-complete and a polynomial-time approximation algorithm, called ${\\tt ConnectID}$, is proposed which produces a connected code of at most twice the size. When the input code is robust to graph distortions, the size of the output code is related to the best error-correcting code of a given minimum distance. The size of the input and output codes converge for increasing robustness, and simulations on various random graphs demonstrate the effectiveness of ${\\tt ConnectID}$. It is found that for Erdős–Rényi random graphs, the connected codes generated are only 25% larger than unconnected codes, and robustness often provides connectivity without increasing the size of the code.
411967	41196723	Efficient re-resolution of SMT specifications for evolving software architectures	This study introduces a method for efficiently handling changes in a component-based software architecture. By using formal descriptions and satisfiability modulo theory (SMT) techniques, the method can identify and carry out the necessary steps to ensure compliance with constraints after each change. The approach is demonstrated on a constraint-satisfaction problem related to cloud-based software services, and is proven to be significantly faster than standard SMT resolution methods. The method streamlines the process of re-resolving system constraints, making it a valuable tool for managing changes in software architectures.
411968	41196818	A description of the diamond grid for topological and combinatorial analysis.	A new coordinate system has been developed for all cells in the diamond grid, allowing for easy retrieval of topological relations between cells. This system can be used in various applications, including image processing and shape analysis, as it simplifies operations such as boundary tracking and computation of the Euler characteristic. The system is based on simple integer operations, making it efficient and suitable for implementation in different contexts.
411969	41196952	Transactional Tasks: Parallelism in Software Transactions.	Many programming languages, including Clojure, Scala, and Haskell, offer different models for handling concurrency. However, when these models are combined, their semantics are not always clearly defined. In this paper, the authors explore the combination of futures and Software Transactional Memory (STM). Currently, when futures are created within a transaction, they cannot safely access the transactional state, which can lead to unexpected behavior and violate the serializability of transactions. To address this issue, the authors propose a new construct called transactional tasks, which allow futures to be created within transactions while still providing safe access to the state. Transactional tasks offer several advantages, such as coordination, maintaining serializability, and eliminating non-determinism. Overall, this approach allows for the full utilization of parallelism in a program while preserving the desirable properties of both futures and STM. 
411970	41197046	Automated Derivation of Translators From Annotated Grammars	This paper introduces a method for automating the creation of translators between operations languages, which are specialized programming languages used for satellite operations procedures. The technique utilizes annotated grammars to identify similarities between languages and create a transformation schema. Additionally, the paper suggests using an intermediate representation shared by all operations languages to further simplify the translation process. To demonstrate the effectiveness of this approach, the authors semi-automatically generated translators between several operations languages using a prototype tool they developed.
411971	41197134	Joint generative model for fMRI/DWI and its application to population studies.	The authors suggest a new approach to combine data from diffusion-weighted MRI tractography and resting-state functional MRI correlations using a probabilistic framework. This framework considers the relationship between anatomical and functional connectivity patterns in the brain and can be applied to group studies. They use a mean-field approximation to fit the model to the data and observe significant differences in connectivity between normal controls and patients with schizophrenia. The proposed method provides a promising tool for investigating brain connectivity in various populations and has potential for further research in this area.
411972	41197266	Automatic Analysis of Programming Assignments	In a virtual university setting, there is a need for advanced support in handling assignments, especially homework assignments, due to the lack of face-to-face communication between teachers and students. In this paper, the AT(x) approach (analyze-and-test) is introduced as a way to automatically analyze and test programs. This approach is used to provide feedback to students working on programming exercises and is tailored to specific programming languages such as Prolog and Scheme. The AT(x) framework is further divided into AT(P) and AT(S) for programs written in Prolog and Scheme respectively. 
411973	41197340	Using answer set programming for a decision support system	ACMI is a decision support system used by a German health insurance company to check medical invoices. It is designed to streamline the invoice verification process and ensure accuracy. The system is implemented using DLV, a logic programming language, and utilizes various modules such as a knowledge base and a rules engine. ACMI helps identify potential errors or discrepancies in the invoices, leading to more efficient and effective processing. It also provides transparency and traceability in the decision-making process. Overall, ACMI serves as a valuable tool for the health insurance company in managing and verifying medical invoices.
411974	4119749	Tutorial: an overview of UML 2	This tutorial provides an overview of the main changes in UML 2, the first major update to the Unified Modeling Language. It covers key features and concepts in a half-day session. The main topics include an introduction to UML 2, new diagram types, enhanced notation, and changes to the metamodel. Other important topics covered are the revised structure of UML 2, the new action language, and the integration of UML 2 with other modeling languages. This tutorial is a concise summary of the major updates and changes in UML 2.
411975	41197530	Application of paraconsistent logic in an intelligent tutoring system	It has been observed that testing students' understanding of new concepts immediately after they are introduced greatly enhances the learning process. The measure of understanding is based on the student's ability to provide accurate answers to questions that require the application of the concept. One solution to determining the level of understanding is to administer tests, which can provide information to intelligent agents and aid in evaluating tests. However, factors such as time and the formulation of questions can greatly affect a student's understanding. To address this, the use of paraconsistent logic is proposed as classical logic is unable to draw conclusions in the presence of inconsistencies.
411976	41197652	A fractional approach for the motion planning of redundant and hyper-redundant manipulators	This paper introduces a new approach to address the issue of drift in joint space trajectory planning for redundant robots, using a fractional differential of order @a. Two performance measures, positional error index and repeatability performance index, are defined to evaluate the effectiveness of this method. The results of testing on redundant and hyper-redundant planar manipulators show that a wide range of values for @a can achieve repetitive joint trajectories. This technique offers a solution to the problem of drift in joint space trajectory planning for redundant robots.
411977	41197739	A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them	Optical flow estimation algorithms have improved significantly, but the basic formulation has remained unchanged since the work of Horn and Schunck. Researchers have analyzed the impact of the objective function, optimization method, and implementation practices on accuracy. Surprisingly, classical methods perform well when combined with modern techniques such as median filtering during optimization. However, this leads to higher energy solutions and does not optimize the original objective function. To address this, a new objective function is developed that includes a non-local smoothness term and information about flow and image boundaries. An asymmetric pyramid downsampling scheme is also introduced to better estimate long-range horizontal motions. The methods are evaluated on various datasets and show promising results.
411978	41197860	The impact of continuous integration on other software development practices: a large-scale empirical study.	Continuous Integration (CI) has brought about significant changes in the software development process. With the right tools and implementation, it has been proven to positively impact pull request throughput and the ability to handle larger projects. However, like any new innovation, adopting CI requires adapting existing practices and implementing "best practices" for maximum benefit. A study was conducted on the adoption and evolution of code writing, submission, issue and pull request closing, and testing practices as Travis CI was adopted by established projects on GitHub. A survey of GitHub developers was also conducted to gain insight into their experiences with adopting CI. The results suggest a more nuanced understanding of how GitHub teams are adapting to and utilizing CI technology compared to previous research.
411979	4119794	Fuzzy Classification of Web Reports with Linguistic Text Mining	This paper introduces a fuzzy system that is able to classify textual web reports. The system utilizes third party linguistic analyzers and builds upon previous work in web information extraction and fuzzy inductive logic programming. The main contributions of the paper include formal models, a prototype implementation of the system, and evaluation experiments. The system shows promise in accurately classifying web reports and has potential for practical applications. 
411980	41198061	Dynamical Systems and Stochastic Programming: To Ordinary Differential Equations and Back	This paper focuses on the relationship between models of biological systems using ordinary differential equations (ODE) and models using stochastic and concurrent constraint programming (sCCP). The authors propose a method to convert between the two types of models and investigate their properties. They show that the conversion from sCCP to ODE's preserves rate semantics for biochemical models and study the invertibility of the mappings. They also examine the preservation of dynamics in the models obtained through the mappings, providing a convergence theorem in the direction from ODE's to sCCP. Several examples are discussed in detail to show that dynamics may not be preserved in the inverse direction. 
411981	41198111	A Delay-Based Piggryback Scheme In Ieee 802.11	Data frames can be combined with control frames to improve channel efficiency in wireless communication. However, this piggyback scheme can lead to decreased efficiency and increased frame transmission delay for other stations with low transmission rates. This is similar to the anomaly phenomenon in multi-rate transmission networks. To address this issue, a delay-based piggyback algorithm is proposed, which considers the delay efficiency in IEEE 802.11 WLAN when deciding whether to use the piggyback scheme for a control frame. Simulation results show that this algorithm can reduce average frame delay and channel utilization by approximately 24% and 25%, respectively, even when there is one station with a low transmission rate. 
411982	41198227	Shape-Simplifying Image Abstraction	This paper presents an algorithm for creating stylistic abstractions of photographs. The method uses mean curvature flow and shock filter to simplify shapes and colors while preserving important features. A constrained mean curvature flow is also developed to better convey directionality and shape boundaries. The algorithm is iterative and allows for intuitive control over the level of abstraction. Users can also incorporate masking to control the speed of abstraction and protect certain regions. Experimental results demonstrate that this method produces highly abstract and feature-preserving illustrations from photographs. 
411983	4119831	Generating counterexamples of model-based software product lines.	A model-based software product line (MSPL) uses a variability model to capture the differences in a domain and base models to represent the core artifacts. Realization models connect the features in the variability model to the base model elements and trigger operations based on a configuration. However, managing the design space of an MSPL can be extremely complex for engineers due to the large number of variants and the need for conformity to rules. To address this, the paper proposes using counterexamples (or antipatterns) to identify invalid product models, which can help in creating guidelines and testing oracles. A generic process using the common variability language (CVL) is provided to randomly search for MSPLs, and a tool called LineGen is developed to support this process. The effectiveness of this approach is validated with different formalisms and in an industrial scenario.
411984	41198418	Towards a more semantically transparent i* visual syntax	The i* modelling language is commonly used in Requirements Engineering to facilitate communication between technical and non-technical stakeholders. However, recent research has revealed flaws in its visual syntax, which can affect the effectiveness of model-mediated communication. In this paper, the focus is on one specific quality criterion, Semantic Transparency, which refers to the ability of notation symbols to convey their meaning. The authors propose an empirical approach to address this issue and present a series of experiments aimed at identifying a new symbol set for i* and evaluating its semantic transparency. While this work does not solve all problems in the i* notation, it serves as an important step towards creating visually effective requirements modelling languages. The authors suggest that this approach can be applied to other quality criteria and other notations in the future.
411985	41198552	Extraction and evolution of architectural variability models in plugin-based systems	Variability management is an important aspect in building and updating software systems, allowing for customization and adaptation to meet customers' needs and deployment contexts. Extensible software systems, built on plugin-based architectures, offer a wide range of configuration options through plugins. To efficiently generate system variants, the software architect must accurately model the variation points and constraints between architectural elements. This task can be time-consuming and prone to errors. In this article, a reverse engineering process is proposed to produce a variability model of a plugin-based architecture. This involves automated techniques to extract and combine different variability descriptions, such as a hierarchical software architecture model, plugin dependency model, and software architect knowledge. By analyzing differences between versions of architectural feature models, the software architect can control the variability extraction and evolution processes. The approach has been successfully applied to a large-scale plugin-based system, FraSCAti, with different versions of its architecture.
411986	41198625	NAP: practical fault-tolerance for itinerant computations	Mobile agents can be used to support itinerant computation, which is a program that moves from host to host in a network. The program can have a pre-defined itinerary or dynamically compute the next host to visit. It can also create multiple copies of itself on a single host and visit the same host repeatedly. However, this type of computation is vulnerable to failures such as processor failures, communications failures, and program bugs. To address this, NAP is a protocol that uses failure detection and recovery to provide fault tolerance for itinerant computations. NAP is implemented in TACOMA and offers guarantees for its effectiveness.
411987	41198720	A hybrid intelligent algorithm for solving the bilevel programming models	This paper presents a hybrid intelligent algorithm that combines genetic algorithm (GA) and neural network (NN) to solve bilevel programming models. GA is used to select potential combinations, while a meta-controlled Boltzmann machine (consisting of a Hopfield network and a Boltzmann machine) determines the optimal solution. The algorithm is applied to solve two-level investment problems, where the upper layer determines optimal company investments and the lower layer decides optimal department investments. Examples are provided to demonstrate the effectiveness and practicality of this method. 
411988	41198844	A framework for integrating real-time MRI with robot control: application to simulated transapical cardiac interventions.	The use of real-time intraoperative image guidance has opened up new possibilities for surgical interventions, such as image-guided robot assistance. However, current methods only allow for visual perception during the procedure. This study introduces a framework for performing robot-assisted interventions with real-time MRI guidance. The framework processes the MRI data in real-time and integrates it with robot control, providing a visualization and force-feedback interface for the operator. Experimental results show that this approach significantly improves the speed, accuracy, and safety of procedures, reducing the duration and incidents of tissue collisions. 
411989	41198951	Power-Aware Computing in Wearable Sensor Networks: An Optimal Feature Selection	Wearable sensory devices are being increasingly used in healthcare and well-being applications. These devices use classification algorithms to detect specific events in real-time. However, selecting the right features for these algorithms is important to ensure efficient use of resources and accurate detection of critical events. In this paper, a power-aware feature selection method is proposed to minimize energy consumption in classification applications. This approach takes into consideration the energy cost of individual features and uses a graph model to represent their correlation and computing complexity. The problem is formulated using integer programming and a greedy approximation is presented to select features in a power-efficient manner. Experimental results show significant energy savings of over 30 percent while maintaining high classification accuracy. 
411990	41199031	Zero-Effort Camera-Assisted Calibration Techniques for Wearable Motion Sensors	Activity recognition using wearable motion sensors is crucial for monitoring health and wellness. However, these algorithms are designed to work with a specific orientation of the sensors on the body. If the sensors are accidentally displaced, it is important to recalibrate their new location and orientation. Traditionally, this requires the user to perform specific movements or manually input information about sensor placement. In this paper, the authors propose a camera-assisted calibration method that eliminates the need for extra effort from the user. The calibration occurs seamlessly when the user appears in front of a camera and performs any activity of their choice. Experimental results show the effectiveness of this approach.
411991	41199145	The minimum number of triangles in graphs of given order and size.	In the 1940s and 50s, Erdős and Rademacher posed the question of finding the maximum number of triangles in a graph with a given number of vertices and edges. This problem has been studied extensively and was recently solved by Razborov in 2008. Further developments by Nikiforov and Reiher have led to an exact solution for large graphs with edge density not approaching one. This result confirms a conjecture made by Lovasz and Simonovits in 1975 for almost all cases. 
411992	41199241	Testing in context: framework and test derivation	The paper discusses the challenge of testing a component within a modular system. The component's "context" is defined as the rest of the system, which serves as its operational or testing environment. The authors propose a framework for testing in context using a model of communicating finite state machines. They address the issues of test executability and fault propagation in the presence of the context by computing an approximation of the component's specification. This approximation allows for tests to be executed and faults to be propagated through the context. The paper also presents a conformance relation for test derivation, based on the reduction relation between an implementation and the approximation of the specification. The authors also provide an approach for test generation in the case of deterministic implementations.
411993	41199337	Experiences of using a PKI to access a hospital information system by high street opticians	This paper discusses a system that allows opticians to access patient data from a hospital's Diabetes Information System through a standard web browser. The system uses strong encryption and digital signatures to ensure the security of the data as it travels through the internet. The paper describes the public key infrastructure and the changes made to the web interface to suit the opticians' needs. The results of pilot testing and feedback from opticians are also discussed. The authors found that a well-designed system with a seamless PKI can improve patient care and data quality, but slow internet speeds and difficulty in creating a user-friendly interface can be challenges. They also highlight the importance of maintaining all components and administrative procedures to ensure the system's availability.
411994	41199431	Subjectively Interesting Component Analysis: Data Projections that Contrast with Prior Expectations	High-dimensional data can be difficult to explore without effective low-dimensional projections. Principal Component Analysis (PCA) is a commonly used method for finding such projections, as it is simple and often captures the most relevant structure in the data. However, not all users may find the dominant structure interesting. To address this, a new method called Subjectively Interesting Component Analysis (SICA) has been introduced. This method uses information theory and a user's prior expectations to find projections that are truly surprising and interesting. The optimization problem is straightforward and results in a balance between explained variance and novelty. Case studies on various types of data demonstrate the effectiveness of SICA in finding subjectively interesting projections. 
411995	4119955	Automating News Content Analysis: An Application to Gender Bias and Readability	This article discusses the use of text-analysis technologies to aid social science research by analyzing patterns in news content. The system collects and annotates a large amount of textual data, allowing for the extraction of trends and patterns. The study focuses on 3.5 million news articles and identifies a relationship between their topics, gender bias, and readability. This demonstrates the potential for pattern analysis technology to automate tasks typically done by humans in the social sciences, making large-scale studies possible. 
411996	41199653	Automating Customisation of Floating-Point Designs	This paper presents a method for customizing the representation of floating-point numbers using re-configurable hardware. The method determines the appropriate size of the mantissa and exponent for each operation, in order to satisfy a cost function with a specified error compared to a reference representation. The implementation of this method is iterative and supports IEEE single-precision or double-precision floating-point representation as the reference. This tool can produce custom floating-point formats with varying sizes of mantissa and exponent, and can be applied to different arithmetic representations and hardware designs. The results demonstrate that this method can improve the speed and size of hardware for calculations with large dynamic ranges compared to using the reference representation. 
411997	41199781	Multilevel logic synthesis for arithmetic functions	Arithmetic functions, a type of Boolean function, can be efficiently described using the AND and XOR operators. This is exemplified by n-bit adders. A new logic synthesis method for arithmetic functions is presented in this paper, using their natural representations in the field GF(2). This method involves algebraic factorization to reduce the number of literals. However, a direct translation of the AND/XOR representations often results in large area costs due to XOR gates. To address this issue, the paper proposes a redundancy removal process that reduces many XOR gates to single AND or OR gates without affecting the function. This process only requires simulating a small set of input patterns and has shown to produce circuits with 17% improvement in area and 50% reduction in run time compared to Berkeley SIS 1.2. The resulting circuits also have good testability and power consumption properties.
411998	41199811	Look-Up-Table Controller Design For Nonlinear Servo Systems With Piecewise Bilinear Models	This paper discusses the use of servo control for nonlinear systems that are approximated by piecewise bilinear models, which are fully parametric. The input-output feedback linearization technique is utilized to stabilize the PB control systems. The paper presents two approaches to designing servo controllers for these nonlinear systems, including a two-degree of freedom controller and a model following controller. The effectiveness of the proposed method is demonstrated through illustrative examples, where the controller is represented as a Look-Up-Table. Overall, this paper provides a practical and effective method for designing servo controllers for nonlinear systems with PB models. 
411999	41199956	Adaptation in open systems: giving interaction its rightful place	Adapting software in open systems, where autonomous and diverse participants interact, is a challenge. In these systems, each participant must adapt, but often needs to interact with others to do so. Current approaches to software adaptation, which rely on control-based abstractions, do not work well in these settings. To address this, the authors build on a recent model of interaction through social commitments. They formalize the idea of a participant's strategy for a goal, including the commitments required, and propose a framework for adaptation based on this strategy. This allows for the use of any selection criteria, such as trust. The authors provide examples from the emergency services field to illustrate their approach.
412000	41200056	The BTR-Tree: Path-Defined Version-Range Splitting in a Branched and Temporal Structure	The BTR-Tree is a new access method for branched-and-temporal databases that allows for the updating of both historic and current versions. It is based on the BT-Tree and is designed to balance space and access time tradeoffs. The BTR-Tree splits at a previous version while maintaining the posting property, reducing redundancy in the structure. It has better space efficiency and similar query efficiency compared to the BT-Tree, with no added complexity in search and posting algorithms. 
412001	4120011	A concern architecture view for aspect-oriented software design	Aspect-oriented programming is gaining popularity, but there has been a lack of support for the independent description and incremental design of aspects. A conceptual framework is proposed where aspects are seen as enhancements that modify an existing design. The Concern Architecture model is used to group aspect designs and identify dependencies and potential conflicts in systems with multiple aspects. Aspects are described generically with required and provided elements, similar to formal parameters, and their binding to an existing design shows where modifications will occur. A Concern Architecture Diagram shows the overlap and partial order of aspects and concerns. An example of a UML profile and the design of a digital sound recorder illustrates the effectiveness of this approach.
412002	41200261	MediaRank: Computational Ranking of Online News Sources.	In today's political climate, the quality of news has become a major concern for both the public and the academic community. Traditional news media is facing growing distrust, making it difficult to establish a common understanding of the truth. In response, a team has developed MediaRank, an automated system that ranks over 50,000 online news sources based on four criteria: peer reputation, reporting bias, financial pressure, and popularity. This paper presents the major contributions of MediaRank, including its open and interpretable rankings for various languages and its new methods for measuring influence and bottomline pressure. Additionally, the paper analyzes the effect of media bias on news sources and finds that despite political differences, high-ranking sources tend to favor left-wing parties in English-speaking countries.
412003	4120034	When the functional composition drives the user interfaces composition: process and formalization	Mashups have simplified the reuse of applications by allowing for easy combination of different applications. However, the resulting composite applications do not allow for data sharing or complex workflows. This can only be achieved by composing applications at the functional level to create new services. However, this process requires the redesign and regeneration of user interfaces in order to interact with the new service. This paper proposes a solution to this issue by implementing a process that abstracts the applications and their functional composition, allowing for the reuse of user interfaces. The process also uses a mixed-initiative framework to solve any composition conflicts, either automatically or with developer intervention.
412004	4120040	Handling Skew in Multiway Joins in Parallel Processing.	Dealing with uneven data distribution, known as skew, is a major challenge in query processing. In distributed environments like MapReduce, minimizing communication cost is a key factor. This refers to the amount of data transferred from mappers to reducers in a MapReduce job. This paper introduces a new method for handling skew in a multiway join computation using only one MapReduce round with minimal communication cost. The technique is based on the Shares algorithm and aims to optimize communication cost in distributed environments. 
412005	41200569	An Efficient Progressive Bitstream Transmission System for Hybrid Channels With Memory	This article discusses progressive transmission over a hybrid channel that introduces both bit errors and packet erasures. The authors analyze existing solutions and extend them to a channel with memory on both bit errors and packet erasures. They then propose a simple and low-complexity coding scheme that transforms the hybrid channel into a single impairment channel, making it easier to optimize. Both rate-based and distortion-based optimization problems are explored and it is shown that the proposed solution has lower complexities compared to known solutions. Simulation results demonstrate the effectiveness of the proposed solution, and numerical results show a close match between rate-based and distortion-based optimization.
412006	4120063	On early detection of application-level resource exhaustion and starvation.	This article discusses a system designed for early detection of application-level exhaustion and starvation attacks. It utilizes a unique detection algorithm based on timed probabilistic finite automata and does not require source code or debugging information. The system is implemented through kernel monitoring and has low overhead and high accuracy compared to a user-space version. Experiments were conducted using synthetic and real-world attacks, such as Apache Killer and Slowloris, on various applications, including the Apache server. The article also includes a theoretical analysis of the potential advantage for attackers who are familiar with the system, measured by a new metric. This metric can also be used to control resources consumed by demanding inputs to protected programs.
412007	412007147	A cooperative immunization system for an untrusting Internet	Viruses and worms are major security threats in computer systems today, and users often rely on anti-virus programs and firewalls to protect their machines. However, these solutions can be unreliable as they rely on manual configurations and human intervention, making them vulnerable to attacks. To address this issue, a cooperative immunization system is proposed where nodes work together to defend against attacks. This system is evaluated using a virus model and simulations, which demonstrate its effectiveness against viruses and malicious participants. Overall, the cooperative immunization system offers a more robust and efficient method for protecting computer systems from attacks.
412008	41200820	Whole brain diffeomorphic metric mapping via integration of sulcal and gyral curves, cortical surfaces, and images.	The paper introduces a new algorithm for registering brain images, called the large deformation diffeomorphic metric mapping algorithm. This algorithm is able to simultaneously map sulcal and gyral curves, cortical surfaces, and intensity images from one brain to another using diffeomorphisms. It is the first time that this type of mapping has been done in a unified manner using a shape space of intensity images and point sets. The paper describes the mathematical equation behind the algorithm and how it is implemented using computationally friendly kernels. The algorithm is then applied to align magnetic resonance brain data and is found to outperform other existing algorithms in terms of mapping accuracy and whole brain alignment.
412009	41200940	Pollution-resilient peer-to-peer video streaming with Band Codes	Band Codes (BC) are a proposed solution for controlled-complexity random Network Coding (NC) in mobile applications, where energy consumption is a major concern. In this paper, the potential of BC is investigated in a peer-to-peer video streaming scenario with both malicious and honest nodes. Malicious nodes launch pollution attacks by randomly altering the content of coded packets, disrupting the video stream for honest nodes. Instead of identifying and isolating malicious nodes, this paper suggests adapting the coding scheme to introduce resilience against pollution propagation. Experimental results show that by adjusting coding parameters, the impact of a pollution attack can be reduced and the quality of the video communication can be restored. 
412010	41201092	Transform Coding Techniques for Lossy Hyperspectral Data Compression	Transform-based lossy compression is a promising method for reducing the size of hyperspectral data. As these data are 3-D, the correlation between dimensions must be carefully considered when designing a compression method. This paper focuses on the design of a 3-D transform and rate allocation for lossy compression of hyperspectral data. The authors first select a suitable transform for the data and then determine the optimal allocation of bits for each transformed coefficient. Experimental results show that this approach achieves better compression performance compared to traditional methods.
412011	4120114	Improved low-complexity intraband lossless compression of hyperspectral images by means of Slepian-Wolf coding	This paper introduces a new lossless compression method for remote sensing images, which is designed to work efficiently within the limited computational resources available. The method uses distributed source coding and Slepian-Wolf coding to achieve high performance while keeping the encoding complexity low. Tests on AVIRIS data demonstrate that this new approach performs similarly to CALIC and outperforms the popular JPEG 2000 method. Overall, this proposed scheme offers a promising solution for compressing multispectral and hyperspectral images in remote sensing systems.
412012	41201253	Network Communities: Something Old, Something New, Something Borrowed …	This paper explores the concept of network communities as a new genre of collaboration in the CHI and CSCW communities. These are robust and persistent communities that bridge the virtual and physical worlds, and require an understanding of both technology and social dynamics. The authors examine various systems and historical examples to describe the advantages they offer to their community of users. Drawing from their own experiences, they expand on the design space by considering three dimensions: boundary negotiations between real and virtual worlds, support for social rhythms, and the emergence and growth of the community. The paper concludes with implications for designers, researchers, and community members based on their findings.
412013	41201344	On the Parameterized Complexity for Token Jumping on Graphs.	The token jumping problem involves transforming one set of tokens on a graph into another set, with each transformation moving one token to a different vertex. The sets must be of equal size and all intermediate sets must also be of the same size. This problem is challenging and has been shown to be PSPACE-complete for planar graphs with maximum degree three. In this paper, the problem is further studied and it is found to be W[1]-hard when parameterized by the number of tokens. However, an algorithm is developed that can efficiently solve the problem for general graphs when parameterized by both the number of tokens and the maximum degree. This algorithm can also find the minimum number of token movements needed to transform the initial set into the target set.
412014	41201426	Statistical leakage modeling for accurate yield analysis: the CDF matching method and its alternatives	This study examines the impact of statistical leakage modeling on the yield of memory designs. Different closed form models are evaluated and the CDF matching method is proposed as a comprehensive and effective approach for accurate modeling. The Schwartz-Yeh method is found to match the body and left tail of the distribution, while the Fenton-Wilkinson method focuses on the right tail. This is important for estimating yield in the presence of leaky bitlines, but the CDF matching method is still shown to be more accurate in practical applications. The estimated error in false-read probability can range from 10x to 147x and is expected to increase as technology advances.
412015	4120153	PaperPhone: understanding the use of bend gestures in mobile devices with flexible electronic paper displays	Flexible displays have the potential to mimic the interaction styles of paper documents, making them a promising technology for use in electronic devices. In this study, researchers evaluated the effectiveness of different bend gestures for performing tasks on a smartphone-inspired flexible E Ink prototype called PaperPhone. They collected 87 bend gesture pairs from ten participants and found that users preferred simpler and less physically demanding gestures that were executed along one axis. They also identified the top three most commonly used bend gesture pairs and found a strong consensus among participants for the polarity of bend gestures in actions that have a strong directional cue. This suggests that bend gestures that take directional cues into account are more natural for users to use.
412016	41201685	Alternative Similarity Functions For Graph Kernels	In collaborative recommendation systems, graph kernels can be used to model the task of recommendation and rating prediction. These kernels are interpreted as inverted squared Euclidean distances in a space defined by the underlying graph. However, other similarity functions can also be used in place of this inverted squared Euclidean function. The exponential diffusion kernel, von Neumann kernel, and random forest kernel have been evaluated in this context and have been found to improve the performance of graph kernels in collaborative item recommendation and rating prediction tasks. This suggests that alternative similarity functions can enhance the effectiveness of graph kernels in these tasks.
412017	412017104	Visual Representation in the Determination of Saliency	This paper explores the impact of visual representation on the behavior of a visual salience model. Different representations have been used to create an early visual representation of image structure, and experiments show that the choice of representation significantly affects the system's behavior. The reasons for these differences are discussed and applied to vision systems in general. When design choices are arbitrary, the properties of visual representation in human early visual processing can provide insights. Overall, the study emphasizes the importance of filter choice and suggests that log-Gabor filters have desirable properties.
412018	41201824	Linear hypertree for multi-dimensional image representation	The linear hypertree is a hierarchical representation of objects in d-dimensional space. It uses a hierarchical locational code to encode the rectangular coordinates of each node. Two decoding techniques are presented - one produces scalar values for the node coordinates, while the other generates them in vector form. The vectorial decoding method only applies coordinate transformations to the universe vertices, making it more efficient. Adjacency concepts in a multi-dimensional environment are defined, and a neighbor-finding algorithm is introduced that operates directly on the locational code to identify adjacent nodes. Procedures for computing the locational codes of larger and smaller neighbors are also included. Finally, an algorithm for finding the shortest path between two nodes is described as an example of the practical use of the decoding formula and neighbor identification.
412019	41201966	Experimenting with a multi-agent e-commerce environment	Agent technology is often touted as the most natural way to automate e-commerce processes. However, despite this claim, human decision-making still plays a crucial role in the success of e-commerce systems. This has led to a lack of large-scale agent-based e-commerce applications that are actually implemented and functioning successfully. To address this, an abstract e-commerce environment is proposed where agents of different types can interact and work towards the goal of supporting e-commerce transactions. A prototype system using the JADE agent platform is described, and experiments with the implemented system are reported.
412020	4120200	Geographic origin of libre software developers	This paper discusses the argument that libre (free, open source) software is a product of global development. While there is evidence that developers work in diverse teams and the community is made up of people from different countries, it is difficult to accurately determine the exact composition of the community. Past studies have relied on surveys, which may not be representative of the entire community. To address this issue, the authors propose using databases to infer the geographical location of developers. By analyzing data from SourceForge and mailing lists, they estimate the geographical origin of over one million individuals involved in libre software development. The paper concludes that this method is a good estimate of the global distribution of libre software developers.
412021	41202120	Segmentation From Natural Language Expressions	This paper presents a novel approach to segmenting images based on natural language expressions, rather than traditional predefined semantic classes. The proposed model uses a combination of recurrent and convolutional neural networks to extract visual and linguistic information and produce pixelwise segmentation for the target object. Unlike previous approaches, this model is not limited to a fixed set of categories or rectangular regions. It has been tested on a benchmark dataset and shown to outperform other methods by a significant margin. This suggests that this model has potential for accurately segmenting images based on natural language descriptions.
412022	4120225	Truth Discovery in Crowdsourced Detection of Spatial Events.	The widespread use of smartphones has led to the rise of mobile crowdsourcing tasks, such as detecting spatial events as users move around in their daily lives. However, the reliability of these events can be compromised by unreliable participants and low-quality data. This presents a challenge in mobile crowdsourcing known as truth discovery, which involves identifying true events from diverse and noisy reports. This problem is unique due to uncertainties in participants' mobility and reliability. Two new unsupervised models, TSE and PTSE, are proposed to address this challenge by considering location popularity, visit indicators, and participant reliability. These models can effectively handle uncertainties and automatically discover truths without location tracking. Experimental results show that these models outperform existing approaches in mobile crowdsourcing environments.
412023	41202360	Rigorous design of robot software: A formal component-based approach	The article discusses the combination of two advanced tools, G^e^noM and BIP, for developing functional modules of robotic systems. While previous research focused on formal methods for the highest level of robot software architecture, the authors address the importance of using these methods for the functional level. They successfully apply the G^e^noM/BIP approach to a complex exploration rover, producing a fine-grained formal model and running it on the real robot to check for deadlock and safety properties. The authors have also extended this approach to include real-time capabilities, distributed processing, user-friendly language for constraints, and integration with a temporal plan execution controller. This approach can now be applied to both the lowest and highest levels of robot architecture. 
412024	41202477	An indirect robust continuous-time adaptive controller with minimal modifications	This paper investigates the effectiveness of an indirect continuous-time adaptive controller by implementing parameter projection to limit the estimated parameters within a known range. The analysis shows that the system remains globally stable in the presence of unmodelled dynamics and external disturbances, without requiring any specific signals in the closed-loop system. The controller also maintains the properties of earlier conventional adaptive controllers when the controlled plant meets certain ideal assumptions. This method does not require prior knowledge of the unmodelled dynamics for implementation.
412025	41202559	Probabilistic object bases	Current object database systems are not equipped to handle uncertain attributes of objects. To address this issue, the authors propose a new algebra to handle object bases with uncertainty. They introduce concepts of consistency and provide an NP-completeness result, along with classes of probabilistic object bases for which consistency can be easily checked. The authors also discuss the importance of conjunctions and disjunctions in operations and how their probabilities depend on the probabilities of primitive events and their relationships. They demonstrate the application of their algebra in query optimization and have developed a prototype server on ObjectStore. Experiments are conducted to evaluate the efficiency of different rewrite rules.
412026	41202620	Retrieving geometric information from images: the case of hand-drawn diagrams.	This paper presents a general algorithmic approach to retrieving meaningful geometric information from image data in any geometric domain. The approach is tailored to the specific domain and can be concretized to yield specific algorithms. As an example, the domain of plane Euclidean geometry is formally specified and concrete algorithms are presented for retrieving typical geometric objects, relations, and labels from hand-drawn diagrams in this domain. The feasibility of these algorithms is demonstrated through experiments. The paper also showcases how this approach can automatically discover implied geometric knowledge from images, illustrated through an example of generating nontrivial geometric theorems from retrieved information.
412027	41202735	Transparent multi-core speculative parallelization of DES models with event and cross-state dependencies	This article discusses the transparent parallelization of Discrete Event Simulation (DES) models on multi-core machines using speculative schemes. The proposed approach allows simulation objects to access and modify the state of any other object, creating what is called cross-state dependency. This differs from traditional PDES platforms where access is limited to the object being processed. To ensure consistency in parallel execution, an advanced memory management architecture and synchronization mechanisms are introduced. This approach has been integrated with the ROOT-Sim platform, but can also be applied more broadly. It aims to improve performance and efficiency while maintaining transparency in handling cross-state and traditional event-based dependencies.
412028	41202813	CrowdScreen: algorithms for filtering data with humans	The problem of filtering a large set of data items based on human-verifiable properties is common in crowdsourcing applications, but has not been formally optimized. Most solutions use heuristics instead of formal optimization. This study presents different versions of the problem and develops deterministic and probabilistic algorithms to optimize the expected cost and expected error. These algorithms outperform other strategies in experimental testing and can be applied in various crowdsourcing scenarios. They can also be integrated into query processors that use human computation.
412029	41202958	Rank-Aware Query Processing and Optimization	This dissertation proposes a framework for supporting ranking in relational database systems through the integration of ranking algorithms and operators in query processing and optimization. Two rank-join algorithms are introduced, one for joining multiple ranked inputs on key attributes and another for general join conditions. These algorithms use individual orders of input relations and result in ordered join results based on a user-specified scoring function. Practical issues and optimization heuristics are addressed to integrate the new operators in query processors. A rank-aware query optimization framework is also introduced, based on a dynamic programming algorithm, to generate optimal rank-aware query plans. A probabilistic model is introduced for estimating the input cardinality and cost of a rank-join operator. Experimental evaluation shows the superior performance of the proposed techniques in joining ranked inputs and the accuracy of the estimation model.
412030	41203017	A declarative approach to distributed computing: Specification, execution and analysis.	The use of logic programming for distributed algorithms is gaining interest for various network applications. These applications involve multiple devices sharing data and computation, with the ability to exchange information and collaborate. A declarative approach is proposed in this paper, where algorithms and communication models can be specified as action theories and executed as collections of distributed state machines. Devices are represented as automata that can communicate through messages. The approach is also analyzed using existing theories on causal theories and Answer Set Programming. The paper also presents results on the application of this approach to different types of network protocols.
412031	41203121	Hybrid Abductive Inductive Learning: A Generalisation of Progol	Progo15 and the Bottom Generalisation method are widely used in Inductive Logic Programming (ILP). However, they are limited in their ability to find hypotheses within the scope of Plotkin's relative subsumption. This paper uncovers a new limitation of Progo15 and proposes a new approach called Hybrid Abductive Inductive Learning that combines ILP and Abductive Logic Programming (ALP). This approach, implemented through the HAIL proof procedure, not only overcomes the limitation of Progo15 but also allows for the generation of multiple clauses from a single example and the derivation of hypotheses outside of Plotkin's relative subsumption. A new semantics, called Kernel Generalisation, is also introduced to encompass the hypotheses produced by HAIL.
412032	412032213	Temporal quality assessment for mobile videos	The paper discusses the challenges of assessing video quality in mobile devices, such as limited computation power and wireless network issues. The authors propose a Temporal Variation Metric (TVM) to measure the temporal information of videos, which shows a high correlation with optical flow. They then use this metric to create a reduced-reference temporal quality assessment metric, Temporal Variation Index (TVI), which can predict users' Quality of Experience (QoE) and estimate network conditions. Subjective assessments show a 92.5% correlation between TVI and Mean Opinion Score (MOS) ratings, and video streaming experiments demonstrate an accuracy of 95% in predicting packet loss and delay.
412033	41203327	MST construction in O(log log n) communication rounds	The article discusses a basic model for overlay networks in which all processes are connected to each other and messages contain a maximum of O(log n) bits. A distributed algorithm is presented for this model, which can construct a minimum-weight spanning tree in O(log log n) communication rounds. This is the first algorithm to achieve a time complexity of less than ω(log n) with small message sizes.
412034	41203442	An interoperable delivery framework for scalable media resources	This article discusses a framework for delivering scalable media resources, such as videos, in a standardized format. The framework supports both video on demand and multicast streaming and uses MPEG-21 Digital Item Adaptation for efficient and interoperable content adaptation. The server and clients of the framework use the MPEG Extensible Middleware and MPEG Query Format for querying available resources. The framework has been integrated into the VLC media player and the architecture for both video on demand and multicast is described. A comparison of the performance of the framework's generic MPEG-21 metadata-based approach to a specific scalable video coding approach is also provided.
412035	4120350	Online learning of correspondences between images.	The proposed method is used for finding corresponding points between images in a sequence. This is done by projecting 3D points onto two images and then determining the corresponding point in the other image. The projections and surface shape are unknown, making it difficult to find correspondences without access to the 3D scene. However, the proposed method uses an iterative approach with a chi-square divergence optimization to accurately and quickly determine the mappings between points. This algorithm outperforms other methods in terms of convergence and accuracy in various experiments and can run in real time.
412036	4120367	The weakest failure detector for solving consensus	In the paper, "Consensus in Asynchronous Distributed Systems with Crash Failures," the authors discuss the minimum amount of information needed to solve the Consensus problem in asynchronous distributed systems with crash failures. They prove that a failure detector, which provides very little information about crashed processes, is sufficient to solve the problem. This is known as the {0 failure detector. The authors also demonstrate that any failure detector must provide at least as much information as {0 in order to solve Consensus. Therefore, {0 is considered the weakest failure detector for solving Consensus in asynchronous systems with a majority of correct processes.
412037	4120370	Efficiently refactoring java applications to use generic libraries	Java 1.5 generics allow for the creation of reusable container classes that ensure safe usage through type enforcement by the compiler. This eliminates the need for potentially unsafe down-casts when retrieving elements from containers. A refactoring technique has been developed to replace raw references to generic library classes with parameterized references. The refactoring uses type constraints to infer type parameters and removes redundant casts. The refactoring has been implemented in the Eclipse development environment and has been evaluated by refactoring Java programs that use the standard collections framework to use Java 1.5 generics. On average, 48.6% of casts are removed and 91.2% of compiler warnings related to raw types are eliminated. This approach is more scalable, can accommodate user-defined subtypes, and is integrated in a popular IDE, setting it apart from previous techniques.
412038	412038169	Conditional Density Estimation with Dimensionality Reduction via Squared-Loss Conditional Entropy Minimization	Regression is a commonly used method for estimating the conditional mean of output based on input. However, it lacks effectiveness when dealing with complex data that has multiple modes, varying levels of variability, and asymmetry. In these cases, it is better to estimate the conditional density itself, but this is difficult in high-dimensional spaces. One approach to address this is to first use dimensionality reduction (DR) and then perform conditional density estimation (CDE). However, this two-step process can lead to significant errors. To address this issue, the authors propose a new method that simultaneously performs DR and CDE by formulating DR as a problem of minimizing a squared-loss version of conditional entropy. This eliminates the need for an additional CDE step and has been tested on various data sets with promising results.
412039	41203952	Bayesian statistical model checking with application to Stateflow/Simulink verification	The article discusses the problem of model checking stochastic systems, which involves determining if a system satisfies a temporal property with a certain probability threshold. The authors propose a Statistical Model Checking (SMC) approach based on Bayesian statistics, which is applicable to a class of hybrid systems with stochastic transitions. This approach combines randomized sampling of system traces with hypothesis testing or estimation to efficiently solve the verification problem. While not guaranteed to be correct, the Bayesian SMC approach can make the probability of giving a wrong answer arbitrarily small. The authors demonstrate the effectiveness of their approach on a fuel control system model and suggest its potential for other stochastic models. 
412040	412040101	Symmetric public-key encryption	Public-key encryption is typically seen as assymetric, as it only allows for messages to be encrypted using a user's public key. However, the use of interactive protocols can allow for a symmetric use of public keys. This means that the same public key can be used for both encrypting messages sent to a user and messages sent by the user to others, without compromising the key. These protocols also allow for probabilistic encryption, which is when the same message can have different encrypted versions. The proposed public-key cryptosystem based on these protocols only requires one key owned by a cryptographic server. Additionally, the protocols allow for authentication of both the sender and receiver of a probabilistically encrypted message, and are secure against chosen-message and chosen-ciphertext attacks. 
412041	41204137	Dynamic dictionary matching	The dynamic dictionary matching problem involves finding patterns in a changing dictionary of strings. The goal is to efficiently search a given text for occurrences of any pattern in the dictionary. An algorithm is presented that can insert or delete patterns from the dictionary with a time complexity of O(m log |D"i|), where m is the length of the pattern and |D"i| is the size of the dictionary after the operation. Searching for patterns in the text has a time complexity of O((n + tocc) log |D"i|), where n is the length of the text and tocc is the total number of occurrences of patterns in the text. 
412042	41204245	Reasoning About Constraint Models	A framework has been proposed to reason about characteristics of models expressed in languages such as AMPL, OPL, Zinc or Essence. However, the framework has shown that certain reasoning problems, like detecting symmetries, redundant constraints, or dualities between models, cannot be solved even for simple problem instances. To assist human modellers, automation of these reasoning tasks would be helpful. Two case-studies have been described to explore the possibility of automating these tasks. The first uses the ACL2 inductive prover to prove the existence of a symmetry in a model, while the second identifies a tractable subset of MiniZinc and uses a decision procedure to prove a parameterized constraint in a model. 
412043	4120433	An empirical simulation-based study of real-time speech translation for multilingual global project teams	Real-time speech translation technology is currently available, but its impact on communication in global software projects is not fully understood. The goal of this study was to investigate the use of combining speech recognition and machine translation to overcome language barriers in remote software requirement negotiations. The study used Google Web Speech API and Google Translate service, two groups of four subjects speaking Italian and Brazilian Portuguese, and a test set of 60 technical and non-technical utterances. The results showed that while there was satisfactory accuracy in terms of speech recognition, it was affected by differences in speakers and utterances. It was also found that accurate transcripts were crucial for successful translations, highlighting the importance of speech recognition in speech translation technology. Overall, the study suggests that speech translation technology can be used to facilitate communication among globally distributed team members in their native languages.
412044	41204413	Building a Knowledge Experience Base for Facilitating Innovation.	In this paper, the authors introduce a framework designed to facilitate knowledge transfer within and outside of an organization for the purpose of innovation. The framework includes a Knowledge Experience Base (KEB) that gathers Knowledge Experience Packages (KEP) to help formalize and package knowledge and experience from innovation stakeholders. This encourages the gradual explanation of implicit knowledge from those who possess it, making the transfer process easier and reducing costs and risks. By using this framework, organizations can improve their ability to transfer knowledge and drive innovation. 
412045	41204546	First GrC model - Neighborhood Systems the most general rough set models	The Neighborhood System (NS) is a formal model that revisits the concept of infinitesimals, which played a crucial role in the development of calculus, topology, and non-standard analysis. It is shown in this paper that Ziarko's variable precision model can be expressed using NS. Additionally, NS is the most comprehensive rough set model, as it includes topology, binary relation, and covering as special cases. A new operation called "and" is introduced, which generates a base of a topology, referred to as a knowledge base. The approximations based on this knowledge base can be interpreted as learning, which distinguishes it from traditional rough set approximations. 
412046	41204684	Designing Directories in Distributed Systems: A Systematic Framework	In this paper, the authors introduce a framework for designing directory-based distributed applications. They evaluate various directory designs using this framework and provide a case study for a multicast application. The framework is based on a model that extends the concept of "process knowledge" in distributed systems. However, they argue that this definition of knowledge is too strong for many applications and propose a weaker concept of "estimation". They define phrases such as "process p in state s estimates with probability 0.9 that process q is active" and use this concept to specify directory design as an optimization problem. This approach allows for systematic analysis of different directory designs, considering factors such as bandwidth, computation, and storage. This paper contributes to the understanding of distributed systems, directories, and design frameworks, as well as providing a practical tool for performance modeling in multicast applications.
412047	41204737	View selection for designing the global data warehouse	A global data warehouse collects data from various databases and sources, and can be seen as a collection of materialized views. The selection of which views to materialize is an important decision in the design of the warehouse. However, commercial products do not have automatic tools for this process. To address this, a method is proposed to generate materialized views that satisfy all the input queries. This process is complex as it involves detecting and utilizing common subexpressions. The method also considers the space allocated for materialization and aims to minimize the overall cost of query evaluation and view maintenance. Algorithms have been designed and tested for this purpose.
412048	41204850	V-scope: an opportunistic wardriving approach to augmenting TV whitespace databases	The article discusses the potential of TV whitespaces for wireless communications and the reliance of secondary users on spectrum occupancy databases to determine available channels. However, the accuracy of these databases may be low due to their reliance on propagation models. The authors present V-Scope, a vehicular sensing framework that uses spectrum sensors on public vehicles to collect and report measurements, to evaluate the accuracy of these databases. The system has been deployed on a public transit bus in a mid-sized US city, and the results show that databases tend to overestimate the coverage of certain TV broadcasts, leading to unnecessary blocking of whitespace spectrum in a large area. The authors also suggest using these measurements to improve existing propagation models in databases.
412049	41204951	Proximity theorems of discrete convex functions	A proximity theorem is a statement that states that an optimal solution to an optimization problem can be found within a certain distance from a solution to its relaxation. These theorems have been utilized in developing efficient algorithms for discrete resource allocation problems. This paper focuses on establishing proximity theorems for L2-convex and M2-convex functions, which are important for solving the polymatroid intersection problem and the submodular flow problem. The results of this study build upon previous findings for L-convex and M-convex functions, expanding the scope of application for proximity theorems in solving discrete convex problems.
412050	41205026	High-performance public-key cryptoprocessor for wireless mobile applications	The article presents a high-speed public-key cryptoprocessor that utilizes three levels of parallelism in Elliptic Curve Cryptography (ECC) over GF(2n). It employs a Parallelized Modular Arithmetic Logic Unit (P-MALU) to accelerate modular operations using two types of parallelism. The scalar multiplication sequence is also sped up through the use of Instruction-Level Parallelism (ILP) and processing multiple P-MALU instructions simultaneously. The system can be programmed for different types of elliptic curves and scalar multiplication algorithms. Synthesis results show that scalar multiplication of ECC over GF(2163) on a generic curve can be computed in 20 and 16 µs with the binary NAF and Montgomery methods, respectively. On a Koblitz curve, the TNAF method can achieve a scalar multiplication of 12 µs. This fast performance enables over 80,000 scalar multiplications per second and enhances security in wireless mobile applications.
412051	41205112	Opportunities from Open Source Search	The internet search industry has a successful business model that offers free services to users. This raises the question of why there should be open source search offerings. This paper explores the concept of open source search and explains why the computer science community should be involved. The Alvis Consortium is developing infrastructure for open source search engines using peer-to-peer and subject specific technology. This approach is based on the belief that there is a sufficient supply of open source components for certain tasks. Open source search is seen as a promising area for information extraction and retrieval components and the use of intelligent agents.
412052	41205232	Emerging opportunities for theoretical computer science	This report highlights the importance of a strong theoretical foundation in computer science and emphasizes the need for a closer integration between theory and practice. It suggests that the value, impact, and funding of theory can be enhanced if guided by this principle. In order to achieve a greater synergy between theory and application, the report recommends increased financial resources for theory and a closer interaction between theoreticians and researchers in other areas of computer science and other disciplines. It also proposes two applied research initiatives, Information Access in a Globally Distributed Environment and The Algorithmic Stockroom, as well as a broader approach to graduate education to better prepare both theoreticians and practitioners for collaboration. 
412053	41205344	Testing Planarity of Partially Embedded Graphs	This article discusses the problem of extending a partial planar drawing of a subgraph to a planar drawing of the entire graph. The authors show that this problem remains polynomial-time solvable, thanks to the "TONCAS" behavior which states that the necessary conditions for planarity are also sufficient. This is determined by the rotation system and containment relationships between cycles, as well as the decomposition of the graph into its connected, biconnected, and triconnected components. The authors also introduce a linear-time algorithm for the simultaneous graph drawing problem, where one of the input graphs or the common graph has a fixed planar embedding. However, generalizations of the problem, such as minimizing the number of edges that need to be rerouted, are proven to be NP-hard.
412054	41205468	Switch-Regular upward planar embeddings of trees	The problem of determining whether a directed graph can be drawn without any crossings, with all edges flowing upwards, is known as the upward planarity testing problem. This has been extensively studied in past research. A new variation of this problem is introduced in this paper: determining whether a directed graph can be drawn in a specific way, known as a switch-regular upward planar drawing. These drawings have practical applications in developing efficient checkers and compaction strategies. The paper presents a characterization for directed trees that can be drawn in this way and proposes an optimal linear-time algorithm for testing and embedding them.
412055	4120553	Clustering cycles into cycles of clusters	The paper focuses on studying clustered graphs with a cycle as their underlying graph. The researchers specifically look at 3-cluster cycles, which have three clusters at the same level. They demonstrate that testing for c-planarity, or the ability to draw the graph without crossing edges, can be efficiently done for this type of clustered graph. A drawing algorithm is also provided. The researchers also use formal grammars to characterize 3-cluster cycles. The study is then expanded to include clustered graphs with a cycle structure at each level of the inclusion tree, and the same efficient c-planarity testing and drawing algorithms are shown to apply.
412056	4120561	On cost-aware monitoring for self-adaptive load sharing	This paper discusses the tradeoff between the utility of monitoring and its cost in self-adaptive load sharing systems, where a stream of jobs is distributed among a group of servers. The authors propose a model called Extended Supermarket Model (ESM) to determine the optimal number of servers to be monitored for minimal service time at an optimal cost. They also present self-adaptive load-sharing algorithms for centralized and distributed settings, and evaluate them using simulations and a real testbed. The results show that self-adaptive load balancing is more effective than cost-oblivious mechanisms, and the proposed algorithms perform well in a distributed setting without a dedicated monitoring component.
412057	41205732	Addressing the challenges of future large-scale many-core architectures	The current trend in processors is to have more cores with varying characteristics, allowing for higher performance in different applications. However, effectively utilizing these processors is a challenge, as assuming all cores are the same for scheduling tasks is no longer valid. To address this, a task assignment policy is proposed that takes into account the different characteristics of cores, such as their grouping and distance from memory controllers. This policy also allows for task migration to optimize both execution time and power consumption. The paper outlines the assignment algorithm and how it will be implemented on a many-core system.
412058	41205819	A relational symbolic execution algorithm for constraint-based testing of database programs	Constraint-based program testing involves using symbolic execution to generate test data for a specific execution path in a program. This technique can be applied to different paths in the code to automatically generate suitable test sets for code units. To improve the reliability of software, this technique can be generalized for programs that manipulate databases. A proposed relational symbolic execution algorithm for testing simple Java methods that interact with a relational database is described. This algorithm models the database tables and method variables as constrained relational variables and uses Alloy constraints to generate test cases. A tool implementing this algorithm is demonstrated with examples.
412059	41205934	Web Application Security Using JSFlow.	Web applications are commonly targeted by code injection attacks and attacks through buggy or malicious libraries. However, current protection measures are often ad-hoc and reactive, resulting in a variety of specialized but fragile mechanisms. This abstract introduces JSFlow, an information-flow aware interpreter for web application security, which focuses on controlling what applications can do with accessed information rather than just restricting access. This removes the need for trust in entities with granted access, making it more resilient to bypassing by third party code and code injection attacks. Through two practical attacks on a web application called Hrafn, the effectiveness of JSFlow is demonstrated. It provides a consistent defense against untrustworthy and malicious code, ensuring confidentiality of sensitive data. 
412060	41206038	LCO-MAC: A Low Latency, Low Control Overhead MAC Protocol for Wireless Sensor Networks	Wireless sensor networks (WSNs) often use a duty-cycling scheme in their medium access control (MAC) protocol to reduce energy consumption. However, this scheme can lead to high end-to-end latency and control packet overhead. To address this issue, a new MAC protocol called LCO-MAC has been proposed. This protocol allows a DATA packet to be transmitted through multiple hops in a single duty cycle, reducing end-to-end latency. Additionally, one control packet can serve multiple roles, acting as an RTS and CTS in the initial transmission period and as an ACK during actual data transmission. Simulations have shown that LCO-MAC improves energy efficiency and decreases end-to-end latency compared to the current RMAC protocol. 
412061	41206114	Collaborative Distributed Admission Control (CDAC) for Wireless Ad Hoc Networks	Admission control algorithms have been studied extensively to ensure Quality of Service (QoS) for multimedia applications over wired Internet. As wireless home networks become more prevalent, it is important to utilize these mechanisms to improve the performance of wireless multimedia applications. This paper proposes a framework for distributed admission control in a collaborative wireless environment, where a device will not add a new flow if it would compromise existing flows due to resource limitations. The authors suggest a modification to 802.11x networks to increase bandwidth efficiency, provide a performance analysis for networks with multiple flows, and propose two distributed admission control algorithms based on transmission opportunity and contention window. Simulation results support the theoretical predictions of the proposed algorithms.
412062	4120621	Designing core ontologies	The lack of a formal basis for modeling complex, structured knowledge in distributed information systems hinders their integration. To address this issue, the use of core ontologies is proposed. These ontologies are highly axiomatized and based on foundational principles, making them precise and modular. They also allow for the reuse and integration of existing domain knowledge. Three core ontologies have been developed for events and objects, multimedia annotations, and personal information management. These ontologies can be simultaneously used and integrated in a complex socio-technical system such as emergency response. The design approach, lessons learned, and aesthetic aspects of these core ontologies are discussed.
412063	41206342	Siamese: scalable and incremental code clone search via multiple code representations	The paper introduces a new code clone search technique, called Siamese, which is accurate, incremental, and scalable to large code bases. Siamese utilizes multiple code representations, query reduction, and a custom ranking function to improve performance. It outperforms seven other clone detection tools with a 95% and 99% mean average precision on two clone benchmarks and can detect the largest number of Type-3 clones compared to other code search engines. Siamese is also scalable, able to search through 365 million lines of code in just 8 seconds. The paper also discusses two use cases for Siamese, online code clone detection and clone search with automated license analysis.
412064	41206460	Improving Color Constancy by Photometric Edge Weighting	Edge-based color constancy methods use image derivatives to estimate the illuminant. Different edge types, such as material, shadow, and highlight edges, exist in real-world images and can greatly affect the performance of illuminant estimation. This paper presents an analysis of the impact of different edge types on edge-based color constancy methods. A taxonomy is introduced to classify edge types based on their photometric properties, followed by a performance evaluation using these edge types. The results show that specular and shadow edges are more important for illuminant estimation than material edges. To address this, the paper proposes an iterative weighted Gray-Edge algorithm that emphasizes these edge types. Results from images in controlled and uncontrolled environments demonstrate improved performance compared to regular edge-based methods, with reductions in median angular error of up to 25% and 11%, respectively. 
412065	41206544	Near-Optimal Distributed Maximum Flow	We have developed a distributed algorithm that can efficiently approximate single-commodity maximum flow in undirected weighted networks. It runs in a nearly optimal number of communication rounds, (D + root n) . n(o(1)), in the CONGEST model. This is a significant improvement over the previous bound of O(n(2)) and is close to the lower bound of (Omega) over tilde (D + root n). The algorithm involves two key results: a distributed construction of a spanning tree with average stretch n(o(1)) in (D + root n) . n(o(1)) rounds, and a distributed construction of an n(o(1))-congestion approximator using O(logn) virtual trees. The algorithm uses randomization and has a high probability of success.
412066	41206631	Serving Online Requests with Mobile Servers.	This study focuses on an online problem where mobile servers must be moved to efficiently serve a set of online requests. There are n nodes and k servers that can be moved between them. Requests are issued one at a time and must be served continuously. The cost of serving requests is determined by the number of servers and requests at each node. The algorithm must ensure that the service cost remains within a certain factor and an additive term of the optimal cost. Two minimization objectives are considered: minimizing server movements and minimizing total cost. It is proven that for every k, the competitive ratio of any deterministic algorithm must be at least Omega(n). However, by extending the objective to include current service cost, a competitive ratio of 1+epsilon can be achieved with a linearly increasing additive term for every constant epsilon > 0.
412067	41206737	Frequent subgraph summarization with error control	Frequent subgraph mining is an important but challenging problem in research. The large number of discovered subgraphs makes it difficult to analyze and understand the patterns. This paper proposes a summarization model that uses an independence probabilistic model to accurately restore frequent subgraphs and their frequencies. The model allows users to specify an error tolerance and uses a top-down approach to discover summarization templates while keeping the frequency restoration error within the tolerance. Experiments on real graph datasets show that the model can effectively control the error within 10% with a concise summary.
412068	41206889	Robust face pose classification method based on geometry-preserving visual phrase	Developing a feature for accurately classifying facial poses is a difficult task due to the varying spatial layouts of key facial points. To address this challenge, a pose classification framework using local geometry-preserving visual phrases (GVP) is proposed. This framework includes a weighting strategy on GVP that improves the discriminability of individual words and spatial layouts within high order phrases. This results in a flexible and robust method that can account for multiple factors affecting facial poses. Experimental results on two databases demonstrate the effectiveness of this approach compared to other methods such as PCA, LDA, and tf-idf weighted Bag-of-Words.
412069	4120696	A Visual Silence Detector Constraining Speech Source Separation	We propose a method for separating speech signals using audio and visual information. Our algorithm first identifies periods of low mouth activity in video recordings and uses a classifier to detect silent intervals during these moments. The source separation problem is then solved for the entire recording. We tested our approach on two speech corpora with two speakers and two microphones, one with simulated room mixing and one with recorded conversations. The results were positive, with the use of the visual silence detector improving the performance of the source separation algorithm. This method shows potential for enhancing speech separation in audiovisual recordings.
412070	4120702	If: An Intermediate Representation For Sdl And Its Applications	The project discussed is focused on improving a specification/validation toolbox by integrating a commercial toolset called ObjectGEODE with other validation tools like CADP and TGV. Due to the complexity of most protocol specifications, the project has explored combining techniques such as static analysis, abstraction, and model-checking. This has resulted in the development of an intermediate representation for SDL called IF, which represents systems as timed automata communicating asynchronously through buffers or synchronization gates. This intermediate representation allows for easier integration with existing tools and provides a way to define different notions of time for SDL. The project's results have been validated through experimentation.
412071	41207164	The Pascal Visual Object Classes Challenge: A Retrospective	The Pascal Visual Object Classes (VOC) challenge is a benchmark for image recognition algorithms, consisting of a dataset with annotations and standardized evaluation software, as well as an annual competition and workshop. The challenge has five categories: classification, detection, segmentation, action classification, and person layout. This paper reviews the challenge from 2008-2012, providing insights for both algorithm designers and challenge organizers. The authors introduce new evaluation methods, such as bootstrapping and normalized average precision, to compare algorithm performance. They also analyze the progress of the community and identify common errors. The paper concludes with recommendations for future challenges and a discussion of the strengths and weaknesses of the VOC challenge.
412072	4120721	Ramsey-Type Results for Unions of Comparability Graphs	The comparability graph of a partially ordered set with $n$ elements always contains either a clique or an independent set of size $\sqrt{n}$. This also applies to graphs that are the combination of two comparability graphs on the same set of vertices, where the minimum size of a clique or independent set is $n^{1 \over 3}$. However, there are cases where the size of these sets is no more than $n^{0.4118}$. This same pattern holds for graphs that are unions of a fixed number $k$ comparability graphs, as well as for unions of perfect graphs.
412073	41207390	Efficient Object Detection and Segmentation for Fine-Grained Recognition	This article presents a new algorithm for detecting and segmenting objects in images for fine-grained recognition. The algorithm first identifies potential object regions and then segments the entire object using propagation. Additionally, the algorithm can zoom in on the object, center it, and normalize it for scale to improve recognition accuracy. The algorithm was tested on various datasets and showed significant improvements in performance, especially for difficult datasets such as bird species. It is more efficient than other methods and has been successfully applied to different types of objects. The algorithm outperforms state-of-the-art methods by as much as 11% on benchmark datasets and consistently improves the performance of a baseline algorithm by 3-4%. It also showed a 4% improvement on a large-scale flower dataset with 578 species and 250,000 images. 
412074	41207410	Multi-Output Regularized Feature Projection	Dimensionality reduction by feature projection is a widely used technique in pattern recognition, information retrieval, and statistics. When there are available outputs, it is beneficial to consider supervised projection, which takes into account both the inputs and target values. This is especially useful in applications with multiple outputs, where several tasks need to be learned simultaneously. In this paper, a new approach called Multi-Output Regularized feature Projection (MORP) is introduced, which preserves input information and captures correlations between inputs and outputs. By using a latent variable model and solving a generalized eigenvalue problem, this approach can greatly improve prediction accuracy and has shown promising results in predicting user preferences for paintings and image/text categorization. 
412075	412075118	A modular multiple classifier system for the detection of intrusions in computer networks	The protection of computer networks is crucial in modern computer systems. To combat threats, various software tools have been created. One of these is Intrusion Detection Systems, which identify intruders that manage to bypass initial security measures. A new approach to network intrusion detection is proposed in this paper, using a pattern recognition system that combines multiple classifiers. The system is designed with a modular structure, with each module detecting intrusions in a specific service offered by the network. Different feature representations of network traffic are fused together, and the effectiveness of this approach is evaluated. The potential for successful intrusion detection using classifier fusion is also discussed.
412076	41207676	PHY modulation/rate control for fountain codes in 802.11a/g WLANs.	The paper examines the combined performance of fountain codes and 802.11a/g PHY modulation and coding. Both theoretical and experimental channel models are considered, with a focus on maximizing goodput and minimizing energy. Unlike studies in cellular networks, it is found that in 802.11a/g WLANs, a cross-layer approach of using higher-layer fountain coding with PHY layer modulation and FEC coding does not result in significant improvements. The optimal PHY modulation/rate for uncoded multicast traffic is also similar to that for fountain-coded multicast traffic in various network conditions. This suggests that in 802.11a/g WLANs, cross-layer design for multicast rate control may not be beneficial and PHY layer rate control can be done without considering the use of fountain coding at higher layers.
412077	41207713	Optimal decision fusion with applications to target detection in wireless ad hoc sensor networks	Decision fusion is a method of decentralized decision making where local decisions are combined to reach a larger decision. A new approach called complementary optimal decision fusion (CODF) has been proposed for target detection in wireless ad hoc sensor networks. This method has been extensively compared to other decision fusion methods using standard datasets, and has been found to have superior performance. The CODF algorithm can also be applied to other signal processing problems involving multiple modalities, agents, and media.
412078	41207837	New Analysis of Manifold Embeddings and Signal Recovery from Compressive Measurements.	Compressive Sensing (CS) is a technique that takes advantage of the fact that a sparse signal can be accurately represented with a small number of compressive measurements. This has been proven through theoretical guarantees, showing that sparse signals can be recovered from noisy compressive measurements. This paper focuses on a different approach, using manifold models instead of sparse models. By using tools from empirical processes, the authors are able to improve upon previous results and establish bounds for embedding low-dimensional manifolds under random measurement operators. They also demonstrate the effectiveness of manifold-based models for signal recovery and parameter estimation in noisy compressive measurements. Overall, this work supports the idea that manifold-based models can be successfully used in compressive signal processing.
412079	41207989	Compressive Sensing for Background Subtraction	Compressive sensing (CS) is a new approach for image recovery that allows for using lower sampling rates than traditional methods. It relies on the sparsity of signals in certain bases, such as wavelets, to reconstruct images from a small set of random projections. This paper presents a method for directly recovering background subtracted images using CS, which has applications in communication constrained multi-camera computer vision problems. The proposed method involves casting background subtraction as a sparse approximation problem and using convex optimization and total variation to find solutions. Unlike traditional methods that learn the background, this approach learns a low dimensional compressed representation of it, making it more efficient. The paper also discusses simultaneous appearance recovery of objects using compressive measurements, and provides results on data captured using a compressive single-pixel camera. The proposed method is also shown to be effective for image coding in communication constrained problems, with results from using data captured by multiple conventional cameras for 2D tracking and 3D shape reconstruction.
412080	41208062	Randomized polygon search for planar motion detection	This paper presents a randomized algorithm for estimating the motion parameters of a planar shape without prior knowledge of point-to-point correspondences. The algorithm searches for points on two shapes measured at different times to determine the centroids, and then randomly searches for congruent polygons to determine the rotation. This approach eliminates the need for manual correspondence matching and improves the accuracy of motion parameter estimation.
412081	41208118	An Integrated Approach For Simulating Interdependencies	This paper discusses the challenges of simulating interdependent critical infrastructures and proposes a solution through a simulation framework that integrates sector-specific simulators into a general environment. The framework allows for the modeling of individual infrastructures and their internal dynamics, as well as capturing inter-domain relationships and merging information from different simulators. This approach aims to overcome the difficulties of modeling multiple heterogeneous infrastructures and expressing their internal dependencies and interdependencies. Overall, the proposed framework provides a comprehensive simulation of critical infrastructures, addressing the complex and important issue of interdependency.
412082	41208215	What groups do, can do, and know they can do: an analysis in normal modal logics	This article discusses various logics that are used to reason about agents' actions, abilities, and knowledge. These logics include Pauly's Coalition Logic CL, Alternating-time Temporal Logic ATL, the logic of 'seeing-to-it-that' (STIT), and their epistemic extensions. The authors introduce a simplification of the STIT language and propose a new semantics for it, as well as extending it to include groups. They also add a temporal operator and standard S5 knowledge operators to this logic, resulting in a new logic called E-X-Ldm G. The authors compare this logic to other epistemic extensions and conclude that it is better suited for expressing agents' abilities and strategies.
412083	41208341	The Dynamics of Epistemic Attitudes in Resource-Bounded Agents	The paper introduces a new logic for reasoning about how beliefs are formed in agents with limited resources. It distinguishes between explicit beliefs and background knowledge, using a non-standard semantics and specific axioms to capture this distinction. The logic includes mental operations of perception and inference, modeled as special model-update operations. The paper also discusses results on axiomatization, decidability, and complexity for the logic. Overall, the logic provides a formal framework for understanding how agents form beliefs through perception and inference, taking into account the limitations of their resources.
412084	41208410	Sufficient dimensionality reduction	Dimensionality reduction is a crucial problem in unsupervised learning, with applications in statistics and cross-classified data analysis. This paper introduces a new approach to this problem using information theory, which aims to preserve the mutual information contained in the original data while reducing its dimensionality. Unlike previous methods, this approach directly extracts continuous feature functions from the co-occurrence matrix, serving as approximate sufficient statistics for one variable about the other. It can also be seen as a generalized, multi-dimensional, non-linear regression, with the resulting dimension reduction described by two conjugate differential manifolds coupled through Maximum Entropy I-projections. An iterative information projection algorithm is presented and proven to converge, demonstrating its effectiveness in various applications such as text categorization and information retrieval. 
412085	41208533	FLUXO: a simple service compiler	In this paper, the authors introduce FLUXO, a system that separates the logical functionality of an Internet service from the architectural decisions made for performance, scalability, and reliability. FLUXO utilizes a dataflow-based programming model, runtime request tracing, and analysis techniques to optimize the service architecture for these factors. The paper presents the authors' vision for simplifying the construction of Internet services and demonstrates how FLUXO can be used to express various performance optimizations. By decoupling the logical functionality from architectural decisions, FLUXO allows for more flexibility and adaptability in designing Internet services.
412086	412086187	The auction: optimizing banks usage in Non-Uniform Cache Architectures	The rise of wire delay in cache design has led to non-constant access latencies in last-level cache banks. To address this issue, Non-Uniform Cache Architectures (NU-CAs) have been proposed. In chip multiprocessor (CMP) architectures, an efficient last-level cache is crucial in reducing requests to off-chip memory due to the speed gap between processor and memory. A bank replacement policy is needed to effectively manage NUCA caches, but the decentralized nature of NUCA has hindered previous policies. A new mechanism called The Auction is proposed, which allows replacement decisions to be spread across the entire cache. This enables global replacement policies, leading to improved performance and reduced energy consumption. Three approaches of The Auction are evaluated and shown to be effective in managing the cache and reducing off-chip memory requests.
412087	41208759	Towards cost-sensitive assessment of intrusion response selection	The concept of cost-sensitive intrusion response has gained attention due to its focus on balancing the potential damage caused by intrusions with the cost of responding to them. However, implementing this approach can be challenging because of the need for consistent and adaptable measurements of cost factors based on system requirements and policies. In this paper, a framework is presented for selecting cost-sensitive intrusion response. This framework includes measurements for potential costs associated with handling intrusions and an evaluation method for response effectiveness, intrusion risk, and cost. An implementation of this framework is also demonstrated using real network traffic. 
412088	41208810	A comparative analysis of biclustering algorithms for gene expression data.	This article discusses the need for new data mining methods to analyze high-dimensional biological data. Biclustering algorithms have been used to discover local patterns in gene expression data, but it is unclear which algorithms are most effective. The article addresses this issue by comparing 12 recently published or lesser-known algorithms using synthetic data sets and gene expression data from the Gene Expression Omnibus. The algorithms were evaluated based on their performance on different conditions, such as varying noise and number of biclusters. Gene Ontology enrichment analysis was also performed on the resulting biclusters. The results suggest that the choice of biclustering method and parameters should be based on the desired model and its ability to handle noise, and that algorithms capable of finding multiple models are more effective in capturing biologically relevant clusters.
412089	41208959	Improving security of virtual machines during live migrations.	Live migration of virtual machines (VMs) is a process that allows a running VM to be transferred to a new hardware component with minimal interruption. This is commonly used in cloud architectures, where users are usually unaware of and unable to prevent live migrations of their VMs. However, if a VM is live migrated to a different data center in another country, it can raise security and privacy concerns. In this paper, the authors propose methods to detect live migrations from within the affected VM and analyze how the migration process can be delayed to allow for security measures to be taken. They have developed a "live migration defence framework" (LMDF) to enforce security policies within a VM. The proposed methods and techniques were evaluated in a cloud setup and partially in the Amazon Elastic Computing Cloud (EC2). 
412090	4120904	MRT—a visualization tool addressing problems “outside” the classical rendering domain	We have developed an object-oriented software architecture for a 3D rendering environment that greatly increases productivity for programmers. The platform, called MRT, is made up of customizable building blocks that make 3D image synthesis more accessible. It is based on objects rather than drawings and has been proven to be highly customizable and extendable for a diverse user population. Our partnership with a German mobile communication network supplier resulted in the creation of a prototype package for simulating 3D radio wave distribution in urban environments within just three weeks. This was made possible by the compact and efficient nature of MRT, which also outperformed existing solutions.
412091	41209151	Learning discrete categorial grammars from structures	The discrete classical categorial grammars are a class of grammars similar to the reversible class of languages proposed by Angluin and Sakakibara. This class can be identified from positive structured examples using a new algorithm that has a quadratic time complexity. This extends previous work by Kanazawa, as our algorithm can handle multiple types associated with a word and has a polynomial time complexity. We provide linguistic examples to demonstrate the applicability of this class.
412092	41209233	Gossip Galore: A Conversational Web Agent for Collecting and Sharing Pop Trivia	This paper introduces a self-learning agent that uses information extraction, web mining, question answering, and dialogue system technologies to gather and update knowledge about celebrity gossip in the music world. The agent can answer questions about musicians, bands, and related people, and uses data mining to create a social network among them. The agent is able to have interactive conversations with users through natural language processing and can provide answers in various forms such as text, graphs, and speech. This is made possible through minimally supervised machine learning for relation extraction and a knowledge-intensive question answering technology. 
412093	41209329	Tight Lower Bounds for st-Connectivity on the NNJAG Model	The directed st-connectivity problem involves determining if there is a path from a specific node s to another node t in a directed graph. A time-space lower bound is proven for the probabilistic NNJAG model, which uses n nodes in the input graph and has a space of S and time of T. The lower bound states that if S is in O(n^(1-delta)), then T is at least 2^(log^2(n/S)); otherwise, T is at least 2^(log^2(nlogn/S)/loglogn) multiplied by the square root of nS/logn. This greatly improves previous lower bounds and is tight when S is in O(n^(1-delta)). As a result, the first tight space lower bound of at least log^2(n) is obtained for the NNJAG model, which was previously unknown. 
412094	41209465	Discovery of Narrativity on the WWW based on Perspective Information Access	The proposed framework aims to discover narrative relationships between objects on the internet by providing perspective information access. While it is easy to access web resources, it can be difficult to find information that meets specific user requests. Most people rely on search engines, but this requires specifying appropriate keywords. The proposed framework is designed for users who are seeking new information related to other information, even if the relationship between them is unknown. It offers a perspective path to guide users to the desired information, creating a narrative between the source and destination. This framework is meant to support users in their search for information.
412095	41209529	A Scenario-View Based Approach to Analyze External Behavior of Web Services for Supporting Mediated Service Interactions	This paper presents a new approach to addressing mismatches in Web service interactions by focusing on both control-flow and data-flow. Current approaches only consider control-flow, but this new approach generates scenarios and views that describe the external behavior of Web services, including data dependencies. A scenario is defined as a set of execution paths for a public process, and a view is generated to analyze this scenario. By describing the external behavior of a Web service as a set of views, this approach is useful for service modelers and users to better understand and resolve behavioral mismatches, ultimately facilitating more effective Web service interactions.
412096	41209615	ATL Transformation for the Generation of SCA Model	The Service Component Architecture specification (SCA) is a technology that supports the development, deployment, and integration of Internet applications. While it can manage dynamic availability and handle differences between components, it cannot solve all problems. As software systems continue to evolve, development and maintenance become more complex. To address this, the Model Driven Engineering (MDE) approach has been used. This paper focuses on applying MDE automation to convert UML 2.0 models to SCA models. Two metamodels, UML 2.0 component and SCA, are studied and transformation rules are defined in the ATL language to ensure traceability between them. This approach aims to simplify the development process and improve system maintenance.
412097	412097132	Optimizing Tasks Assignment on Heterogeneous Multi-core Real-Time Systems with Minimum Energy	Embedded real-time systems, especially for mobile devices, face the challenge of balancing system performance and energy efficiency. To address this issue, a study was conducted on the relationship between energy consumption, execution time, and completion probability of tasks on heterogeneous multi-core architectures. An Accelerated Search algorithm was proposed, using dynamic programming, to find a combination of task schemes that can be completed within a given time with minimum energy consumption and a desired confidence level. A Directed Acyclic Graph (DAG) was used to represent task dependencies and a Minimum-Energy Model was developed to determine the optimal task assignment. The heterogeneous multi-core architectures can adjust voltage levels with Dynamic Voltage and Frequency Scaling (DVFS), resulting in varying execution times and energy consumption. The experimental results showed that this approach outperformed existing algorithms, with a maximum improvement of 24.6%.
412098	41209858	Crafting urban camouflage	The workshop aims to explore the concept of controlling personal visibility in public spaces by using computer vision tracking systems. Participants from various fields are invited to engage in hands-on activities and discuss strategies for managing personal visibility in relation to design and technology. The focus is on creating speculative design scenarios rather than implementable designs, with the goal of expanding designers' understanding of presence in public spaces beyond the physical to include digital representations. This workshop hopes to stimulate new ideas and perspectives on the intersection of architecture, technology, and personal privacy in public spaces.
412099	41209935	Control and Data Flow Visualization for Parallel Logic Programs on a Multi-window Debugger HyperDEBU	A fine-grained highly parallel program involves multiple threads of execution, making it important to understand the overall execution situation in order to debug it effectively. This is where visualization comes in, and HyperDEBU, a debugger designed for the parallel logic programming language Fleng, helps with this by visually representing the control and data flows of the program based on the user's intention. Breakpoints, which represent the user's intention or perspective, are used by HyperDEBU to visualize the program's execution. This enables efficient debugging through the use of visual examination and manipulation tools.
412100	41210069	Teaching Petri Nets Using P3	The paper introduces P3, a Petri net software tool designed for teaching the Architecture and organization of computers (AOC) course. P3 offers a graphical modeling interface and interactive simulation with conflict resolution, as well as four analysis tools including two new ones developed specifically for learning. P3 also has the ability to share models with other Petri net tools through XML/XSLT support. The paper discusses the AOC course and compares the outcomes of students who used P3 with those who did not. It also includes feedback from teachers and students on P3's features. Overall, P3 is a useful tool for teaching and learning about Petri nets in the AOC course.
412101	41210178	The CMUnited-98 champion small-robot team.	The article discusses the research opportunities presented by robotic soccer and highlights the achievements of the CMUnited-98 small robot team. The team utilizes a multiagent system with global perception and distributed cognition and action. The hardware design of the physical robots includes differential drive, a robust mechanical structure, and a kicking device. The team has also developed a new motion algorithm that allows for successful collision-free motion in the dynamic soccer environment. At the strategic level, the team utilizes role-based behaviors and a collaboration algorithm to improve team performance. The CMUnited-98 team performed well in the RoboCup-98 games, scoring 25 goals and only suffering 6 in 5 games.
412102	41210290	A Filtering Approach to Stochastic Variational Inference.	SVI is a method that uses stochastic optimization to efficiently compute Bayesian models with large datasets. It can be viewed as an approximate parallel coordinate ascent, balancing bias and variance to approach the optimal solution from batch variational Bayes. A new model is proposed to automate this process by inferring the next optimum from a sequence of noisy observations. This allows the variational parameters to be updated using Bayes rule, rather than a pre-defined schedule. When the model is a Kalman filter, it recovers the original SVI algorithm and can also incorporate additional assumptions such as heavy-tailed noise. In experiments, this method outperforms traditional SVI and other adaptive algorithms in two different domains.
412103	412103108	Improving accuracy in path delay fault coverage estimation	A new method has been developed to estimate path delay fault coverage by counting newly sensitized path faults in a simulated vector pair. However, this estimate may be pessimistic due to shared paths. To improve accuracy, a range of approximate methods have been proposed, with higher accuracy requiring more CPU time. These methods use flags to indicate if a path segment has already been included in a previously detected fault. A fault is only counted as newly detected if it includes an unflagged segment. This method has a small overhead when there are few fan-in and fan-out branches per gate, and accuracy improves as the length of path segments increases. Results show that this approach provides accurate estimates when using short path segments.
412104	41210451	Privacy preserving decision tree learning over multiple parties	Data mining over multiple data sources poses challenges due to legal and competition concerns. Cryptographic methods have been proposed as a solution, but their complexity makes them impractical for large numbers of sources. This paper proposes an efficient algorithm for building a decision tree over an arbitrary number of distributed sources while ensuring privacy. The algorithm is based on the ID3 algorithm and allows data sources to run data mining algorithms over the union of their data without revealing any extra information to other sources. This addresses the classification problem and enables data mining in scenarios where multiple sources are involved.
412105	4121052	Robust satisfaction of temporal logic over real-valued signals	The article discusses the use of temporal logic formulae to specify constraints on the behavior of continuous and hybrid dynamical systems with uncertain parameters. It introduces different measures of robustness that indicate how close a system's trajectory is to satisfying or violating a property. A method is presented for computing these measures and their sensitivity to system and formula parameters. This approach can aid in verifying nonlinear and hybrid systems against temporal properties through simulation. It can also be applied to identify parameter subsets that guarantee a desired level of robustness in satisfying a formula.
412106	41210623	Efficient mining of recurrent rules from a sequence database	This study introduces a new problem of identifying significant recurrent rules from a sequence database. Recurrent rules follow the pattern of "whenever a series of events occurs, eventually another series of events occurs" and are useful in capturing behaviors in various domains, such as software specifications. These rules are an improvement on existing sequential and episode rules as they consider repeated events within and across sequences, and do not have a "window" limitation. The rules are formalized in linear temporal logic and a new concept of rule redundancy is introduced to efficiently mine a representative set of rules. Performance studies and a case study have demonstrated the scalability and usefulness of this approach.
412107	41210743	A stepwise optimization algorithm of clustered streaming media servers	Optimizing Clustered Streaming Media Servers (CSMS) is crucial for achieving cost-effective and high-performing systems. This involves balancing performance, quality of service (QoS), and costs. To address this, a stepwise optimization algorithm is proposed, which models the problem as a directed acyclic graph and uses a divide and conquer approach to reduce complexity and speed up the process. A simulation system is also developed based on a theoretical performance model and practical parameters to accurately generate information. A case study is presented to demonstrate the effectiveness of the algorithm and guide the design of practical CSMS systems. This approach has a significant impact on the practicality and efficiency of CSMS.
412108	41210832	Computing with semi-algebraic sets represented by triangular decomposition	This article builds upon previous work on triangular decompositions of semi-algebraic systems and introduces new theoretical results that improve these decomposition algorithms. The authors also introduce a technique called "relaxation" which simplifies the decomposition process and reduces the number of redundant components. Additionally, they propose procedures for basic set-theoretical operations on semi-algebraic sets represented by triangular decomposition. Experimentation shows that these techniques are effective. The authors also present new results on the theory of border polynomials of parametric semi-algebraic systems, including a geometric characterization of its "true boundary." These contributions aim to optimize the computation of triangular decompositions and make them more practical for real-world applications.
412109	41210936	Fast Regular Expression Matching Using Small TCAM	This paper introduces a hardware-based approach for regular expression (RE) matching using ternary content addressable memory (TCAM). This is a popular method for packet classification in modern networking devices. The proposed approach uses three techniques - transition sharing, table consolidation, and variable striding - to improve TCAM space utilization and RE matching speed. The effectiveness of these techniques was tested on eight real-world RE sets and the results showed that even small TCAMs can store large deterministic finite automata (DFAs) and achieve high RE matching throughput. For example, a 0.59-Mb TCAM chip can store eight DFAs with 25,000 states each, while a single 2.36-Mb TCAM chip can process multiple characters per transition and achieve a potential RE matching throughput of 10-19 Gb/s for each DFA. 
412110	4121100	Preventing and unifying threats in cyberphysical systems	The topic of CPS security has been widely researched, but only a few studies focus on the general aspects of threats. Most of the existing work is centered on attack detection, which helps to identify threats but does not offer solutions to prevent them. Current system approaches involve detecting and correcting inputs and outputs, but with a large number of inputs and distributed systems, it is important to identify and stop threats at their point of entry. This can be achieved by utilizing a Security Reference Architecture, which has been used to trace attacks but not to stop them. The authors propose using this approach to identify and prevent threats in CPSs, as many of them have similar effects and can be stopped in similar ways. Using threat patterns is seen as a more effective way of describing attacks compared to other models.
412111	41211141	Open Code Coverage Framework: A Framework For Consistent, Flexible And Complete Measurement Of Test Coverage Supporting Multiple Programming Languages	Test coverage is a crucial aspect in determining the effectiveness of software testing. However, current measurement tools for test coverage have various issues, including high costs, inconsistency, and inflexibility. To address these problems, a new measurement framework called the Open Code Coverage Framework (OCCF) has been proposed. This framework supports multiple programming languages and allows for independent addition of language and test coverage criteria support. Additionally, OCCF offers two methods for changing measurement range and elements, making it more flexible. In a sample tool implementation for C, Java, and Python, OCCF was able to measure four test coverage criteria and was also confirmed to support C#, Ruby, Java Script, and Lua. In a comparison experiment with non-framework-based tools, OCCF was able to reduce the required lines of code and time for implementing new test coverage criteria significantly.
412112	41211217	Validating Security Design Patterns Application Using Model Testing.	Software developers often lack specialized knowledge in security, which can result in improperly applied security patterns. These patterns, which contain reusable security knowledge, may not effectively mitigate threats and vulnerabilities if applied incorrectly. To address this issue, a new method is proposed for validating the application of security patterns. This method includes extended security patterns, such as requirement- and design-level patterns, as well as a model testing process. Early in the development process, developers specify the threats and vulnerabilities in the system, and the method then assesses whether the security patterns have been properly applied and if the vulnerabilities have been resolved.
412113	41211313	The impacts of personal characteristic on educational effectiveness in controlled-project based learning on software intensive systems development	This study examines the impact of team composition and learning process on educational effectiveness in software-intensive business systems courses. The lack of an established method to determine optimal team composition prompted the use of the Five Factors and Stress theory and modified grounded theory approach to measure personal characteristics and identify the learning process of team members. The study found that having team members with different personal characteristics leads to increased knowledge and skills acquisition. Moreover, teams that focus on a smaller number of learning process topics also demonstrate higher educational effectiveness. These findings have implications for improving educational effectiveness in similar practical courses.
412114	41211480	Towards a semantic model for Java wildcards	Java wildcards are a way to express more types in Java and expand the range of programs that can be typed. While previous research has focused on syntactic models and proofs for type systems related to Java wildcards, there has been little study on the semantics of wildcards. This paper presents a semantic model for Java wildcards based on semantic subtyping, which interprets types as sets of possible values. The model is defined in terms of runtime types, rather than the structure of runtime values, to reflect the nominal type system of Java. The model also accounts for variance introduced by wildcards and shows the soundness of syntactic subtyping. However, completeness is not guaranteed in the general case, but a restricted type language is identified for which syntactic subtyping is both sound and complete. 
412115	41211575	Lazy Bayesian Rules: A Lazy Semi-Naive Bayesian Learning Technique Competitive to Boosting Decision Trees	LBR is a learning technique that aims to improve the performance of naive Bayesian classifiers by addressing the issue of attribute interdependence. It does this by creating a conjunctive rule and using a subset of training examples to induce a local naive Bayesian classifier for classifying a test example. This approach has been found to be effective in reducing the bias and variance of the naive Bayesian classifier, resulting in improved performance. LBR is a useful tool for dealing with the limitations of naive Bayesian classification and can lead to better classification results. 
412116	41211619	Contractible subgraphs in 3-connected graphs	The article discusses a conjecture by McCuaig and Ota regarding the existence of a contractible subgraph in 3-connected graphs. A subgraph H of a 3-connected graph G is considered contractible if H is connected and G with H removed is still 2-connected. The conjecture states that for any value of k, there exists a function f(k) such that any 3-connected graph with at least f(k) vertices will have a contractible subgraph with k vertices. The article presents a proof of this conjecture for k ⩽4 and explores its implications in various types of graphs, such as maximal planar graphs, Halin graphs, and AT-free graphs.
412117	41211755	A context management infrastructure with language integration support	Context-management systems have been developed to support context-aware applications. These systems usually have APIs and query languages for analyzing context. However, they do not have effective support for reacting to context changes within the constraints of the framework. In this paper, a new context-management system is presented that combines context reasoning with context-dependent behavior using dynamic adaptation techniques like aspect- and context-oriented programming. The system offers various levels of integration with programming language extensions and allows for dynamic aggregation of local and distributed context sources. A query library for the JCop language has been implemented as a first step, and an example application is demonstrated to showcase its capabilities.
412118	41211841	Another Look at Complementation Properties	This paper discusses a variety of attacks based on the complementation property of DES. The authors use symmetry relations in the key schedule and rounds to create distinguishers for any number of rounds, leading to a generalization of the complementation property. They also explore the use of fixed points in these relations to develop new types of attacks. The paper presents a self-similarity property in the SHA-3 candidate Lesamnta, which reveals unexpected weaknesses in its compression function. The authors demonstrate how this property can be used to find collisions much faster than generic attacks. Additionally, a related-key differential attack on round-reduced versions of the XTEA block-cipher is proposed, exploiting weaknesses in the key-schedule to recover the secret key more efficiently. The authors also identify a class of weak keys that can be used to attack a large number of rounds in the cipher.
412119	41211973	A Logic Programming Approach to Scientific Workflow Provenance Querying	Scientific workflows are essential for facilitating and expediting scientific discoveries. These workflows integrate and organize diverse data and services for conducting virtual experiments. To ensure the reproducibility and validation of scientific results, the management and querying of provenance information has become crucial. This paper presents a logic programming approach, FLOQ, for querying and managing scientific workflow provenance. The authors identify important characteristics for a provenance query language and demonstrate how FLOQ meets these requirements. They also show how their previous model, virtual data schema, can be easily translated into FLOQ. The paper includes examples to demonstrate the effectiveness and versatility of FLOQ, including solving the provenance challenge queries. 
412120	41212059	CiNCT: Compression and retrieval for massive vehicular trajectories via relative movement labeling.	This paper introduces a compressed data structure for representing moving object trajectories in a road network. Unlike existing methods, this structure supports pattern matching and decompression from any position while maintaining high compression rates. It is based on the FM-index, a data structure known for its fast and compact pattern matching abilities. To further improve compression, the authors incorporate the sparsity of road networks and introduce the concepts of relative movement labeling and PseudoRank. Experimental results show that their method outperforms existing trajectory compression methods and FM-index variants in terms of data size and query processing time.
412121	41212136	Managing Process Model Complexity Via Abstract Syntax Modifications.	The use of Business Process Management (BPM) technology has become increasingly popular, requiring stakeholders to understand and agree upon the process models used in BPM systems. However, users often struggle with the complexity of these models. The main challenge is to enhance the understanding of process models, and there is a significant amount of literature devoted to this topic.
412122	41212227	Achieving side-channel protection with dynamic logic reconfiguration on modern FPGAs	Reconfigurability is a key feature of modern FPGA devices, allowing for different hardware circuits to be loaded on demand. This makes it difficult for attackers to predict what will happen at a specific location in the FPGA. The authors present a hardware implementation of the PRESENT cipher with built-in side-channel countermeasures using dynamic logic reconfiguration. Their design utilizes Configurable Look-Up Tables (CFGLUT) in Xilinx FPGAs to quickly change the hardware internals of the cipher, improving resistance against side-channel attacks. Experimental results on a Spartan-6 platform show that even with 10 million recorded power traces, the first-order leakage cannot be detected with state-of-the-art methods.
412123	4121235	ASSIST: adaptive social support for information space traversal	The paper discusses the challenge of finding relevant information in a vast hyperspace, which has been extensively studied. The use of social systems, such as Web 2.0 technologies, for retrieval tasks has become increasingly popular. These systems collect and use community knowledge to benefit users. The paper proposes a form of retrieval that combines multiple sources of social wisdom, specifically social search and social navigation. A personalized engine called ASSIST is introduced, which integrates social support mechanisms for information repositories. The goal is to improve retrieval in the hyperspace and an empirical study of the technology's effectiveness is presented.
412124	41212498	Set similarity join on massive probabilistic data using MapReduce	This paper discusses the challenge of efficiently performing set similarity join on large probabilistic datasets using the MapReduce paradigm. The authors propose two approaches, Hadoop Join by Map Side Pruning and Hadoop Join by Reduce Side Pruning, which use different methods to filter out candidate pairs and reduce the comparison cost. They also present a hybrid solution that combines both approaches. Experiments on Hadoop-0.20.2 show that these approaches outperform the naive method of Block Nested Loop Join and have good scalability. This is the first work to address this problem using MapReduce and provides a new method for processing massive probabilistic data.
412125	41212538	An abstract formalization of correct schemas for program synthesis	This paper discusses the importance of incorporating both structured program design principles and domain knowledge into program schemas for hierarchical program synthesis. While most current approaches only focus on the syntactic aspect of schemas, this paper proposes a semantic approach that includes a formalized template and a first-order axiomatization of the problem domain. This framework allows for a formal definition of correctness and enables the use of correct schemas to guide program synthesis. By incorporating both syntactic and semantic elements, this approach aims to improve the effectiveness and accuracy of program synthesis.
412126	41212690	Provably secure authenticated key agreement scheme for distributed mobile cloud computing services.	Mobile cloud computing has become increasingly popular, but this also brings concerns about security in distributed environments. In 2015, Tsai and Lo proposed a privacy-aware authentication scheme for these services, but it was found to be vulnerable to server impersonation and leaks of sensitive information. To address these issues, a new authentication scheme with stronger security features was proposed. It was rigorously analyzed and found to be secure against various attacks. The scheme also has lower computational costs for mobile users and was evaluated using a network simulator. This new scheme is considered more suitable for practical applications compared to the previous one proposed by Tsai and Lo.
412127	41212745	Exploring the Design Space of AAC Awareness Displays.	Augmentative and alternative communication (AAC) devices are crucial for individuals with speech disabilities. However, one challenge with these devices is their inability to convey nonverbal communication cues that usually complement or substitute for verbal speech. This paper explores the design of awareness displays that can supplement AAC devices, considering their impact on how the users are perceived by others. The design process involved creating prototypes and receiving feedback from people with ALS, their caregivers, and communication partners. The study found a tension between abstractness and clarity in the designs and also raised concerns about how these displays may further label individuals using AAC as "different." Overall, this research provides insights into the design of AAC awareness displays to enhance communication for individuals with disabilities.
412128	41212862	Vehicle Orientation Analysis Using Eigen Color, Edge Map, And Normalized Cut Clustering	The paper introduces a new method for estimating the orientation of vehicles in still images. This is achieved through a clustering framework that utilizes eigen color and edge mapping. A novel color transform model is used to segment vehicles from their background, which is invariant to changes in contrast, background, and lighting. However, this model alone cannot accurately extract the shape of the vehicle, so the distributions of edges and colors are combined to form a high-dimensional feature space. This is then reduced using normalized cut spectral clustering, which minimizes dissimilarity between groups. The orientation can be determined by analyzing the eigenvectors. Unlike traditional methods, this approach only requires one still image and has shown promising results in experiments.
412129	41212948	Ten Blue Links on Mars.	In this paper, the authors discuss the challenge of providing a high-quality search experience on Mars, where the speed-of-light propagation delays result in significant latency. They explore the feasibility of overcoming this challenge and providing a tolerable user experience by considering the tradeoff between waiting for responses from Earth and pre-fetching or caching data on Mars. The authors present two case studies that use publicly available data to demonstrate the effectiveness of baseline techniques. They argue that this research problem is not only relevant for potential Mars colonization, but also for Earth-based scenarios with similar constraints, such as searching from rural villages in India. 
412130	41213052	Designing a Physical Aid to Support Active Reading on Tablets	Tablet computers and eReaders are becoming more popular for reading, but it is believed that print documents are better for in-depth understanding. Previous attempts to improve digital reading have focused on mimicking physical features or adding digital tools, but this paper proposes combining both approaches. A study was conducted to identify the needs of users while actively reading on tablets, resulting in the design of a smart bookmark to support active reading. The paper also outlines an interaction space for the bookmark and presents a user study to evaluate its effectiveness.
412131	41213160	Novel Modalities For Bimanual Scrolling On Tablet Devices	The paper discusses two studies that examine the use of new methods for scrolling on tablet devices using both hands. The techniques involve a combination of physical dial, touch, and pressure input to control the speed and direction of scrolling. The effectiveness of these techniques is compared to traditional one-handed techniques in a targeted task. The results show that participants were able to select targets more quickly and with less effort when using the bimanual techniques. This suggests that bimanual scrolling may be a more efficient and user-friendly option for tablet devices.
412132	41213241	Two-way source coding with a common helper	The two-way rate-distortion problem involves a helper sending a limited-rate message to both users using side information. This results in a Markov form of Helper-User 1-User 2. The achievable rates and distortions for this setup are characterized, and it is found that a binning scheme similar to Wyner-Ziv can be used to achieve the optimal rate. The side information at the decoder is the "further" user, User 2. The proof of this result uses a new technique involving undirected graphs to verify the Markov relations.
412133	41213349	Extended analysis of motion-compensated frame difference for block-based motion prediction error.	In the past, designing and optimizing hybrid video codecs often relied on trial and error. However, having a theoretical model is important in order to understand and improve existing codecs. In this paper, a first-order Markov model is used to create an approximate model for the block-based motion compensation frame difference signal. The model assumes that pixel deformation is directional rather than uniform within a block. The study also shows that the accuracy of motion-compensated codecs is affected by imperfect block-based motion compensation. Experimental results demonstrate that the derived model accurately describes the statistical characteristics of the MCFD signal and reveals the impact of imperfect motion compensation on the performance of video codecs.
412134	41213434	Recommendation system for HBB TV: Model design and implementation	Hybrid broadcast and broadband (HBB) television has opened up new possibilities for services, including personalized recommendation systems. With the vast number of programs available, it is impossible for viewers to keep track of everything in real time. Recommendation engines were created to address the issue of information overload and make it easier for viewers to find content they are interested in. This paper presents the design and implementation of a recommendation system for HBB TV. The system uses an enhanced Naïve Bayes model to predict user ratings based on past observations. It is flexible and robust, able to handle sparse data sets and has shown promising results in experiments using a Yahoo movie data set. 
412135	41213516	Visual tracking via shallow and deep collaborative model.	The paper proposes a robust tracking method that combines a generative model and a discriminative classifier. Features are learned by shallow and deep architectures, with the generative model using a block-based incremental learning scheme and the discriminative model using deep learning techniques. The generative model also incorporates a local binary mask to handle occlusion, while the discriminative model learns generic features that are robust to background clutters and foreground appearance variations. The two models work together to achieve a balance in handling occlusion and target appearance change, which are challenging factors in visual tracking. The proposed method is evaluated against state-of-the-art algorithms and shown to be accurate and robust.
412136	41213648	Generic Attacks on Secure Outsourced Databases.	Various protocols have been suggested for securely outsourcing database storage to a third party server, ranging from those with strong cryptographic security to more practical implementations. However, new attacks have shown that confidentiality of the data can still be compromised. The need for a formal understanding of the efficiency and privacy trade-off in outsourced database systems is recognized, and abstract models are proposed to capture them. These models identify two sources of leakage - access patterns and communication volume. Generic reconstruction attacks are developed for systems supporting range queries, where either of these sources is leaked. These attacks are shown to be optimal and can successfully recover secret attributes of all records in a database after a certain number of queries. Experimental results demonstrate the effectiveness of these attacks on real datasets.
412137	41213769	Automatic initialization for facial analysis in interactive robotics	This paper discusses the importance of the human face in communication and presents a soft real-time vision system that allows a robot to analyze faces and recognize facial expressions as non-verbal cues. The system uses a robust detection scheme to identify faces and basic facial features, and then extracts facial parameters using active appearance models and support vector machine classifiers to identify individuals and their facial expressions. The paper evaluates four different methods for initializing the AAM algorithm and their performance in classifying facial expressions using the DaFEx database and real-world data from a robot's perspective.
412138	41213869	How to forget a secret	A new type of attack has been discovered that can affect any cryptographic protocol. This attack exploits the physical memory of a participant, allowing the attacker to access all previous states. To protect against this type of attack, a new cryptographic primitive called "erasable memory" has been introduced. This primitive allows for the secure deletion of secret information. By using a small amount of erasable memory, a large non-erasable memory can be transformed into a large erasable memory. This transformation can be achieved using a simple assumption and can be implemented using a block cipher, making it efficient. Suggestions for implementing small amounts of erasable memory are also provided.
412139	412139108	The principled design of large-scale recursive neural network architectures--dag-rnns and the protein structure prediction problem	The article proposes a methodology for designing large-scale recursive neural network architectures called DAG-RNNs. This methodology involves three steps: representing a domain using directed acyclic graphs, parameterizing relationships between variables using feedforward neural networks, and applying weight-sharing within subsets of the graphs. This approach allows for efficient processing and training of various data structures with different sizes and dimensions. The resulting models are probabilistic, but their internal dynamics are deterministic, making them suitable for tackling large-scale problems. The article also discusses specific classes of DAG-RNN architectures based on lattices, trees, and structured graphs, and their applications in predicting protein structural features. The article concludes by discussing extensions, relationships to graphical models, and implications for neural architecture design. The protein prediction servers can be accessed online at www.igb.uci.edu/tools.htm.
412140	4121404	On Cognitive Dynamic Systems: Cognitive Neuroscience and Engineering Learning From Each Other	The concept of cognitive dynamic systems combines engineering and cognitive neuroscience to create a better understanding of cognitive perception and control. Bayesian inference is used to improve sparse coding, a method used in neuroscience, and incorporate information filtering to enhance cognitive perception. Bellman's dynamic programming is then applied to cognitive control, resulting in a new reinforcement learning algorithm with desirable properties. The paper also addresses how to integrate cognitive control and perception, as well as how to manage risk, by using probabilistic reasoning and a preadaptive control mechanism. This mechanism involves a closed-loop feedback structure that utilizes past experiences to control motor actions and executive attention.
412141	41214132	The Prediction of Student First Response Using Prerequisite Skills	Educational data analytics has mostly focused on predicting student next problem correctness, but this may not be the most useful for teachers. Instead, it is important to predict more meaningful aspects of student knowledge over a longer period of time. This paper proposes a method that uses prerequisite information, recorded by learning systems like ASSISTments and Khan Academy, to predict students' initial knowledge on a subsequent skill. Comparing this method to the standard Knowledge Tracing model and majority class, the results show that it is a reliable way to predict student knowledge without sacrificing accuracy. This method could help teachers better understand and support their students' learning.
412142	41214276	Stochastic optimization for collision selection in high energy physics	Artificial intelligence is becoming increasingly important in basic science research, particularly in high energy physics. AI methods can assist in precision measurements, such as determining the mass of the top quark. This is achieved by selecting collisions at high energy particle accelerators that produce top quarks, while minimizing other particles (background). Current methods for collision selection rely on heuristics or supervised learning, which are not always optimal. A new approach using stochastic optimization has been developed to directly search for selectors that minimize statistical uncertainty in the top quark mass measurement. This method has been shown to significantly improve the accuracy of the measurement, contributing to our understanding of the top quark. 
412143	41214335	Weighted Online Problems with Advice.	The class is concerned with online problems where each request must be accepted or rejected and the goal is to minimize or maximize the number of accepted requests while maintaining a feasible solution. All -complete problems have the same advice complexity. This paper studies weighted versions of problems in , where each request has a weight and the aim is to minimize or maximize the total weight of accepted requests. Unlike unweighted versions, there is a significant difference in the advice complexity of complete minimization and maximization problems. The techniques for dealing with weighted requests can also be applied to non-complete problems like Matching and scheduling, achieving better results.
412144	4121449	Asynchronous Multiparty Computation: Theory and Implementation	The proposed protocol is an asynchronous method for multiparty computation that ensures perfect security. It has a communication complexity of n^2|C|k, with n being the number of parties, |C| being the size of the arithmetic circuit, and k being the size of elements in the underlying field. The protocol guarantees termination by allowing a preprocessing phase to finish without releasing any information. It is as efficient as a passively secure solution, and can withstand an adaptive and active adversary corrupting less than n/3 players. A software framework called VIFF is also presented, which allows for automatic parallelization of secure operations. Benchmarking results show that this protocol can be applied to practical and complex computations.
412145	41214563	An ASP-Based Data Integration System	Information integration systems play a key role in combining data from different sources to provide users with a unified view, known as a global schema. While simple integration scenarios have been extensively studied and efficient systems exist, challenges arise when constraints on the data quality are imposed, leading to ambiguous results. Significant research has been devoted to addressing these challenges, but no efficient system has been implemented yet. This paper presents a new data integration system utilizing Answer Set Programming (ASP) and various optimizations to enable consistent query answering (CQA) for large datasets. This system aims to improve the integration process and provide more reliable results.
412146	41214633	The DLV system for knowledge representation and reasoning	Disjunctive Logic Programming (DLP) is a powerful formalism for representing and reasoning about knowledge, capable of expressing properties of finite structures that are decidable in the complexity class ΣP2 (NPNP). This makes DLP more expressive than normal logic programming, which is limited to properties decidable in NP. Disjunction in DLP allows for simpler and more natural representation of problems of lower complexity. DLV is a widely used implementation of DLP and is able to handle complex problems up to ΔP3-complete problems. It also allows for the use of weak constraints to express optimization problems. DLV has been extensively tested and benchmarked, showing its potential for applications such as knowledge management and information integration.
412147	4121476	Lexical Selection for Cross-Language Applications: Combining LCS with WordNet	This paper discusses experiments conducted to test the usefulness of large-scale resources for lexical selection in machine translation (MT) and cross-language information retrieval (CLIR). The authors propose that verbs with similar argument structures have shared meaning components that are more relevant for argument realization than for individual verb meanings. This is supported by the finding that verbs with similar argument structures, as represented in Lexical Conceptual Structure (LCS), are rarely synonymous in WordNet. Based on this, the authors developed an algorithm for cross-language selection of lexical items that combines the strengths of both LCS and WordNet. The algorithm was implemented using the Parka Knowledge-Based System and tested on a knowledge base containing both types of information.
412148	41214852	On minmax theorems for multiplayer games	The authors present a generalization of von Neumann's minmax theorem to a class of separable multiplayer zero-sum games. These games are polymatrix, where every edge represents a two-player game with a zero total sum of payoffs. This generalization implies convexity of equilibria, polynomial-time tractability, and convergence of no-regret learning algorithms to Nash equilibria. This class of games is the broadest class to which tractability results can be applied. The authors also explore generalizations to non-constant-sum games, specifically polymatrix games with strictly competitive games on their edges. They show that the complexity of these games is PPAD-complete, indicating a significant difference from zero-sum games. Additionally, they examine the role of coordination in networked interactions and show that finding a pure Nash equilibrium in coordination-only polymatrix games is PLS-complete, while combining coordination and zero-sum games results in PPAD-completeness. 
412149	41214962	Extending FolkRank with content data	The article discusses the problem of recommending tags for new or untagged documents in real-world tagging datasets. Most existing approaches focus on artificially created datasets where the user and document information is known, rather than addressing the challenge of recommending tags for new documents. In this paper, the authors propose a novel adaptation to the FolkRank algorithm by incorporating content data. This allows FolkRank to recommend tags for new documents based on their textual content. The results show that this adaptation improves FolkRank's performance on full tagging datasets, but a simpler content-aware tag recommender outperforms it. The authors suggest that further optimization of FolkRank's weighting method is needed for better results.
412150	41215015	Visually exploring movement data via similarity-based analysis	The article discusses the importance of data analysis and knowledge discovery in moving object databases, particularly for tasks like traffic management and location-based services. Similarity search over trajectories is crucial for these tasks, but existing methods often use generic metrics that do not consider the complexity of trajectory data. To address this issue, the authors propose a framework that includes various trajectory similarity measures based on both primitive and derived parameters. These measures can be used for trajectory data mining, such as clustering and classification. The proposed measures are evaluated through experiments on synthetic and real trajectory datasets, highlighting their effectiveness and potential for visual analytics in trajectory data analysis. 
412151	41215169	Recursive Structure And Motion Estimation From Noisy Uncalibrated Video Sequences	This paper introduces a new approach for estimating structure and camera focal length and motion. The method combines discrete and continuous methods to effectively handle image noise and outliers. It builds upon an existing framework and incorporates a simple structure estimation scheme. This allows for the system to handle varying focal lengths of the camera. The structure obtained from previous image frames is used to improve estimates for the current frame, and a RANSAC outlier rejection scheme is utilized. The system's performance is demonstrated through simulated experiments.
412152	41215265	Sensor Noise Modeling Using The Skellam Distribution: Application To The Color Edge Detection	The Skellam distribution is proposed as a noise model for CCD or CMOS cameras, derived from the Poisson distribution of photons that determine the sensor response. It can measure intensity differences in both spatial and temporal domains, and the parameters are linearly related to pixel intensity. This allows for automatic detection of color differences and edge detection in images. The approach does not require Gaussian smoothing and is able to extract fine details of image structures, such as edges and corners, without being dependent on camera settings. The algorithm only needs a confidence interval for a hypothesis test, making it a more efficient and accurate method.
412153	41215358	Using evolutionary neural networks to test the influence of the choice of numeraire on financial time series modeling	This study introduces an evolutionary approach to testing the impact of different numeraires on financial time series modeling. The method utilizes evolutionary neural networks, which evolve both the network structure and connection weights, along with a new similarity-based crossover technique. The study focuses on two highly traded financial time series expressed in their trading currency, as well as alternative numeraires such as gold, silver, and the euro. The goal is to compare the performance of these different numeraires and their effect on financial time series modeling. This approach may provide insights into the importance of choosing the right numeraire in financial analysis. 
412154	41215468	Modifying edges of a network to obtain short subgraphs	The paper addresses the problem of finding an optimal reduction strategy for a given edge weighted graph with a budget constraint. The goal is to minimize the total length of a minimum spanning tree in the modified network while staying within the budget. The problem is proven to be NP-hard, but the paper presents polynomial time approximation algorithms for a broad class of cost functions. Improved algorithms are also presented for a specific class of graphs with linear cost functions. These results can be applied to other network design problems such as Steiner trees and generalized Steiner networks. 
412155	41215545	Topology control in constant rate mobile ad hoc networks	Topology control is a crucial issue in ad hoc networks, where power levels must be assigned to nodes to create a desired network topology while minimizing energy consumption. While previous research has focused on stationary networks, this paper addresses topology control in mobile wireless ad hoc networks (MANETs). A model called the constant rate mobile network (CRMN) is introduced, where the speed and direction of each moving node are known. The goal is to minimize the maximum power used by any node in maintaining a specific graph property, such as network connectivity. The paper presents general frameworks for solving both the decision and optimization versions of the topology control problem under the CRMN model, with efficient algorithms for specific graph properties like network connectivity. This work paves the way for developing efficient and reliable distributed algorithms for topology control in MANETs.
412156	41215623	Strategies for mapping dataflow blocks to distributed hardware	Distributed processors must strike a balance between communication and concurrency when distributing instructions among processors. The level of concurrency plays a significant role in determining the importance of this balance. Higher concurrency allows for wider distribution of instructions, while lower concurrency benefits from mapping dependent instructions close together to reduce communication costs. This study examines these tradeoffs for distributed Explicit Dataflow Graph Execution (EDGE) architectures, which execute dataflow instructions in blocks. The results show that the best approach for mapping blocks to cores varies based on the width of the cores. An adaptive strategy that varies the number of cores for each block improves performance over fixed strategies for single and dual-issue cores, but the benefits decrease as the cores' issue width increases. These findings suggest that choosing the right runtime block mapping strategy can increase average performance by 18% while reducing average operand communication by 70%, making it a promising method for balancing communication and concurrency in distributed processors. 
412157	4121579	Transparent Offloading and Mapping (TOM): Enabling Programmer-Transparent Near-Data Processing in GPU Systems.	Modern GPU systems face a critical bottleneck in main memory bandwidth due to limited off-chip pin bandwidth. 3D-stacked memory architectures have the potential to alleviate this bottleneck by directly connecting a logic layer to the DRAM layers with high bandwidth connections. Recent research has shown that offloading bandwidth-intensive computations to a GPU in each logic layer can significantly improve performance. However, the challenge lies in enabling computation offloading and data mapping without burdening the programmer. To address this, the paper introduces two new mechanisms: a compiler-based technique to automatically identify code for offloading and a software/hardware cooperative mechanism to predict and place frequently accessed memory pages in the closest memory stack. These mechanisms, known as TOM, are able to transparently improve performance by up to 76% without requiring any program modifications.
412158	4121584	An Algorithm for Multi-Criteria Optimization in CSPs	Constraint Satisfaction and Optimization are crucial concepts in Artificial Intelligence, but in real-life applications, it may be necessary to optimize multiple functions simultaneously. This paper introduces a new algorithm for solving Multi-Criteria Optimization problems in this scenario. The algorithm is complete, meaning it can find all non-dominated solutions, and it does not rely on any assumptions about the constraints or objective functions. It utilizes Point Quad-Trees to efficiently represent the non-dominated frontier and provides superior results compared to other commonly used methods.
412159	41215983	Complexity Analysis of Late Binding in Dynamic Object-Oriented Languages	This article discusses the complexity of implementing late binding in dynamic object-oriented programming languages. Late binding refers to the process of resolving procedure calls to method definitions at runtime, rather than during compilation. This is a crucial feature of object-oriented programming and is commonly found in languages like CLOS and Prototype-based languages. The article explores the challenges and potential solutions for efficiently supporting late binding in these types of languages.
412160	412160230	Joint Channel Estimation and Data Detection for MIMO-OFDM Two-Way Relay Networks	This paper discusses multi-input multi-output (MIMO) two-way relay networks that use orthogonal frequency-division multiplexing (OFDM) modulation. These networks involve two end users exchanging information through a relay in two phases. The relay amplifies and broadcasts signals in the second phase, reducing the time needed for information exchange compared to traditional one-way relay networks. The paper proposes an iterative algorithm for each end user to jointly estimate channel information and detect transmitted data. This involves estimating composite channels and noise covariance matrices resulting from the relay's amplification. The Expectation Conditional Maximization (ECM) algorithm is used for this estimation. Simulation results show that the proposed algorithm performs similarly to having perfect channel information. 
412161	4121619	DBGlobe: a service-oriented P2P system for global computing	The DBGlobe project aims to address the challenge of peer-to-peer computing by developing a data management system for data and services hosted by distributed, autonomous, and possibly mobile peers. The approach used is a service-oriented one, where data is encapsulated in services. The system also supports direct querying of data through an XML-based query language. The research covers topics such as infrastructure support for mobile peers and context-dependent communities, metadata management for services and peers, efficient routing of path queries on hierarchical data, and querying using the AXML language which includes service calls within XML documents.
412162	41216272	Image-Based Food Calorie Estimation Using Knowledge on Food Categories, Ingredients and Cooking Directions.	This paper discusses the importance of accurately estimating food calories from images for mobile applications. It highlights the challenges of current methods that require human help or limited food categories and multiple images. To address this, the paper proposes using deep learning to simultaneously learn food calories, categories, ingredients, and cooking directions. Two datasets were created for the experiment, one from Japanese recipe sites and one from an American recipe site. The results showed that the multi-task CNN outperformed the single-task CNN in both food category and calorie estimation. This was seen in a 0.039 improvement in correlation coefficient for the Japanese dataset and a 0.090 improvement for the American dataset.
412163	41216318	Metamodel-Based Decision Support System For Disaster Management	Software model developers typically use a general purpose language, such as Unified Modelling Language (UML), to create domain application models. However, when these models do not fully meet their needs, a more specific domain modelling language can be a better option. In this paper, a Disaster Management (DM) metamodel is introduced, which can be used to develop a disaster management language. This metamodel serves as a representation of DM expertise and can be used to create a decision support system for DM. The paper outlines the process of creating the metamodel and explains how it can be used to combine and match different DM activities for a specific disaster. This metamodel is a crucial component in creating a decision support system that streamlines access to DM expertise.
412164	41216431	OPPL-Galaxy, a Galaxy tool for enhancing ontology exploitation as part of bioinformatics workflows.	Biomedical ontologies are important for organizing and sharing data in the Life Sciences Semantic Web. However, working with and enriching these ontologies requires flexible and efficient tools. The Ontology Pre Processor Language (OPPL) is an OWL-based language designed to automate changes in ontologies. A new tool, OPPL-Galaxy, integrates OPPL into the Galaxy framework, allowing for automated ontology manipulation. This combination of OPPL and Galaxy can enhance the analysis and exploitation of biomedical ontologies, including automated reasoning. Use cases have demonstrated the effectiveness of OPPL-Galaxy for enriching, modifying, and querying ontologies. Overall, this integration opens up new possibilities for advanced biological data analysis.
412165	41216558	A methodology for extracting ontological knowledge from spanish documents	The paper discusses a method for extracting knowledge from Spanish natural language texts using a combination of NLP techniques, ontological technology, and case-based reasoning. This approach has been applied in the field of oncology and the results are presented and discussed. The method involves analyzing text fragments, representing knowledge using ontologies, and applying a case-based reasoning methodology. The paper highlights the effectiveness of this approach and its potential for extracting knowledge from other domains as well.
412166	41216617	Robust Higher Order Potentials for Enforcing Label Consistency	The paper introduces a new framework for labelling problems that combines multiple segmentations using higher order conditional random fields. This method uses potentials defined on pixel sets generated by unsupervised segmentation algorithms to enforce label consistency in image regions. The framework utilizes the Robust P n model, which is more general than the commonly used P n Potts model, and optimal swap and expansion moves can be computed by solving a st-mincut problem. This allows for the use of powerful graph cut algorithms for inference. The framework is tested on multi-class object segmentation and results show significant improvements in object boundary definition. The method has potential for use in other labelling problems. 
412167	41216716	AC: composable asynchronous IO for native languages	The paper discusses AC, a set of language constructs that allow for composable asynchronous IO in native languages like C and C++. This approach allows for multiple IO requests to be issued by a single thread and for long-latency operations to overlap with computation. Unlike traditional asynchronous IO interfaces, AC maintains a sequential programming style without requiring multiple threads or chains of callbacks. The language includes an "async" statement for concurrent IO operations, a "do..finish" block for waiting until async work is complete, and a "cancel" statement for cancelling unfinished IO. The paper provides an operational semantics for the language and evaluates its performance on the Barrelfish research OS and Microsoft Windows. Results show that AC offers comparable performance to existing interfaces while providing a simpler programming model.
412168	41216822	Re-structuring, re-labeling, and re-aligning for syntax-based machine translation	The article discusses how the structure of bilingual material used in standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems. To improve the accuracy of these systems, the authors propose three modifications to the training data: re-structuring the syntactic structure of training parse trees, re-labeling bracket labels, and re-aligning word alignments across sentences. These modifications are implemented using the EM algorithm and have shown to lead to an overall 1.48 BLEU improvement on the NIST08 evaluation set in Chinese/English translation. Combining all three techniques results in the greatest improvement. 
412169	41216956	Locating causes of program failures	Software failures can be caused by various defects in the program. To identify the specific defect that leads to a failure, programmers compare the program states of a failing run and a successful run. However, these state differences can occur anywhere in the program run, making it difficult to pinpoint the exact cause. To address this issue, programmers focus on the relevant variables and values that are associated with the failure, as well as the moments where these variables undergo transitions. This method has been found to be twice as effective in locating the defect compared to previous methods. For example, if the variable argc was initially 3, and at shell_sort(), variable [2] was 0, this would lead to a program failure.
412170	41217016	Iterative criteria-based approach to engineering the requirements of software development methodologies	Software engineering involves developing and designing software based on the requirements of the target software. Similarly, engineering a software development methodology (SDM) also requires identifying the requirements of the target methodology. The authors propose an iterative method for eliciting and specifying the requirements of a SDM by using existing methodologies as supplementary resources. This method is part of a larger methodology engineering process aimed at designing and implementing a target methodology. The initial set of requirements is identified through analyzing the development situation and refining them through iterative application to relevant methodologies. The final set of requirements is based on the qualities desired in the target methodology, using knowledge gained from existing methodologies. This approach results in a more complete and rigorous set of requirements for the target methodology. An example is provided to demonstrate the effectiveness of this method in identifying the requirements of a general object-oriented SDM. 
412171	4121713	A conditional neural fields model for protein threading.	Alignment errors are a major issue in current protein modeling methods, particularly for proteins with low sequence identity. A new method, called CNFpred, has been developed to improve the accuracy of sequence-template alignment. It uses a probabilistic graphical model called a Conditional Neural Field (CNF) and a non-linear scoring function that considers various protein sequence and structure features. This method has been trained using a quality-sensitive approach, resulting in improved alignments compared to traditional methods. CNFpred performs well on both small and large datasets, and is especially effective for proteins with sparse sequence profiles. This approach can also be applied to protein sequence alignment. 
412172	4121725	Establishing an Informed S-BPM Community.	Subject-oriented Business Process Management (S-BPM) is a new approach to modeling and executing business processes that involves a shift from traditional functional flow-oriented methods. This paradigmatic shift requires organization designers and developers to change their mindset from thinking in terms of functions to thinking in terms of actor interactions. To facilitate this change, a Community of Practice based on education is proposed as an initiative to trigger a shift towards S-BPM. By revisiting traditional methods and experiencing alternatives, this approach aims to effectively implement S-BPM in organizations.
412173	4121731	Not Just An Empty Threat: Subgame-Perfect Equilibrium In Repeated Games Played By Computationally Bounded Players	This article discusses the problem of finding subgame-perfect equilibria in repeated games. Previous research has shown that finding a Nash equilibrium in these games is efficient if players are computationally bounded, but becomes extremely difficult if this assumption is removed. However, Nash equilibrium is considered a weak solution concept for extensive-form games. This study introduces a new notion of subgame-perfect equilibrium for computationally bounded players and demonstrates an efficient method for finding such equilibria in repeated games, assuming standard cryptographic hardness assumptions. Additionally, the algorithm can be applied to games with a finite number of players and constant-degree graphical games. 
412174	41217443	Quantifying paedophile activity in a large P2P system	The presence of paedophile activity on peer-to-peer (P2P) systems is a major concern for society, as it has significant implications for child protection, policy making, and internet regulation. However, due to limited traces and analysis methods, current knowledge of this activity is limited. To address this, researchers focused on the eDonkey P2P system and collected millions of keyword-based queries. They developed a tool to detect paedophile queries and established its accuracy through expert assessment. Using this tool, they estimated that 0.25% of all queries on eDonkey are paedophile in nature, and that over 0.2% of users enter such queries. These are the most precise and reliable statistics on this issue to date. 
412175	4121758	StarT-Voyager: A Flexible Platform for Exploring Scalable SMP Issues	StarT-Voyager is a machine used for research in cluster system communication. It consists of a network interface unit (NIU) that connects a PowerPC-based SMP to the MIT Arctic network. The NIU is highly flexible, with its functions easily modifiable through firmware or programmable hardware. This allows for comparison of different communication interfaces and strategies. The NIU has a fast embedded processor and large FPGAs, making it efficient and able to perform primitive operations in hardware to reduce firmware overhead. It currently supports four forms of message passing, S-COMA, and NUMA shared memory, but can be reconfigured to introduce new mechanisms for better usability and performance.
412176	4121766	Approximation Algorithms for MAX SAT: Yannakakis vs. Goemans-Williamson	MAX SAT, also known as the maximum satisfiability problem, is a problem where the goal is to find a truth assignment that satisfies the largest number of clauses, each with assigned weights. This paper discusses approximation algorithms for MAX SAT proposed by Yannakakis and Goemans-Williamson. The authors present an improved algorithm based on Yannakakis' original algorithm. While Yannakakis' algorithm has the same performance guarantee as Goemans-Williamson, the improved algorithm has a better performance guarantee and can achieve a 0.770-approximation. This shows that the improved algorithm is more efficient in finding a near-optimal solution for MAX SAT.
412177	41217792	A SIMD optimization framework for retargetable compilers	Retargetable C compilers are commonly used for new embedded processors and to explore processor architecture. However, this approach often results in lower code quality compared to hand-written compilers or assembly code due to a lack of dedicated optimization techniques. To address this issue, flexible and retargetable code optimization techniques can be designed specifically for a range of target architectures. This article focuses on optimizing SIMD instruction support, which is common in embedded processors for multimedia applications but can be challenging due to nonuniform architectures and limitations on data types and memory alignment. The article presents an efficient and quickly retargetable SIMD code optimization framework integrated into an industrial retargetable C compiler. Experimental results show significant improvements in code quality for various processors, approaching the theoretical limit. 
412178	41217865	Joint Multiple Target Tracking and Classification Using Controlled Based Cheap JPDA-Multiple Model Particle Filter in Cluttered Environment	This paper addresses the challenge of tracking and classifying multiple targets in a cluttered environment. The approach involves using a multiple model particle filter (MMPF) to handle nonlinear filtering with switching dynamic models. Joint probabilistic data association (JPDA) is also utilized to determine the origin of measurements. The efficiency of using Fitzgerald's Cheap JPDA is proven in the literature. Additionally, a controller based on the quality of innovation is implemented to adjust the number of particles. Monte Carlo simulations with two maneuvering targets demonstrate the feasibility and effectiveness of this approach. 
412179	41217916	Jgroup-ARM: a distributed object group platform with autonomous replication management	This paper discusses Jgroup-ARM, a distributed object group platform that uses a measurement-based assessment technique to ensure its fault-handling capability. Jgroup is built on top of Java RMI and is designed for use in partitionable systems. ARM aims to improve system dependability through a fault-treatment mechanism and focuses on deployment and operational aspects. The combination of Jgroup and ARM makes it easier to develop, deploy, and manage partition-aware applications. The paper presents experimental results that demonstrate Jgroup-ARM's ability to recover applications to their initial state in various failure scenarios, including multiple network partitions. 
412180	41218063	Weakening the language bias in LINUS	Propositional inductive learning algorithms have limitations in their ability to incorporate background knowledge and the expressiveness of their knowledge representation. This paper proposes a solution in the form of LINUS, a system that utilizes a more expressive logic programming framework to effectively incorporate background knowledge in learning both propositional and relational descriptions. This allows for the learning of logic programs in the form of constrained deductive hierarchical database clauses and determinate deductive database clauses. This method bridges the gap between propositional learning and more complex knowledge representation, leading to improved learning capabilities.
412181	41218114	Using DEMO to Identify IT Services	IT Service Management (ITSM) has become increasingly important as organizations invest in their Information Technology (IT) Infrastructure and customers become more demanding. The Service Catalog is a crucial element of ITSM, formally documenting the available services provided by IT organizations. However, accurately identifying these services can be challenging and can have serious consequences if done incorrectly. This paper proposes a method using the Design & Engineering Methodology for Organizations (DEMO) to identify IT services. The Design Science Research Methodology and customer feedback were used to evaluate the method, showing that the new services list was preferred by customers compared to the previous one. This highlights the importance of accurately identifying IT services to meet customer demands and improve service quality.
412182	41218218	Fat-trees: Universal networks for hardware-efficient supercomputing	The author introduces fat-trees, a new type of universal routing network that can connect processors in a parallel supercomputer. These networks can be customized to support varying levels of communication, allowing for efficient use of hardware and potential cost savings for applications like finite-element analysis. The author proves that fat-trees are the most optimal routing network of a given size and demonstrates this using a VLSI model that considers wiring as a direct cost. It is also shown that a fat-tree built from a specific amount of hardware can simulate any other network built from the same amount of hardware with only a slightly longer time (polylogarithmic factor greater).
412183	41218373	Drift-free tracking of rigid and articulated objects	Model-based 3D trackers are used to estimate the position, rotation, and joint angles of a model from video data. However, they often suffer from drift due to small errors accumulating over time. This is especially problematic for human motion capture and tracking when there are multiple moving objects and occlusions present. To address this issue, the authors propose an analysis-by-synthesis framework that combines patch-based and region-based matching to track both structured and homogeneous body parts. The method is demonstrated to work well for various scenarios, including fast movements, self-occlusions, and clutter. The study also includes a quantitative error analysis and comparison with other model-based approaches.
412184	41218437	Routing for energy minimization in the speed scaling model	Network optimization aims to minimize energy consumption in telecommunication networks by adjusting the processing speed of network elements according to the amount of traffic they carry. Existing research on this topic focuses on individual elements, but we aim to optimize the entire network. We specifically examine routing with the goal of providing guaranteed speed/bandwidth for a given demand while minimizing energy use. The energy curve, which shows how energy is consumed as processing speed increases, plays a crucial role in determining the optimal routes. We show that for certain energy curves, there is a constant approximation for routing, but for others with a non-zero startup cost, there is no feasible constant approximation. However, we present two other approximations that provide good results. 
412185	41218524	Learning collaboration strategies for committees of learning agents	In multi-agent systems, it is important for agents to decide when and with whom to cooperate. This paper focuses on systems with learning agents, whose goal is to accurately predict solutions to problems. Each agent must determine whether to solve a problem alone or collaborate with other agents. The paper presents a proactive learning approach that allows agents to learn when to form committees and which agents to invite. Experiments show that this results in smaller committees with maintained or improved problem solving accuracy compared to committees composed of all agents. This approach is beneficial for efficient collaboration in multi-agent systems.
412186	412186146	Listing triconnected rooted plane graphs	A plane graph is a type of drawing of a graph in which no edges cross each other. A rooted plane graph has a specific outer vertex. The set G3(n, g) includes all triconnected rooted plane graphs with n vertices and inner faces of size at most g. This paper presents an algorithm for listing all plane graphs in G3(n, g). The algorithm takes constant time per output and outputs the difference from the previous output.
412187	41218743	The asymptotic number of labeled graphs with given degree sequences	The article discusses the number of n × n symmetric non-negative integer matrices subject to certain constraints. These constraints include specified and bounded row sums, bounded entries, and a set of specified zero entries. The article provides asymptotic estimates for this number and relates it to incidence matrices for labeled graphs. This result has potential implications in various fields such as combinatorics and graph theory.
412188	41218875	Analysis of human behavior to a communication robot in an open field	This study explores human behavior towards an interactive robot at a science museum. Understanding how people interact with robots is crucial in developing communication robots for everyday use. The researchers analyzed visitor behavior towards a simple interactive robot at the museum, using information from sound level and range sensors. They identified factors that influence how people approach, maintain distance, and physically and verbally interact with the robot. This allowed them to extract useful information from the sensor data and apply it to communication robots. Overall, this research contributes to the development of robots that can predict and optimize their interactions with humans based on their behavior.
412189	41218930	Diagonal Unloading Beamforming for Source Localization.	In sensor array beamforming, algorithms are used to estimate the location of a radiating source. Diagonal loading of the beamformer covariance matrix is commonly used to improve accuracy and robustness. This paper proposes a new method, called diagonal unloading (DU), which adds an additional constraint to the array output vector's covariance matrix. This regularization is achieved by subtracting a set amount of white noise from the matrix's main diagonal. The DU beamformer aims to separate the signal subspace from the noisy signal space and is calculated by enforcing a negative definite covariance matrix. The proposed DU method is compared to other beamforming methods in terms of computational cost and localization performance. The results show that the DU method is comparable to existing methods, making it a simple, effective, and efficient option for array processing.
412190	41219047	Can network characteristics detect spam effectively in a stand-alone enterprise?	Prior research has demonstrated that the network dynamics of an email packet and the entire connection can be used to classify the email as spam or not. However, this research has mainly focused on data collected from various sources. In this paper, the authors re-examine these techniques using data from a single enterprise and find that packet properties are not as effective. They also show that incorporating flow characteristics into the model of packet features can improve the accuracy, indicating that some flow features are already captured by the packet features.
412191	41219152	Parallel non-linear optimization: towards the design of a decision support system for air quality management	Large numerical simulation codes have become increasingly popular for solving complex scientific and engineering problems. In the environmental field, these simulations have been used to predict the outcomes of various scenarios and aid in decision making for strategies that balance financial gain with environmental impact. To optimize these decisions, environmental decision support systems have been developed, using high performance computing to evaluate different policy options and find the best solution. This involves solving non-linear optimization problems with discrete and continuous parameters. In this paper, the authors focus on the optimization component of a decision support system and assess the effectiveness of using the quasi-Newton BFGS method, which can be parallelized for faster computation. Through a case study in air quality management, the authors demonstrate the success of this approach, showing significant performance gains and feasibility for solving real-world problems using parallel and distributed supercomputers.
412192	41219227	Efficient Large-Scale Distributed Key Generation against Burst Interruption	A distributed key generation scheme distributes a secret key among key servers and computes a corresponding public key. Canny and Sorkin proposed a probabilistic threshold scheme for a large number of key servers with lower communication cost. However, their scheme has some limitations that can be addressed using randomness technique. This paper introduces a secure scheme against dishonest key servers and better performance. Simulation experiments show that their scheme and the proposed scheme can be used in different scenarios.
412193	41219318	A container yard storage strategy for improving land utilization and operation efficiency in a transshipment hub port.	This paper discusses the challenges of managing a storage yard in a busy transshipment hub where there are high levels of loading and unloading activities. Due to the large amount of container traffic and limited land availability, efficient services are difficult to provide. A consignment strategy with a static yard template has been used, but it sacrifices land utilization. To address this issue, the paper proposes two space-sharing approaches that dynamically reserve storage space for different vessels during different shifts. This also considers workload balancing to reduce traffic congestion. The proposed framework integrates space reservation and workload assignment, resulting in more efficient container handling and reduced yard crane deployment. Experimental results support the effectiveness of this approach.
412194	41219473	Slid pairs in the initialisation of the A5/1 stream cipher	A5/1 is a stream cipher used in the GSM system to ensure privacy. It uses a shift register and majority clocking rule for updating its registers. In this study, the initialisation process of A5/1 is analysed. The researchers found that the cipher has a sliding property, where every valid internal state can also be used as a legitimate loaded state. This property allows for multiple key-IV pairs to produce phase shifted keystream sequences. The paper also discusses a possible attack on the cipher based on this property, using only ciphertext.
412195	4121957	Computing loci of rank defects of linear matrices using Gröbner bases and applications to cryptology	The MinRank problem, also known as computing loci of rank defects of linear matrices, is a challenging task in linear algebra with diverse applications in fields such as Cryptology, Error Correcting Codes, and Geometry. This problem involves finding points where the rank of a given square linear matrix is less than a specified integer. The paper aims to develop an efficient algorithm to solve this problem by studying two algebraic formulations: one that generates bi-homogeneous equations and another that involves a determinantal ideal. Under certain assumptions, the paper provides new bounds on the degree of regularity of these formulations, allowing for an estimation of the complexity of computing Gröbner bases. The results have practical implications, such as breaking a cryptographic challenge and evaluating the security of a cryptosystem. The determinantal ideal formulation is found to be more suitable for Gröbner bases computations. 
412196	41219686	Spectral Analysis of Quasi-Cyclic Product Codes	In this paper, we analyze a linear quasi-cyclic product code made up of two quasi-cyclic codes with relatively prime lengths over finite fields. We explore the spectral analysis of this code using the spectral analysis of the row and column codes. We also present a new lower bound for the minimum Hamming distance of a given quasi-cyclic code and introduce a new algebraic decoding algorithm. Additionally, we prove the explicit basis of this product code in terms of the generator matrix using a special order form. Our findings extend previous work on cyclic product codes and we also provide a conjecture for the generator matrix of a general quasi-cyclic product code. We apply our spectral analysis to a specific case and propose a new lower bound for the minimum Hamming distance of a given quasi-cyclic code. Finally, we develop a decoding algorithm for phased burst errors with guaranteed decoding radius.
412197	41219727	Making learning designs happen in distributed learning environments with GLUE!-PS	The article discusses the limited availability of virtual learning environments (VLEs) that allow teachers to create and reuse learning designs. Many VLEs are not accessible to most teachers due to institutional decisions and contextual constraints. The GLUE!-PS architecture and data model aims to address this issue by allowing teachers to design learning activities using various tools and deploy them in different distributed learning environments. The article demonstrates this by showcasing two authentic learning designs with different pedagogical approaches and how GLUE!-PS helps set up the necessary ICT infrastructure in two different environments (Moodle and wikis).
412198	41219831	Collisions for the WIDEA-8 compression function	WIDEA is a family of block ciphers based on the IDEA block cipher. It uses multiple parallel instances of IDEA and an improved key schedule to create larger block sizes. The design is suggested as a compression function for the Davies-Meyer mode. This paper examines the security of WIDEA when used as a compression function. By taking advantage of the slow diffusion in the key schedule, the paper presents practical free-start collisions for WIDEA-8, with a complexity of 213.53.
412199	41219942	Commutativity analysis for XML updates	The article discusses the use of XQuery extended with update operations as an effective approach to support XML updates. While this approach is user-friendly, it poses difficulties in optimization and analysis due to the undecidability of commutativity in most existing XML update languages. To address this issue, the article proposes a conservative analysis method that uses path analysis to determine commutativity. The method involves computing upper bounds for accessed or modified nodes in an expression and can be used to identify commuting expressions. The article also provides a theorem to support this technique and showcases its application in query optimization with updates.
412200	41220049	Towards Fresh and Hybrid Re-Keying Schemes with Beyond Birthday Security.	Fresh re-keying is a security protocol that splits the task of protecting encryption/authentication schemes against side-channel attacks into two parts. One part, the re-keying function, must have certain properties and is based on an algebraic structure that can withstand side-channel attacks. The other part, a block cipher, provides resistance against mathematical attacks and only needs to be secure against single-measurement attacks. This protocol is cheap, stateless, and does not require synchronization between communication parties. However, a recent attack has shown that the first instantiation of fresh re-keying only provides birthday security. In response, this paper proposes two solutions that provide provable security against this attack, one using classical block ciphers and the other using tweakable block ciphers. The paper also discusses the use of fresh/hybrid re-keying for encryption and authentication and warns about their vulnerability to side-channel attacks.
412201	41220113	Exploiting coding theory for collision attacks on SHA-1	This article discusses the use of coding theory in the cryptanalysis of hash functions, specifically focusing on the SHA-1 algorithm. The authors introduce various linear codes that are used to identify low-weight differences that can lead to a collision. They also incorporate recent findings in the field to improve upon existing approaches. Through their method, they are able to identify differences with extremely low weight and make a conjecture about the complexity of a full collision attack on SHA-1. 
412202	41220242	Dynamic constraints and database evolution	The relational model, commonly used for databases, has been extended to study the impact of dynamic constraints on database evolution. This extension incorporates both static and dynamic constraints, with a focus on functional dependencies (fd's) and their dynamic counterparts. The interaction between these constraints is explored, considering the influence of the database's past history on the static constraints and the connection between the static constraints and future database evolution. The concepts of age, age-closure, survivability, and survivability-closure are used to analyze these effects. This extension provides insight into the behavior of databases under dynamic constraints. 
412203	4122037	Organizations Analysis with Complex Network Theory	In this paper, the authors propose network measures and analytical procedures for modeling the structure and behavior of different types of organizations, including line, functional, line-and-staff, project, and matrix organizations. They develop network generators for each type to gather information about employee connectivity and organizational properties. The authors also examine the roles and groups of employees within the network and assess their social position and impact. They analyze the structure of the network to identify employees with similar roles and potential for action within the organization. Additionally, the authors estimate the confidentiality of the network by removing certain communication links between employees and determining the percentage of communications that would disconnect the organization into unconnected parts.
412204	41220424	Efficient Algorithms and Implementations for Optimizing the Sum of Linear Fractional Functions, with Applications	This paper discusses an improved algorithm for solving the sum of linear fractional functions (SOLF) problem in both 1-D and 2-D. The key subproblem is the off-line ratio query (OLRQ) problem, which requires finding optimal values for a sequence of linear fractional functions subject to linear constraints. The algorithm uses geometric properties and parametric linear programming to solve the OLRQ problem in O((m+n)log (m+n)) time. This improves upon previous methods which had a time complexity of O(m(m+n)). The improved algorithm has been shown to outperform other approaches in most cases and has also been applied to problems in computational geometry and other areas. 
412205	41220516	PurTreeClust: A purchase tree clustering algorithm for large-scale customer transaction data	Clustering customer transaction data is a crucial step in understanding customer behavior in retail and e-commerce companies. This involves organizing products into a product tree, with leaf nodes being individual products and internal nodes representing product categories. To better analyze customer transactions, the concept of a "personalized product tree" or purchase tree is introduced. The PurTreeClust algorithm is then proposed to efficiently cluster large-scale customer data from these purchase trees. This algorithm utilizes a new distance metric and a cover tree for indexing and initial cluster center selection. Additionally, a gap statistic method is proposed to estimate the number of clusters. Experiments on ten large-scale transaction datasets confirm the effectiveness and efficiency of this method compared to other clustering algorithms.
412206	41220638	A generic flash-based animation engine for prob	Writing a formal specification for real-life, industrial problems can be challenging and prone to errors, even for experts in formal methods. It is important to involve domain experts in the process to ensure that the formal model is accurate and in line with their expectations. However, understanding formal models written in a specification language like B may require mathematical knowledge that domain experts may not possess. To address this issue, a new tool has been developed that visualizes B Machines using the ProB animator and Macromedia Flash. This tool allows specifiers to create domain-specific visualizations that can be easily understood by domain experts, helping to ensure that the specification meets their needs.
412207	41220745	Deriving Linear Size Relations for Logic Programs by Abstract Interpretation	The proposed method is an automatic approach for determining linear size relations between the arguments of atoms in a definite Horn clause program's least Herbrand model, based on a given norm. It utilizes abstract interpretation, with an abstract domain consisting of affine subspaces or linear varieties and operations based on linear algebra. The main purpose of this technique is for automatic termination analysis, but it can also be used for complexity and granularity analysis, as well as specialisation of constraints in constraint logic languages. 
412208	41220852	Migration of information systems in the Italian industry: A state of the practice survey	The paper discusses the challenges and complexity of software migration, particularly towards web and distributed architectures, which has become increasingly relevant due to the widespread use of the internet and mobile devices. The objective of the paper is to report on a survey conducted among 59 Italian IT companies to identify their experiences, tools used, and emerging needs and problems in software migration projects. The results show that migration, especially towards the web, is highly important for Italian companies and they are increasingly using free and open source solutions. However, the adoption of specific tools for migration is limited due to lack of skills and knowledge or inadequate options. The paper concludes by suggesting the need for technology transfer between academia and industry to improve the adoption of software migration techniques and tools.
412209	41220935	Code Your Own Game: The Case of Children with Hearing Impairments.	The computer science community emphasizes the importance of teaching children coding skills to foster their creativity and not just consume games. However, Deaf and Hard of Hearing children have specific needs when it comes to coding and making activities, even with accessible visual translations. A workshop program was developed and evaluated with 12 DHH children to improve the teaching of coding in current learning environments. A set of guidelines was created through a focus group and content analysis to aid special education teachers, curriculum designers, and K-12 education developers in providing appropriate workshops for DHH children. This initial evaluation shows promise in enhancing the learning experience for DHH students in coding.
412210	41221031	Leftover Hash Lemma, revisited	The Leftover Hash Lemma (LHL) is a well-known concept that states that universal hash functions can be used as good randomness extractors. However, LHL-based extractors have two limitations: large entropy loss and large seed length. These limitations can be overcome in certain scenarios, such as deriving secret keys for cryptographic applications. Additionally, a general computational extractor has been developed that can be used for any cryptographic application with low entropy loss. The expand-then-extract approach, which uses a pseudorandom generator to expand a short seed into a longer one before using it in the LHL, has also been studied. It is not always sound, but it can be used with a small number of bits or in certain cases, such as minicrypt. This suggests that it may be secure to use practical pseudorandom generators, even without a proof of security.
412211	41221145	Fixed Parameter Algorithms For The Minimum Weight Triangulation Problem	This article discusses and compares four different algorithms for finding the minimum weight triangulation of a simple polygon with a total of n vertices, where (n - k) vertices are on the perimeter and k vertices are in the interior. All four algorithms use a divide-and-conquer approach and dynamic programming to improve efficiency. The algorithms are based on two observations about triangulations - triangle splits and path splits. The first two algorithms use only one type of split, while the last two combine both types for better performance. The authors aim to simplify and deepen the understanding of these algorithms by discussing them and presenting experimental results from a Java implementation.
412212	41221273	Semantic representation: search and mining of multimedia content	The ability to understand the meaning of multimedia content is crucial for effective access to digital media data. Semantic content descriptions greatly improve the value of large media repositories by making them searchable. However, automatic semantic understanding is a difficult task and most databases rely on low-level features or manual annotations. Recent techniques focus on detecting semantic concepts in videos, such as indoor, outdoor, face, etc. This requires a fixed lexicon and annotated training examples. This paper discusses the use of semantic concept detection to map videos into semantic spaces, creating a compact representation of the content. Experiments on a large video corpus show improved performance in retrieval, classification, visualization, and data mining using this approach. 
412213	41221334	Conjugacy in Free Inverse Monoids.	Conjugacy, a concept commonly used in groups, can also be applied to monoids in two ways: the first version defines conjugacy as two elements being equal when one can be obtained by rearranging the other, while the second version defines it as two elements being equal when they can be transposed. By using Munn's characterization of elements in free inverse monoids, it is shown that when restricted to non-idempotent elements, the relation of conjugacy is the transitive closure of the relation of transposition. Additionally, it is proven that testing for conjugacy between two elements in a free inverse monoid can be done in linear time.
412214	41221417	The weight hierarchy of the Kasami codes	The Kasami codes are a group of codes with lengths of [2 2 m − 1, 3 m , 2 2 m −1 − 2 m −1 ]. These codes are commonly used to create sequences with ideal correlation values. The weight hierarchy of the Kasami codes is fully defined, and it has been proven that the chain condition applies to these codes.
412215	41221545	Analysis of the 802.11e enhanced distributed channel access function	We have developed a mathematical model to analyze the performance of the enhanced distributed channel access (EDCA) function in the IEEE 802.11e standard. Our model uses a discrete-time Markov chain (DTMC) and takes into account important quality-of-service (QoS) features such as contention window, arbitration interframe space, and transmit opportunity differentiation. This model considers the state of the MAC layer buffer, as well as MAC differentiation mechanisms, to provide a comprehensive analysis of performance under different traffic loads. Our results, obtained through both analytical and simulation methods, demonstrate the accuracy of our model for various EDCA parameters and MAC layer buffer sizes. 
412216	41221687	Optimal Neighborhood Kernel Clustering with Multiple Kernels.	Multiple kernel k-means (MKKM) is a clustering algorithm that aims to improve performance by learning an optimal kernel, which is a linear combination of pre-specified base kernels. However, this approach may limit the kernel's representation capability and not fully consider the negotiation between kernel learning and clustering, resulting in unsatisfactory performance. To address these issues, an optimal neighborhood kernel clustering (ONKC) algorithm is proposed, which enhances the representability of the optimal kernel and strengthens the negotiation between learning and clustering. Theoretical justification shows that existing MKKM algorithms are a special case of ONKC and that it can be extended for better clustering algorithms. An efficient algorithm is designed to solve the optimization problem with proven convergence. Experimental results demonstrate the superior performance of ONKC compared to state-of-the-art algorithms.
412217	41221739	Design, Implementation and Deployment of State Machines Using a Generative Approach	The approach discussed in this article involves using a single abstract model to generate a family of finite state machines for designing and implementing a distributed system. The state machines are used to formalize the interactions between system components, leading to increased confidence in correctness. The generated artefacts include diagrams, protocol implementations, and documentation. This methodology allows state machines to be applied to problems that may not have been suitable for them before. The authors demonstrate this approach with a case study of a Byzantine-fault-tolerant commit protocol in a distributed storage system. They explain how the abstract model is defined in terms of an abstract state space and state transitions, and propose a general methodology for using this technique in system development.
412218	41221813	Automatic Reactive Adaptation of Pervasive Applications.	 the vision of a technology-rich environment that provides seamless and distraction-free support for everyday tasks.Pervasive Computing is a concept that envisions seamless and distraction-free support for everyday tasks through distributed applications. This is achieved by leveraging the resources of the users' environment and adapting to changes in the execution environment. To efficiently create adaptive applications, developers require a suitable framework. This paper presents and evaluates an approach for adapting a pervasive computing application during its execution. The approach is based on the PCOM system and an algorithm for automating the initial configuration of a component-based application. The paper describes modifications to the component model to enable automatic adaptation, proposes a cost model to capture the complexity of adaptations, and presents an online optimization heuristic to choose a low-cost configuration. The ultimate goal of Pervasive Computing is to create a technology-rich environment that seamlessly supports everyday tasks without causing distraction.
412219	41221922	Information security underlying transparent computing: Impacts, visions and challenges	The advancement of computer networks and social information has created both opportunities and challenges in information security. As more people share information and services, the need to strengthen security measures has become crucial. This paper proposes a new network security mechanism called transparent computing, which is based on the extended von Neumann architecture. This separates program storage and execution in the network environment, utilizing a new generation server and client BIOS called EFI BIOS, and coordinated with the MetaOS management platform and related devices. By controlling data and instructions in a block-streaming way, it effectively manages security and protects against malicious software such as worms and Trojan horses. Detailed examples are provided to showcase the benefits and potential of this security approach.
412220	41222028	On Store Languages of Language Acceptors.	Pushdown automata are a type of language acceptor that use a stack to keep track of intermediate steps in a computation. The "store language" of a pushdown automaton, which includes all possible state and stack configurations during a computation, is known to be a regular language. This concept is explored further by examining different models of language acceptors and their store languages. Connections between one-way and two-way machines, as well as between deterministic and nondeterministic machines, are also discussed. The results of these studies have practical applications, such as a technique for proving that families of languages accepted by deterministic machines are closed under right quotient with regular languages. Additionally, lower bounds on the space complexity of Turing machines with non-regular store languages are determined. 
412221	41222147	Coalgebraic Hybrid Logic	This article introduces a framework called coalgebraic hybrid logic, which allows for reasoning about individual states in a model using modal logics with nominals and satisfaction operators. This framework is as general as coalgebraic modal logic and encompasses various logics with non-normal modal operators such as probabilistic, graded, or coalitional modalities and non-monotonic conditionals. The authors prove a generic finite model property and a weak completeness result, and provide a semantic criterion for decidability in PSPACE. They also present a fully internalised PSPACE tableau calculus. These generic results can be applied to specific hybrid logics, resulting in new findings such as the decidability of probabilistic and graded hybrid logics in PSPACE.
412222	41222262	Twitinfo: aggregating and visualizing microblogs for event exploration	Microblogs, such as Twitter, contain a vast amount of user-generated content about world events. However, the chronological organization of posts makes it difficult for users to get a detailed understanding of an event. TwitInfo is a system that addresses this issue by providing a timeline-based display of tweets, highlighting peaks of high activity. A unique streaming algorithm automatically identifies and labels these peaks using text from tweets. Users can explore further through geolocation, sentiment, and popular URLs. The system also includes a recall-normalized sentiment visualization for more accurate overviews. Evaluations have shown that users can reconstruct meaningful event summaries quickly, and a Pulitzer Prize-winning journalist has confirmed its usefulness for understanding long-running events and identifying eyewitnesses. Quantitatively, the system can identify 80-100% of manually labeled peaks, providing a comprehensive view of each event.
412223	41222351	Multiple Coloring of Cone Graphs	This paper discusses the $k$th chromatic number of cone graphs, which are created by adding a "cone" of vertices to an existing graph. The $k$th chromatic number, denoted by $\chi_k(G)$, is the minimum number of colors needed to assign to each vertex in a $k$-fold coloring of the graph $G$. An upper bound for $\chi_k(\mu_m(G))$, the $k$th chromatic number of the cone graph of $G$, is presented in terms of $\chi_k(G)$, $k$, and $m$. It is also shown that there is a connection between the $k$th chromatic number of the cone graph and the circular chromatic number of $G$. Surprisingly, if $\chi_k(G)$ and $\chi_c(G)$ are even, then for sufficiently large $m$, the $k$th chromatic number of the cone graph is equal to the $k$th chromatic number of $G$.
412224	41222416	Parametric Yield Analysis and Constrained-Based Supply Voltage Optimization	Parametric yield loss is a major concern in technologies where leakage is dominant. This paper discusses the impact of leakage on parametric yield and how it can shrink the yield window. The authors present a mathematical framework for estimating yield under device process variation with power and frequency constraints. The model is validated through simulations and shown to have less than 5% error. The importance of optimal supply voltage selection for maximizing yield is also demonstrated, along with the sensitivity of yield to frequency and power constraints. Finally, the framework is applied to the problem of maximizing shipping frequency while meeting yield and power constraints.
412225	41222540	A GPU-based Streaming Algorithm for High-Resolution Cloth Simulation.	Our team has developed a GPU-based streaming algorithm for efficient and accurate cloth simulation. By utilizing GPU-based kernels and data structures, we were able to map all components of the simulation pipeline, including time integration, collision detection and response, and velocity updating. Our algorithm can handle both intra-object and inter-object collisions, as well as contacts and friction, resulting in realistic simulations of folds and wrinkles. We also discuss the challenges in achieving high throughput on many-core GPUs and demonstrate the parallel performance of our algorithm on various GPU generations. On a high-end NVIDIA Tesla K20c, we observed a significant improvement in performance compared to a single-threaded CPU-based algorithm and a 16-core CPU-based parallel implementation. Our algorithm can accurately simulate a cloth mesh with 2M triangles using only 3GB of GPU memory.
412226	41222628	Soft Articulated Characters With Fast Contact Handling	The handling of soft articulated characters in computer animations is a difficult task due to the complex interaction between the character's skeleton and surface deformation. To address this issue, a new algorithm has been developed that uses a layered representation to simulate the movement and deformation of high-resolution characters in real time. This algorithm effectively captures the dynamic relationship between the skeleton and skin by incorporating elastic deformation into the character's pose space. It also overcomes computational challenges by separating skeleton and skin computations and efficiently handling collisions using the layered representation. With this approach, the algorithm can handle large contact areas, produce realistic surface deformations, and accurately simulate the impact of collisions on the character's skeleton. 
412227	41222730	StereoPasting: interactive composition in stereoscopic images.	"StereoPasting" is a proposed method for efficiently combining 2D and stereoscopic images. By painting "disparity" on a 2D image, a disparity map is gradually created and blended with the target stereoscopic image. The process takes into account expected disparities and perspective scaling, and the 2D object is warped to generate an image pair. This warping is solved in real time through energy minimization. An interactive composition system allows users to edit disparity maps and view results instantly. Experiments have shown the effectiveness and versatility of this method in various applications.
412228	4122281	Analyzing settings for social identity management on Social Networking Sites: Classification, current state, and proposed developments.	The increasing use of Social Networking Sites (SNS) has led to new privacy challenges and has prompted users to manage their online identity. This is referred to as Social Identity Management (SIdM), which involves deliberately disclosing personal information to a select group of contacts or other users on the SNS. However, protection against entities such as the site operator or advertisers is not covered by SIdM and needs to be addressed separately. While some SNS have implemented features and settings for SIdM, there is a lack of integration and a reference framework to guide users in managing their identity. This article proposes a reference framework for SIdM based on identity theory, literature analysis, and existing SNS. It also examines the current SIdM capabilities of popular SNS and suggests possible improvements. Lastly, the idea of developing a metric to compare the SIdM capabilities of different SNS is discussed.
412229	41222957	Extinction-based shading and illumination in GPU volume ray-casting.	Direct volume rendering is a popular method for visualizing volumetric data, but it is still a challenge to incorporate sophisticated illumination models without sacrificing interactive frame rates. This paper presents a new approach for advanced illumination in direct volume rendering using GPU ray-casting. This method includes directional soft shadows, ambient occlusion, and color bleeding effects while maintaining competitive frame rates. It also supports dynamic lights and interactive transfer function changes. Unlike the commonly used a-blending method, which simplifies the original volume rendering integral, this approach uses the original exponential extinction coefficient, which can be cleverly implemented on the GPU to achieve volume lighting effects.
412230	41223045	Freeness analysis in the presence of numerical constraints	This paper discusses a new method of abstract interpretation for dealing with mixed systems of numerical and unification constraints. The proposed abstraction is able to effectively handle the interaction between these two types of constraints, extending the concept of possible sharing information in logic programs to include possible dependencies established by constraint systems. This information can be used for low-level optimizations and to delay the addition of constraints in computation, allowing for constraint specialization. When combined with other analyses, this derived information can greatly improve the compilation of Constraint Logic Programming (CLP) programs, reducing the need for a general constraint solver. 
412231	41223141	Vertex characterization of partition polytopes of bipartitions and of planar point sets	The partition problem involves dividing a set of vectors in d-dimensional space into p parts in order to maximize a convex objective function. This problem has practical applications in various fields such as clustering, inventory management, scheduling, and statistical testing. The convexity of the objective function allows the problem to be reduced to maximizing it over a polytope, which is the convex hull of all possible solutions. This article explores the vertices of the partition polytope when d=2 or p=2, and determines the maximum number of vertices for these cases. The results provide insights into the time complexity needed to solve the partition problem using an oracle. 
412232	41223235	The Vapnik-Chervonenkis dimension of graph and recursive neural networks.	The Vapnik-Chervonenkis dimension (VC-dim) is a measure of the sample learning complexity of a classification model and is often used to evaluate its generalization capability. While the VC-dim has been studied for common feed-forward neural networks, this paper explores its application to Graph Neural Networks (GNNs) and Recursive Neural Networks (RecNNs). These models are capable of processing graph inputs, which are a more general form than sequences and vectors. The paper provides upper bounds on the VC-dim of GNNs and RecNNs, showing that their generalization capability increases with the number of connected nodes. This finding is similar to previous results for recurrent neural networks, which also have a higher VC-dim compared to traditional neural networks. 
412233	412233128	CASA-Based Robust Speaker Identification	Conventional speaker recognition systems struggle to accurately identify speakers in noisy environments. To address this, researchers have turned to computational auditory scene analysis (CASA), which uses a time-frequency mask to separate speech signals. In this study, the researchers introduce a new speaker feature called gammatone frequency cepstral coefficient (GFCC), which is based on an auditory model and outperforms traditional speaker features in noisy conditions. To improve performance, CASA separation is applied and either reconstructed or marginalized components are used based on the mask. Combining these methods results in a highly effective system that outperforms other similar systems in a variety of noise levels.
412234	41223416	A Note on Multiple-Access Channels with Strictly-Causal State Information	This content discusses a new inner bound for the capacity region of a memoryless multiple-access channel that is influenced by a known, strictly causal state. The proposed bound includes previous bounds and is shown to be strictly better through an example. A variation of this example is applied to a scenario where the channel is influenced by two independent state sequences, with each transmitter having knowledge of one of the states. This proves a previous conjecture by Li et al. that their inner bound for this scenario is indeed better than previous bounds.
412235	41223532	Power efficient rate monotonic scheduling for multi-core systems	Current real-time systems offer more computational power to handle CPU intensive applications, but this leads to increased energy consumption and heat dissipation. To address these issues, system speed is adjusted in real-time to meet application deadlines and reduce overall energy consumption. The use of multi-core technology also presents opportunities for energy reduction through efficient scheduling, but there is limited research on this topic. This paper proposes a technique for finding the lowest core speed for scheduling tasks, which is shown to be superior to existing methods. Additionally, a task shifting policy is used to balance core utilization and determine a uniform system speed for a given task set, ensuring that all tasks meet their deadlines and energy consumption is reduced.
412236	41223628	Dynamic Organization Schemes for Cooperative Proxy Caching	In a cooperative caching architecture, web proxies work together in a mesh network. If a proxy cannot fulfill a request, it is sent to other nodes in the network. However, since local caches can only satisfy a small percentage of requests, this can result in a high volume of queries being directed to neighboring nodes, which can strain system resources. Due to variations in traffic, overlapping content, and distance between nodes, it is not necessary for each proxy to cooperate with every node in the mesh. Limiting the number of neighboring nodes can also help with scalability, as having N proxies in a mesh can lead to a high number of queries (N2). Therefore, it is beneficial to restrict the number of neighbors for each proxy to a certain value, k.
412237	41223750	MM: A bidirectional search algorithm that is guaranteed to meet in the middle.	Bidirectional search algorithms combine two separate searches, one starting from the initial state and the other starting from the goal state. While adding a heuristic to unidirectional search greatly reduces the search effort, bidirectional heuristic search has not been as successful despite years of research. This is due to the lack of a comprehensive understanding of its nature. This paper introduces a new bidirectional heuristic search algorithm called MM, which guarantees that the forward and backward searches will meet in the middle. This allows for a unique comparison with A* and their brute-force variations by dividing the state space into sections based on their distance from the start and goal. Experimental results support the theoretical analysis and identify the conditions where each algorithm is most effective.
412238	4122385	Stochastic link and group detection	Link detection and analysis has been crucial in the social sciences and government intelligence community. While most efforts focus on analyzing known networks, the abundance of circumstantial data from the internet and other sources has led to the need for probabilistic evidence of links. To address this, a probabilistic model of link generation based on group membership has been proposed. This model takes into account both observed link evidence and demographic information of the entities. The parameters of the model are learned through a maximum likelihood search, which is made tractable through various heuristics. The model and optimization methods are tested on both synthetic data with known ground truth and a database of news articles.
412239	412239134	Productivity of test driven development: a controlled experiment with professionals	The popularity of Extreme Programming has led to a rise in interest for test driven development (TDD). As a result, many experiments have been conducted to determine if TDD is more effective than the traditional method of testing code after it has been written (TAC). However, research on TDD is still in its early stages and there is limited knowledge on the subject. This paper presents an experiment conducted in a Spanish software company to compare productivity between TDD and TAC. The results suggest that TDD may be more efficient, but further research is needed to fully understand its benefits.
412240	41224060	Assessing the usability of a visual tool for the definition of e-learning processes	The paper discusses a usability study of a visual language-based tool called ASCLO-S, which is designed for developing adaptive e-learning processes. The tool uses flow diagrams to define different classes of learners and determine the most suitable adaptive learning process for each class. The study used a questionnaire-based survey and an empirical analysis to gather feedback and measure the usability of the tool. The results showed that both the visual notation and the system prototype were user-friendly for instructional designers, regardless of their experience with computers or e-learning tools. The empirical analysis also revealed that the effort required to develop e-learning processes was not affected by the instructional designer's experience, but rather by the size of the process.
412241	41224134	On Relating Heterogeneous Elements from Different Ontologies	The majority of distributed ontology integration formalisms utilize mapping languages to express semantic relations between concepts of different ontologies. These mappings, known as homogeneous mappings, can handle a large portion of heterogeneities between ontologies. However, there is a need for mapping languages that can also express semantic relations between properties, known as heterogeneous mappings. This is necessary to reconcile different representations of the same real-world aspect in different ontologies. The authors have proposed a rich mapping language that incorporates both homogeneous and heterogeneous mappings, extending the semantics of Distributed Description Logics (DDL). They provide a sound and complete axiomatization for these mappings, which is a crucial step towards representing an arbitrary network of ontologies.
412242	41224290	A fast algorithm to binarize and filter documents with back-to-front interference	This paper discusses a common issue known as "back-to-front interference" that occurs when documents are written on both sides of translucent paper. This interference makes it difficult to accurately binarize the documents, resulting in unreadable images. To address this problem, the paper presents a fast and effective segmentation-based method that uses the entropy of the image's histogram. This approach produces high-quality binarized images, making the documents easier to read and analyze. Overall, the proposed method offers a solution for effectively binarizing documents with back-to-front interference.
412243	41224357	Fair Rate Allocation In Some Gaussian Multiaccess Channels	The article discusses how to achieve all points in the capacity region of Gaussian multiple access channels through successive decoding and time-sharing. It proposes a criterion of fairness based on the theory of majorization, also known as the Lorenz order in economics. The article shows that a unique solution exists for this criterion in a large class of channels, and this fair solution is the same as the well-known Nash bargaining solution. This is due to the special structure of the capacity region. The article also presents a fast algorithm to compute this fair point in certain cases, making it a strong operational choice.
412244	41224485	On RAC drawings of 1-planar graphs.	1-planar graphs are graphs that can be drawn with each edge crossing at most once. A k-bend RAC drawing is a special type of drawing where each edge has at most k bends and edges only cross at right angles. A graph is considered k-bend RAC if it has a k-bend RAC drawing. A 0-bend RAC graph is also known as a straight-line RAC graph. The relationship between 1-planar and k-bend RAC graphs has been studied, and it has been shown that not all 1-planar graphs are straight-line RAC and not all straight-line RAC graphs are 1-planar. The complexity of deciding whether a graph has a drawing that is both 1-planar and straight-line RAC is still an open question, and it has been proven to be NP-hard. However, it has been shown that every 1-planar graph has a drawing that is both 1-planar and 1-bend RAC. This paper answers both of these questions by proving the NP-hardness of the first question and providing a drawing algorithm for 1-planar graphs that satisfies both conditions.
412245	41224554	Achieving the degrees of freedom of 2×2×2 interference network with arbitrary antenna configurations	This paper examines the degrees of freedom (DoF) region in a 2 × 2 × 2 interference network, consisting of two sources, two relays, and two destinations, each with varying numbers of antennas. The authors demonstrate that using linear transceivers, the cut-set outer bound can be reached without requiring additional symbol extensions, except for one specific system setup with a one-DoF gap. Different techniques such as interference avoidance, cancelation, neutralization, and alignment are employed by the transceivers to achieve the outer bound, depending on the antenna configuration. These findings provide insight into the optimal design of transceivers for interference networks with multiple antennas.
412246	41224639	Chunk parsing revisited	Chunk parsing is a method used in natural language processing to identify and group words into meaningful units or "chunks". While it has been a theoretically appealing approach, its practical performance has been lacking. However, a new study introduces a simple sliding-window method and maximum entropy classifiers to improve the accuracy and speed of chunk parsing. The results from testing this method on the Penn Treebank corpus show high precision and speed. Additionally, the study proposes a parsing method that considers the probabilities output by the classifiers, which further enhances the accuracy of the parsing. This suggests that chunk parsing can be a viable option for practical use with the right techniques. 
412247	41224788	SimSem: fast approximate string matching in relation to semantic category disambiguation	This study explores the use of fast approximate string matching to address issues with spelling variations and to utilize large-scale lexical resources for semantic class disambiguation. The researchers incorporate string matching results into a machine learning-based disambiguation approach by creating a unique set of features that measure the distance between a given text and the closest match in a collection of lexical resources. They collect these resources from various biomedical sources, resulting in a combined database of over 20 million lexical items. By utilizing a fast and efficient string matching algorithm, the researchers are able to query these resources without significantly impacting system performance. The results are evaluated on six different corpora, showing significant improvements on one corpus while being modest or negative on others. The researchers provide potential explanations for these results and suggest future research directions. Their lexical resources and implementation are available for research purposes at: http://github.com/ninjin/simsem.
412248	41224852	Haptic interaction becomes mainstream	For over two decades, the human-technology interaction research community has recognized multimodal interaction as a potential successor to graphical user interfaces. With the rapid development of personal computers and mobile devices, there has been an increase in computing power and more accurate output through various channels like display and audio. This has also led to the emergence of new ways of interaction for everyday tasks. Recently, there has been a lot of research and development in haptic interaction, which involves touch-based feedback. In this paper, the author introduces their talk on the progress and potential of haptic interaction technology in mainstream information and communication technology.
412249	41224936	A New Method For Efficient Symbolic Propagation In Discrete Bayesian Networks	This paper introduces a new method for dealing with uncertainty in discrete Bayesian networks using symbolic expressions instead of numeric calculations. The method involves representing the conditional probabilities of a set of nodes as ratios of linear polynomials in parameters. This allows for efficient computation of the expressions using standard numerical algorithms. An alternative method called the numeric canonical components method is also proposed, which is faster and simpler. Redundancy is avoided by using message-passing methods. The method can also be used to obtain lower and upper bounds for the symbolic expressions. The paper also addresses the issue of symbolic evidence and presents examples to illustrate the methodology.
412250	4122502	Validating Orchestration of Web Services with BPEL and Aggregate Signatures	This paper presents a framework to ensure integrity and authentication in secure workflow computation using BPEL Web service orchestration. Despite the focus on security for Web services, there is a lack of practical and standardized solutions for securing workflows. The authors propose using aggregate signatures to validate the orchestration, with all partners signing the result of their computation. This method does not require any changes to service implementation and can be applied to linear workflows. A prototype implementation is evaluated for performance and a more general scheme is also proposed for validating generic workflows.
412251	41225137	Agent-Oriented Visual Modeling And Model Validation For Engineering Distributed Systems	Recent Agent-Oriented methodologies for engineering distributed systems follow a model-based approach to software development. This means that they use a specific set of models for each step in the analysis and design phases. To implement this approach effectively, clear guidelines and flexible modeling tools are needed. This paper introduces a modeling environment that integrates an Agent-Oriented modeling tool with other tools, such as a model-checker for verification and a library for graph transformation techniques. The design of this environment follows recommendations from the OMG's Model-Driven Architecture initiative. Examples are provided to illustrate how modeling and validation are supported by this environment.
412252	41225236	Shadowed c-means: Integrating fuzzy and rough clustering	A new method of partitive clustering called shadowed clustering has been developed, which uses shadowed sets to reduce computation compared to traditional fuzzy clustering. Unlike rough clustering, the threshold parameter is automatically chosen. The number of clusters is optimized using validity indices. Shadowed clustering can handle overlapping clusters and model uncertainty in class boundaries well, and is robust against outliers. A comparison with other partitive clustering methods shows that this approach is superior, as demonstrated by experiments on both synthetic and real data sets. 
412253	412253105	Evaluation and analysis of term scoring methods for term extraction.	This study evaluates five different methods for automatically extracting terms from four different types of text collections: personal documents, news articles, scientific articles, and medical discharge summaries. Each collection has a specific use case, such as identifying authors, suggesting query terms, and expanding patient queries. The goal is to determine which term scoring method is most suitable for a given collection, and how these methods perform on collections that they were not specifically designed for. The results show that collection size and the presence of multi-word terms are important factors in the success of a term scoring method. The most effective method overall is pointwise Kullback-Leibler divergence for both informativeness and phraseness. This study demonstrates that unsupervised term scoring methods can be applied successfully in a variety of contexts.
412254	41225429	The effect of imperfect error detection on reliability assessment via life testing	Life testing is a method used to measure the reliability of software by running it on a large number of test cases and recording the results. The number of failures observed is used to determine the failure probability, even if no failures are observed. However, in practice, failures can occur without being detected. This paper examines the impact of imperfect error detection on the confidence level of the failure probability bound. The results show that imperfect error detection does not necessarily limit the ability of life testing to accurately estimate failure probability in critical systems. However, achieving high confidence levels may require unrealistic assumptions about error detection. Overall, life testing may be more useful for situations where extremely high confidence levels are not necessary.
412255	41225560	Parallel Decomposition of Generalized Series-Parallel Graphs	Generalized series-parallel (GSP) graphs are a type of decomposable graphs that can be represented by their decomposition trees. This allows for efficient solving of graph-theoretic problems. A parallel algorithm has been developed for constructing a decomposition tree of a GSP graph in O(log n) time using C(m, n) processors on a CRCW PRAM. This algorithm relies on the number of processors needed to find connected components of a graph with m edges and n vertices in logarithmic time. Additionally, this algorithm can be used to derive properties for GSP graphs that may be useful in other contexts. 
412256	41225685	Data memory minimisation for synchronous data flow graphs emulated on DSP-FPGA targets	 environment for designing and implementing digital signal processing systems on programmable DSP processors and FPGAs. The paper presents an algorithm for determining the smallest possible data buffer sizes for synchronous data flow applications, ensuring a deadlock-free schedule. This is crucial for mapping applications on FPGAs, where there is limited availability of register resources. The algorithm fits into the design flow of GRAPE, making it a valuable tool for efficient DSP system design.
412257	41225748	A Clustering Approach to Generalized Tree Alignment with Application to Alu Repeats	The Generalized Tree Alignment Problem is a formalization of the multiple sequence alignment problem that focuses on its evolutionary aspect. It involves finding a phylogenetic tree and ancestral sequences that minimize the amount of change needed to explain the given data. This problem is difficult to solve, but a heuristic algorithm has been developed for it. This algorithm uses a combination of agglomerative clustering techniques and sequence graphs to align the sequences and achieve good results according to the scoring function. The effectiveness of this approach is demonstrated on a set of Alu repeats, producing biologically meaningful answers.
412258	41225852	Clustering Large Graphs via the Singular Value Decomposition	The article discusses the problem of dividing a set of points in a multi-dimensional space into a fixed number of clusters, aiming to minimize the total distance between each point and its cluster center. This is the objective of the popular k-means clustering algorithm. The article proves that this problem is difficult to solve even when the number of clusters is only 2. To overcome this difficulty, a continuous relaxation of the problem is considered by finding the best subspace that minimizes the distances of the points to that subspace. This can be efficiently solved using the Singular Value Decomposition (SVD) of a matrix representing the points. The article also introduces a fast randomized algorithm for this problem, which can be applied to large-scale applications.
412259	41225920	Comparing two notions of simulatability	This work explores the relationship between two security notions, standard simulatability and universal simulatability, for cryptographic protocols. A protocol is considered secure if every attack on it can be simulated by an idealized version of the protocol. Two common formalizations of this concept are standard simulatability, which requires that no user can distinguish the protocol from the idealized version, and universal simulatability, which allows for a single user to distinguish. It is shown that universal simulatability implies standard simulatability for perfect security, but not for computational or statistical security. Additionally, a formal definition of a time-lock puzzle is presented, which has implications for the separation of standard and universal simulatability in terms of computational security.
412260	41226046	Hot or not: Interactive content search using comparisons	The article discusses a method for searching for a specific object in a database using interactive content search through comparisons. The user is asked to select the most similar object from a small list, and then a new list is presented based on her previous selections. This process continues until the target object is included in the list, and the search terminates. The study focuses on the scenario of heterogeneous demand, where target objects are selected from a non-uniform probability distribution. The proposed algorithm uses a doubling metric space and can efficiently search for objects with a cost proportional to the doubling constant and the entropy of the demand distribution. The average search cost is shown to scale with a bound of O(c5H), which is an improvement from previous bounds and is order optimal for a constant c.
412261	41226154	Robust graph regularized nonnegative matrix factorization for clustering.	Nonnegative matrix factorization (NMF) and its graph regularized extensions have been popular in machine learning and data mining. However, they are prone to errors and noise due to the use of squared loss function in measuring graph regularization and data reconstruction. To address this issue, a new robust graph regularized NMF model (RGNMF) is proposed. It assumes that there may be corrupted data entries, but the corruption is sparse. An error matrix is introduced to capture this sparse corruption, resulting in a more accurate factorization of the data. Additionally, the use of -norm function helps to reduce the impact of unreliable regularization errors. An iterative updating algorithm is proposed to solve the optimization problem, and experimental results show that RGNMF outperforms other state-of-the-art methods.
412262	41226234	Algorithmic Barriers from Phase Transitions	In many random constraint satisfaction problems, there are tight estimates for the largest constraint density for which solutions exist. However, known polynomial-time algorithms for these problems stop finding solutions at much lower densities. For example, coloring a random graph requires twice as many colors as its chromatic number, which has yet to be improved upon despite efforts from researchers. This "resilience" of the factor of 2 against algorithms is explored by analyzing the evolution of the set of k-colorings of a random graph. The factor of 2 is shown to correspond to a phase transition in the geometry of the set, and this phenomenon is also observed in other random CSPs. A general technique is developed to prove this using statistical physics principles.
412263	41226361	Dynamic frequency scaling algorithms for improving the CPU's energy efficiency	This paper focuses on improving the energy efficiency of service center server CPUs by using dynamic frequency scaling to balance computational performance and power consumption. Two algorithms, an immune inspired algorithm and a fuzzy logic based algorithm, were created and tested. The immune inspired algorithm uses a model based on human antigens to classify the server's power/performance state and determine actions to optimize power consumption. The fuzzy logic algorithm adjusts the CPU's performance based on workload, while also filtering workload spikes to avoid excessive p-state transitions. These techniques aim to reduce the overall energy consumption of service center server CPUs.
412264	41226459	Clustering Sparse Graphs.	The development of a new algorithm for clustering sparse unweighted graphs is presented. The goal is to partition nodes into clusters with high density within clusters and low density across clusters. This is challenging due to the small edge densities within and across clusters. The algorithm takes into account the noisiness of sparse graphs and uses a different penalization approach for missing edges within clusters and present edges across clusters. It is tested on the well-known "planted partition" model and is shown to outperform previous methods in terms of clustering sparser graphs with smaller clusters. This is also supported by empirical evidence.
412265	41226568	Personalized recommendation of social software items based on social relations	This study focuses on personalized recommendations for social software items such as web-pages, blog entries, and communities. The recommendations are based on the user's social network information, which is collected from different sources within the organization. The research compares the effectiveness of recommendations based on the user's familiarity network and their similarity network. Additionally, the impact of providing explanations for recommended items, including related people and their relationship to the user and the item, is also examined. Results from a user survey and a field study suggest that recommendations based on the familiarity network are more effective and that providing explanations can increase interest in recommended items. 
412266	41226624	Characterizing VeSFET-Based ICs With CMOS-Oriented EDA Infrastructure	This paper discusses the application of standard cell design methodology to create vertical slit field effect transistor (VeSFET)-based ASICs using modern CMOS EDA tools. The study focuses on a family of VeSFET canvases, specifically chain canvases, that offer improved performance and power consumption compared to circuits using isolated transistor canvases. The designs were compared to those implemented with a commercial low power CMOS library and corresponding VeSFET libraries. Results showed significant power reduction in VeSFET-based designs compared to CMOS-based designs with the same performance. This highlights the potential for VeSFETs to be a promising alternative to CMOS technology in ASIC design.
412267	41226726	Event-Triggered Optimal Control With Performance Guarantees Using Adaptive Dynamic Programming.	This paper focuses on event-triggered optimal control (ETOC) for continuous-time nonlinear systems. A new event-triggering condition is proposed, which allows for the design of ETOC methods based on the Hamilton-Jacobi-Bellman (HJB) equation. Performance guarantees are provided through upper and lower bounds. An adaptive dynamic programming (ADP) method is developed using a critic neural network (NN) to approximate the value function of the HJB equation. It is proven that semiglobal uniform ultimate boundedness can be achieved for states and NN weight errors with the ADP-based ETOC. Simulation results show the effectiveness of this method.
412268	41226826	Automatic metadata generation for active measurement.	The Internet is a challenging environment for conducting empirical research due to potential bias introduced by local conditions such as CPU or network load. In response, the authors propose a framework for local environment monitoring during Internet measurement experiments. This framework aims to provide a more comprehensive understanding of measurement results and increase the reproducibility of findings. The tool SoMeta is used to implement this framework and evaluate its runtime costs. Through a series of intentional perturbations of the local environment during active probe-based measurements, the authors demonstrate how simple local monitoring can reveal biases in the results. They also discuss the potential for expanding this framework to provide metadata for a wider range of Internet measurement experiments. 
412269	412269248	Stable Delaunay Graphs.	The concept of stability of edges in a Euclidean Delaunay triangulation is introduced, with a parameter $$\\alpha$$ being used to determine if an edge is $$\\alpha$$-stable. A subgraph G is defined as a $$(c\\alpha, \\alpha)$$-stable Delaunay graph if every edge in G is $$\\alpha$$-stable and every $$c\\alpha$$-stable edge in the triangulation is also in G. This concept can also be applied to Delaunay triangulations under different convex distance functions. It is shown that if an edge is stable in the Euclidean Delaunay triangulation, it is also stable in the triangulation under a different distance function, and vice-versa. This relationship leads to a linear-size kinetic data structure for maintaining a $$(8\\alpha, \\alpha)$$-stable Delaunay graph as points move, with a time complexity of $$O(\\log n)$$ for each event during the motion. It is also shown that many properties of the Euclidean Delaunay triangulation are retained in any stable Delaunay graph.
412270	41227015	Software-directed power-aware interconnection networks	Interconnection networks are used in various parallel computer systems, but recent technology trends have led to increased power consumption. Power-aware networks are needed to address this issue, but current techniques are limited and not tailored to specific applications. Research on power-aware parallelizing compilers is also in its early stages. In this paper, the authors propose software techniques that allow for dynamic voltage scaling of network links during application runtime, based on offline DVS settings generated during static compilation. Simulations show that this approach can reduce link power consumption by up to 76.3&percnt;, with minimal impact on network performance.
412271	4122711	Feature selection for ensembles applied to handwriting recognition	The use of feature selection in creating ensembles has been proven to be an effective strategy due to its ability to produce high-quality subsets of features. This paper introduces a new approach to ensemble feature selection using a hierarchical multi-objective genetic algorithm. The algorithm operates in two levels, first selecting features to generate a set of classifiers and then choosing the best team of classifiers. It has been tested in both supervised and unsupervised feature selection scenarios, specifically in the recognition of handwritten digits and month words. Comparisons with traditional methods such as Bagging and Boosting showed that this approach leads to significant improvements in low error rate situations. These improvements are demonstrated through recognition rate comparisons.
412272	41227243	Using Context Similarity for Service Recommendation	Recommender systems have been successful in solving the issue of information overload for consumers by providing recommendations for goods and services. However, existing systems for service recommendation do not consider both service qualities and contextual dimensions simultaneously. To address this gap, a new approach is proposed that combines both criteria using concept abduction as a reliable measure for context similarity. This approach aims to improve the effectiveness of recommendations in situations with limited feedback. Experiments using real-world data demonstrate the usefulness of this method in addressing data sparsity by incorporating rankings from nearby context segments.
412273	4122737	Mobile access to cultural heritage: mobile-CH 2016.	The Mobile-CH workshop, taking place alongside the MOBILE HCI Conference, will serve as a platform for the intersection of cultural heritage research and personalization using technology, particularly mobile and ubiquitous devices. The workshop aims to bring together researchers and practitioners working on cultural heritage to explore the potential of cutting-edge technology in enhancing the experience at heritage sites. The outcome of the workshop will be a collaborative research agenda that will guide future research and encourage partnerships among different disciplines.
412274	41227448	Coping with WORDNET Sense Proliferation	WORDNET is renowned for its detailed word sense distinctions, but this can pose a challenge for computational tasks like word sense disambiguation. In order to address this issue, one approach is to reduce the number of senses by grouping them into equivalence classes. However, the authors of this paper propose a different approach. They acknowledge that some of the sense distinctions in WORDNET may be questionable, but they prefer to maintain its semantic richness and instead suggest extensions to make word sense disambiguation more manageable.
412275	41227582	Capacity Theorems for Distributed Index Coding	Index coding is a communication method in which a server sends messages and their respective receivers use side information to reduce the amount of communication needed. Distributed index coding is an extension of this, where multiple servers broadcast different subsets of messages. This paper studies the optimal tradeoff between message and server broadcast rates, known as the capacity region, for a general distributed index coding problem. Inner and outer bounds on the capacity region are established, with matching sum-rates for various scenarios. The proposed inner bound uses a distributed composite coding scheme, while the outer bound is based on the polymatroidal axioms of entropy and functional dependences. These bounds improve upon existing ones and have features demonstrated through examples.
412276	412276112	Coalitional Games for Transmitter Cooperation in MIMO Multiple Access Channels	Wireless networks rely on cooperation between nodes to achieve higher throughputs. This paper focuses on determining the feasibility and stability of cooperation between rational nodes in a wireless network using cooperative game theory. The stability of the grand coalition of transmitters is examined, taking into account external interference and different receiver strategies. The core of the game is used to determine stability, which is a complex problem. Results show that cooperation is stable for a single user decoding receiver, but unstable for an interference cancelling receiver with a fixed decoding order. However, allowing for time sharing between decoding orders can make cooperation stable at high signal-to-noise ratios. This research highlights the importance of cooperation in wireless networks and its potential to improve individual user rates.
412277	41227752	A theory of the motion fields of curves	This article discusses a study on the motion field generated by moving 3-D curves observed by a camera. The relationship between optical flow and motion field is explored, showing that the assumptions made in computing optical flow are difficult to defend. The motion field of a nonrigid curve is then studied, introducing the concept of isometric motion and analyzing the spatiotemporal surface and its differential properties. It is shown that the full motion field of a curve cannot be recovered from this surface, but equations can be used to characterize it up to a rigid transformation. The study also examines the use of two cameras for disambiguating stereo correspondences.
412278	41227857	The First International Workshop on Non-Functional System Properties in Domain Specific Modeling Languages (NFPinDSML2008)	The workshop aimed to bring together researchers and practitioners from different fields to discuss the integration of non-functional properties in software systems. This included those from language engineering and experts in Domain Specific Modeling Languages. The focus was on expanding the principles of reasoning about non-functional properties in software systems and model-driven engineering. 
412279	41227949	Feature Curve Metric for Image Classification	The paper introduces a new classifier, called the feature cure metric (FCM) which combines elements from existing classifiers for hand gesture and face recognition. It uses a distance metric between the test sample and prototype samples to improve accuracy. Experiments on various databases show that FCM outperforms other well-known classifiers, such as nearest neighbor, nearest feature line, and center-based nearest neighbor. The proposed approach achieves higher recognition rates on Jochen Triesch Static Hand Posture, Yale face, and JAFFE face databases.
412280	4122803	An event based approach to web service design and interaction	The paper proposes a new approach to web service design and interaction, where web services participate in shared business events instead of one-to-one method invocations. These events are broadcasted to all participating web services simultaneously, and there is a distinction between transactional and non-transactional events. The concept of business events is highlighted as essential in the analysis and design phase, and the paper demonstrates how this approach can be implemented using SOAP messaging. This approach has the potential to improve efficiency and effectiveness in web service interactions.
412281	41228150	Relaxed and Hybridized Backstepping.	This technical note discusses nonlinear control systems with structural obstacles that make it difficult to design traditional continuous backstepping feedback laws. Instead, a new approach is proposed where the origin of the closed-loop system is not globally asymptotically stable, but a suitable attractor is practically asymptotically stable. A hybrid feedback law is designed using a combination of backstepping and locally stabilizing controllers. The nonlinear dynamics are represented using a differential inclusion method, and the results are demonstrated on a system that cannot be stabilized globally using backstepping alone. 
412282	41228228	EnrichNet: network-based gene set enrichment analysis.	EnrichNet is a new approach and web-application that aims to improve the analysis of functional associations between gene/protein sets and databases. It addresses four limitations of the commonly used over-representation-based enrichment analysis: it can score non-overlapping gene/protein sets, considers genes with missing annotations, takes into account physical interactions between the gene/protein sets, and recognizes tissue-specific associations. EnrichNet combines a graph-based statistic and interactive sub-network visualization to prioritize putative functional associations and provide a biological interpretation of the results. When applied to sets of genes involved in human diseases, EnrichNet identified new pathway associations by highlighting a dense sub-network of interactions between their corresponding proteins.
412283	41228322	Piggyback CrowdSensing (PCS): energy efficient crowdsourcing of mobile sensor data by exploiting smartphone app opportunities	Mobile crowdsourcing is a rapidly evolving field thanks to the widespread use of sensor-enabled smartphones. This includes systems that provide information on highway congestion, but participation in these systems can drain battery resources. To address this issue, the authors propose Piggyback CrowdSensing (PCS), which collects sensor data during phone calls or app usage to minimize energy usage. They use a user-specific prediction model to determine the best times to collect data and found that PCS can collect large-scale sensor data while using significantly less energy compared to existing methods. 
412284	41228418	BeWell+: multi-dimensional wellbeing monitoring with community-guided user feedback and energy optimization	This paper discusses the development of a new smartphone app, BeWell+, which monitors three aspects of health: sleep, physical activity, and social interaction. The app uses persuasive feedback through an ambient display on the phone's wallpaper to promote better behavioral patterns. The new version of the app includes two new mechanisms: community adaptive wellbeing feedback, which can be tailored to different user communities, and wellbeing adaptive energy allocation, which prioritizes monitoring and feedback on specific health dimensions. The app was tested in a 19-day field trial with 27 participants, and results showed that the app was successful in promoting healthier lifestyles through user understanding and response to feedback. 
412285	41228557	Sliding Mode Model Semantics and Simulation for Hybrid Systems	This article discusses the characterization and simulation of dynamic physical systems operating in sliding regimes, which involve complex continuous behaviors at multiple temporal and spatial scales. To simplify behavior generation, models use time scale and parameter abstraction techniques, resulting in hybrid systems with both discrete and continuous behaviors. Mode transitions are caused by internal state changes and external control signals, and can sometimes exhibit chattering behaviors. This presents challenges for conventional simulation methods, so the article proposes an efficient and adaptive algorithm that considers the model semantics at the discontinuous boundaries. Simulation results show that this algorithm is more accurate and efficient for sliding mode systems compared to conventional integration methods.
412286	41228629	A Design Model For Lifelong Learning Networks	The implementation of lifelong learning facilities has become a top priority for higher education institutions and distance learning programs, aiming to meet the needs of both industry and society. With the help of ICT networks, these facilities can be accessed seamlessly and ubiquitously at home, work, and in schools and universities. This requires a shift from traditional course and program-focused models to a learner-centered and learner-controlled approach. A conceptual model based on self-organization theory, learning communities, agent technologies, and learning technology specifications has been developed, and an exploratory implementation has been put into practice. Further reflection and development is needed to fully support lifelong learning.
412287	41228745	Probabilistic Sensor Fusion for Ambient Assisted Living.	There is a growing need to improve the current methods of providing healthcare, especially in the home setting. As part of the Sensor Platform for HEalthcare in Residential Environment (SPHERE) Interdisciplinary Research Collaboration (IRC), researchers are developing a multi-sensor platform with various network connections. However, this presents challenges in combining data from different sensors. To address this, Bayesian models for sensor fusion are introduced, which can identify the most useful modalities and relevant features for specific activities. The model also combines location prediction and activity recognition, resulting in a more efficient and accurate system. Testing on data from the SPHERE house shows its effectiveness and superiority over other benchmark models.
412288	41228868	A Systematic Approach to Platform-Independent Design Based on the Service Concept	This paper discusses the importance and benefits of using the service concept in the model-driven design of distributed applications. A service is defined as the observable behavior of a system without imposing constraints on its internal structure. By utilizing services and designing application parts based on them, the design is not limited by interaction patterns provided by a specific middleware platform. This allows for platform-independence and potential reuse of application parts across various middleware platforms. The service concept is also used in defining an abstract platform, which considers the characteristics of potential target middleware platforms for platform-independent design. The paper also explores the trade-offs and alternatives for platform-specific realization in this approach.
412289	41228917	A reconstruction and extension of Maple's assume facility via constraint contextual rewriting	Maple's symbolic evaluator and assume facility allow for powerful conditional rewriting. Previous research showed that Maple's evaluation can be seen as constraint contextual rewriting (CCR), which integrates decision procedures through a specific interface. This study extends the analysis to the general solver, which handles problems beyond linear arithmetic. A flaw was discovered that causes Maple to give incorrect results in certain contexts, as it falsely assumes the general solver uses all available assumptions. While a quick fix would weaken the assume facility, a more comprehensive approach based on CCR techniques avoids this issue and results in stronger simplification abilities.
412290	41229071	A Theory of Singly-Linked Lists and its Extensible Decision Procedure	The availability of a decision procedure for pointer-based data structures is essential for effective reasoning. However, current solutions only offer approximate solutions that lack precision in verification techniques. In this paper, a new theory called TLL is introduced for singly-linked lists, which can accurately express both data and reachability constraints. The decidability of TLL is ensured and a practical decision procedure is designed, making it compatible with existing decision procedures for first-order logic theories. This provides a more precise and efficient approach to reasoning about pointer-based data structures.
412291	412291104	A First Approach to Provide Anonymity in Attribute Certificates	This paper discusses the importance of authorization and anonymity in internet applications. Traditional authorization methods are not always effective, but attribute certificates proposed by ITU-T offer a suitable solution. As internet transactions are easily recorded, anonymity has become a desirable feature. The paper proposes a solution to enhance X.509 attribute certificates to include conditional anonymity. A protocol is also presented for obtaining these certificates while protecting user anonymity through a blind signature scheme. The use of these certificates is explored, along with potential problems and open issues. 
412292	41229212	Automatically Improving SAT Encoding of Constraint Problems Through Common Subexpression Elimination in Savile Row.	Efficient solving of Propositional Satisfiability (SAT) problems relies heavily on the formulation of the problem instance. Preprocessing and inprocessing techniques have been developed to improve the efficiency of SAT solvers, but these methods operate on the low-level representation of the problem which makes it difficult and costly to identify higher-level patterns. Instead of directly reformulating the SAT representation, this approach uses automated reformulations on a higher level representation of the problem called a constraint model. This approach, known as Common Subexpression Elimination (CSE), has been shown to significantly improve the formulation of constraint satisfaction problems and can also have similar benefits when encoding the reformulated model to SAT and solving it with a state-of-the-art solver, resulting in speed improvements of over 100 times in some cases. 
412293	41229325	Collaborative work with linear classifier and extreme learning machine for fast text categorization	This paper discusses the importance of fast text categorization in the digital age and explores current methods that either sacrifice accuracy for speed or vice versa. The authors propose a new approach that combines a linear classifier and an extreme learning machine (ELM) to achieve both speed and accuracy. The linear classifier, obtained through a modified non-negative matrix factorization algorithm, quickly maps documents from the original term space into the class space. The ELM then uses nonlinear and linear transformations to further improve classification accuracy. Experimental results show that this approach is not only accurate but also significantly faster, with a 180% increase in speed compared to previous methods.
412294	41229462	Post-processing scheme for improving recognition performance of touching handwritten numeral strings	This paper outlines a method for improving recognition accuracy of handwritten numeral strings that are touching. The approach involves using verification factors, such as structural features and recognition probability, to identify and correct mis-segmented digits. A reconsideration condition is introduced, along with an additional segment digit set, to further refine the recognition results. Through experiments on a database of touching handwritten numeral strings, the proposed method showed promising results in terms of both accuracy and reliability.
412295	41229530	Timed default concurrent constraint programming	Synchronous programming is a powerful approach for programming reactive systems. It is based on the idea that processes are relations extended over time, and it provides a simple but effective model for timed, determinate computation. This model, called tcc, is extended to include strong time-outs, which are supported in synchronous programming languages. These operations are non-monotonic, but a compositional semantics is provided using Reiter's default logic. This framework, called Default cc, has a basic set of combinators and supports multiform time, orthogonal pre-emption, and executable specifications. It can also be read as logical formulae and compiled into finite state automata, allowing for separate compilation and run-time tradeoffs.
412296	412296123	Maximizing bichromatic reverse nearest neighbor for  -norm in two- and three-dimensional spaces	The Bichromatic Reverse Nearest Neighbor (BRNN) problem has been widely studied in spatial database research. This paper introduces a related problem called MaxBRNN, which involves finding the optimal region that maximizes the size of BRNNs for the L p-norm in two- and three-dimensional spaces. This problem has practical applications, such as finding the best location for a new server to attract the most customers based on proximity. Previous approaches have been limited due to the large number of possible points. However, the authors propose an efficient algorithm called MaxOverlap that takes advantage of certain properties of the problem. Extensive experiments demonstrate the effectiveness of this algorithm. 
412297	41229728	Mosaicing-by-recognition for video-based text recognition	Recognizing text from a hand-held video camera can be difficult because the camera may capture multiple frames with overlapping text. To improve the quality of the text image, the frames must be registered and mosaiced, accounting for camera shakes. This paper proposes a mosaicing-by-recognition technique that combines video mosaicing and text recognition into a single optimization problem. Using a dynamic programming-based optimization algorithm, this technique can accurately estimate distortions and achieve high character recognition accuracy, even with various camera movements. Experimental results show that the proposed technique can achieve around 90% perfect distortion estimation and over 95% character recognition accuracy. 
412298	41229842	Real-Time Retrieval for Images of Documents in Various Languages Using a Web Camera	The proposed method is a real-time retrieval approach for document images in different languages. It uses queries in the form of images captured by a web-camera to retrieve corresponding document images from a database. This method is an extension of a previous method for English documents, which only used centroids of word regions as feature points and was not applicable to languages like Japanese and Chinese. The proposed method introduces additional features to enable real-time retrieval for document images in various languages. 
412299	41229925	Quantified reading and learning for sharing experiences.	This paper discusses two main topics: the "experiential supplement" project, which aims to capture and transfer human experiences to others, and sensing technologies for producing these supplements in the context of learning. The focus is on reading, and methods for quantifying reading activities and estimating English ability and confidence are presented with experimental results. These measurements are taken using various sensors, including eye trackers, EOG, EEG, and first-person vision. The paper highlights the potential of using these technologies to enhance the learning experience.
412300	41230043	Towards a Conscious Choice of a Similarity Measure: A Qualitative Point of View	This paper examines the importance of similarity measures in various applications, such as case based reasoning and data mining. The authors propose studying these measures based on the ordering relation they create between objects. They use a method from measurement theory to determine the conditions for a numerical measure or a class of measures to represent a given ordering relation. The focus is on the various conditions of independence that must be met. This research highlights the significance of carefully selecting a similarity measure in order to accurately represent the relationship between objects in different contexts. 
412301	41230147	Acyclic Directed Graphs to Represent Conditional Independence Models	This paper focuses on conditional independence models that follow the graphoid properties. It explores the use of acyclic directed graphs (DAGs) to represent these models and proposes a new algorithm for constructing a DAG based on an ordering of random variables. The benefits and unique features of using DAGs in this context are discussed. The paper also presents conditions that guarantee the existence of perfect maps, which can be used to develop a procedure for finding perfect maps for certain types of independence models.
412302	412302108	EMD-L1: an efficient and robust algorithm for comparing histogram-based descriptors	EMD-L1 is a fast algorithm for calculating the Earth Mover's Distance (EMD) between two histograms. It simplifies the original formulation by reducing the number of unknown variables to O(N) and the number of constraints by half. The algorithm is formally equivalent to the original EMD with L1 ground distance and uses an efficient tree-based approach to compute the distance. It has a time complexity of O(N2), making it much faster than previous algorithms. EMD-L1 allows for comparing histogram-based features, which was previously not feasible. Experiments on shape recognition and interest point matching show that EMD-L1 outperforms previous state-of-the-art methods in these tasks.
412303	41230322	DAILY PHYSIOLOGICAL SIGNAL MONITORING SYSTEM FOR FOSTERING SOCIAL WELL-BEING IN SMART SPACES	The proposed system is a socially acceptable way to monitor physiological signals using a natural sensing interface, BioPebble, and an intuitive information display, Rainbow display. The BioPebble is a simple and enjoyable stone-type interface embedded in everyday objects, while the Rainbow display uses an intuitive visualization method for novice users. The information is wirelessly transferred to a mobile personal station and mapped according to individual differences. The experiment showed that the BioPebble is more comfortable and aesthetically pleasing than disposable wired interfaces, and the Rainbow display is easily understood by novice users. This system aims to promote social well-being by encouraging regular monitoring of physiological signals for early detection of abnormalities.
412304	41230429	A Logical Framework For Set Theories	Axiomatic set theory is widely accepted as the foundation of mathematics and is useful for managing mathematical knowledge. However, there are gaps between the official set theory formulations and actual mathematical practice. This work presents a new framework for formalizing different strengths of axiomatic set theories, from basic to full ZF. It allows the use of set terms, but ensures their validity through a static check. The framework is based on two set-theoretical principles: extensionality and comprehension, with the option to include induction and the axiom of choice. Comprehension is formulated as a relation between elements and formulas. Different systems vary in their use of safety relations, which are defined syntactically and based on the safety relation used in commercial query languages for databases.
412305	412305223	Ranking queries on uncertain data: a probabilistic threshold approach	Uncertain data is a common occurrence in certain applications like environmental surveillance and mobile object tracking. Top-k queries, which rank the data based on a certain criteria, are useful in analyzing uncertain data in these applications. This paper focuses on the problem of answering probabilistic threshold top-k queries, which calculates the probability of at least p for an uncertain record to be in the top-k list. The paper presents three algorithms - an exact one, a fast sampling one, and a Poisson approximation based one - for efficiently answering these queries. An empirical study using real and synthetic data shows the effectiveness of probabilistic threshold top-k queries and the efficiency of the proposed algorithms.
412306	41230628	Unfolding Based Minimal Test Suites for Testing Multithreaded Programs	This paper discusses the challenge of finding the smallest set of tests that cover all executable statements in a multithreaded program. Previous research has used unfoldings to accurately represent the concurrency in such programs and generate test cases. Building on this, the paper presents a method for using unfoldings to generate the minimal test suite that covers all executable statements. However, this problem is shown to be NP-complete in the size of the unfolding. The paper also proposes using SMT-encodings to solve this problem and provides preliminary results for several benchmarks. Additionally, the paper shows that finding the minimal test suite for any terminating safe Petri net is also NP-complete in the size of its unfolding. 
412307	41230714	Oblivious Network RAM and Leveraging Parallelism to Achieve Obliviousness.	ORAM, or Oblivious RAM, is a cryptographic tool that allows a trusted CPU to access untrusted memory without revealing any information about the data being accessed. It has many potential uses in secure processor design and secure multi-party computation for managing large amounts of data. However, due to a lower bound on its efficiency, ORAM incurs a moderate cost in practical applications. To address this issue, the Oblivious Network RAM model of computation is proposed, where a CPU communicates with multiple memory banks in a way that hides the specific address being accessed. This reduces the overall cost of obliviousness and has potential applications in secure processor design and distributed storage with a network adversary. New constructions for simulating programs in this model are also presented.
412308	41230857	Reducing power density through activity migration	Modern microprocessors have uneven distribution of power dissipation, resulting in localized hot spots with higher temperatures than surrounding areas. This can lead to decreased reliability and even failure. To address this issue, activity migration is proposed, which involves moving computation between multiple replicated units to reduce peak junction temperature. By incorporating a thermal model that considers the temperature-dependent leakage power, it is shown that this technique can increase sustainable power dissipation by almost two times while maintaining the same junction temperature limit. Alternatively, peak die temperature can be reduced by 12.4°C at the same clock frequency. The model suggests that migration intervals of 20-200 μs are necessary for maximum sustainable power increase. Different forms of replication and migration policy control are evaluated. 
412309	41230938	Optimal maintenance of a spanning tree	This article discusses the importance of keeping track of history in dynamic network protocols and presents a communication optimal maintenance algorithm for a spanning tree in a dynamic network. This algorithm has an amortized message complexity of O(V), where V is the number of nodes, and a message size of O(log &verbar;ID&verbar;). This efficient maintenance of a spanning tree can improve various distributed algorithms such as broadcast, multicast, routing, and termination detection. The authors also introduce a novel technique to save communication by using past information to deduce present information. This technique is one of the main contributions of the study.
412310	41231038	Transliteration by bidirectional statistical machine translation	The paper discusses a system that uses statistical machine translation to transliterate between all language pairs in a shared task. This technique does not rely on language-specific assumptions or dictionaries, and instead directly translates sequences of tokens from the source language to the target language. The system is comprised of two phrase-based SMT decoders, one generating the target from the first token to the last, and the other from the last to the first. Results show that a combination of these two approaches is most effective, with the optimal choice depending on the specific languages involved.
412311	41231135	Using ensemble method to improve the performance of genetic algorithm	Ensemble methods, which involve combining multiple learning algorithms to improve performance, have been extensively researched and applied in machine learning. The basic concept is to use a "weak" algorithm that performs slightly better than random guessing, and "boost" it into a highly accurate "strong" algorithm. Building upon this idea, a paper proposes a new hybrid optimization algorithm called GA ensemble, which uses ensemble methods to enhance the performance of genetic algorithms. This involves running multiple genetic algorithms on the same problem and using bagging to sample from a pool of solutions. Experimental results on various optimization problems demonstrate that the ensemble approach greatly improves the performance of genetic algorithms.
412312	41231213	The Mystery of Structure and Function of Sensory Processing Areas of the Neocortex: A Resolution.	This paper discusses various neural models that have been proposed to explain memory in primates. While there is no direct evidence to support one model over others, the authors suggest that models can be evaluated based on their complexity and predictive power. The authors propose a computational framework that assumes neural information processing is done by generative networks. This framework suggests a possible relationship between functional memory units and information encoding in primates, as well as the entorhinal-hippocampal loop. The authors also explore the idea that different layers of the cortex may have different functions. Finally, the concept of the homunculus fallacy is discussed in relation to this computational framework. Keywords include auto-associator, information processing, entorhinal-hippocampal loop, and information maximization.
412313	41231345	Communication Efficient Coresets for Empirical Loss Minimization.	This paper addresses the issue of minimizing empirical loss with l2-regularization in distributed settings where communication cost is significant. Traditional methods like SGD are commonly used, but their high communication cost can lead to decreased performance. To address this, the paper proposes a new approach that uses a small summary of the data, called coreset, to reduce communication cost while maintaining good convergence. The paper presents a general framework for analyzing coreset-based optimization and offers insights into existing algorithms. A new coreset construction is also introduced, along with its convergence analysis for problems like logistic regression and support vector machines. Preliminary experiments on real-world datasets show promising results for this algorithm.
412314	412314111	From Color Sensor Space to Feasible Reflectance Spectra	The interaction between light and object surfaces produces color signals that are responsible for the outputs of digital acquisition systems. In order to reverse this process and retrieve wavelength information, a lot of research has been done since 1964. The main approach is to use linear models to establish a one-to-one relationship between sensor data and reflectance spectra, while ensuring the quality and naturalness of the recovered spectrum. In this paper, the authors propose a solution to improve the quality of the recovered spectrum by considering smoothness and physical feasibility, which is achieved through the use of linear models. This strategy can be applied to any generic recovery method. 
412315	41231542	CAESAR_SOLVE: A generic library for on-the-fly resolution of alternation-free Boolean equation systems	Boolean equation systems (Bess) are a useful tool for modeling verification problems on finite-state concurrent systems. These problems, such as equivalence checking and model checking, can be solved on the fly without explicitly constructing the state space of the system. A demand-driven approach is used for construction and resolution of the Bess. A generic software library has been developed for on-the-fly resolution of alternation-free Bess. It currently offers four resolution algorithms, including specialized ones for handling specific types of Bess in a memory-efficient way. The library is developed within the Cadp verification toolbox and is used for on-the-fly equivalence checking, model checking, and state space reduction.
412316	41231666	Factors Influencing Digital Forensic Investigations: Empirical Evaluation of 12 Years of Dubai Police Cases.	Digital forensics is a crucial aspect of investigations, where minimizing the number of person-hours spent while ensuring the authenticity of evidence is important. However, the literature shows that there are several challenges that lead to an increase in person-hours. This paper examines these challenges and argues that they do not fully explain the increase in investigation time. The study uses real case records from the Dubai Police and identifies other factors that contribute to the increase in person-hours. The paper concludes by highlighting the various factors that affect person-hours, contrary to what is commonly proposed in the literature.
412317	41231762	The Phenomenology of Remembered Experience: A Repertoire for Design.	The study aims to understand how people experience autobiographical remembering and how design-driven research can benefit from this understanding. Through interviews using the repertory grid technique, the study identifies five categories of personal constructs that characterize people's remembered experiences: contentment, confidence/unease, social interactions, reflection, and intensity. These categories align with previous classifications of personal constructs and models of human emotion. The study suggests that this categorization can be used to evaluate interactive technologies for supporting remembering and highlights the potential for future improvements.
412318	41231832	The International Children's Digital Library: Description and analysis of first use	The International Children's Digital Library (ICDL) is a five-year research project aimed at providing children with access to a diverse collection of children's books from around the world. The paper discusses the importance of this research and outlines the development of new interface technologies to make the ICDL accessible to children. It also compares the ICDL to other digital libraries for children and presents an analysis of the first seven weeks of its public use on the web. The ICDL hopes to promote literacy and cultural understanding among children through its international collection of books.
412319	41231946	(Almost) Tight bounds and existence theorems for single-commodity confluent flows	This article discusses the concept of confluent flow, where the flow of a commodity leaves a node along a single edge. It focuses on single-commodity confluent flow problems, specifically the minimization of congestion in routing node demands to a single destination. The article presents approximation algorithms and hardness results for this problem, as well as a polynomial-time algorithm for determining a confluent flow with congestion at most 1 + ln(k) in a directed graph. It also considers a demand maximization version of the problem and shows that the gap between confluent and splittable flows is smaller in k-connected graphs. The existence of confluent flows with low congestion is proven using topological techniques. 
412320	41232041	Parallel checkpoint/recovery on cluster of IA-64 computers	ChaRM64 is a high availability parallel run-time system designed for parallel programs on a cluster of IA-64 computers. It uses a user-level, single process checkpoint/recovery library and implements a coordinated checkpointing and rollback recovery mechanism, as well as quasi-asynchronous migration and dynamic reconfiguration. This allows it to effectively handle cluster node crashes and hardware faults. The system has been implemented for PVM on Linux and an MPI version is currently being developed. ChaRM64 is one of the few projects that have been successfully completed for IA-64 architecture. 
412321	41232194	A Discriminative Model With Multiple Temporal Scales For Action Prediction	Intelligent systems need to quickly recognize actions in order to react effectively. This is especially important in applications like detecting criminal activity, where decisions must be made based on incomplete videos. A new model has been proposed that can predict the action class from a partially observed video by considering the temporal dynamics of human actions and incorporating both past and present features. This model also enforces label consistency between smaller segments and the overall video. Experiments on public datasets have shown that this approach outperforms other methods for predicting actions.
412322	4123225	Tractable dataflow analysis for distributed systems	Automated behavior analysis is a useful tool for developing and maintaining distributed systems. This paper presents a dataflow analysis technique that can detect unreachable states and actions in distributed systems. The technique is based on an approximate approach by Reif and Smolka but is more accurate due to the use of action dependency and history sets. While it may not detect all errors, it can efficiently detect nontrivial errors in systems with loops and nondeterministic structures. This makes it a practical and tractable tool for analyzing preliminary designs of distributed systems and can be integrated into software development tools. The technique is demonstrated through case studies and a prototype implementation is presented.
412323	41232376	An Efficient Policy System for Body Sensor Networks	Body sensor networks (BSNs) are a promising technology for healthcare, where biosensors continuously monitor a user's physiological parameters. Unlike conventional sensor networks for environmental monitoring, BSNs need to be adaptive and easily managed. Security is also crucial in BSNs, leading to the development of a policy system that enables policy-driven management at the sensor level. This system allows for adaptability through dynamic loading and the ability to enable and disable policies without shutting down nodes. It also enables fine-grained access control through authorization policies. The design and implementation of this policy system are discussed, and experimental results show its viability and potential to accelerate application development for BSNs in healthcare.
412324	41232462	A Computational Method for Segmenting Topological Point-Sets and Application to Image Analysis	A new computational method has been proposed for segmenting topological subdimensional point-sets in scalar images of any spatial dimension. This method involves calculating the homotopy class defined by the gradient vector in a subdimensional neighborhood around each image point. This neighborhood is defined as a linear envelope created by a given subdimensional vector frame. By using the first largest principal directions of the Hessian, this method can segment critical points such as extrema and saddle points, as well as positive and negative ridges and other types of critical surfaces. The result is a hierarchy of point-sets of different dimensionalities that can be used for geometrical grouping using only local measurements. This method is also efficient and has been demonstrated in two examples where an additional image coordinate or local orientation parameter was used.
412325	41232525	Spatio-temporal segmentation using dominant sets	Pairwise data clustering techniques are becoming increasingly popular for image segmentation problems, but they are too computationally demanding for video segmentation due to their scaling behavior with data quantity. This poses a challenge for large datasets, as the number of potential comparisons scales with O(N2), making these approaches unfeasible. To address this issue, strategies to reduce the number of comparisons required by subsampling the data and extending the grouping to out-of-sample points are needed. In this paper, the authors present an approach to out-of-sample clustering using the dominant set framework and apply it to video segmentation. Results show that this approach is comparable to other recent methods in terms of segmentation quality, but much faster.
412326	41232614	Fast Generation of Random Permutations via Networks Simulation	The paper discusses the problem of generating random permutations with a uniform distribution, specifically on two models of parallel computations: the CREW PRAM and the EREW PRAM. The main result is an algorithm that runs in O(log log n) time and uses O(n 1+o(1) ) processors on the CREW PRAM, which is the first o(log n) -time algorithm for this problem. On the EREW PRAM, a simpler algorithm is presented that generates a random permutation in O(log n) time using n processors and O(n) space. Both algorithms use a random switching network and simulate it on the PRAM model efficiently. This algorithm is an improvement over previously known algorithms for the exclusive write PRAMs.
412327	41232734	Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning	The challenges of learning, planning, and representing knowledge at different levels of time are important issues in the field of AI. In this paper, the authors propose using options, which are closed-loop policies that allow for actions to be taken over a period of time, to address these challenges within the framework of reinforcement learning and Markov decision processes. Options can be used interchangeably with primitive actions in planning and learning methods. The authors also present results for using options during execution, learning about options, and improving options. These results are established in a simpler and more general setting, without committing to any particular approach, making them applicable to a wider range of problems.
412328	41232817	Rectilinear Shortest Paths in the presence of Rectangular Barriers	This paper addresses the problem of finding the shortest L1 path from a given source point to a query point, without crossing any of the given barriers (disjoint isothetic rectangles) in the plane. A previous solution for a restricted version of this problem had a time complexity of O(n2). The authors propose a new approach that involves preprocessing the source point and barriers to create a planar subdivision, allowing for quick traversal from the query point to the source point. By proving the monotonicity of the path in at least one direction, a plane sweep technique is applied to divide the plane into O(n) rectangular regions. The resulting algorithm has a complexity of O(n logn) preprocessing time, O(n) space, and O(logn+k) query time, with an additional O(logn) query time for finding the length of the path. A lower bound of Θ(n logn) is shown for the case where the source and destination points are known in advance, making this algorithm optimal for that scenario. 
412329	41232950	Enumerating Global Roundings of an Outerplanar Graph	The article discusses the concept of global rounding in a connected weighted graph, where a hypergraph is created based on the shortest paths in the graph. A real assignment is given to the vertices of the graph and a global rounding is a binary assignment that satisfies certain conditions. The authors of the paper proposed a conjecture that there are at most \V\ + 1 possible global roundings for the hypergraph. The paper proves that this conjecture holds true for outerplanar graphs and provides a polynomial time algorithm for enumerating all the global roundings in such graphs.
412330	41233032	Variational Markov chain Monte Carlo for Bayesian smoothing of non-linear diffusions	This paper introduces new Markov chain Monte Carlo algorithms for Bayesian smoothing of partially observed non-linear diffusion processes. The algorithms use a deterministic approximation of the posterior distribution as a proposal for a mixture of independence and random walk sampling. This approximation is obtained by simulating a time-dependent linear diffusion process using the variational Gaussian process approximation method. The algorithms are tested on two diffusion processes and compared to state-of-the-art hybrid Monte Carlo methods. The results show that the new algorithms are accurate and more computationally efficient, except in cases of large observation errors and low observation densities. Overall, the variational approximation assisted sampling algorithm outperforms hybrid Monte Carlo.
412331	41233120	Redundant bit vectors for quickly searching high-dimensional regions	Audio fingerprinting applications require searching in high dimensions to find similar items in a database. This task often results in negative answers, as many queries do not match any items in the database. To address this problem, a new method called Redundant Bit Vectors (RBVs) is proposed. This method involves approximating high-dimensional regions as hyperrectangles, partitioning the query space, and using bit vectors for efficient storage and searching. RBVs outperformed linear scan and locality-sensitive hashing in terms of speed, especially for large databases and when queries are not frequently found in the database. In a data set of 239369 audio fingerprints, RBVs was 109 times faster than linear scan and 48 times faster than locality-sensitive hashing.
412332	41233217	Two-Level Volume Rendering	The paper introduces a two-level approach for volume rendering called two-level volume rendering. This approach allows for using different rendering techniques for different portions of a 3D data set. This means that different structures within the data set can be rendered using different methods such as DVR, MIP, surface rendering, value integration, or nonphotorealistic rendering. These results are then combined globally through a merging step, typically compositing. This approach allows for selectively choosing the most suitable rendering technique for each object within the data set while maintaining a reasonable level of information in the image. This is particularly useful for visualizing inner structures with semi-transparent outer parts, similar to the focus-plus-context approach seen in information visualization. The paper also presents an implementation of this approach, which allows for interactive exploration of volumetric data at high frame rates.
412333	41233348	Multiple Spike Time Patterns Occur at Bifurcation Points of Membrane Potential Dynamics.	In vitro studies have shown that repeated injections of current into neurons can produce a reliable sequence of action potentials. This response can be interpreted as the activity of a group of similar neurons receiving the same input. To study the consistency of this response under different conditions, researchers injected varying levels of current into cortical neurons and found that increasing the amplitude of the fluctuations resulted in more reliable spike times. However, at certain points, small changes in the stimulus caused large shifts in spike times, revealing multiple spike patterns. Increasing the DC offset also increased reliability and produced earlier spike times. Although reliability was reduced at these points, the information about the input was actually increased.
412334	41233465	Querying capability modeling and construction of deep web sources	This paper discusses the importance of analyzing the querying capability of deep Web sources, which can be accessed through query interfaces. This is crucial for Web applications that need to interact with these sources, such as deep Web crawling and comparison-shopping. The authors propose a querying capability model based on the concept of atomic query, which is a valid query with a minimal attribute set. They also present an approach for automatically constructing this model by identifying atomic queries for a given query interface. The experimental results demonstrate the accuracy of their algorithm.
412335	41233528	Density-Based Clustering over an Evolving Data Stream with Noise	Clustering is an important task in analyzing evolving data streams. Due to limited memory and one-pass constraints, stream clustering must meet certain requirements such as being able to handle outliers and discovering clusters with arbitrary shape. Many algorithms have been proposed for stream clustering, but none have addressed all of these requirements. In this paper, the Den Stream approach is introduced, which uses "dense" micro-clusters to summarize clusters with arbitrary shape and introduces potential core-micro-clusters and outlier micro-clusters to identify clusters and outliers. A pruning strategy is also developed to maintain the precision of micro-cluster weights with limited memory. Experiments with real and synthetic data sets show the effectiveness and efficiency of this method.
412336	41233623	Uncovering functional dependencies in MDD-compiled product catalogues	A functional dependency is a relationship between attributes in a table of data where the values of certain attributes determine the values of others. This is commonly used in database design and can also be applied to product catalogues. However, traditional methods for identifying functional dependencies require a tabular representation of the data, which may not be feasible for large combinatorial catalogues. This paper presents a new approach that uses decision diagrams to efficiently compute functional dependencies in compiled knowledge representations, making it possible to analyze even very large product catalogues. The algorithm was tested on different types of catalogues and revealed potential anomalies that could be useful for improving product recommendations.
412337	41233722	Preserving mapping consistency under schema changes	The article discusses the challenges of adapting data mappings in dynamic environments, such as the Web, where data sources can change their data, schemas, semantics, and query capabilities. The authors propose a new framework and tool, ToMAS, for automatically updating mappings as schemas evolve. This approach takes into account not only local schema changes, but also changes that may affect multiple components of a schema. The algorithm identifies affected mappings and generates alternative rewritings that are consistent with the new schemas. It also considers user preferences and strives to maintain them as schemas and mappings evolve. In cases where there are multiple potential rewritings, the algorithm may prioritize those that are closest to the existing mappings. 
412338	41233834	Performance limitations for linear feedback systems in the presence of plant uncertainty	This paper aims to examine the performance limits for feedback control systems by considering the impact of plant uncertainty. Previous studies have assumed an exact model of the plant, but it is believed that plant uncertainty should also be taken into account. By utilizing stochastic embedding, the paper formulates the problem and provides results that allow for evaluation of the best achievable performance in the presence of uncertainty. This also helps to determine whether uncertainty or other factors, such as nonminimum phase behavior, have a more significant impact on the system's performance. Keywords include performance limitations, stochastic embedding, and uncertainty. 
412339	41233910	Soundness of Formal Encryption in the Presence of Active Adversaries	This paper presents a method for proving the security of cryptographic protocols against active adversaries. The method uses a simple logic-based language to express security properties and does not require dealing with probability distributions or asymptotic notation. The paper shows that the method is sound, meaning that logic statements can be interpreted in the computational setting to accurately represent the behavior of an adversary. This is the first paper to provide a framework for translating security proofs from the logic setting to the standard computational setting for protocols with powerful active adversaries. 
412340	41234051	Examining the technology acceptance model using physician acceptance of telemedicine technology	Organizations worldwide are heavily investing in information technology (IT), making user acceptance a crucial issue in technology implementation and management. While previous research has focused on user acceptance, there is a need for further examination in different contexts. This paper reports on a study that looked at the applicability of the Technology Acceptance Model (TAM) in explaining physicians' acceptance of telemedicine technology in the health-care industry. This study was significant as it involved a new technology, user group, and organizational context. Results showed that TAM was able to explain physicians' intention to use telemedicine technology, with perceived usefulness being a significant factor. However, the relatively low explanatory power of the model suggests the need for additional factors or integration with other IT acceptance models. The study has implications for both user acceptance research and telemedicine management.
412341	41234115	A Fast Optimal Robust Path Delay Fault Testable Adder	This paper discusses the complexity of testing the adder function under the robust path delay fault model. The authors prove a lower bound of Omega(n2) for the minimum number of tests required for a complete test set for an n-bit adder. This applies to all known adder designs. They also introduce a fast O(sqrt(n)) adder that can be fully tested for robust path delay faults with a test set of size Theta(n^2). 
412342	41234238	Property analysis and design understanding	Verification is a crucial aspect of circuit and system design, and formal methods such as bounded model checking (BMC) are effective in ensuring high-quality verification. However, formal verification requires in-depth knowledge of the design implementation and finding the right set of properties can be a laborious and time-consuming task. To address these challenges, this paper introduces two techniques that aid in writing properties for BMC. The first technique helps in analyzing and refining properties to avoid redundancy and handle different scenarios. The second technique, known as inverse property checking, automatically generates valid properties based on the desired behavior. These techniques are integrated with a coverage check for BMC, resulting in a more efficient and streamlined verification process. This ultimately reduces the number of iterations needed for full coverage, saving time and effort.
412343	41234364	Optimizing Majority-Inverter Graphs With Functional Hashing	The Majority-Inverter Graph (MIG) is a new type of logic representation that allows for efficient optimization. It has been shown to outperform existing approaches in reducing logic depth and achieving superior synthesis results. This paper introduces a new MIG optimization algorithm that uses functional hashing and precomputed minimum MIG representations for functions up to 4 variables. Experimental results demonstrate that this algorithm can further minimize heavily-optimized MIGs in size. When used as a starting point for technology mapping, the optimized MIGs can improve both depth and area for arithmetic instances of the EPFL benchmarks, surpassing the current results of state-of-the-art logic synthesis algorithms. This highlights the effectiveness of the proposed methodology. 
412344	412344109	Robust sample preparation on digital microfluidic biochips.	Digital microfluidic biochips (DMFBs) are a promising platform for sample preparation, but errors in fluidic operations can lead to concentration errors. To address this issue, researchers have developed two dilution-chain structures that can generate droplets with desired concentrations even if there are volume variations during droplet splitting. These structures are more effective than current error-recovery methods, which require on-chip sensors and additional time for re-execution. The experimental results demonstrate the success of the proposed method in reducing errors and improving efficiency compared to previous methods. This advancement in sample preparation on DMFBs has potential to greatly improve the accuracy and speed of various bioassays and analyses.
412345	41234548	Bayesian Active Clustering with Pairwise Constraints	This paper focuses on improving clustering using pairwise constraints, which specify similarities between pairs of instances. Randomly selecting constraints can be inefficient and can negatively impact clustering performance. To address this issue, the paper introduces a Bayesian clustering model that learns from pairwise constraints and presents an active learning framework. This framework selects the most informative pair of instances to query an oracle for labels and updates the model based on the obtained constraints. The paper introduces two criteria for selecting informative pairs, one based on uncertainty and the other on maximizing information gain. Experiments show that this method outperforms existing techniques on benchmark datasets.
412346	41234650	Reachability under contextual locking	The pairwise reachability problem is a fundamental issue in the analysis of multi-threaded programs, which aims to determine if two control locations can be reached simultaneously during program execution. This problem is crucial for detecting concurrent statements and is typically undecidable, even with abstracted data and finite lock synchronization. However, certain programming paradigms have been identified that make the problem solvable. This paper introduces a new programming paradigm called contextual locking, which restricts lock usage to specific calling patterns within each thread. The main result is that the pairwise reachability problem is decidable in polynomial time for this paradigm.
412347	41234750	Tracking performance of the RLS algorithm applied to an antenna array in a realistic fading environment	This paper explores the application of frequency domain techniques to the recursive least squares (RLS) algorithm in an adaptive antenna array for mobile wireless communication. The RLS algorithm is used to track changes in a fading environment, and the frequency domain approach is applied to the interference canceling problem that arises with antenna arrays in a mobile setting. The results show that using frequency domain techniques can improve the tracking performance of the RLS algorithm in nonstationary environments, making it a promising approach for mobile wireless communication.
412348	41234866	Invariant Features for 3D-Data based on Group Integration using Directional Information and Spherical Harmonic Expansion	As the amount of 3D data increases, there is a need for effective classification and search in databases. However, the varying representations and positions of 3D objects make it challenging to compare and classify them. Invariant features, generated through Group Integration, provide a solution to this problem. Two extensions to this approach are proposed in this paper - incorporating local directional information and using Spherical Harmonic Expansion for more descriptive features. The effectiveness of these extensions is demonstrated through their application to 3D-volume data (Pollen grains) and 3D-surface data (Princeton Shape Benchmark).
412349	41234989	Identity-based online/offline key encapsulation and encryption	The Identity-Based Online/Offline Encryption (IBOOE) scheme splits the encryption process into two phases to make it more efficient for devices with limited computation power. The first phase involves heavy computations, while the second phase only requires light computations and does not require knowledge of the plaintext or receiver's identity. This scheme also allows for preparation of ciphertext without certificate verification. The authors propose new schemes with improved efficiency, assuming random oracles. These include a scheme secure against chosen-plaintext attack and a new scheme called ID-based Online/Offline KEM (IBOOKEM) which splits the key encapsulation process into offline and online stages. Additionally, they present a generic transformation to achieve security against chosen-ciphertext attack. These schemes are the most efficient in terms of online computation and ciphertext size, making them suitable for deployment on devices with limited computation power and expensive communication bandwidth. 
412350	412350126	Learning to handle negated language in medical records search	The use of negated language by medical practitioners to indicate the absence of a medical condition can lead to incorrect results when using traditional information retrieval systems. These systems do not distinguish between positive and negative contexts of terms, resulting in non-relevant medical records being ranked highly. To address this issue, a novel learning framework is proposed that takes into account the context of terms within a document when weighting their relevance. This prevents non-relevant documents from being retrieved. The framework was evaluated using the TREC 2011 and 2012 Medical Records track test collections and showed significant improvements over existing baselines. When combined with other techniques, the framework can achieve results comparable to the best TREC systems, but does not address other challenges in medical records search.
412351	4123512	A Structured Learning Approach for Medical Image Indexing and Retrieval.	The VisMed approach is a framework for automatic indexing and retrieval of large medical image databases. It involves creating vocabularies of meaningful medical terms and matching images based on their distributions of these terms. This approach allows for both similarity-based retrieval with visual queries and semantics-based retrieval with text queries. By combining the results from both types of retrieval, the VisMed approach achieves a higher Mean Average Precision (MAP) compared to using just one type of retrieval. In a study using this approach on a subset of medical images, the best MAP was 0.2821 for mixed retrieval. 
412352	41235225	Lookahead and pathology in decision tree induction	The standard method for creating decision trees involves making optimal decisions at each node without considering future nodes. A new approach was tested where the algorithm looks ahead one level to decide which test to use at a node. This method was compared to the standard method using a large number of artificial data sets. The results showed that the standard method consistently produced trees with the same level of accuracy as the more expensive lookahead method. Additionally, the lookahead method often created trees that were larger and less accurate than those produced without it.
412353	41235370	Eco-friendly reduction of travel times in european smart cities	The article introduces an innovative solution for reducing polluting gas emissions from road traffic in modern cities. The solution is based on a new Red Swarm architecture, composed of intelligent spots with WiFi connections that can suggest customized routes to drivers. The proposal has been tested in four European smart cities using a micro-simulator and an evolutionary algorithm. Results show a significant reduction in gas emissions and travel times when vehicles are rerouted according to the Red Swarm indications. This low-cost solution has the potential to engage the interest of citizens and municipal authorities.
412354	41235420	So-Grid: A self-organizing Grid featuring bio-inspired algorithms	So-Grid is a set of bio-inspired algorithms designed for decentralized construction of a Grid information system. It uses swarm intelligence, where individual entities perform simple tasks but together create advanced intelligence. So-Grid has two main functions: logical reorganization of resources, inspired by ants and termites, and resource discovery, inspired by ants searching for food sources. These functions work together to facilitate intelligent dissemination and efficient discovery of resources. The algorithms are implemented by ant-like agents that autonomously travel the Grid through P2P connections and use biased probability functions. Simulation analysis shows that So-Grid can reduce system entropy and efficiently disseminate content. It also allows users to reach Grid hosts with a larger number of useful resources in a shorter time. This approach has self-organization, scalability, and adaptivity, making it suitable for dynamic and unreliable distributed systems.
412355	41235531	A field study of API learning obstacles	A study was conducted on over 440 professional Microsoft developers to determine the obstacles they face when learning new APIs. The study found that documentation and other learning resources were the most significant challenges. Five key factors were identified for effective API documentation: documentation of intent, code examples, matching APIs with scenarios, penetrability of the API, and format and presentation. These factors can help prioritize API documentation efforts in order to improve the learning experience and increase programmer productivity.
412356	41235642	Asymmetric rendezvous on the plane	The concept of rendezvous problems involves two players moving on a plane and trying to minimize the time it takes for them to meet. In one scenario, the players are aware of their distance apart but do not know which direction to travel in. Another scenario involves one player knowing the other's initial position while the other player only knows the initial distance between them. Results are also discussed for situations where one player's initial position is randomly chosen from a set of points. Overall, the goal is to find strategies that will allow the players to meet as quickly as possible.
412357	41235717	Cutlike Semantics For Fuzzy Logic And Its Applications	Fuzzy sets can be represented by a nested system of ordinary sets, known as a-cuts. There is a lot of research on whether operations and properties of fuzzy sets can be reduced to those of their a-cuts. This includes questions about reducing operations and properties of fuzzy relations and whether a fuzzy concept can be represented by a collection of crisp concepts. Klir and Yuan (1995) refer to this as "cutworthiness". The authors propose a solution to this problem by using a nested system of crisp structures to define the semantics of fuzzy predicate logic. This approach can provide answers to these questions, as demonstrated by examples such as the extension principle and properties of binary fuzzy relations.
412358	41235878	Taking part: role-play in the design of therapeutic systems	Understanding user needs is crucial in designing effective human-computer interaction (HCI). However, in fields like mental health care, it can be challenging to access end-users. This is due to the sensitive nature of mental illness and the stigma associated with it, as well as the protected therapeutic setting. Role-play, a technique used in HCI, can be useful in these settings. It allows researchers to gain a deeper understanding of the user's experience, simulate therapy sessions, and train therapists in using technology. This paper explores various role-play formats adapted from therapeutic role-play, drawing upon literature and therapist input. It also discusses the benefits of using role-play in generating empathy and providing feedback on designs. 
412359	41235936	Composition attacks and auxiliary information in data privacy	Privacy is becoming increasingly important in data publishing, but there are challenges in reasoning about privacy. One major issue is the auxiliary information that an adversary can gather from other sources, which can breach privacy even with current anonymization techniques. This paper explores how to reason about privacy when faced with realistic sources of auxiliary information. It investigates composition attacks, where an adversary uses multiple independently released anonymized data sets to breach privacy. The experiments show that even simple composition attacks can be successful against commonly used anonymization schemes. However, certain randomization-based notions of privacy, such as differential privacy, are resistant to composition attacks and allow for modular design without explicitly considering other releases. This expands the range of protocols that can be used for privacy protection.
412360	41236039	Open distributed processing: an architectural basis for information networks	The ODP Reference Model is an important part of the information network architecture being developed by TINA-C. This model is based on key principles and concepts that are crucial for the design and construction of information networks. One of these principles is binding, which is essential for the structure of an information network. A generic binding protocol is also introduced, which can accommodate the diverse types of binding in an information network. Overall, the ODP Reference Model is a significant framework for building efficient and effective information networks.
412361	41236124	Variations in the binding pocket of an inhibitor of the bacterial division protein FtsZ across genotypes and species.	The rise of antibiotic resistance in pathogenic bacteria has led to a need for new approaches to drug development. Targeting the mechanisms of action of proteins involved in bacterial cell division, such as FtsZ, has shown promise. However, the mechanism of action and effectiveness of FtsZ inhibitors, such as PC190723, on different bacterial species is not fully understood. Using a statistical method and molecular dynamics simulations, researchers examined the structural environment of the PC190723 binding site on FtsZ. They found that the binding site varies significantly between species, genetic mutations, and polymerization states, providing valuable information for the development of FtsZ inhibitors. This highlights the need for a tailored approach to drug development, taking into account species-specific differences in drug targets.
412362	41236292	Coupled-learning convolutional neural networks for object recognition.	Convolutional neural networks (CNNs) have gained a lot of attention for their use in computer vision tasks. These networks are inspired by the human brain and have similar properties to its learning process. However, a key difference is that CNNs operate independently while humans rely on effective communication between individuals for their visual system. To address this, a new approach called Coupled-learning Convolutional Neural Network (Co-CNN) has been proposed for object recognition. This method utilizes the dynamic interaction between neural networks to improve their discriminative capability. Unlike other network architectures, Co-CNN optimizes multiple networks simultaneously and incorporates a coupled-learning mechanism to prevent over-fitting. Evaluations on various datasets demonstrate the superiority of Co-CNN over existing algorithms. 
412363	412363162	Image coding using dual-tree discrete wavelet transform.	This paper explores the use of 2-D dual-tree discrete wavelet transform (DDWT) for image coding. Three methods for reducing DDWT coefficients are compared and noise shaping is found to be the most efficient. The dependence among DDWT coefficients is analyzed and three coding methods are evaluated, with TCE performing the best. The DDWT _ TCE scheme outperforms JPEG2000 and other directional filter bank-based image coders. To improve efficiency, the DDWT is extended to anisotropic dual-tree discrete wavelet packets (ADDWP) and is coded with TCE. Experimental results show that ADDWP _ TCE provides even better performance, outperforming JPEG2000 by up to 2.00 dB. The reconstructed images of these schemes are visually more appealing due to the directionality of wavelets.
412364	412364111	Anatomical labeling of the anterior circulation of the Circle of Willis using maximum a posteriori classification.	This paper discusses a method for automatically labeling the arteries that make up the Circle of Willis, a key structure in the brain. This labeling is important for comparing different individuals and identifying risk factors for vascular diseases. The method uses pre-labeled examples and machine learning to detect the five main vessel bifurcations in the anterior part of the Circle of Willis. The labeling process is formulated as a maximum a posteriori solution, incorporating both local and global anatomical variations. The method was tested on 30 subjects and had a success rate of 90%. This technique is effective for labeling the arterial segments of the Circle of Willis and can handle anatomical variations. 
412365	4123654	Concurrent flexible reversibility	The concept of concurrent reversibility has been explored in various fields, but has only been limited to a "rigid" form where a past state can be revisited and the same computation can be restarted, even if it leads to a different outcome. In this paper, the authors introduce croll-π, a concurrent calculus that allows for flexible reversibility. This means that alternatives to a computation can be specified and used upon rollback, and these alternatives are attached to messages. The authors demonstrate the strength of this approach by encoding more complex methods for flexible reversibility and showcasing its application in a calculus of communicating transactions.
412366	41236617	Constrained Equational Deduction	Symbolic computation plays a crucial role in automated deduction, algebraic specification, and declarative programming. In order to design a deduction mechanism within equational programming logic, symbolic equational deduction needs to be extended to incorporate semantic information from abstract symbols. This paper proposes constrained equational deduction as a framework for this extension, within a general constraint equational logic programming framework. By utilizing hierarchical constraint information and linking symbolic equational deduction with various constraint solving mechanisms, constrained equational deduction bridges the gap between symbolic and concrete computation. The paper presents a constructive approach that combines a constraint system with symbolic equational constraints to create the constraint equational logic programming paradigm. Computational models for this paradigm, called constrained equational deduction models, are also introduced.
412367	41236723	Finding corresponding objects when integrating several geo-spatial datasets	The paper discusses the use of join algorithms in integrating geo-spatial datasets, particularly for finding corresponding objects. It proposes methods that can be applied to multiple datasets and compares two approaches that use object locations. One approach processes datasets sequentially, while the other processes them simultaneously. The paper provides join algorithms for both approaches and evaluates their performance in terms of recall and precision. These algorithms are designed to handle imprecise locations and datasets that do not represent all real-world entities. Results from experiments show that one of the algorithms consistently performs well and outperforms the traditional one-sided nearest-neighbor join. 
412368	41236894	A General Framework for Automatic Termination Analysis of Logic Programs	The paper discusses a framework for automatically analyzing the termination of logic programs, focusing on the finiteness of the LD-tree constructed for a given program and query. A general property of mappings from a subset of infinite LD-tree branches into a finite set is proven, leading to various termination theorems. These theorems are then applied to specific cases, such as predicate and atom dependency graphs. The paper also introduces a new method for proving termination in programs involving arithmetic predicates, which combines a finite abstraction of integers with the technique of query-mapping pairs. Several potential extensions to this framework are also mentioned. 
412369	41236930	Elastic correction of dead-reckoning errors in map building	.The paper discusses a technique called elastic correction that aims to address the problem of imprecision in sensor measures during map building. It involves using a relational graph to represent the environment being explored, with landmarks and inter-landmark routes represented as vertices and arcs respectively. The approach is based on the idea of treating the map as a truss, with each route acting as an elastic bar and each landmark as a node. By taking inconsistent measures and modeling the uncertainty of odometry as elasticity parameters, errors can be corrected through the deformations induced within the structure. This technique shows promise in improving the accuracy of maps created by robots navigating unknown environments.
412370	41237047	Fingerprint Classification by Directional Image Partitioning	This work presents a new method for automatic fingerprint classification. The directional image is divided into connected regions based on fingerprint topology, creating a synthetic representation for classification. Dynamic masks and an optimization criterion guide this partitioning process. The resulting numerical vectors can be seen as a continuous classification of fingerprints. Different search strategies are explored for efficient retrieval of both continuous and exclusive classifications. Experimental results show superior performance and high robustness compared to other existing methods, making it the best approach for continuous fingerprint classification and retrieval. Tests were conducted on commonly used fingerprint databases.
412371	41237130	On the use of words and fuzzy sets	This paper proposes a rethinking of fuzzy sets that does not change their fundamental nature as mathematical entities that extend predicates. The key idea is that predicates organize the universe of discourse, and when this organization is a preorder, a numerical degree can be defined to represent the L-set. The ultimate goal is to expand the current theories of fuzzy sets to encompass language and reasoning, in order to further understanding of the relationship between language and its representations for the advancement of computing with words and perceptions.
412372	41237220	Characterization of Fuzzy Implication Functions With a Continuous Natural Negation Satisfying the Law of Importation With a Fixed t-Norm.	The law of importation is a significant aspect of fuzzy implication functions that has practical applications in fields such as approximate reasoning and image processing. It has been extensively studied and there are open problems related to this property that have been proposed in previous research. In this paper, the authors address one of these open problems by providing a partial solution. They specifically focus on the case of a fixed t-norm and characterize all fuzzy implication functions with continuous natural negation that satisfy the law of importation with this t-norm. The results are detailed for different types of t-norms, including continuous and noncontinuous ones, and also provide characterizations for well-known fuzzy implication functions. 
412373	41237352	Distributed Model Predictive Control	The paper discusses a distributed model predictive control (DMPC) scheme where controllers use model predictive control (MPC) policies to control their local subsystems. These controllers exchange their predictions through communication and incorporate information from other controllers to coordinate their actions. The DMPC scheme ensures stability for controllable systems that satisfy a matching condition by imposing stability constraints on the next state in the prediction. An example application of multi-area load-frequency control is used to demonstrate the performance of the DMPC scheme. The results show that this approach can effectively coordinate the actions of multiple controllers and improve system performance.
412374	41237444	Polarized Resolution Modulo	The authors propose a modified version of Resolution modulo where clauses can directly rewrite to other clauses without needing to be transformed into clause form. This requires extending the rules for negative and positive atomic propositions. This method is a combination of clause and literal selection restrictions and can be viewed as a variation of Equational resolution. It is not an instance of Ordered resolution, unlike many other restrictions of Resolution.
412375	41237530	Collaborative Metacomputing with IceT	The IceT metacomputing system utilizes collaborative resource sharing, lightweight software frameworks, and efficient execution in heterogeneous environments. It extends the traditional model for distributed computing to enable controlled and secure exchange of hardware and software resources and supports programming paradigms that balance portability and efficiency. The current system is Java-based and allows non-persistent collections of resources, users, and processes to work together for distributed problem solving. The paper discusses the architecture and design principles of IceT, its operational model, and support for executing distributed applications. It also covers the development of subsystems that support the concept of cooperative metacomputing.
412376	41237652	Graph Treewidth and Geometric Thickness Parameters	This paper discusses the relationship between the classical graph parameter thickness, the geometric thickness, and the book thickness, which are all based on the number of colors needed to draw a graph without crossing edges. The main result is that for graphs with treewidth k, the maximum thickness and geometric thickness are both equal to ⌈k/2⌉. This also applies to the more restrictive setting of geometric thickness. Another main result is that for graphs with treewidth k, the maximum book thickness is equal to k if k ≤ 2 and k + 1 if k ≥ 3. This contradicts a previous conjecture. Similar results are also shown for outerthickness, arboricity, and star-arboricity.
412377	41237722	Sequential Feature Explanations for Anomaly Detection.	Anomaly detection systems are commonly used in various applications to detect unusual data instances. However, when presenting the most anomalous instance to a human analyst, these systems often do not provide any explanation as to why it was considered anomalous. This leaves the analyst with no guidance on how to investigate further. To address this issue, researchers have studied the concept of sequential feature explanations (SFEs) for anomaly detectors. These are sequences of features that are presented to the analyst one at a time until enough information is provided for them to confidently determine the anomaly. The quality of an explanation is measured based on the number of features required for the analyst to attain confidence. To evaluate these explanations, a framework has been developed using real and simulated data sets. This has led to insights into various novel explanation approaches.
412378	41237818	Content-based image retrieval by using tree-structured features and multi-layer self-organizing map	The study presents a new approach for content-based image retrieval (CBIR) using a tree-structured image representation and a multi-layer self-organizing map (MLSOM). The tree structure contains global and local features, providing more comprehensive information for image retrieval. MLSOM efficiently organizes and compresses the image data, allowing for faster retrieval compared to traditional methods. A relevance feedback scheme is also implemented to further improve accuracy. Results show that the proposed system is robust against image alterations and outperforms other CBIR systems in terms of accuracy, speed, and robustness. 
412379	41237970	Multi-Objective Optimization Based on Brain Storm Optimization Algorithm	The paper discusses the development of various evolutionary and population-based algorithms for solving multi-objective optimization problems. The authors propose a new algorithm, called multi-objective brain storm optimization, which uses a clustering strategy in the objective space rather than the solution space as in the original brain storm optimization algorithm for single objective problems. Two versions of the algorithm were tested and the results showed the effectiveness of the proposed approach in solving different multi-objective problems. The authors conclude that the new algorithm shows promise in solving various multi-objective optimization problems.
412380	41238067	Negotiation as an Interaction Mechanism for Deciding App Permissions.	Apps on the Android platform often collect personal data from users, including location, contacts, and photos, and use it as part of their business model. However, many users are not aware of the permission settings or do not make changes to them. This is due to the difficulty of checking and modifying settings for all apps on a device, as well as the lack of flexibility in deciding what happens to their data. To address this issue, the authors conducted a study to explore the impact of more discretionary permission settings at the time of app installation. The pilot experiment showed that allowing users to negotiate which data they are comfortable sharing results in higher satisfaction compared to the traditional all-or-nothing approach. This suggests that negotiating consent can be an effective way to engage users and find a balance between privacy and pricing concerns.
412381	41238149	An evaluation framework for assessing the dependability of Dynamic Binding in Service-Oriented Computing	Service-Oriented Computing (SOC) is a flexible approach to building applications using distributed services. One of its main advantages is Dynamic Binding, which allows for abstract requests to be connected to specific services at runtime, increasing flexibility and adaptability. However, there is a lack of research on the evaluation of Dynamic Binding Systems (DBS), particularly in terms of system failure and dependability. This paper introduces a new evaluation framework for DBS, utilizing a fault model to simulate potential failures and observe the system's behavior. By treating the DBS as a black box and using distributed components, the evaluation is not limited by the technology or location of the system. Experiments on the NECTISE Software Demonstrator (NSD) show that this framework can detect and provide important information for improving the dependability and performance of DBS.
412382	41238294	Mapping an unfriendly subway system	This article looks at creating a map of a dynamic network, specifically an urban subway system, in less than ideal conditions. The focus is on a team of asynchronous computational entities, or mapping agents, trying to locate black holes in the constantly changing graph of the subway system. The goal is to solve this problem with the minimum number of agents and movements possible. The article presents and analyzes a solution protocol, and also establishes lower bounds on the number of movements required in the worst case scenario, showing that the protocol is move-optimal.
412383	41238336	Level-k Phylogenetic Networks Are Constructable from a Dense Triplet Set in Polynomial Time	This paper addresses the question of whether there exists a phylogenetic network that is consistent with a given dense triplet set, and if so, can we construct one effectively. Previous research has provided answers and algorithms for networks of levels 0, 1, and 2, but for higher levels, only partial answers were available. The authors present a complete solution to this problem, using a special property of SN-sets in a level-k network. As a result, they are able to find a level-k network with the minimum number of reticulations, if one exists, in polynomial time. 
412384	41238431	Split decomposition and graph-labelled trees: Characterizations and fully dynamic algorithms for totally decomposable graphs	This paper explores the split decomposition of graphs and presents new findings for the class of totally decomposable graphs, as well as two subclasses: cographs and 3-leaf power graphs. The authors provide structural and incremental characterizations, resulting in efficient dynamic recognition algorithms for changes to vertices and edges within each class. These results are based on a new framework of graph-labelled trees, which represent the split decomposition of graphs. The paper also discusses how this framework allows for a bijective mapping between the graph classes and graph-labelled trees, with labels representing cliques and stars. This approach leads to an intersection model for distance hereditary graphs. 
412385	41238575	Delineation of rock fragments by classification of image patches using compressed random features	Monitoring the fragmentation of rocks is crucial for the mining industry. Current methods involve physically sieving rock samples or using 2D image analysis software, which is time-consuming and requires skilled operators. Recent research has shown promise in using 3D image processing, but it is not feasible for all mines to switch to this technology. This paper proposes a new method that uses compressed Haar-like features and a support vector machine to accurately delineate rock fragments from 2D images. The optimal image patch size and number of compressed features have been determined through experimentation. Results show that this method outperforms existing algorithms and is more computationally efficient, making it a viable solution for mining companies.
412386	41238639	Particle swarm optimisation assisted classification using elastic net prefiltering	A new algorithm has been developed for constructing a linear-in-the-parameters classifier in two stages, specifically for noisy two-class classification problems. The first stage aims to create a prefiltered signal that will be used as the desired output for the second stage. The first stage uses a two-level algorithm to maximize the model's generalization capability, incorporating an elastic net model identification algorithm and particle swarm optimization to select the two regularization parameters. This approach emphasizes the principle of "Occam's razor". The second stage uses an orthogonal forward regression with the D-optimality algorithm to construct the sparse classifier. Extensive experiments have shown that this approach is effective and competitive for handling noisy data sets.
412387	41238716	Argument-Based expansion operators in possibilistic defeasible logic programming: characterization and logical properties	Possibilistic Defeasible Logic Programming (P-DeLP) is a logic programming language that integrates argumentation theory and logic programming, incorporating possibilistic uncertainty and fuzzy knowledge. It allows for non-monotonic inference, making it useful for modeling complex reasoning. This paper focuses on two non-monotonic operators in P-DeLP that add new weighed facts associated with argument conclusions and warranted literals to a given program, and compares them to a traditional logic system. The analysis of these operators provides a useful framework for evaluating and comparing argumentation frameworks.
412388	41238839	Parallelism and synchronization in two-level metacontrolled substitution grammars	The concept of synchronized substitution is introduced in two-level metacontrolled substitution grammar, as explained in [6]. In contrast to [6], where the top level is independent of the grammar, this mechanism is highly influenced by the chosen grammars. This is exemplified using parallel grammars like E0L systems, and their language-generating capability is compared to ET0L systems and E0L iterated systems.
412389	41238934	Constructive hypervolume modeling	This paper discusses the modeling of point sets with attributes in geometric spaces of arbitrary dimensions. A point set is a representation of a real or abstract object, while an attribute represents a property of the object at any point in the set. The paper provides a survey of various modeling techniques related to point sets with attributes, such as solid modeling, heterogeneous objects modeling, and volume graphics. Based on this survey, the paper proposes a general model for hypervolumes (multidimensional point sets with multiple attributes) and discusses its components, including objects, operations, and relations. The model uses a function representation approach, where the geometry and attributes are represented by real-valued scalar functions evaluated at points using a constructive tree structure. The paper also demonstrates the application of this model in the context of texturing and discusses its implementation and a specialized modeling language used for hypervolume objects. 
412390	4123902	Certified Information Access	Certified Information Access (CIA) is a method for users to verify the accuracy of information received from a database. This is achieved by the database owner providing a proof that the information matches the actual content of the database. Existing solutions for this problem require a lengthy setup process. Two secure distributed implementations of CIA are described, one where the database owner distributes the evaluation of a computationally intensive function among untrusted peers, and another where the entire computation is outsourced. The main concern in both cases is ensuring confidentiality and correctness of the data. A new primitive, the Verifiable Deterministic Envelope, is introduced as a potential solution.
412391	41239195	Computational Anatomy Gateway: Leveraging XSEDE Computational Resources for Shape Analysis	Computational Anatomy (CA) is a field that uses mathematical and computational techniques to study the variability in biological shape. The key algorithm used in CA is Large Deformation Diffeomorphic Metric Mapping (LDDMM), which assigns numerical descriptors and a metric distance to anatomical shapes. CA is widely used in the neuroimaging and cardiovascular imaging communities. The process involves estimating a template and calculating diffeomorphic transformations for each subject in a population. This is computationally expensive and requires parallel computing resources, such as those provided by XSEDE's Stampede cluster. The use of NVIDIA Tesla GPUs and Xeon Phi Co-processors can further speed up these calculations. This will have a significant impact on the neuroimaging and cardiac imaging fields, as these shape analysis tools become available through a webservice (www.mricloud.org) using XSEDE's resources through the Computational Anatomy Gateway.
412392	4123928	Secure and efficient data transmission in the Internet of Things	The Internet of Things (IoT) has gained popularity in both industry and academia. This concept involves millions of interconnected objects with sensors that collect data and send it to servers for analysis. To ensure the accuracy of this collected data, it is important to establish a secure communication channel between the sensors and servers. In this paper, a heterogeneous ring signcryption scheme is proposed to achieve confidentiality, integrity, authentication, non-repudiation, and anonymity in a single step. This scheme is proven to be secure against different types of attacks and can be used for data transmission in the IoT, including between different types of cryptographic systems.
412393	41239392	Towards confidentiality of ID-Based signcryption schemes under without random oracle model	Signcryption is a cryptographic method that ensures both confidentiality and authenticity in an efficient way. It is a single step process that offers better computation and communication costs to enhance security. This paper analyzes two different signcryption schemes - one by Yu et al. and another by Liu et al. - under the without random oracles model. The chosen plaintext attacks prove that these previous schemes are not able to resist indistinguishability on chosen ciphertext attacks, thus failing to provide semantic security for confidentiality. This highlights the need for improved signcryption schemes that can effectively resist such attacks and ensure stronger security measures. 
412394	41239417	Adaptive pricing for customers with probabilistic valuations	This paper discusses the problem of determining discriminatory prices for customers with uncertain values and a seller who has identical copies of a product. It is shown that, with certain assumptions, this problem can be reduced to the continuous knapsack problem and a new algorithm is developed to solve it. The algorithm is able to handle asymmetric concave reward functions and can also be applied to pricing problems with overlapping goods. Additionally, a framework for learning customer valuation distributions is proposed and tested on real-world pricing scenarios. The results demonstrate that the algorithm outperforms a previously proposed heuristic in terms of speed and accuracy.
412395	41239523	Embodiment enables the spinal engine in quadruped robot locomotion	The spinal engine hypothesis suggests that locomotion is mainly controlled by the spine, with the legs providing support. Based on this, a quadruped robot called Kitty was created with a compliant, multi-degree-of-freedom spine and no leg actuation. The study demonstrates how the spine's movements, through embodiment, can generate versatile behaviors such as bounding, trotting, and turning. Information theory was also used to analyze the spine's internal dynamics and its effect on the bounding gait, using three different spinal morphologies. The results showed that a rear virtual spinal joint improved locomotion by allowing more freedom for the rear legs to move forward. The study also discusses the relationship between the robot's behavior and its information structure, which varied based on the spine's morphology.
412396	4123969	Fully automated analysis of padding-based encryption in the computational model	Computer-aided verification is a useful method for assessing the security of cryptographic primitives. However, achieving automated analyses that are effective against computational attacks has been difficult. This paper addresses this challenge by presenting proof systems for analyzing the security of public-key encryption schemes that use trapdoor permutations and hash functions. By combining techniques from both computational and symbolic cryptography, the authors have developed a toolset that allows for fully automated proof and attack finding algorithms. This toolset has been used to create a database of encryption schemes, documenting attacks against insecure schemes and providing proofs with concrete bounds for secure ones.
412397	41239750	Non-revisiting genetic algorithm with adaptive mutation using constant memory	The continuous non-revisiting genetic algorithm (cNrGA) incorporates the entire search history and parameter-less adaptive mutation to improve search performance. However, as the number of fitness evaluations increases, memory management becomes necessary. To address this, two pruning mechanisms, least recently used and random pruning, are proposed to keep memory usage constant. These strategies also serve as parameter-less adaptive mutation operators. Experimental results show that cNrGA with constant memory outperforms other algorithms and can be extended to handle larger fitness evaluations without affecting performance. This expands the applicability of cNrGA to solve more complex problems.
412398	4123983	Sound and complete relevance assessment for XML retrieval	In information retrieval research, test collections consisting of documents, user requests, and relevance assessments are necessary for comparing retrieval approaches. Obtaining accurate and complete relevance assessments is crucial for this comparison. In XML retrieval, the challenge of obtaining relevant assessments is complicated by the structural relationships between retrieval results. Unlike flat document retrieval, the relevance of elements in XML is dependent on related elements. As such, creating reliable relevance assessments for XML retrieval has been a focus of the INEX evaluation campaign. After trying various methods, the campaign has settled on a highlighting method for marking relevant passages within documents, which has proven to be the most reliable approach.
412399	41239940	How well do line drawings depict shape?	This paper examines the use of sparse line drawings in representing 3D shapes. A study was conducted where participants were shown images of 12 objects depicted in 6 different styles and asked to orient a gauge to match the surface normal of the object. Results were compared to ground truth data from a 3D surface model and analyzed for accuracy and precision. The study collected a large data set of 275,000 measurements and found that people are able to interpret certain shapes effectively from line drawings, which can be as effective as shaded images or artist's drawings. Errors in depiction were found to be localized and related to specific properties of the lines used. The collected data will be made available for future studies on this topic.
412400	41240054	Design and fabrication of materials with desired deformation behavior	This paper presents a data-driven approach for designing and creating materials with specific deformation properties. The process involves measuring the deformation properties of base materials and using this information to create a non-linear stress-strain relationship in a finite-element model. The validity of this measurement process is confirmed by comparing simulated and actual deformations of fabricated material stacks. Then, an optimization process is used to design layered combinations of base materials that meet desired deformation criteria. Various strategies are employed to streamline the search for optimal solutions. The complete process is demonstrated by designing and fabricating objects using modern multi-material 3D printers.
412401	41240140	Evolving mach 3.0 to a migrating thread model	The authors have made changes to the Mach 3.0 operating system to improve its handling of cross-domain remote procedure calls (RPCs). This involves treating RPCs as a single entity instead of a sequence of message passing operations, and using a migrating thread model instead of a static one. This model allows for better transfer of control during RPCs and simplifies kernel code. The new system also provides more control over thread manipulation and improves RPC performance. The authors have kept the old thread and IPC interfaces for compatibility and have demonstrated the effectiveness of their changes with a functional Unix server and clients. The changes have reduced logical complexity and improved local RPC performance. They conclude that the migrating thread model and kernel-visible RPC are beneficial for improving operating systems. 
412402	4124020	Coupled binary embedding for large-scale image retrieval.	Visual matching is a crucial part of image retrieval using the bag-of-words (BoW) model. However, the traditional method of matching based on SIFT descriptors has limitations in terms of discriminative power and only describing local texture features. To address this issue, this paper proposes incorporating multiple binary features at the indexing level and using a multi-IDF scheme to model their correlation. This allows for the integration of binary feature-based matching methods, like Hamming embedding, into the framework. The inclusion of binary color features also improves the precision of visual matching and reduces the impact of false positive matches. Experimental results on four benchmark datasets show that this method outperforms the traditional approach and requires acceptable memory usage and query time. When global color features are also integrated, the proposed method performs competitively with other state-of-the-art methods. 
412403	41240388	Texture Features for Browsing and Retrieval of Image Data	Image content based retrieval is a growing field that has practical applications in digital libraries and multimedia databases. This paper specifically focuses on the use of texture information for browsing and retrieving large volumes of images. The authors propose using Gabor wavelet features for texture analysis and provide a thorough evaluation of their effectiveness. Results show that Gabor features outperform other texture features when applied to the Brodatz texture database, demonstrating their accuracy in pattern retrieval. An example of using this approach for browsing large air photos is also presented. 
412404	41240451	Asking for more than an answer	The study discussed in this article focuses on the situational context that affects the quality of answers in online Q&A services. The researchers used a mixed method analysis to investigate this issue and found that users have three primary expectations when asking a question: quick responses, additional or alternative information, and accurate or complete information. By understanding these expectations, it may be possible to develop personalized approaches to improve information relevance and satisfaction for users of online Q&A services. This research addresses a gap in the current research on online Q&A and sheds light on how users engage in information seeking within this context. 
412405	41240565	Barriers to organizing information during cancer care: &quot;I don't know how people do it.&quot;.	Patients are receiving a large amount of health information and are expected to take an active role in managing it. However, both patients and clinicians are frustrated with the lack of effective organization and use of this information. A study was conducted on cancer patients to identify barriers they face in organizing their health information. Four types of barriers were found: emotional, scalable, temporal, and functional. Suggestions are provided for reducing these barriers through technology or social changes. This study offers guidance for health-information providers, technology designers, and patients and their caregivers.
412406	41240638	Fixed-Points of Social Choice: An Axiomatic Approach to Network Communities.	This article introduces a social choice theory approach to defining communities within social networks. The authors use a preference network framework in which members rank each other based on their preferences. They develop two approaches to studying communities: one indirectly through preference aggregation functions and the other directly through community axioms. These approaches reveal that community rules form a bounded lattice and have a structural characterization. However, defining community rules based solely on preference aggregation can lead to violations of desirable community axioms. The authors also provide a polynomial-time rule that satisfies seven axioms and weakly satisfies the eighth.
412407	4124078	Left and right convergence of graphs with bounded degree	The theory of convergent graph sequences has been studied in two different cases, dense graphs and bounded degree graphs. Convergence can be defined in terms of counting homomorphisms from fixed graphs into members of the sequence (left-convergence) or counting homomorphisms into fixed graphs (right-convergence). In the dense case, it has been shown that left-convergence and right-convergence are equivalent under certain conditions. This paper establishes a similar equivalence in the bounded degree case, with restrictions on the set of graphs in the right-convergence definition. This implies that for a left-convergent sequence, partition functions of a wide range of statistical physics models will converge. The proof utilizes techniques from statistical physics such as cluster expansion and Dobrushin Uniqueness. This work was supported by OTKA (67867) and ERC (227701).
412408	41240821	Building member attachment in online communities: applying theories of group identity and interpersonal bonds	This article discusses the importance of online communities for organizations and the general public, and the lack of research on what makes some communities more successful than others. The authors applied theory from social psychology to understand how online communities develop member attachment, a key factor in community success. They tested two sets of community features aimed at strengthening either group identity or interpersonal bonds, and found that both sets of features increased participants' visit frequency and self-reported attachment. However, features intended for identity-based attachment had a greater impact, particularly for newcomers. This research highlights the potential for applying social science theories to improve the success of online communities.
412409	41240910	Compilation for explicitly managed memory hierarchies	A new compiler has been developed for machines with a managed memory hierarchy. Its main purpose is to organize and schedule bulk operations at different levels of the application and the machine. The performance of the compiler has been tested using various benchmarks on a Cell processor.
412410	4124106	Policy auditing over incomplete logs: theory, implementation and applications	This article presents an algorithm, called reduce, that checks audit logs for compliance with privacy and security policies. The algorithm addresses two main challenges in compliance checking: realistic policy applicability and incomplete audit logs. Reduce operates on policies expressed in a first-order logic that allows restricted quantification and uses logic programming to identify the restricted form of quantified formulas. It can handle all 84 disclosure-related clauses of the HIPAA Privacy Rule. The algorithm proceeds iteratively and outputs a residual policy that can only be checked with additional information. The article proves correctness, termination, time and space complexity results for reduce and provides an optimized implementation using two heuristics for database indexing. Experimental results show that the algorithm is fast enough for practical use.
412411	4124112	Digestor: device-independent access to the World Wide Web	Digestor is a software system that allows for easy access to the World-Wide Web on small screen devices such as PDAs and cellular phones. It works by automatically reformatting web pages to fit the screen size of the device, using a planning algorithm and structural page transformations. This means that users can view web pages on their small screen devices without having to zoom in or scroll. Digestor is implemented as an HTTP proxy, making it a convenient and efficient solution for device-independent web browsing on the go.
412412	41241242	"…is it normal to be this sore?":: using an online forum to investigate barriers to physical activity	Regular physical activity is crucial for maintaining overall health, but many adults in the U.S. lead sedentary lifestyles. Lowering perceived barriers to physical activity is an important part of interventions aimed at increasing physical activity. Through a systematic qualitative coding of an online forum used in a three-month healthy lifestyle intervention, the top five barriers to physical activity were identified. Two of these barriers have not previously been reported in the literature. This paper provides design considerations for technologies that can support and encourage physical activity, based on the identified barriers. Understanding the needs of a population is essential in designing effective interventions, and this paper offers valuable insights for those working in this field.
412413	4124130	UWB-Based Tracking of Autonomous Vehicles with Multiple Receivers.	This paper discusses real-time tracking of an Autonomous Guided Vehicle (AGV) in an indoor industrial setting. The AGV's on-board odometer provides information about its position and orientation, while an external Ultra Wide Band (UWB) wireless network compensates for any error drift. Two new solutions for real-time tracking are proposed: a classical Time Differences of Arrivals (TDOA) approach with a single receiver, and a "Twin-receiver" TDOA (TTDOA) approach that requires two independent receivers on the AGV. The effectiveness of these algorithms is evaluated in realistic conditions, showcasing the tradeoff between the frequency and quality of UWB measurements.
412414	4124147	Natural language analysis for semantic document modeling	This paper discusses a method for organizing and retrieving web documents through semantic document classification. This approach uses a combination of linguistic tools and a conceptual domain model to create a controlled vocabulary for a document collection. Users can browse the model and select relevant fragments to classify documents. Natural language tools are then used to analyze the text and suggest model fragments based on the selected concepts and relations. These fragments are refined by users and stored in RDF-XML format for retrieval. A prototype of the system is presented, with examples from a medical document collection. Lexical analysis is also used to refine search queries using the domain model.
412415	41241525	Characterizing network events and their impact on routing	Network events, also known as incidents, are disruptions to the usual functioning of elements in an IP network. These elements, such as routers, network interface cards, and IP links, can fail or malfunction due to various reasons. For instance, a router may need to be rebooted for a software upgrade, an interface card may crash, or an IP link may become overloaded due to a denial-of-service attack. These incidents can affect the traffic of customers, resulting in packet loss, delays, or even complete loss of connectivity to parts of the network. To address these issues, network operators must diagnose the problem and find a solution. The study of network events, their causes, and their impact was conducted using data from a large European provider's Virtual Private Network (VPN) backbone, which connects over ten thousand enterprise networks but does not connect to the internet.
412416	41241637	Instant Places: Using Bluetooth for Situated Interaction in Public Displays	Instant Places is a study that explores the use of Bluetooth presence and naming as tools for interactive experiences around public displays. The authors not only use Bluetooth names for identity representation, but also as a means of triggering content on situated displays. Through deploying a prototype in a bar for several weeks, the study examines the effectiveness of these techniques and the resulting social practices. The findings suggest that the simplicity of these techniques does not hinder their ability to sustain interaction and can be easily adapted for new forms of social engagement. 
412417	4124175	Information Holodeck: Thinking In Technology Ecologies	This paper discusses the potential for technology to support human thinking, learning, and sensemaking through device ecosystems (DDEs). The authors propose a theoretical framework that incorporates digital information into Vygotsky's sign mediation triangle, introducing the concept of objectification where perceivable objects can be associated with signs to aid in thinking. A qualitative study with graduate students was conducted to evaluate the effectiveness of DDEs in supporting objectification. The findings suggest that DDE technologies can afford objectification in two distinct ways. This work advances a method for examining device ecologies in terms of their potential for learning and offers insights for designing technology that can enhance higher thought processes.
412418	41241825	IDSET: Interactive Design System using Evolutionary Techniques	The paper proposes a system called IDSET, which supports the creation of tools using evolutionary techniques. The system has two stages - one where fundamental shapes are generated and another where they are combined. The system generates new shapes at each stage and displays them to the user, stimulating their creativity. Unlike other systems, IDSET starts with creating parts instead of using pre-prepared parts, resulting in a variety of novel tools. The user only needs to evaluate the displayed tools, rather than creating them. The system was tested through computer experiments, and various tools and artistic shapes were successfully created, some of which were not easily imaginable by humans. This demonstrates the effectiveness of IDSET.
412419	41241956	Experimental Evaluation of Ontology-Based Test Generation for Multi-agent Systems	Software agents are becoming increasingly popular for managing complex, distributed systems. To ensure the reliability of these systems, there is a growing need for methodologies and techniques that can effectively test multi-agent systems. A new approach, using an agent interaction ontology, has been developed and incorporated into a testing framework called eCAT. This framework can automatically generate and evolve test cases, and continuously run them. In this paper, the authors evaluate the effectiveness of this ontology-based approach through experiments using two BDI agent applications. The results demonstrate the framework's performance and its ability to identify faults in the tested systems.
412420	41242018	How knowledge representation meets software engineering (and often databases)	This paper examines various personal research projects that utilized concepts from Knowledge Representation and Reasoning to solve problems in Software Engineering and Databases. It explores the degree to which these ideas were directly applicable and how often new techniques had to be developed. The projects demonstrate the potential for cross-disciplinary collaboration and the importance of incorporating diverse perspectives in finding solutions to complex problems.
412421	41242195	Optimally fast parallel algorithms for preprocessing and pattern matching in one and two dimensions	This article discusses optimal parallel algorithms for string matching problems, specifically in one dimension. The authors have designed an algorithm that can efficiently compute a substring in constant time, which was previously a bottleneck in pattern preprocessing. They have also improved the preprocessing time for constant-time text search and developed algorithms for deterministic and probabilistic string matching in certain cases. Additionally, the article presents a constant expected time Las-Vegas algorithm for string matching, solving a major issue in this field.
412422	41242264	Building and Testing a Statistical Shape Model of the Human Ear Canal	The current design of custom in-the-ear hearing aids relies on individual experience and skills rather than a systematic understanding of the variation in ear canal shape. This paper presents a method for creating a detailed model of the human ear canal using laser scanned ear impressions and expert-placed anatomical landmarks. The model is used to analyze potential differences in ear canal size and shape between genders. By warping a template mesh onto the ear canal shapes in the training set and using the resulting vertices to create a 3D point distribution model, a more accurate and reliable means of designing custom hearing aids can be achieved. 
412423	41242396	Effective User Relevance Feedback for Image Retrieval with Image Signatures.	CBIR has become increasingly popular as a method for searching and retrieving digital images, due to the large number of images now available online. Relevance feedback (RF) is a technique used in search engines that involves gathering feedback from users about the relevance of search results, and using this information to improve future searches. This paper presents a new approach for using RF with signature based image retrieval, which involves the user in the process and allows them to refine their search queries. Empirical experiments have shown that this approach improves the performance of CBIR in terms of accuracy, speed, and scalability. This is a significant contribution as it addresses the problem of effectively reformulating image queries, for which there is currently no effective solution.
412424	41242472	Performance of a Policy-Based Management System in IPv6 Networks Using COPS-PR	The implementation of a centralized server is the most effective way to ensure the correct Quality of Service (QoS) in new generation core networks. These networks commonly use DiffServ due to its scalability, making it suitable for large networks. A Bandwidth Broker architecture is a policy-based management system designed to centrally manage DiffServ networks through admission control and policy delivery. This paper showcases the implementation of this architecture for managing QoS in DiffServ dual networks (IPv4/IPv6). Developed using JAVA and COPS-PR as an intra-domain communication protocol, the system allows administrators to configure policies based on SLA/SLS. The performance of the architecture was evaluated using a real IPv6 DiffServ testbed, and the results and conclusions are presented.
412425	41242534	Evaluating and improving semistructured merge	This article compares unstructured and semistructured merge tools in terms of conflict detection and resolution. While previous studies have shown that semistructured merge tools result in fewer reported conflicts, it is unclear if this actually leads to increased productivity and quality. To address this, the authors reproduce over 30,000 merges from 50 open source projects and analyze the false positives and false negatives of each approach. Their findings show that semistructured merge tools have significantly fewer false positives and that these are easier to resolve, but there is no significant difference in false negatives. Based on these results, they develop an improved semistructured merge tool that reduces the number of reported conflicts, has no additional false positives, and has fewer false negatives compared to unstructured merge. 
412426	41242681	On exploiting static and dynamically mined metadata for exploratory web searching	This paper discusses the limitations of current web search engines (WSEs) in meeting users' exploratory search needs, as they are designed for targeted, keyword-based searches. The authors propose a solution that incorporates both static and dynamically generated metadata to enhance the exploration aspect of web searching. This involves using online results clustering to provide users with an overview of top results, as well as utilizing various static metadata such as domain, language, date, and file type through a dynamic taxonomy and faceted exploration approach. Incremental algorithms are also designed to improve the speed of the exploration process. An experimental evaluation and user study show promising results in terms of effectiveness, flexibility, and user satisfaction. 
412427	41242739	Contextualization as an independent abstraction mechanism for conceptual modeling	The concept of context is relevant not only in computer science, but also in other fields. This paper proposes a framework for representing context in information modeling. Context is defined as a set of objects with names and possible references to other contexts. The framework allows for structuring context through traditional abstraction mechanisms such as classification, generalization, and attribution. The paper also explores the relationship between context and traditional abstraction mechanisms and presents a theory for contextualized information bases, including validity constraints, a model theory, and inference rules. The core theory can be expanded to support embedding of specific information models within the contextualization framework.
412428	4124287	A New Lower Bound on the Number of Perfect Matchings in Cubic Graphs	The article discusses the number of perfect matchings in $n$-vertex cubic bridgeless graphs. It is proven that every such graph must have at least $n/2$ perfect matchings. A list of all 17 graphs with less than $n/2+2$ perfect matchings is provided. These results have implications in the study of perfect matchings and cubic graphs.
412429	4124293	Sliding Mode Congestion Control for data center Ethernet networks	Data centers are increasingly adopting Ethernet as the unified switch fabric, known as Data Center Ethernet. One important enhancement is the end-to-end congestion management, for which Quantized Congestion Notification (QCN) has been established as the standard. However, experiments have shown that QCN can experience queue oscillation at bottleneck links, resulting in decreased link utilization. This is due to the fact that QCN's equilibrium point is mainly reached through sliding mode motion, which is influenced by system parameters and network configurations. To address this issue, the Sliding Mode Congestion Control (SMCC) scheme has been proposed, which is simple, stable, and has a fast response time. SMCC can easily replace QCN and outperforms it in variable traffic and network conditions. Experiments on the NetFPGA platform have demonstrated the superiority of SMCC.
412430	41243071	The Effect of Information Utilization: Introducing a Novel Guiding Spark in the Fireworks Algorithm.	The fireworks algorithm (FWA) is a popular swarm intelligence algorithm used in various applications. A new feature called the guiding spark (GS) is introduced to enhance its performance by utilizing more information. This is achieved by constructing a guiding vector (GV) using the objective function's information from explosion sparks, and adding it to the position of the firework to generate an elite solution called a GS. The new version, called guided FWA (GFWA), shows improved performance in both exploration and exploitation compared to previous versions and other similar algorithms. The simplicity and effectiveness of the GS make it applicable to other population-based algorithms.
412431	41243118	An implementation for small databases with high availability	This technique presents a method for implementing small databases in operating systems and distributed systems using the checkpointing mechanism. It combines elements of stable storage and master-slave approaches for fault tolerance and is well-suited for systems that use the processor pool paradigm. It has been successfully used to create a monitoring service for a distributed system, with the benefits of being simple, efficient, reliable, and available.
412432	4124328	Non-redundant Multi-view Clustering via Orthogonalization	This paper proposes a new clustering paradigm for explorative data analysis that aims to find all non-redundant clustering views of the data. It acknowledges that in real-world applications, data can have multiple reasonable and interesting groupings from different perspectives, especially in high-dimensional data. The proposed framework includes two approaches: orthogonal clustering, which finds alternative clusterings by seeking orthogonality in the cluster space, and clustering in orthogonal subspaces, which seeks orthogonality in the feature space. The paper presents results from testing the framework on synthetic and high-dimensional data sets, demonstrating its ability to discover varied and meaningful solutions. This approach is referred to as multi-view clustering or non-redundant clustering, as it allows for different interpretations of the data without committing to a single clustering solution. 
412433	41243313	Automated cellular annotation for high-resolution images of adult Caenorhabditis elegans.	Recent advancements in high-resolution microscopy have opened up the possibility of studying gene expression at the individual cell level. The fixed lineage of cells in adult Caenorhabditis elegans makes it an ideal model for investigating complex biological processes such as development and aging. However, manually annotating individual cells in images of adult C. elegans is time-consuming and requires expertise. Automating this task is crucial for conducting high-resolution studies on a large scale. In this study, researchers developed an automated method for labeling a subset of 154 cells in high-resolution images of adult C. elegans. They used a combinatorial optimization approach and a learning algorithm to compare cells in test images with a manually annotated training atlas. The results showed an 84% median accuracy, demonstrating the feasibility of automating cell annotation in adult C. elegans. 
412434	41243455	Dividing Protein Interaction Networks by Growing Orthologous Articulations	The growing amount of data on protein-protein interaction networks has led to increased interest in comparing these networks across different species. To do this, researchers have proposed various models and algorithms for network alignment, which involves identifying conserved modules between networks. Traditionally, this involves creating a merged representation of the networks and using greedy techniques to find conserved modules. However, a new approach has been proposed that divides the networks into smaller subnets based on both graph theory and biological indicators, and then performs network alignment on these subnets. This approach has shown promising results in accurately identifying conserved modules and highlights the importance of considering orthology and articulation in comparative network analysis. 
412435	41243543	Object Reconstruction By Incorporating Geometric Constraints In Reverse Engineering	This paper presents a new method for reconstructing 3D geometric models of objects from range data. It focuses on improving the overall shape of the model by incorporating geometric constraints based on feature positions. The proposed framework allows for incremental addition and integration of constraints, resulting in a balance between minimizing shape fitting error and satisfying constraint tolerances. The paper introduces sets of constraints for planar and quadric surfaces, and demonstrates the effectiveness of the approach through experiments on synthetic and real objects. This work has significant implications for reverse engineering and manufactured object modelling, where feature relationships are commonly present. 
412436	41243689	Probabilistic management of OCR data using an RDBMS	The digitization of scanned documents is changing the way enterprises manage data. To integrate this data with existing enterprise data, OCR software is often used to convert images to ASCII text and store it in a relational database. However, OCR software can produce errors, leading to failed queries. State-of-the-art OCR programs use probabilistic models to account for these errors, but these models are usually discarded once the data is stored in the database. This study proposes retaining these probabilistic models in the database, but this poses a challenge due to the large size of the data. To address this, a new approximation scheme called Staccato is proposed, allowing users to balance query performance and accuracy. This scheme is integrated with standard RDBMS text indexing. 
412437	41243760	Human and Machine Collaboration in Creative Design	The article discusses the idea of collaboration between humans and machines in the creative design process. It argues that machines cannot be creative in the same way as humans, but proposes a computational model that allows for collaboration between the two. The article then delves into the specific architectural challenges of such a system, emphasizing the complexity of design and the need for both human and machine contributions. It proposes an open and distributed architecture using software agents to facilitate collaboration and creativity. The article also addresses the roles of different agents and the challenges encountered in developing the system, including how to integrate human and machine contributions in the design process.
412438	41243815	Glyph: Visualization Tool for Understanding Problem Solving Strategies in Puzzle Games.	Understanding player strategies is crucial for both academic researchers and industry practitioners when analyzing player behavior. For game designers and researchers, it is important to compare intended strategies with emergent strategies to identify glitches or undesirable behaviors. This is especially relevant in serious games used for education, as player strategies can indicate their cognitive progress towards learning goals. However, current methods for analyzing player strategies have limitations, such as difficulty in scaling up for large numbers of players and subjective biases. To address these issues, a new visualization technique has been proposed that allows for both an overview of strategies and examination of individual player behaviors. This approach has been tested on data from a commercial educational puzzle game and initial results show its effectiveness in understanding player strategies.
412439	41243923	CATaLog Online: Porting a Post-editing Tool to the Web.	CATaLog online is a new web-based translation and post-editing tool that can be accessed through a web browser. It is a free tool that only requires a simple registration. It offers editing and log functions similar to the desktop version of CATaLog, as well as new features that are described in detail in this paper. This tool allows users to post-edit both translation memory segments and machine translation output. It also provides a comprehensive set of log information, which can be used for project management and studying the translation process and translator's productivity. 
412440	41244026	Opinion Fluctuations and Disagreement in Social Networks	The article discusses a model of opinion dynamics that involves two types of agents: regular agents who update their beliefs based on information from their social neighbors, and stubborn agents who never change their opinions. In societies with both types of agents, there is no consensus among the regular agents and beliefs continue to fluctuate. The structure of the social network and the placement of stubborn agents influence the dynamics of opinions. The expected belief vector is shown to follow a differential equation and converge to a harmonic vector, while expected cross products of beliefs can be characterized through Markov chains. In large societies, a condition of homogeneous influence emerges, leading to similar distributions of beliefs among most regular agents.
412441	41244189	An Efficient Approach Toward the Asymptotic Analysis of Node-Based Recovery Algorithms in Compressed Sensing	The paper proposes a framework for analyzing node-based verification-based algorithms, where the signal length and number of non-zero elements scale with infinity. This framework is applied to study the performance of recovery algorithms in compressive sensing with random sparse matrices. It is found that there is a success threshold for the density ratio of the signal, below which the recovery algorithms are successful and above which they fail. This threshold depends on both the graph and the recovery algorithm. The results also show that there is a strong agreement between the asymptotic behavior and finite length simulations for large signal lengths. 
412442	41244258	Comparison of LRD and SRD traffic models for the performance evaluation of finite buffer systems	The paper compares different traffic models to determine their ability to accurately represent real traffic in a single server queue with limited buffer space. Models with long range dependence (LRD), short range dependence (SRD), and a combination of both are evaluated. The results show that models such as circulant modulated Poisson processes and fractional autoregressive integrated moving average are effective in simulating heavy load conditions, but may not accurately represent traffic under low to medium load conditions. Additionally, the accuracy of the circulant modulated Poisson process model may be compromised when fitting actual traffic data.
412443	41244310	How Does Software Process Improvement Address Global Software Engineering?	SPI programs have been used for decades to improve the quality and speed of software development. There are many different SPI approaches and a lot of experience available to guide and measure these projects. SPI covers a wide range of aspects from individual developer skills to entire organizations, including optimizing software lifecycle activities and creating organization awareness and project culture. Recently, there has been a growing interest in the topic of Global Software Engineering (GSE) in relation to SPI. A systematic mapping study was conducted and 30 papers were selected for an in-depth analysis. These papers discuss the use of cultural models and agile approaches in GSE, as well as identifying success factors and barriers for implementing SPI in a GSE context. 
412444	41244428	Assessing the usability of a visual tool for the definition of e-learning processes	The paper discusses a usability study of a visual language-based tool called ASCLO-S, which is used for creating adaptive e-learning processes. This tool utilizes flow diagrams to define different classes of learners and their suitable adaptive learning processes. The study uses a questionnaire-based survey and an empirical analysis to gather feedback from instructional designers. The results show that both the visual notation and the system prototype are user-friendly for designers with and without experience in computer usage and e-learning tools. The empirical analysis also reveals that the effort required to develop e-learning processes is not affected by the designer's experience, but rather by the complexity of the process. Overall, the tool is considered effective in assisting designers in creating adaptive e-learning processes.
412445	41244532	Weakly-private secret sharing schemes	Secret-sharing schemes are important in cryptography for creating secure protocols. However, the size of the shares in the best known schemes for general access structures is too large to be practical. The lower bound for sharing an l-bit secret with n parties is ω(ln/log n), but no progress has been made in closing this gap. To address this issue, researchers have investigated a weaker notion of privacy in secret-sharing schemes where unauthorized parties cannot rule out any secret, rather than not learning any probabilistic information. Surprisingly, they were able to construct schemes with smaller shares for various access structures, with the size becoming more efficient as the secret size grows. For example, for a specific access structure, they were able to construct a scheme with shares of size l + n log n, and for a 2-out-of-n threshold access structure, they were able to construct a scheme with 2-bit shares, compared to ω(log n) in perfect secret-sharing schemes.
412446	41244690	Join query optimization techniques for complex event processing applications	Complex event processing (CEP) is a technology used in modern applications to monitor and track important events in large data streams. CEP engines examine real-time information and try to identify combinations of events that match pre-defined patterns. This is similar to how traditional data management systems execute multi-join queries. However, there has been little research on using existing join optimization methods to improve the performance of CEP systems. This paper presents the first study of the relationship between these two areas. It proves that the CEP Plan Generation problem is equivalent to the Join Query Plan Generation problem and can be solved more efficiently using join query optimization techniques. Experiments show that these techniques outperform existing strategies for CEP optimization in terms of speed and memory usage. 
412447	4124472	(FPT-)Approximation Algorithms for the Virtual Network Embedding Problem.	The Virtual Network Embedding Problem (VNEP) is a common issue in cloud resource allocation, where request graphs need to be mapped onto a physical infrastructure. In the offline setting, the main objectives are profit maximization and cost minimization. This problem is a generalization of routing and call admission problems, but it is not well-understood. This paper presents the first fixed-parameter tractable approximation algorithms for VNEP, which use a novel linear program formulation to account for flexible mapping options and arbitrary request graph topologies. The algorithms have an exponential runtime in the request graphs' maximal width, but for fixed extraction width, they provide the first polynomial-time approximations. Additionally, the paper introduces the concept of extraction orders and shows that computing minimal width extraction orders and decomposable LP solutions is NP-hard.
412448	41244890	Query Answering in Expressive Variants of DL-Lite	The paper discusses the use of ontologies in different application domains, such as Data Integration and the Semantic Web, where they provide access to large amounts of data. This poses a challenge in balancing the expressive power of a Description Logic with the efficiency of reasoning. The DL-Lite family of logics was designed to meet these requirements, with a focus on data complexity. The paper proposes an extension of DL-Lite called DL-Litebool, which includes full Booleans and number restrictions. Using a novel reduction to the one-variable fragment of first-order logic, the paper studies the computational and data complexity of reasoning in DL-Litebool and its sub-logics. The results provide insights into the properties of these logics and show an extension of the LogSpace upper bound for answering unions of conjunctive queries in DL-Lite to positive queries and number restrictions.
412449	41244967	: A Tool for Automatic Composition ofServices Based on Logics of Programs	This paper presents a technique for automatic service composition and introduces a prototype software, called ESC, that implements this technique. The approach utilizes a finite state machine to characterize the behavior of a service. By using satisfiability in a variant of Propositional Dynamic Logic, the technique is able to automatically compose a client's desired service using a given set of available services. The system is open-source and can work with services described in WSDL and finite state machine languages. This allows for the creation of a composite service that fully realizes the client's specification.
412450	41245048	Project talk: coordination work and group membership in WikiProjects.	WikiProjects have played a significant role in the success of Wikipedia, but their specific contributions and methods of coordination have not been thoroughly examined. In a study analyzing discussions from 138 WikiProjects, it was found that they go beyond just content production and involve non-members in their work. This research suggests that WikiProjects have a more flexible and inclusive approach to collaboration compared to other virtual teams, and may operate similarly to open source software projects rather than traditional groups.
412451	41245123	The Ethics of Knowledge Transfers and Conversions: Property or Privacy Rights?	This article discusses the ethical considerations surrounding the transfer of knowledge within organizations. It explores the question of whether organizations have the right to own and use the knowledge of their employees, or if this knowledge is the property of the individual and protected by human rights. The article suggests that organizational knowledge may be viewed as intellectual property, while personal knowledge is subject to personal privacy rights. This may lead to different knowledge management practices within organizations. Ultimately, the article highlights the importance of considering both ethical and legal implications when managing knowledge within organizations. 
412452	41245250	Designing an Appropriate Information Systems Development Methodology for Different Situations	The use of information systems development methodologies has grown significantly, but there is no one-size-fits-all approach that works for all situations. This raises the question of when to use which methodology. To address this issue, a design research approach was used to create a radar diagram with eight dimensions. Three action research cycles were then conducted to validate the design in three projects involving IT project managers. The result is an artefact that can guide the selection of a specific methodology for a particular situation. This can help practitioners and researchers make informed decisions about which methodology to use.
412453	41245394	AMiner: Toward Understanding Big Scholar Data.	In this talk, the speaker introduces AMiner, a new academic search and mining system that goes beyond traditional document search. AMiner focuses on modeling researchers, their publications, and the venues where they publish, creating a comprehensive network. The system automatically extracts researcher profiles and integrates them with publication data. Currently, AMiner has collected data on over 130 million researchers and 100 million papers from various databases. The system also connects with professional social networks, enhancing the metadata. A unified topic modeling approach is used to analyze different entities and provide topic-level expertise search. AMiner also offers various researcher-centered functions, such as social influence analysis and collaboration recommendations. Since its launch in 2006, AMiner has attracted over 7 million IP accesses from 200 countries/regions. 
412454	41245463	Web page title extraction and its application	This paper discusses the automatic extraction of titles from HTML documents. While authors should accurately define titles in the title field, this is often not the case. The paper proposes a supervised machine learning approach to extracting titles and outlines a specification for what constitutes a proper HTML title. Two learning methods are used, one utilizing features from the DOM Tree and the other using visual features. Combining these methods results in significant improvements in title extraction accuracy compared to a baseline method. The paper also explores the application of extracted titles in web page retrieval, showing that using both extracted titles and title fields leads to better results than using title fields alone, particularly in the task of finding named pages. 
412455	41245516	A Feedback Mechanism for Network Scheduling in LambdaGrids	The future of e-Science applications will require the ability to transfer large amounts of data quickly between various computing centers and data repositories. This can be achieved through the use of a Lambda-Grid, which offers dedicated, high-speed, point-to-point connections that can be reserved for specific applications. However, this can lead to congestion at the end-systems, as their processing speeds have not kept up with networking speeds. To address this issue, researchers have proposed a lightweight end-system protocol that utilizes performance monitoring to detect potential congestion and provide feedback to the sending end-system. This protocol, called RBUDP+, improves the performance of data transfer over Lambda-Grids and has been demonstrated to be effective through network emulation.
412456	4124561	Minimizing the Data Transfer Time Using Multicore End-System Aware Flow Bifurcation	This study examines the impact of high speed network traffic, commonly used in data centers, on a commodity multicore machine. The researchers found that in certain scenarios, packet loss and degraded throughput can occur due to the machine's inability to process incoming data quickly enough. To address this issue, they propose a flow bifurcation technique that optimizes data transfer time using rate-based protocols and determines the optimal number of parallel flows and rates to utilize available bandwidth. This approach outperforms the widely used GridFTP protocol in computational grids, particularly when end-system losses occur in the receive ring buffer.
412457	412457152	Dynamic Traffic Grooming in Elastic Optical Networks	Spectrum elastic optical networks offer flexible central frequency and spectrum allocation for lightpaths. However, when provisioning a new connection in these networks, the control plane must solve the problems of electrical-layer routing and optical-layer routing and spectrum assignment (RSA). This involves determining how to route the connection through new and existing lightpaths and establishing new lightpaths with limited spectrum availability. The study proposes a multi-layer auxiliary graph that addresses these issues and allows for different traffic-grooming policies. A spectrum reservation scheme is also proposed to efficiently use the flexibility of lightpaths and reduce operational costs. The results show that incorporating the reservation scheme can improve spectrum efficiency and reduce costs in various traffic-grooming policies.
412458	41245820	Energy-Efficient and Thermal-Aware Resource Management for Heterogeneous Datacenters.	This paper focuses on studying the management of resources in heterogeneous datacenters, considering energy efficiency, thermal control, and performance optimization. As datacenters become more diverse, with variations in computing power, power consumption, and cooling systems, it is important to address all these factors for optimal resource management. The paper introduces the concept of a heat distribution matrix to account for the influence of servers on each other, and proposes a heuristic solution for server placement and a greedy framework for online scheduling. It also presents single-objective heuristics and a fuzzy-based priority mechanism to balance performance, energy usage, and cooling. The results, based on simulations and real measurements, demonstrate the effectiveness of the proposed approach. 
412459	4124598	Requirements specification via activity diagrams for agent-based systems.	Goal-oriented agent systems are becoming increasingly popular for developing complex applications in dynamic environments. However, like any software, these systems require clear and well-defined system requirements in order to be successful. In this paper, the authors propose a method for improving the understandability and maintainability of requirements in a popular agent design methodology called Prometheus. This method involves automatically generating UML activity diagrams from existing requirements models, such as scenarios and goal hierarchies. The authors believe that this approach can help reduce ambiguity in the current requirements specification and provide a more structured way of representing variations. The evaluation of this approach through user experiments showed that it enhances understanding of requirements, makes it easier to modify them, and helps ensure coverage in the detailed design of the agents. This approach can also be applied to other methodologies that use similar concepts for specifying requirements.
412460	41246031	How difficult are exams? A framework for assessing the complexity of introductory programming exams	The level of difficulty of exam questions has a significant impact on student performance. To better understand the expected level of skills and knowledge at the end of a course, this paper explores using the assessment of question difficulty as a gauge. A subjective assessment and a specially designed complexity classification system were used to measure the difficulty of exam questions, specifically in introductory programming courses. The system considers factors such as external references, explicitness, linguistic and conceptual complexity, length of code, and intellectual complexity. The study examined 20 exam papers from five countries and found a wide range of difficulty levels. All measures of complexity were found to correlate with the assessment of difficulty, highlighting the importance of considering these factors in assessing learning standards in programming courses.
412461	41246121	Interactive Obstruction-Free Lensing For Volumetric Data Visualization	Volumetric visualization faces challenges with occlusion, which hinders the direct visualization of the area of interest. Existing techniques like transfer functions and volume segmentation have limitations in addressing occlusion, especially when dealing with similar density datasets. A new technique has been proposed to address this issue by allowing the user to define an area of interest and using a lens to reveal hidden volumetric data. The lens is modified with a fish-eye deformation, providing a better view of the surroundings. The technique has been implemented using GPU acceleration for real-time exploration and has been demonstrated in different scenarios like baggage inspection and 3D fluid flow visualization. 
412462	41246229	Role of calibration, validation, and relevance in multi-level uncertainty integration.	Calibrating model parameters is crucial in predicting complex systems, but lack of data makes it difficult. This paper proposes a method for quantifying uncertainty in system level predictions by integrating calibration, validation, and sensitivity analysis at different levels. The approach considers the validity of models used for parameter estimation at lower levels and their relevance to the system level prediction. A model reliability metric is used to evaluate validity, and Sobol indices are used to measure relevance. These results are then integrated in a roll-up method to predict the system output. This approach allows for more accurate predictions by considering the validity and relevance of lower level tests.
412463	41246345	Eudaemonic Computing ('underwearables')	This paper discusses a framework for wearable computing that prioritizes being unobtrusive and integrated into everyday clothing. This approach, known as 'eudaemonic computing', is exemplified through the creation of the 'underwearable computer'. This computer system is designed to be worn within or under regular clothing and has evolved into a tank-top shape. This design was chosen for its even weight distribution, privacy, and proximity to the body for sensing and output capabilities. The paper also explores the use of vibrotactile technology, specifically the VibraVest, as a means of assisting the visually impaired. The success of this technology suggests the potential for other unobtrusive wearable devices in various aspects of daily life.
412464	41246422	First symposium on the Personal Web	The First Symposium on the Personal Web, sponsored by IBM CAS Research and colocated with CASCON 2010, aims to bring together key researchers and practitioners to focus on the research directions and challenges of the Personal Web as the next step in the evolution of the Smart Internet. The Smart Internet is envisioned as a platform that automatically aggregates data and services to support individual users' goals and tasks. The Personal Web focuses on the user's perspective, allowing for seamless integration across the web based on their context and interests. The research initiatives for the Smart Internet include Smart Interactions, which focuses on discovering and delivering relevant data and services, and Smart Services, which addresses challenges in the web architecture and infrastructure. The Personal Web emphasizes the use of semantically-linked data to enable user-driven integrations and presents new research challenges.
412465	41246565	On triple systems and strongly regular graphs	A Steiner triple system of order v is represented by a (v(v-1)/6,3(v-3)/2,(v+3)/2,9) strongly regular graph. While most strongly regular graphs with these parameters are the block graph of a Steiner triple system, some exceptions exist for small orders. These exceptions can be explained using the concept of switching. Similarly, Latin squares can also be treated using switching. By applying switching and constructing graphs with specific automorphisms, many new strongly regular graphs have been discovered, such as (49,18,7,6), (57,24,11,9), (64,21,8,6), (70,27,12,9), (81,24,9,6), and (100,27,10,6), which do not come from Steiner triple systems or Latin squares.
412466	41246637	HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion	Research on human motion and pose estimation has made significant progress in recent years, but there has been no comprehensive evaluation of different methods to determine the current standard. To address this, the HumanEva datasets were created, which consist of synchronized video and ground-truth 3D motion data captured using a specialized hardware system. These datasets contain multiple subjects performing predefined actions with multiple repetitions, resulting in a large amount of data. Standard error measures were defined to evaluate 2D and 3D pose estimation and tracking algorithms. A baseline algorithm for 3D articulated tracking was also developed, using Bayesian filtering and optimization techniques. Experiments showed that image observation models and motion priors are important factors in performance, and that Bayesian filtering performs well in a laboratory environment with initialization. The datasets and software are available to the research community, providing a foundation for the development and comparison of new methods and establishing the current state of the art in human pose estimation and tracking.
412467	4124679	Empty pseudo-triangles in point sets	The study focuses on empty pseudo-triangles in a set of n points in the plane. These are triangles with vertices at the points and no points inside. The minimum and maximum number of these triangles is determined, with different bounds depending on whether the points are inside a triangle or not. The study also looks at optimization problems, such as finding the minimum or maximum perimeter or area of these pseudo-triangles. The time complexity for solving these problems ranges from O(n^3) to O(n^6) depending on the given conditions. Additionally, the number of star-shaped pseudo-triangles is also considered, with different bounds for their minimum and maximum numbers.
412468	41246862	Probabilistic Wire Resistance Degradation Due to Electromigration in Power Grids.	Electromigration (EM) is a growing issue in on-chip interconnects, especially in the computing and automotive industries. It can cause wire resistances to increase, leading to performance failure in a product's lifetime. Existing EM models have limitations in accurately capturing modern copper dual-damascene (Cu DD) metallization and accounting for circuit resilience. This paper presents a new method to analyze EM's impact on wire resistance in a circuit. It considers factors such as susceptibility and statistical behavior of void evolution. The proposed criterion identifies wires at risk of failure, taking into account steady state and system lifetime. The study also shows the robustness of on-chip power grids in maintaining supply integrity despite EM failures. 
412469	41246971	High-level algorithm and architecture transformations for DSP synthesis	This survey paper discusses the use of high-level transformation techniques to enhance the performance of digital signal and image processing architectures and circuits implemented using VLSI technology. It highlights the importance of carefully selecting algorithms, architectures, implementation styles, and synthesis techniques in designing successful VLSI processors. These transformations can help reduce silicon area or power consumption while maintaining speed, or increase speed while using the same amount of area. The paper reviews various transformation techniques such as pipelining, parallel processing, retiming, unfolding, and others, and how they can improve the suitability of an algorithm for a specific architectural style. 
412470	4124709	srcType: A Tool for Efficient Static Type Resolution	The article introduces an efficient tool for resolving static types, which is based on srcML, an XML representation of source code. This tool can determine the type of every identifier in a given code body, providing a dictionary for easy lookup. The type information includes various metadata such as constness, class membership, and file information. The tool is scalable and can generate a dictionary for large codebases like Linux in under 7 minutes. It is open source and can be downloaded from srcML.org under a GPL license.
412471	412471152	Efficient User Support with DACS Scheme	The university network faces various problems when it comes to operation and management. One of these problems is user support, which often arises during changes in setups by users. This can be a time-consuming task for system administrators as they have to provide individual support to each user. Another issue is dealing with annoying communication from certain users, which requires a lot of effort and time to identify the source. To address these challenges, a new user support system called DACS (Destination Addressing Control System) Scheme is being considered. This system aims to simplify user support tasks and make network operation and management more efficient in university networks.
412472	41247216	Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation	The WMT10 and MetricsMATR10 shared tasks focused on machine translation, system combination, and evaluation. 104 machine translation systems and 41 system combination entries were manually evaluated, and their rankings were used to measure the correlation between automatic metrics and human judgments of translation quality. The study also explored hiring non-expert annotators through Amazon's Mechanical Turk to increase the number of human judgments. 26 metrics were used to assess translation quality.
412473	41247323	Bit-split string-matching engines for intrusion detection and prevention	Network Intrusion Detection and Prevention Systems are crucial for providing security to those connected to a network. A key component of these systems is the string-matching algorithm, which allows for decisions to be made based on the actual content flowing through the network. However, as network speeds increase, this task becomes computationally intensive. To address this issue, a specialized device is needed that can maintain performance, be updated with new rules seamlessly, and be efficient enough to be integrated into existing network chips or wireless devices. Our approach involves a special purpose architecture and optimized string-matching algorithms, resulting in a system that is 10 times more efficient than existing methods. This is achieved by breaking down the database of strings into small state machines that search for a portion of the rules and bits of each rule.
412474	41247418	Semi-Supervised Classification with Graph Convolutional Networks.	Our approach for semi-supervised learning on graph-structured data uses a convolutional neural network that works directly on the graph. We use a localized first-order approximation of spectral graph convolutions to guide our choice of convolutional architecture. Our model has linear scalability with the number of graph edges and learns hidden layer representations that capture both the local graph structure and node features. Through experiments on citation networks and a knowledge graph dataset, we show that our approach outperforms similar methods by a significant margin. This demonstrates the effectiveness of our scalable approach for semi-supervised learning on graph data.
412475	41247532	Network-level dynamics of diffusively coupled cells	This article discusses the study of molecular dynamics in populations of diffusively coupled cells with fast diffusive exchange. The authors propose conditions for boundedness and ultimate boundedness in systems with a singular perturbation, which expands upon existing stability results. Using these results, they demonstrate that commonly used models of intracellular dynamics lead to coordinated behavior in cell populations, with all cells converging to a common equilibrium point. The authors then focus on a specific example of coupled cells functioning as bistable switches, where the population as a whole exhibits bistable behavior by converging to a state where all cells are close to one of two equilibrium points. Finally, the practical implications of these findings for the robustness of cellular decision making in coupled populations are discussed.
412476	412476130	Transform coding for hardware-accelerated volume rendering.	Hardware-accelerated volume rendering is the standard method for real-time rendering, but limited graphics memory can be a challenge when dealing with large volume data sets. Volumetric compression, where decompression is linked to rendering, has been proven to be an effective solution. However, most existing techniques were developed for software volume rendering and are not suitable for real-time hardware-accelerated rendering. In this paper, a new block-based transform coding scheme is proposed that addresses this issue by consolidating the inverse transform with dequantization. This approach allows for precomputation of most of the reprojection, making decompression fast without sacrificing compression quality. Additionally, a new block classification scheme is introduced to preserve important features during compression. The end result is an asymmetric transform coding scheme that allows for real-time decompression of large volumes while rendering on the GPU.
412477	41247743	Ensuring Time in Service Composition	This paper discusses the significance of time in service compositions, particularly in situations where meeting a deadline is essential. It introduces a framework that takes into account the execution time of services in order to improve the overall composition. The reliability of service execution times is also emphasized, considering factors such as workload and availability. This framework aims to enhance service compositions by incorporating timely and reliable service execution. 
412478	412478109	Why Do Internet Services Fail, and What Can Be Done About It?	The article discusses the architecture, operational practices, and failure characteristics of three large-scale Internet services. Research was conducted through interviews with architects and operations staff, as well as examining operations databases and failure reports. The architecture of these services includes front-end and back-end nodes, redundancy, and custom software. Operationally, there is a close relationship between service developers and operators, and coordination is necessary for problem detection and repair. Operator error is the main cause of failures, and front-end nodes experience more problems but result in less unavailability. The article suggests that implementing online testing and better detection of component failures could help reduce system failure rates.
412479	41247914	An 8.69 Mvertices/s 278 Mpixels/s tile-based 3D graphics SoC HW/SW development for consumer electronics	The paper discusses a 3D graphics system-on-a-chip (SoC) that supports OpenGL ES 1.0 and has a high performance of 8.69 Mvertices/s and 278 Mpixels/s. It also includes features such as embedded circuitry for monitoring performance and detecting bus errors, as well as the ability to capture bus traces with compression up to 98%. The SoC has a compact size of 15.7 mm2 and operates at a frequency of 139 MHz. 
412480	41248036	Predicting Instructor'S Intervention In Mooc Forums	Instructor intervention plays a crucial role in MOOCs, where personalized interaction is limited. This paper explores the issue of predicting when instructors will intervene in student discussion forums. The proposed models take into account various factors unique to MOOCs, such as course information, forum structure, and post content. These models use latent categories to summarize individual posts and improve the accuracy of intervention prediction. Experiments on data from two Coursera MOOCs show that incorporating thread structure into the learning process results in better predictive performance. Overall, this study highlights the importance of instructor intervention and offers potential solutions for predicting it in MOOCs.
412481	41248137	What does classifying more than 10,000 image categories tell us?	Image classification is a crucial task for both humans and computers, but one of the challenges is the large scale of the semantic space. While humans can recognize tens of thousands of object classes and scenes, no computer vision algorithm has been tested at this scale. This paper presents a study of large scale categorization with more than 10,000 image classes. The authors find that computational issues are crucial in algorithm design, conventional wisdom from smaller datasets may not hold, and the structure of WordNet is strongly related to the difficulty of visual categorization. They also argue for the importance of dataset scale, category density, and image hierarchy in developing automatic vision algorithms for recognizing tens of thousands or even millions of image categories.
412482	41248264	A packet loss estimation model and its application to reliable mesh-based P2P video streaming	This paper presents a model for estimating packet loss probability in a mesh-based P2P network. The model considers factors such as channel packet drop rate, peer dynamics, and FEC protection, to account for the complex and heterogeneous packet loss behavior in a mesh network. The proposed model is able to accurately estimate packet loss and is used to develop a peer selection mechanism that can reduce packet loss propagation by selecting the most reliable child-peers to transmit redundant substreams. Simulation results demonstrate the effectiveness of the proposed model and peer selection mechanism in improving the overall performance of a mesh-based P2P network.
412483	41248376	Rate-Distortion Cost Estimation for H.264/AVC	This paper discusses the usefulness of rate-distortion cost estimation in H.264/AVC applications, particularly for rate-distortion optimization and rate-control. The authors propose a new rate-prediction model and adaptive algorithm to improve the accuracy of estimating the number of coding bits for intra and inter-block encoding. Their approach uses a linear combination of existing coding parameters, which are more closely related to entropy coding and transform coefficients. They also introduce a cost estimation function and block classification method to further enhance the accuracy of their approach. The results show that their proposed schemes outperform previous methods in terms of bit-rate and rate-distortion cost estimation.
412484	41248449	Reinforcement learning for robot soccer	Batch reinforcement learning methods are a valuable tool for teaching autonomous robots to learn efficiently and effectively. The authors' recent work focuses on applying this approach to a difficult and complex domain. Their paper explores different versions of the batch learning framework, specifically designed for using multilayer perceptrons to estimate value functions in continuous state spaces. These methods have proven successful in teaching essential skills to their soccer-playing robots, which have competed in the RoboCup tournament. The paper presents three case studies demonstrating the effectiveness of the batch learning approach.
412485	41248550	Differential variational inequalities	The paper discusses the use of differential variational inequalities (DVIs) in modeling various applied problems that involve dynamics, inequalities, and discontinuities. It unifies different mathematical problem classes such as ordinary differential equations (ODEs), differential algebraic equations (DAEs), and dynamic complementarity systems. Conditions are presented for converting DVIs to equivalent ODEs and for numerically solving them using an Euler time-stepping procedure. The convergence of this procedure is established for initial-value and boundary-value DVIs with a maximum index of two and absolutely continuous solutions. The paper also addresses a class of DVIs for which traditional methods are not applicable. 
412486	41248632	A shape-based camera angle-invariant retrieval scheme for 3D objects	 Camera angle-invariant object retrieval is essential in various applications such as security, surveillance, and place recognition. However, to achieve this, we need object images at every angle, leading to a large image database. Existing indexing schemes are not suitable for handling such a huge database, resulting in performance issues. To address this problem, a new indexing and matching scheme is proposed in this paper, focusing on two types of camera movements: horizontal rotation and fixed rotation. This scheme involves two levels of indexing structures to reduce the space requirement, namely static indexing for horizontal angle invariance and dynamic indexing for rotation invariance. Implementation of this scheme in a prototype image retrieval system showed better performance compared to other methods.
412487	41248763	Clairvoyance: look-ahead compile-time scheduling.	Hardware designs have been developed to improve the performance of memory-bound applications by hiding memory latency. This is achieved through the use of out-of-order (OoO) execution engines, but this comes at the cost of increased energy consumption. Contemporary processor cores offer a range of performance and energy efficiency options, from fast and power-hungry OoO processors to slower, but more efficient in-order processors. This proposal suggests using a simple OoO core, which strikes a balance between performance and energy efficiency, to achieve higher performance for memory-bound applications. This is made possible by using a new compile-time technique called Clairvoyance, which overcomes limitations such as unknown dependencies, insufficient independent instructions, and register pressure. Clairvoyance can improve execution time by up to 13% while maintaining high performance for compute-bound applications.
412488	41248814	Vehicle routing in a Spanish distribution company: Saving using a savings-based heuristic.	The article discusses a Vehicle Routing Problem (VRP) faced by a large distribution company in Northeast Spain. The company has a diverse fleet of vehicles with different capacities and makes multiple trips per day to distribute products to 400 stores across the country. This variant of the VRP, known as Heterogeneous Fleet and Multi-trip VRP, has not been extensively studied. To solve the problem, the company uses a simple algorithm based on the savings heuristic with a biased-randomization effect and three local search operations. This approach has resulted in savings of around 12% in transportation costs, equivalent to €30 000 per week.
412489	41248911	A Novel Mutual Authentication Scheme For Rfid Conforming Epcglobal Class 1 Generation 2 Standards	RFID technology has become increasingly popular in recent years, with many applications being discovered for its use. However, there are still security concerns that need to be addressed. A mutual authentication scheme is proposed in this paper to address issues such as privacy, replay attack, forward security, and user location privacy. The scheme utilizes the tag as a storage media based on EPCglobal Class 1 Generation 2 (CIG2) standards. Through analysis, it is determined that the proposed scheme is able to withstand known attacks and can be implemented in low-cost RFID systems using current technology.
412490	41249060	Software tool combining fault masking with user-defined recovery strategies	.The voting farm is a distributed software tool that allows for parallel message passing systems to implement a fault tolerant voting mechanism. It was developed as part of the EFTOS project and can be used alone or with other EFTOS tools. The tool can be used to implement various recovery strategies, such as restoring organs or N-modular redundancy systems with N-replicated voters. By combining these strategies with the fault masking capabilities of the voting tool, it is possible to create complex fault tolerant systems. The tool offers replication transparency, flexibility, ease-of-use, and good performance, making it a valuable resource for improving reliability. 
412491	41249119	A Privacy-Preserving Solution for Tracking People in Critical Environments	In highly sensitive environments, strict security measures are necessary to control physical access and monitor the location of individuals. The use of RFID technology allows for tracking and logging of people's movements, providing crucial information in case of a security incident. However, this solution raises privacy concerns. This paper proposes an RFID-based technique that allows for tracking while preserving privacy by introducing uncertainty into the tracking process. This approach implements the k-anonymity property, where the probability of correctly identifying a person's location at a given time is k-1. The proposed technique is cost-effective and has been successfully evaluated in experiments.
412492	41249258	Reasoning with minimal models: efficient algorithms and applications	The paper discusses the challenges of reasoning with minimal models in knowledge-representation systems, even for simple theories. The authors introduce the elimination algorithm, which can efficiently find and check minimal models for a subset of positive CNF theories called positive bead-cycle-free (HCF) theories. They also show that this algorithm is more effective for minimal entailment than for all positive CNF theories. The paper concludes by demonstrating how variations of the elimination algorithm can be used for efficient querying in disjunctive deductive databases and default theories. This research was published in 1997 by Elsevier Science B.V. 
412493	41249320	TSEB: More Efficient Thompson Sampling for Policy Learning	In this paper, the authors propose a new algorithm called TSEB, which is based on Thompson Sampling and aims to solve the problem of learning in an unknown environment with tighter PAC guarantees. The algorithm maintains distributions over the model parameters and uses an exploration bonus to generate more informative exploration. This allows for better PAC bounds and helps to update the posterior over the model parameters. The authors provide a detailed analysis of the algorithm's PAC guarantees and show that the adaptive exploration bonus encourages additional exploration for better performance. Empirical analysis on simulated domains supports the effectiveness of the proposed approach.
412494	41249416	Intrinsically Motivated Hierarchical Skill Learning in Structured Environments	In this article, the authors propose a framework for reinforcement learning agents to learn abstract skill hierarchies in structured environments. This can greatly improve the efficiency of the agents in solving complex tasks. By understanding the causal relationships between actions and their effects on different features of the environment, agents can learn incrementally and autonomously using Bayesian network structure and structured dynamic programming algorithms. The authors also introduce a novel active learning scheme that uses intrinsic motivation to maximize the efficiency of learning the structure of the environment. This approach allows for a developmental learning process in which agents continuously acquire new knowledge and skills as they explore their environment.
412495	41249532	Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach	The paper discusses the challenge of learning a regression model using a fixed-rank positive semidefinite matrix and its nonlinear search space. It focuses on scalability for high-dimensional problems and uses the theory of gradient descent algorithms with Riemannian geometry. Unlike previous work, no restrictions are placed on the range space of the matrix, resulting in linear complexity and invariance properties. The proposed algorithms are applied to learning a distance function with successful results on standard benchmarks. 
412496	4124966	Planar piecewise algebraic curves	The article introduces a method for connecting sections of planar algebraic curves with continuous derivatives. It explores the use of piecewise algebraic curves in area modelling, which is the two-dimensional equivalent of solid modelling. The approach involves representing a planar rational parametric curve as a segment of an algebraic curve. The article also presents a formula for calculating the maximum distance between two algebraic curves, with one potentially being a parametric curve, within a defined area. This approach has potential applications in computer graphics and geometry.
412497	41249753	Directed and undirected network evolution from Euler–Lagrange dynamics	This paper investigates the evolution of undirected and directed networks using the Euler-Lagrange equation and the von Neumann entropy. The equation is used to develop a variational principle based on the entropy, and correlations in the degree difference are used to determine changes in entropy over time. The model is applied to three complex network models and is found to effectively capture both undirected and directed structural transitions. It is then applied to real-world networks, including stock price data from the NYSE and gene regulatory networks in Drosophila. The model accurately simulates degree statistics and detects anomalous network evolution. Overall, the model accurately captures topological variations in evolving networks.
412498	41249845	Conceptual clustering in information retrieval	Clustering is a technique used in information retrieval systems to organize documents into groups based on their associations with each other. This is achieved by examining the index terms or user feedback on queries. Characterizing these clusters can further enhance the retrieval process. In this paper, the authors propose a method for developing clusters and characterizations using a user viewpoint elicited through a structured interview based on personal construct theory. This approach results in a more effective way of organizing and assigning documents to clusters, improving both the query process and the assignment of new documents. 
412499	41249945	Smashing the Gadgets: Hindering Return-Oriented Programming Using In-place Code Randomization	The rise of non-executable page protections in popular operating systems has led to an increase in return-oriented programming (ROP) attacks, which allow for arbitrary code execution without injecting any code. Existing defenses against ROP exploits require source code or have a high runtime overhead, limiting their effectiveness for third-party applications. This paper introduces in-place code randomization, a practical mitigation technique that can be applied directly to third-party software. By using narrow-scope code transformations that do not change the location of basic blocks, in-place code randomization can safely randomize stripped binaries without adding any code. This technique effectively eliminates and breaks a large portion of useful instruction sequences found in PE files. It also does not introduce any measurable runtime overhead, making it compatible with other exploit mitigations such as address space layout randomization. The evaluation showed that in-place code randomization successfully prevented the exploitation of vulnerable Windows 7 applications and thwarted attempts to bypass it through alternative ROP payloads.
