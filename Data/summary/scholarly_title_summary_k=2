410	41065	Very Sparse Stable Random Projections, Estimators and Tail Bounds for Stable Random Projections	Stable random projections are widely used in data streaming, data mining, and machine learning. They provide a unified and efficient method for approximating norms and distances between data streams. This paper focuses on improving stable random projections in three ways. Firstly, by using very sparse projections, the processing and storage costs can be significantly reduced. This is achieved by replacing the α-stable distribution with a mixture of a symmetric α-Pareto distribution and a point mass at the origin. The rate of convergence is analyzed as a function of the parameters β, α, and data regularity. For example, when α = 1 and data has bounded second moments, a small value of β can lead to a fast convergence rate.
410	41059	Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections.	Stable random projections are a popular method in various areas of computing, such as data mining and machine learning, for efficiently calculating distances using limited memory space. This study proposes two algorithms, one using the geometric mean estimator and the other using the harmonic mean estimator, for computing distances with different values of alpha. The study also provides a general sample complexity bound for alpha values other than 1 and 2, which was previously a difficult task. The study also presents a practical and optimal algorithm for alpha values approaching 0, which is useful for boolean data. Additionally, the study provides precise theoretical analysis and practical implications, including exact constants and variances, to assist practitioners in choosing accurate sample sizes. 
411	41187	On cognitive mechanism of the eyes: the sensor vs. the browser of the brain	This paper discusses the role of the eyes in neuropsychology, specifically as a sensory organ and a cognitive browser. While it is known that the eyes contribute to 70% of sensory input to the brain, their function as a browser for memory and cognition has not been fully recognized. The paper argues that the eyes serve as a bi-directional organ, both sensing external stimuli and providing internal perceptual abilities such as consciousness, memory, motivation, and emotion. This sheds light on the important role of the eyes in the thinking process and has practical implications for understanding various cognitive mechanisms. 
411	411128	On the cognitive process of human problem solving	Problem solving is a crucial cognitive process that involves various other processes such as abstraction, learning, and decision making. It is a search process in the brain that aims to find a solution to a problem or reach a goal. This process is facilitated by the internal knowledge representation through the object-attribute-relation (OAR) model. A cognitive model and a mathematical model have been developed to explain this process, using real-time process algebra (RTPA) and concept algebra. This research is part of a larger project on cognitive computing, which seeks to understand and simulate the brain's mechanisms and processes in order to develop advanced methods and cognitive computers with thinking, learning, and perception abilities.
412	41268	Joint code-encoder-decoder design for LDPC coding system VLSI implementation	This paper discusses a design approach for implementing low-density parity-check (LDPC) coding systems using both irregular LDPC code construction and VLSI implementations of encoder and decoder. The goal is to construct efficient LDPC codes that can be easily implemented in hardware. To achieve this, a heuristic algorithm is proposed for constructing implementation-aware irregular LDPC codes that can provide strong error correction. The paper also presents the corresponding hardware architectures for the encoder and decoder. This approach offers a promising solution for implementing LDPC coding systems in hardware with high performance and low complexity.
412	41270	Block-LDPC: a practical LDPC coding system design approach.	The paper proposes a new approach called Block-LDPC for designing low-density parity-check (LDPC) codes, encoders, and decoders in practical LDPC coding systems. This approach involves constructing LDPC codes with specific constraints that ensure efficient hardware implementation of the encoder and decoder. The authors develop a set of constraints that are tailored to hardware implementations, allowing for a simpler and more effective design process. The resulting Block-LDPC codes have the potential to improve the performance and efficiency of LDPC coding systems in practical applications.
413	41392	Efficient Algorithms and Implementations for Optimizing the Sum of Linear Fractional Functions, with Applications	The paper introduces a new algorithm for solving the sum of linear fractional functions (SOLF) problem in 1-D and 2-D. The algorithm focuses on solving the off-line ratio query (OLRQ) problem, which involves finding optimal values for a sequence of linear fractional functions subject to linear constraints. By utilizing geometric properties and parametric linear programming, the algorithm can solve the OLRQ problem in O((m+n)log (m+n)) time. This can significantly speed up iterations in a known iterative SOLF algorithm, improving the overall efficiency in solving the SOLF problem. The algorithm has been tested and shown to outperform other commonly-used approaches in most cases. It has also been successfully applied to problems in computational geometry and other fields, improving upon previous results.
413	41396	Free-Form Surface Partition in 3-D	The problem of partitioning a spherical representation of a 3-D surface into two hemispheres is studied in this research. This problem is important in surface machining for manufacturing. The researchers have developed an algorithm that combines data structures, geometric observations, and algorithmic techniques to solve this problem. The algorithm has a time complexity of O(m^2n log log m) for handling multiple off-line sequences of insertions/deletions of convex polygons and point queries. This is a significant improvement over the previous best-known algorithm with a time complexity of O(m^2n^2). Additionally, the algorithm can also process O(n) insertions/deletions of convex polygons and queries on their common intersections in O(n^2 log log n) time, which is an improvement over the standard solution. The techniques used in this algorithm may also be applicable to other problems.
414	41467	Network Coding-Based Broadcast In Mobile Ad Hoc Networks	In multi-hop wireless networks, broadcast operation is crucial for disseminating information to all nodes. Previous methods for broadcast support have been either probabilistic (with nodes randomly rebroadcasting packets) or deterministic (with nodes pre-selecting specific neighbors for rebroadcasting). This paper introduces the use of network-coding in deterministic approaches, resulting in a significant reduction in the number of transmissions. Two algorithms are proposed, using local topology information and opportunistic listening: 1) a simple XOR-based coding algorithm with up to 45% gains compared to non-coding, and 2) a Reed-Solomon based algorithm with up to 61% gains in simulations. This coding-based deterministic approach outperforms a previously proposed coding-based probabilistic approach.
414	41447	Gossip-based ad hoc routing	Ad hoc routing protocols often use flooding, but this can result in unnecessary message propagation. To reduce overhead, a gossiping-based approach is proposed where nodes only forward messages with a certain probability. In large networks, gossiping has bimodal behavior where the message either dies out quickly or is received by many nodes. By using a gossiping probability between 0.6 and 0.8, almost every node can receive the message in almost every execution. This protocol uses 35% fewer messages than flooding and can be combined with other optimizations for even better performance. Simulations show that adding gossiping to AODV results in significant improvement in networks of 150 nodes, with even more potential in larger networks.
415	41518	The role of the velocity-slowness mapping in fan filtering of image sequences	A new method for designing fan filters for moving objects has been introduced, using a mapping from velocity sets to slowness sets. The main challenge in using fan filters for image sequences is due to the lack of one-to-one mapping between velocity and slowness, making it difficult to specify filter responses at all velocities. However, the fan filter shows potential for being a useful tool in analyzing image sequences. The strengths and limitations of this approach are discussed.
415	41526	Optimal detection of known moving objects in a noisy image sequence with velocity uncertainty	This paper presents an optimal algorithm for detecting a moving object in a noisy image sequence, taking into account prior knowledge of the object's velocity and intensity distribution. The algorithm involves formulating a convex mini-max optimization problem to find the optimal 3-D detection filter, which is expressed in terms of a 2-D Lagrange multiplier function. When the object's velocity is assumed to be within a certain set, V, the optimal filter can provide a significant signal-to-noise ratio improvement compared to a 3-D matched filter designed for the center velocity of the object. For a circular disk, the optimal filter can achieve a 6.7 dB improvement in mini-max SNR and cover an area 22 times greater in the velocity plane compared to a bank of 22 3-D matched filters.
416	41678	Beamsteering on Mobile Devices: Network Capacity and Client Efficiency	In this study, the focus is on the use of beamsteering technology to improve the efficiency and capacity of wireless networks for mobile devices. The current omni directional communication limits energy efficiency and causes interference, so the researchers propose using beamsteering to make mobile devices directional. This approach is feasible for devices such as Netbooks and eBook readers and can lead to a balance between device efficiency and network capacity. The team developed a distributed algorithm, called BeamAdapt, to help mobile clients reach their optimal operating point without central coordination. Simulations show that using BeamAdapt can significantly reduce power consumption while maintaining network throughput in large-scale networks.
416	41686	Beamforming on mobile devices: a first study	This study explores beamforming, a method of directional communication, on mobile devices. The researchers demonstrate that beamforming is already possible on mobile devices in terms of size, mobility, and power efficiency. Surprisingly, they find that beamforming can be more power-efficient than single antenna communication by optimizing the tradeoff between transmit and circuit power. The researchers also propose a solution called BeamAdapt, which allows each mobile device to determine the optimal number of active antennas for power efficiency. They test BeamAdapt on a prototype and in simulations, showing a significant reduction in power consumption while maintaining network throughput. Overall, this work establishes the feasibility and effectiveness of beamforming on mobile devices.
417	41728	IQ estimation for accurate time-series classification	Time-series classification is an important research topic in data mining and computational intelligence due to its numerous applications. The simple k-NN classifier using dynamic time warping (DTW) distance has been found to be competitive among other state-of-the-art time-series classifiers. However, in our research, we observed that using a fixed number of nearest neighbors (k) may not always result in optimal performance. This is because the complexity and characteristics of time-series data can vary from region to region. To address this issue, we propose a new method called individual quality (IQ) estimation, which estimates the expected accuracy for each time series and k value. We then combine the results of multiple k-NN classifiers using the IQ estimations to make a final prediction. Our experiments on 35 benchmark data sets demonstrate the effectiveness of our proposed IQ-MAX and IQ-WV algorithms, which outperform two baseline methods.
417	417100	INSIGHT: efficient and effective instance selection for time-series classification	Time-series classification is a crucial task in data mining with various applications in different fields. Recent research has found that the simple nearest-neighbor classifier using Dynamic Time Warping (DTW) as a distance measure is highly effective, often surpassing more complex algorithms. One method for improving the efficiency of this classifier is instance selection, which involves choosing representative instances from the training set to use during classification. This paper introduces a new instance selection method that takes advantage of the hubness phenomenon in time-series data, where a small number of instances are frequently the nearest neighbors. This method, called INSIGHT, combines a score-based selection approach with a principled method for optimizing training data coverage. The paper discusses the theoretical basis for this method and compares it to a leading instance selection method, FastAWARD, in terms of accuracy and execution time. The results show significant improvements in both categories.
418	41839	Clothes Co-Parsing Via Joint Image Segmentation and Labeling With Application to Clothing Retrieval.	This paper presents a new system for clothing co-parsing (CCP) that aims to jointly parse unsegmented clothing images into semantic configurations. The system consists of two phases: image cosegmentation, which extracts consistent regions using exemplar-SVM, and region colabeling, which constructs a graphical model to assign labels based on context. The system was evaluated on the Fashionista dataset and a new dataset called SYSU-Clothes, achieving high segmentation accuracy and recognition rates. The system was also applied to a challenging task of cross-domain clothing retrieval, where it outperformed previous methods in accurately identifying clothing items from online stores based on fine-grained parsing results. 
418	41896	Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network	The problem of cross-domain image retrieval is addressed with a focus on practical application. The goal is to retrieve similar clothing items from online shopping stores based on a user's photo. This is challenging due to the differences between online shopping images and user photos. To address this, a Dual Attribute-aware Ranking Network (DARN) is proposed, consisting of two sub-networks driven by semantic attribute learning. A triplet visual similarity constraint is also imposed to improve retrieval accuracy. A large-scale dataset is created from customer review websites, with around 450,000 online shopping images and 90,000 offline user photos with fine-grained clothing attributes. Extensive evaluation shows that DARN outperforms popular solutions using pre-trained CNN features. 
419	41954	Cache-Oblivious R-Trees	A cache-oblivious data structure has been developed for efficiently storing and retrieving a set of axis-aligned rectangles in the plane. The structure uses an axis-aligned bounding-box hierarchy and has provable performance guarantees. It can efficiently find all rectangles in the set that intersect with a given query rectangle or point. If the set does not contain a large number of points within each rectangle, the structure can answer rectangle and point queries with a limited number of memory transfers. A variant of the structure can achieve the same performance even with overlapping rectangles. The performance of the rectangle query matches that of the best-known linear-space cache-aware structure. 
419	419116	Significant-Presence Range Queries in Categorical Data	The traditional colored range-searching problem involves storing a set of n objects with m distinct colors and reporting all colors that have at least one object intersecting a given range. However, this can result in including 'outlier' objects that do not represent the majority of their color class. To address this, a variant of the problem was proposed where only colors with a certain fraction of objects intersecting the range, denoted by tau, need to be reported. This article focuses on an approximate version of the problem, where colors with a fraction (1 - epsilon)tau intersecting the range can also be reported, for a fixed epsilon > 0. Efficient data structures for this problem are presented for orthogonal query ranges in sets of colored points and for point stabbing queries in sets of colored rectangles. 
4110	41106	Multi-marker tracking for large-scale X-ray stereo video data.	Analyzing large amounts of video data is a major challenge in the era of big data. In medical research, for instance, tracking infected cardiac movements in animals using X-ray sequences is time-consuming and difficult. To address this, a two-stage graph-based data association method is proposed. This approach considers challenges such as occlusions, low contrast, and inaccurate detections, and uses a directed acyclic graph to connect 3D observations. Tracklets are then linked into longer tracks using global features. The method is tested on X-ray datasets of beating sheep hearts and outperforms standard tracking approaches in accuracy and efficiency. It can also be applied to other video data.
4110	411026	A Framework For Actively Selecting Viewpoints In Object Recognition	The article discusses the challenges in object recognition in computer vision, which is typically based on processing single images. The authors propose a method for active object recognition, where a camera is moved around the object to capture different viewpoints. This approach aims to improve classification accuracy while reducing the number of views needed. The optimization criterion is the gain of class discriminative information from each new image. An unsupervised reinforcement learning algorithm is applied, and the authors also present a sequential fusion algorithm for combining image information. Experimental evaluations on synthetic and real objects demonstrate the effectiveness of the proposed method in improving recognition rates. 
4111	411155	A Method For Verifying Concurrent Java Components Based On An Analysis Of Concurrency Failures	Java is a programming language that supports concurrent (simultaneous) execution of multiple tasks. However, verifying the correctness of concurrent programs is more challenging than sequential programs due to non-determinism and specific problems like interference and deadlock. The ConAn testing tool was developed to test concurrent Java components, but it has limitations in detecting certain types of failures. Other verification tools have been proposed, but they also have their own strengths and weaknesses. This paper suggests a method that combines ConAn with other static and dynamic verification techniques to effectively verify concurrent Java components. It uses a Petri-net model to analyze common concurrency problems and failures, and then determines suitable tools for detecting them. This method aims to provide a comprehensive approach for verifying concurrent Java components.
4111	411124	Maximising the information gained from a study of static analysis technologies for concurrent software	Empirical studies in Software Engineering are crucial for both researchers and practitioners, but they face limitations such as context specificity and high costs. However, by replicating and maximizing existing studies and using power analyses for accurate sample sizes, empirical studies can be effective. A controlled experiment was conducted to examine the combination of automated static analysis tools and code inspection in verifying and validating concurrent Java components. The results showed that this combination is cost-effective and by using a strategy to maximize information gained, conclusive results were obtained despite the size of the study. This contributes to the research on V&V technology evaluation.
4112	411252	A general framework for assembly planning: the motion space approach	Assembly planning involves finding the most efficient way to put together a product using its individual parts. A new method, called motion space, has been developed to help with this task. This framework breaks down assembly motions into smaller, independent movements that can be easily identified and do not interfere with each other. The motion space is then divided into cells, with each cell containing fixed blocking relations to prevent collisions between parts. This approach has been applied to various types of assembly motions and has been found to be efficient and effective in complex assemblies. However, there are still some challenges that need to be addressed.
4112	411272	Polyhedral Assembly Partitioning Using Maximally Covered Cells In Arrangements Of Convex Polytopes	The problem being studied is whether a collection of 3D polyhedral parts can be moved without colliding with the rest of the parts, indicating whether the object can be taken apart with two hands. This can be used by an assembly sequence planner, but may not provide a complete assembly operation. The problem is transformed into traversing an arrangement of convex polytopes and identifying maximally covered cells. An algorithm is devised which is guaranteed to find a solution, is simple, and improves upon previous solutions. An implementation of the algorithm is described and experimental results are reported.
4113	4113126	A federated approach to distributed network simulation	The article discusses the use of federated simulation techniques to create parallel simulations of computer networks. This approach involves dividing the topology and protocol stack of the network into submodels and creating a simulation process for each one. The runtime infrastructure software provides services for communication and synchronization. The article addresses issues that arise in both homogeneous and heterogeneous federations, including differing packet representations and incomplete implementations of network protocol models. The authors propose a dynamic simulation backplane mechanism to facilitate interoperability among different network simulators. Two methods for using the backplane are discussed: cross-protocol stack and split-protocol stack. Results from experiments on two computing clusters demonstrate the scalability of this approach. The article concludes that federated simulations are an effective method for creating efficient parallel network simulation tools.
4113	411365	On the parallel simulation of scale-free networks.	Scale-free networks have become a hot topic due to their widespread use in various fields such as social networks, biology, and the Internet. In this study, researchers explore the use of conservative parallel discrete event simulation techniques in simulating scale-free networks. They develop an analytical model to analyze the parallelism available in these simulations using a conservative time window synchronization algorithm. The performance of two different synchronization algorithms is evaluated, showing the impact of network topology on simulation performance. The results also highlight challenges such as performance bottlenecks that need to be addressed for efficient parallel execution. This study suggests that new approaches to parallel simulation of scale-free networks can greatly improve performance.
4114	4114102	Hazy Image Modeling Using Color Ellipsoids	This paper introduces a new framework for analyzing single image dehazing methods using color ellipsoids. Existing methods lack in explaining why a particular method was chosen. The research uses both synthetic and real world images to demonstrate the properties of color ellipsoids, which can indicate the amount of haze in an image. This framework is then applied to analyze a specific dehazing method, showcasing its usefulness. Overall, the use of color ellipsoids offers a promising approach to understanding and improving single image dehazing methods.
4114	411492	Fast Single Image Fog Removal Using The Adaptive Wiener Filter	This paper introduces a rapid method for removing fog from single images. It utilizes a unique approach to improve the estimation of fog amount by using the Locally Adaptive Wiener Filter. The paper also addresses the issue of correlated noise by using a decorrelation technique with a roughly estimated defogged image. The results show that this method is significantly faster than other existing methods and is comparable in quality to the Spectral Matting smoothed Dark Channel Prior method.
4115	411512	mClock: handling throughput variability for hypervisor IO scheduling	This paper discusses the allocation of resources in virtualized servers, which run a variety of virtual machines with different needs. The hypervisor is responsible for managing these resources and ensuring isolation between VMs. While current methods provide control over CPU and memory allocation, there is limited support for IO resource management. This paper introduces a new algorithm, mClock, for IO resource allocation in hypervisors. It allows for proportional sharing, minimum reservations, and maximum limits on IO allocations for VMs. The algorithm has been implemented in the VMware ESX server hypervisor and shows promising results in terms of VM performance and application latency. An adapted version, dmClock, is also presented for distributed storage environments.
4115	411515	A flexible approach to efficient resource sharing in virtualized environments	Cloud-based storage and computing have become popular due to the cost benefits of using a centralized infrastructure. However, the use of virtual-machine-based server consolidation in data centers presents challenges in resource management, capacity provisioning, and ensuring application performance. The fluctuating nature of workloads means that peak capacity needed for short bursts can be much higher than the long-term average. This often leads to over-provisioning and low utilization, resulting in higher costs. To address this issue, the authors propose a method for efficiently managing multiple bursty workloads on a shared storage server. This is particularly useful in virtual machine environments where the hypervisor must allocate IO bandwidth among multiple VMs. Their solution involves decomposing bursts to provide each workload with a graduated QoS and dynamically scheduling the decomposed portions of all the workloads.
4116	4116144	Inter-domain stealthy port scan detection through complex event processing	Large enterprises today are comprised of complex interconnected software systems that span multiple domains. This presents a challenge for implementing effective security defenses. A paper proposes an Intrusion Detection System (IDS) architecture that uses an open source Complex Event Processing engine called Esper to detect inter-domain stealthy port scans. The architecture includes software sensors deployed in different domains that send events to Esper for correlation. The IDS uses a Rank-based SYN (R-SYN) port scan detection algorithm, which combines and adapts three detection techniques to identify malicious host behavior. Evaluation results show that the algorithm is accurate in detecting port scan activities with low false positive rates. This approach offers a low cost of ownership and high flexibility for enterprises.
4116	411615	An event-based platform for collaborative threats detection and monitoring	Organizations face various threats to their information systems and often rely on isolated defenses such as firewalls, intrusion detection, and fraud monitoring systems. However, this approach is not always effective as similar organizations in the same market, such as financial institutions and telecommunication providers, tend to experience similar cyber crimes. Sharing and correlating information between these organizations can aid in early detection and mitigation of these crimes. The Semantic Room (SR) abstraction is a collaborative event-based platform that allows for controlled sharing and correlation of data from different information systems to detect coordinated internet-based security threats and frauds. Two SRs are proposed and validated in the paper: one for detecting inter-domain port scan attacks and another for online fraud monitoring in Italy. Real data traces are used to demonstrate the effectiveness of these SRs, with the first achieving high detection accuracy and the second providing new evidence and tools for law enforcement agencies.
4117	411747	Spliced Video and Buffering Considerations for Tune-In Time Minimization in DVB-H for Mobile TV	 ther encapsulated into RTP packets.A new video splicing method has been proposed to reduce the tune-in time of mobile TV in DVB-H. Tune-in time is the time between receiving a broadcast signal and starting to play the media. This method uses a random access point picture to minimize the time between decoding and outputting the media. In IPDC over DVB-H, an additional stream of IDR pictures is sent to the IP encapsulator to replace pictures in the bitstream, ensuring compliance with HRD requirements. A video rate control system is also proposed to satisfy HRD requirements while achieving good video quality. This method aims to optimize the broadcast experience for handheld devices with limited battery life. 
4117	411765	Tune-in Time Reduction in Video Streaming Over DVB-H	The proposed method aims to reduce the tune-in time for channel zapping in IP datacasting over DVB-H. This is achieved by using a time-sliced transmission scheme in DVB-H, which allows the receiver to turn off radio reception during time-slices that are not of interest to the user. The key factor in tune-in time is the time from media decoding to correct output, which can be minimized by starting a time-slice with a random access point picture. To achieve this, an additional stream of IDR pictures is transmitted to the IP encapsulator, which replaces pictures in the bit stream to align with time-slice boundaries. This can cause error propagation, which is addressed through an HRD-compliant video rate control system. Simulation results show that this method achieves a good average quality of decoded video with minimal tune-in delay and complexity.
4118	411842	Caching-aided coded multicasting with multiple random requests	Caching networks, where a source communicates with multiple users through a shared link, have recently gained attention. One specific type, the shared link caching network, involves a source with access to a file library and users who can store segments of these files. The source sends a coded multicast message to satisfy all users' requests at once. The objective is to find the smallest possible average codeword length to fulfill these requests. This paper focuses on the case where each user makes independent requests according to a common distribution. An achievable scheme using random vector caching placement and multiple groupcast index coding is proposed, which is shown to be order-optimal in certain scenarios. The impact of the number of requests per user on the performance of these schemes is also analyzed. 
4118	4118226	On the average performance of caching and coded multicasting with random demands	This article discusses the use of caching side information at users in a network with one sender and multiple receivers to efficiently satisfy simultaneous demands for different files. It introduces different strategies, such as deterministic and random caching, and linear coding, which have been proven to be optimal in worst-case demand scenarios. The focus is then shifted to the case of random user demands, specifically following a Zipf popularity distribution. The problem is framed as finding the minimum average number of equivalent message transmissions. The article presents a new decentralized random caching placement and coded delivery scheme, which have been shown to achieve optimal performance in this scenario. This is the first time that an order-optimal solution has been found for the caching and coded multicasting problem with random demands. 
4119	411981	Distributed Multicell and Multiantenna Precoding: Characterization and Performance Evaluation	This paper discusses downlink multiantenna communication using cooperative precoding done by base stations in a distributed manner. Previous research has assumed that transmitters have complete knowledge of data symbols and channel state information (CSI). However, in this study, each base station only has local CSI. For instantaneous CSI, a method to achieve the outer boundary of the achievable rate region is proposed for two multi-antenna transmitters and two single-antenna receivers. The paper also presents distributed versions of traditional beamforming techniques and shows how the design can be improved using the virtual SINR framework. Similar results are derived for the case of local statistical CSI. The paper provides heuristics for power allocation and demonstrates the performance through numerical simulations.
4119	411938	Egoistic vs. altruistic beamforming in multi-cell systems with feedback and back-haul delays.	Base station cooperation is a technique used to improve the efficiency of multi-cell systems. However, obtaining accurate channel state information (CSI) at the base station can be challenging due to delay and limited feedback techniques. This paper compares competitive and cooperative beamforming strategies in the presence of imperfect CSI. The impact of delay and finite codebook size on the achieved sum rate is analyzed and closed-form expressions are derived. A mode switching criterion is proposed based on SNR, delay, Doppler frequency, and codebook size to determine whether competitive or cooperative beamforming is preferred. The study shows that competitive beamforming is preferred at low SNR, while cooperative beamforming is more beneficial at high SNR and low Doppler frequencies.
4120	412036	Robust computations with dynamical systems	In this paper, the computational power of Lipschitz dynamical systems that are robust to infinitesimal perturbations is discussed. The previous study on this topic was limited to systems that were not considered natural from a classical mathematical perspective. However, the authors prove that their previous results can be extended to Lipschitz and computable dynamical systems. They also show that if robustness to infinitesimal perturbations is required, the reachability problem for these systems becomes decidable. This means that undecidability of verification does not apply to Lipschitz, computable, and robust systems. The authors also demonstrate that the perturbed reachability problem is co-recursively enumerable and complete, even for C∞-systems.
4120	412053	Using local planar geometric invariants to match and model images of line segments	Image matching is a fundamental process in computer vision that involves finding corresponding features in different images. This paper presents a line segment matching algorithm that assumes the apparent motion between two images can be approximated by a planar geometric transformation. Local planar invariants are used to match segment configurations in the images, and a global constraint is added to ensure consistency among all matches. The algorithm can also be applied to multiple images and is able to handle noisy and imprecise images, as well as uncalibrated cameras with unknown motion. The algorithm is based on known techniques such as geometric hashing and Hough algorithms, and is most effective for planar objects or man-made objects with coplanar elements. The paper also discusses verification and completion stages, as well as modeling different aspects of an object using image matching. Results of the algorithm are presented in the corresponding sections.
4121	412191	Connected graph searching	The graph searching game involves searchers trying to capture a fugitive on a graph by moving along edges, while the fugitive tries to avoid capture by moving along unguarded paths. The minimum number of searchers needed to guarantee the capture of the fugitive is known as the search number. This paper focuses on the game under the restriction of connectivity, where the locations of the graph that are clean must remain connected during the search. Many traditional mathematical tools do not work under this restriction, and the paper also explores the "price of connectivity," or the additional number of searchers needed for a connected search. The study provides estimations and tight bounds for the price of connectivity on general graphs and trees, and also presents a complete characterization for connected graph searching on trees. It is shown that the connected search number can be computed efficiently on trees and has connections to other tree invariants such as the Horton-Strahler number. 
4121	4121136	Connected treewidth and connected graph searching	The article presents a constructive proof that shows the equality between treewidth and connected treewidth. This proof is achieved through an algorithm that can be applied to any n-node width-k tree-decomposition of a connected graph G, resulting in a connected tree-decomposition of G with a width of at most k. This proof has practical applications in graph searching problems, where it can be used to find the connected search number of a connected graph G, which is at most logn+1 times larger than its search number. Additionally, the proof is used to design an efficient approximation algorithm for connected search, with a time complexity of O(t(n)+nk3log3/2k+mlog n) for graphs of treewidth at most k.
4122	412259	Exploiting selective placement for low-cost memory protection	Embedded processing applications, such as those used in the automotive and medical fields, require low-cost and reliable hardware designs. Traditionally, reliable memory systems have used coded storage techniques like ECC, which can detect and correct memory faults but come with a high cost overhead. In this article, the authors propose a new partial memory protection scheme that offers high-coverage fault protection for program code and data at a lower cost compared to traditional methods. Their approach involves profiling program code and data usage to identify critical elements, which are then placed into limited protected storage resources. Through fault-injection experiments, the authors confirm that their approach provides high levels of fault protection (99&percnt; coverage) with limited memory protection resources (36&percnt; protected area).
4122	412251	Reliability-aware data placement for partial memory protection in embedded processors	Embedded systems are vulnerable to soft errors, especially in memory elements. To address this issue, a new technique has been proposed for protecting embedded memory systems at a lower cost. This method involves placing program variables in either protected or non-protected memory areas based on a profile-driven liveness analysis. By using this approach instead of traditional methods that offer complete memory protection, a satisfactory level of fault tolerance can be achieved while minimizing area and power overhead. The effectiveness of this technique is evaluated through fault injection experiments using a Monte Carlo simulation framework, which measures the coverage of the partial protection scheme. 
4123	412359	Small-time scaling behavior of Internet backbone traffic	In this study, the authors conduct a wavelet analysis of Internet backbone traffic traces to investigate the causes of small-time scaling phenomena. They find that the second-order scaling exponents at small time scales are mostly close to 0.5, indicating uncorrelated traffic fluctuations. However, some traces exhibit larger exponents, suggesting correlation. The traces also display mostly monofractal behaviors at small time scales. Through analyzing flow composition, the authors identify dense flows as the main factor in causing correlation at small time scales. They also find that the proportion of dense vs. sparse flows greatly influences small-time scalings. These findings have important implications for networking modeling, service provisioning, and traffic engineering.
4123	4123155	Long-term forecasting of internet backbone traffic.	A new methodology has been developed to predict when and where upgrades are needed in an IP backbone network. By analyzing SNMP statistics collected since 1999, the aggregate demand between two adjacent points of presence (PoPs) is calculated and examined at time scales of 1 hour or more. It is found that IP backbone traffic shows long-term trends, periodicities, and variability at various time scales. This approach uses wavelet multiresolution analysis and linear time series models to identify the overall trend and fluctuations in the data. A multiple linear regression model is then used to predict the inter-PoP demand, which accounts for 98% of the total energy and explains 90% of the variance. This model can accurately forecast the trend and fluctuations of traffic for up to 6 months in the future.
4124	412463	Packet audio playout delay adjustment: performance bounds and algorithms	The paper discusses adaptive playout delay adjustment in packet audio applications, where packets are buffered and delayed to compensate for variable network delays. The goal is to minimize the playout delay while avoiding excessive loss due to late packet arrivals. The paper presents efficient algorithms for computing upper and lower bounds on the optimum average playout delay, as well as a new adaptive delay adjustment algorithm that tracks network delays and uses delay percentile information to dynamically adjust playout delay. This algorithm outperforms existing ones and performs close to the theoretical optimum in measured audio delay traces.
4124	412485	Opportunism vs. cooperation: Comparing forwarding strategies in multihop wireless networks with random fading	A variety of forwarding strategies have been created for multi-hop wireless networks to account for the unreliable and time-varying transmission quality caused by the broadcast nature of the wireless medium and random fading. Two popular strategies are opportunistic forwarding, which selects an overhearing relay as a forwarder, and cooperative forwarding, which uses synchronized transmissions to enhance signal strength. However, there has been no comprehensive comparison of their performance in a realistic SINR setting with multiple network flows. This paper introduces Markovian models for these protocols in various network setups and finds that opportunism generally outperforms cooperation. A fixed-point model is also presented for efficiently estimating network throughput, with interference from more transmissions being a hindrance to the potential benefits of cooperative forwarding.
4125	4125389	MSPlayer: Multi-Source and multi-Path LeverAged YoutubER.	The popularity of online video streaming through mobile devices has increased significantly in recent years. YouTube has reported a significant rise in the percentage of its traffic streaming to mobile devices. However, users often face limited bandwidth which affects their streaming experience. The deployment of content delivery networks (CDNs) has helped to improve this by replicating popular videos at different sites, allowing users to stream from nearby locations with low latency. With the availability of multiple wireless interfaces on mobile devices, a new solution called MSPlayer has been proposed. It takes advantage of multiple video sources and network paths to provide high quality and robust video streaming. The solution has been tested on a testbed and through YouTube with positive results.
4125	412548	Optimal multicast smoothing of streaming video over the Internet	Many applications, such as internet video broadcasts and distance learning, require streaming video to be transmitted to multiple users simultaneously over a network. This poses a challenge due to the high bandwidth requirements and variable nature of compressed video. To make such applications more accessible and widespread, scalable techniques are needed to efficiently deliver video to different devices on a diverse network. One proposed solution is to use multicasting and differential caching, which involves smoothing the video and storing it at intermediate nodes in the distribution tree. Algorithms have been developed to compute optimal transmission schedules and buffer allocations for heterogeneous networks. Performance evaluations using MPEG-2 traces have shown significant benefits from this approach, with potential reductions in transmission bandwidth requirements by more than three times compared to traditional multicasting methods.
4126	412687	Truth, justice, and cake cutting.	Cake cutting is often used as a metaphor for dividing a diverse and divisible resource. There have been many studies on how to fairly divide a cake, but only a few have considered the self-interest of individuals and the strategic implications. However, these studies have a limited concept of truthfulness. This paper aims to address the problem of dividing a cake in a way that is both truthful and fair, while also being Pareto-efficient. The authors propose a notion of dominant strategy truthfulness, which is commonly used in social choice and computer science. They present both deterministic and randomized mechanisms for cake cutting that are truthful and fair, depending on the agents' valuation functions. 
4126	412644	Hidden Market Design	The next decade will see a rise in intelligent systems, particularly market-based ones. Users will unknowingly interact with these markets while performing everyday tasks. To ensure their success, a new approach must be taken in designing these systems, specifically by hiding the complexities of the market and creating a seamless user experience. This paper introduces the concept of "Hidden Market Design" and aims to understand the balance between increasing market efficiency and decreasing user complexity. Several examples are provided to illustrate this concept, and it is hoped that this will inspire further research in this direction. Overall, the goal is to create more successful market-based systems in the future.
4127	412764	Key management for restricted multicast using broadcast encryption	This content discusses the problem of securely communicating with a group of users over an insecure broadcast channel, specifically in the contexts of satellite/cable pay TV and the Internet MBone. The main concerns are the number of key transmissions and the number of keys held by each receiver. Previous schemes have limitations, such as high setup costs or the need for receivers to keep a large number of keys. The proposed approach addresses these issues by using a single key structure and allowing a controlled number of non-target users to receive the broadcast. This is achieved through -redundant establishment key allocations, which limit the total number of recipients and prevent unauthorized access. The performance of these schemes is measured by the number of key transmissions, redundancy, and the likelihood of free-riders being able to decrypt the broadcast. New lower bounds and establishment key allocations are presented and evaluated through simulation. 
4127	412722	Topology aggregation for directed graphs	This paper proposes a solution for aggregating the topology of a sub-network in a compact manner while minimizing distortion. This problem arises in hierarchical networks where sub-networks need to advertise routing costs between border nodes. The straightforward solution of advertising exact costs is not practical due to its quadratic cost. The paper addresses the realistic scenario of bidirectional links with potentially different costs in each direction. It presents a solution with distortion bounded by the logarithm of border nodes and the square-root of link cost asymmetry. This is the first theoretical bound for undirected graphs. The paper also suggests heuristics that perform better than the proven solution, and demonstrates its application to PNNI. 
4128	412870	Advanced Classification And Rules-Based Evaluation Of Motion, Visual And Biosignal Data For Patient Fall Incident Detection	This paper discusses the importance of monitoring human physiological data in emergency situations, particularly for elderly individuals living alone. Various techniques have been proposed for identifying distress using motion, audio, and video data. The paper presents an integrated patient fall detection system that combines visual data from overhead cameras with motion and physiological data from the subject's body. Trajectory tracking and acceleration data are utilized to detect falls, while biosignals like ECG and SPO2 can indicate the severity of the incident. The paper also evaluates different classifiers and meta-classifiers for their accuracy in detecting falls, and includes a user-based evaluation. 
4128	412836	Enabling human status awareness in assistive environments based on advanced sound and motion data classification	The paper describes a system that uses sensors to collect and analyze motion and sound data from patients in order to detect and respond to emergencies, such as falls and distress speech. This is achieved through the use of accelerometers and microphones attached to the patient's body, which wirelessly transmit data to a monitoring unit. The system uses Short Time Fourier Transform and spectrogram analysis to detect fall incidents, and classification of the data is done using Support Vector Machines. The results of evaluations show that the system is accurate and effective. It is also noted that the system can be expanded to include other types of patient data for further context.
4129	4129148	ForeGraph: Exploring Large-scale Graph Processing on Multi-FPGA Architecture.	Large-scale graph processing faces challenges such as poor locality, scalability, random access pattern, and data conflicts. FPGAs have characteristics that make them a promising solution for accelerating these applications, such as high throughput for random data access. However, the limited on-chip memory and off-chip bandwidth on a single FPGA chip can hinder large-scale processing. A multi-FPGA architecture can improve this, but data partitioning and communication schemes must be carefully considered to ensure locality and reduce data conflicts. In this paper, the authors propose ForeGraph, a framework for large-scale graph processing using a multi-FPGA architecture. Each FPGA board only stores a partition of the graph, reducing communication. Under their scheduling scheme, each FPGA chip can process the graph in parallel without conflicts. Experimental results show that ForeGraph outperforms previous FPGA-based systems by 4.54x when executing PageRank on a 1.4 billion edge Twitter graph. The average throughput is over 900 MTEPS, which is 2.03x higher than previous work.
4129	4129139	Going Deeper with Embedded FPGA Platform for Convolutional Neural Network.	Convolutional neural networks (CNNs) have become a popular and powerful tool in computer vision, but their high computational and resource requirements make it challenging to implement them on embedded systems like smart phones, smart glasses, and robots. FPGA is a promising platform for accelerating CNNs, but its limited bandwidth and on-chip memory size can limit performance. This paper proposes a CNN accelerator design for ImageNet classification using an embedded FPGA platform. The authors analyze different CNN models and propose a dynamic-precision data quantization method and efficient convolver design to improve bandwidth and resource utilization. Their results show that the proposed methods only introduce a small accuracy loss and outperform previous approaches in terms of performance. They also demonstrate a successful implementation of the largest and most accurate CNN model, VGG16-SVD, on an FPGA platform with high frame rate and accuracy.
4130	413044	Primal Cutting Plane Algorithms Revisited	Dual fractional cutting plane algorithms are commonly used to tighten a linear relaxation of an integer program in order to solve it more efficiently. However, there are also primal cutting plane algorithms that were developed in the 1960s, but have received little attention. These algorithms aim to improve the feasible solution of the original problem through the use of cutting planes. In this paper, the authors propose a new primal algorithm for 0-1 problems, utilizing strong valid inequalities, and provide promising computational results. They also discuss the potential for extending this approach to general mixed-integer programs.
4130	413031	Decorous Lower Bounds for Minimum Linear Arrangement	Minimum linear arrangement is a classic combinatorial optimization problem that has proven to be very difficult to solve in practice. Despite numerous attempts to find the optimal solution, the exact value for most benchmark instances is still unknown. In this paper, the authors introduce a new method using linear programming to calculate lower bounds on the optimum solution. This approach reveals that the current best-known solutions are very close to the optimal solution for most benchmark instances.
4131	413165	On probabilistic models for uncertain sequential pattern mining	This article discusses the use of uncertainty models in sequential pattern mining. It explores situations where there is uncertainty about a source or an event, and proposes the use of probabilistic databases to model these uncertainties. Two notions of frequentness are considered, namely expected support and probabilistic frequentness, and their interestingness criteria are described. The article also examines the complexity of evaluating these criteria, showing that in cases of source-level uncertainty, evaluating probabilistic frequentness is #P-complete and cannot be solved in polynomial time, while the remaining cases can be evaluated in polynomial time. 
4131	413114	Lower bounds for set intersection queries	This article discusses the set intersection reporting problem, which involves maintaining a collection of initially empty sets while processing a sequence of updates (insertions and deletions) and queries (reporting the intersection of two sets). The problem is studied in the arithmetic model of computation, and it is shown that any algorithm must take time ~'/(q + nx/q) to process n updates and q queries, with a tight bound in this model. The article also considers a variant of the problem with limited memory, and a tight bound of O(n2/m 1/3) is shown for a sequence of n operations. The problem can be cast in a framework involving a monoid and associative and commutative operator, and various variations, such as reporting the size of the intersection, can be obtained.
4132	4132125	Multi-user multiple-input single-output downlink transmission systems exploiting statistical channel state information	Multi-user multiple antenna systems are popular due to their high spectral efficiency. However, they require receiver and transmitter channel state information (CSI), especially for the downlink channel. In this study, the authors explore the ergodic sum rate and propose a low complexity adaptive transmission scheme for multiple-input single-output downlink multi-user systems with statistical CSI at the base station (BS). They develop an approximation for the ergodic sum rate and determine the optimal constraint for each user's statistical CSI to maximize this approximation. Based on this, they derive a statistical beamforming space division multiple access transmission scheme and propose new adaptive transmission algorithms. These algorithms only require the BS to have knowledge of the statistical CSI of each user, and are shown to perform well.
4132	413273	Multi-base station cooperation downlink statistical eigenmode transmission	This paper studies cooperative cellular networks with multiple base stations and one mobile station, considering a correlated MIMO channel model and using statistical channel state information at the base stations. The optimal transmission scheme is proposed to maximize the ergodic sum capacity, where the transmit signals of all base stations are independent and the optimal signaling directions are the eigenvectors of the channel's transmit correlation matrix. A tight upper bound for the ergodic sum capacity is derived using matrix permanents. A low-complexity power allocation solution is developed using convex optimization techniques and a simple iterative water-filling algorithm. Simulations show that the cooperative transmission schemes can significantly increase the downlink system sum capacity.
4133	413356	Native Double Precision LINPACK Implementation on a Hybrid Reconfigurable CPU	The use of double precision (DP) arithmetic on embedded CPUs without native support can significantly decrease performance and power efficiency. However, hybrid reconfigurable CPUs, which can change their instruction set at runtime, offer a promising solution for applications requiring unsupported instructions. Experiments on a Stretch S6 platform have shown that limited reconfigurable resources are enough to provide native support for DP arithmetic. By implementing a DP fused multiply-accumulate (FMA) extension instruction, the S6 achieved a peak performance of 200 MFlop/s and a sustained performance of 22.7 MFlop/s at a clock frequency of 100 MHz, outperforming LINPACK's software-emulated DP arithmetic by a factor of 5.7 with higher numerical accuracy. Additionally, the S6 can also implement multiple floating-point operators in parallel for single precision operations.
4133	413335	Peak performance model for a custom precision floating-point dot product on FPGAs	FPGAs have a unique ability to translate reduced resource usage of single operators into increased parallelism. This is particularly useful for floating-point (FP) operators, where reducing the mantissa bit width can lead to significant resource savings. The researchers in this study set out to explore the maximum number of operands that can be used in a parallel dot product architecture on an FPGA, using different custom precision FP number formats. They found that a dot product for input vector sizes 21, 57, and 123 can be implemented for double-, single-, and half-precision, respectively, with peak performances of 1, 3.2, and 9.9 GFlop/s. They also developed a model to predict the maximum peak performance of a dot product based on the precision used, revealing similarities and differences among different generations of FPGAs.
4134	413454	Semi-Supervised Clustering via Matrix Factorization	In recent years, there has been a growing interest in semi-supervised clustering methods, which use supervisory information to group data. This information is typically in the form of constraints that indicate the similarity or dissimilarity between data points. This paper introduces a new approach to semi-supervised clustering using matrix factorization. The algorithm can also be extended to co-cluster data sets of different types with constraints. Experiments on UCI data sets and real world BBS data sets demonstrate the effectiveness of this approach.
4134	413455	Clustering with Local and Global Regularization	Clustering has been a well-studied topic in data mining and machine learning. Traditional methods for clustering can be divided into two categories: local and global. This paper introduces a new clustering method, called Clustering with Local and Global Regularization (CLGR), which combines both local and global information in the data set. The goal is to minimize a cost function that balances the local and global costs. This can be achieved by using eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved using iterative methods. The effectiveness of the proposed method is demonstrated through experiments on various data sets.
4135	4135143	A general graph-based semi-supervised learning with novel class discovery	The paper proposes a general graph-based semi-supervised learning algorithm that not only aims to achieve semi-supervised learning but also discovers latent novel classes in the data. The algorithm uses normalized weights on a data graph to output probabilities of data points belonging to labeled classes or the novel class. It is theoretically interpreted through three viewpoints on the graph: regularization framework, label propagation, and Markov random walks. Experiments on toy examples and benchmark datasets show the effectiveness of the algorithm. 
4135	413532	Soft constraint harmonic energy minimization for transductive learning and its two interpretations	Transductive learning is a technique that uses both labeled and unlabeled data to improve classification performance. It has gained attention from researchers and a novel algorithm has been proposed in this paper. The algorithm is based on the harmonic energy minimization method and works on graphs with soft labels and constraints. By relaxing the label to a real value and softening the constraint for labeled data, the algorithm becomes more tolerant to noise in labeling. The solution derived for the algorithm can be interpreted through label propagation and random walks on graphs, making it more intuitive. The paper also discusses related issues and provides experimental results to demonstrate the effectiveness of the algorithm.
4136	4136111	On the (dis)similarity of transactional memory workloads	The use of transactional memory as an alternative to traditional concurrent programming in multicore systems has posed challenges for computer and software engineers. Previous research has used microbenchmarks or manually converted multithreaded workloads to evaluate transactional memory designs, but there is no consensus on how to describe their runtime characteristics. This has led to an unknown level of redundancy in the workloads used for evaluation. To address this, the authors propose a set of architecture-independent transaction-oriented workload characteristics and use clustering algorithms to analyze a set of transactional memory programs. The results show that using these characteristics can reduce the number of simulations needed and help identify specific feature subsets for evaluation in future transactional memory designs. 
4136	4136223	Exploring GPGPU workloads: Characterization methodology, analysis and microarchitecture evaluation implications	This research focuses on characterizing and analyzing the growing number of GPGPU workloads to better understand their impact on GPU microarchitecture design. The researchers propose a set of microarchitecture agnostic GPGPU workload characteristics and evaluation metrics to accurately evaluate the design space. Through a correlated dimensionality reduction process and clustering analysis, they demonstrate the usefulness of this approach in providing meaningful and thorough simulations for proposed GPU architecture designs. They also explore the diversity of GPU benchmark suites and find that certain workloads show varied characteristics in different workload spaces, such as memory coalescing and branch divergence. This research provides valuable insights for architects to choose the most appropriate workloads to test their intended functional blocks of a GPU microarchitecture.
4137	413719	A Bandwidth-Efficient Coded Cooperative Communications System	The proposed coding scheme aims to increase spectral efficiency and improve diversity in repetitive information transmission. It uses a parallel concatenation of two bit-interleaved coded M-ary modulators (BICM) with scalable repetition of information symbols. This allows for a tradeoff between bandwidth and performance by adjusting the fraction of repeated information and the spectral efficiency of M-ary modulation. Simulation results for different M-PSK and M-QAM systems in block fading scenarios show that this scheme can significantly improve performance for a given bandwidth efficiency. 
4137	413718	Suboptimal Decoding of Vector Quantization Over a CDMA Channel with M-Ask	The article discusses the transmission of vector quantization over a frequency-selective Rayleigh fading code-division multiple access (CDMA) channel. It suggests suboptimal decoding strategies for systems that use M-ary amplitude shift keying (M-ASK) to increase spectral efficiency. The performance loss in achieving better spectral efficiency is analyzed and simulated. The results highlight the trade-off between spectral efficiency and performance in such systems.
4138	413896	Nonsmooth Optimization for Beamforming in Cognitive Multicast Transmission	The optimal beamforming problems for cognitive multicast transmission are typically nonconvex and difficult to solve. The conventional method involves reformulating the problems as convex semi-definite programs with nonconvex and discontinuous rank-one constraints. However, this approach often fails to find satisfactory solutions and requires the use of randomization techniques. In contrast, this paper presents a new approach that formulates the problems as SDPs with continuous reverse convex constraints. A nonsmooth optimization algorithm is then used to find the optimal solution, which has been shown to be almost globally optimal in simulation results. This approach is more efficient and requires less computational load than the conventional method. 
4138	413824	D.C. iterations for SINR maximin multicasting in Cognitive radio	In cognitive multicast transmission, transmit beamforming vectors are designed to optimize the signal-to-interference-plus-noise ratios (SINR) at secondary receivers. This involves maximin optimization of quadratic fractional functions, but there is currently no efficient solver for this problem. However, a new approach presented in this paper shows that the problem can be effectively represented as a canonical d.c. (difference of convex functions) program of the same size. This allows for the use of d.c. iterations to find the optimal solution, which has been shown to offer almost global optimality with a relatively low computational load. Numerical examples demonstrate the effectiveness of this method.
4139	413930	Mobile text entry using three keys	This content describes six techniques for three-key text entry, which use Left- and Right-arrow keys and a Select key to maneuver a cursor over characters. The keystrokes per character (KSPC) range from 10.66 to 4.23. Two techniques, Method #2 and Method #6, were chosen for formal evaluation. Method #2 arranges characters alphabetically while Method #6 uses linguistic enhancement to minimize cursor distance after each entry. Both methods place SPACE on the left and utilize a snap-to-home cursor mode. In an experiment with ten participants, both techniques had entry rates of 9-10 words per minute. The challenges and opportunities for using linguistic knowledge and typamatic keying strategies are also discussed. 
4139	413972	Metrics for text entry research: an evaluation of MSD and KSPC, and a new unified error metric	In this article, the authors discuss two recently introduced statistics for measuring accuracy in text entry evaluations, the minimum string distance error rate and keystrokes per character. They identify shortcomings in these measures and propose a new framework for error analysis that takes into account the presented text, input stream, and transcribed text. The framework includes a unified total error rate, as well as other measures such as error correction efficiency, participant conscientiousness, utilised bandwidth, and wasted bandwidth. The authors also provide a case study to demonstrate the effectiveness of this new methodology. This framework aims to overcome the weaknesses of previous measures and provide a more comprehensive and accurate analysis of text entry accuracy. 
4140	414044	Pointing at 3d target projections with one-eyed and stereo cursors	This study compares the use of stereo and mono-rendered cursors for selecting 2D-projected 3D targets. Two mouse-based and two remote pointing techniques were tested in a 3D Fitts' law pointing experiment, with the first experiment using fixed depth targets. The results showed that one-eyed cursors only improved screen-plane pointing techniques, and target depth did not affect pointing throughput. A second experiment with varying depth targets and only screen-plane pointing techniques found that screen-space projections of Fitts' law parameters resulted in consistent throughput, regardless of target depth. This suggests that in the absence of stereo cue conflicts, projecting Fitts' law parameters onto the screen can accurately predict performance and is not influenced by target depth differences.
4140	414076	Factors Affecting Mouse-Based 3D Selection in Desktop VR Systems	The study presents two experiments on using a mouse cursor in a desktop virtual reality system with stereo display and head-tracking. They also evaluate the use of a one-eyed (mono) cursor to address potential issues with the stereo cursor. The results show that while a one-eyed cursor eliminates depth conflicts, it may offer worse performance than stereo cursors due to eye discomfort. However, in situations without depth conflicts, the one-eyed cursor performs slightly worse than stereo cursors. The study suggests that this difference is not solely due to the transparency of the cursor, and may be caused by eye fatigue or similar factors. 
4141	41418	A Boosting-Based Framework for Self-Similar and Non-linear Internet Traffic Prediction	Internet traffic prediction is crucial for network management and optimization, but the complex nature of network traffic makes accurate prediction challenging. This paper proposes a boosting-based framework for predicting self-similar and non-linear traffic by treating it as a regression problem. The framework uses Ada-Boost and Principle Component Analysis to take advantage of self-similarity while avoiding its drawbacks. A feed-forward neural network is used to capture the non-linear relationship in the traffic. Real network traffic experiments show that this framework is effective in predicting traffic.
4141	414144	Anomaly internet network traffic detection by kernel principle component classifier	Anomaly detection is a critical aspect of computer network security and is gaining increasing attention from both practical and theoretical perspectives. A new anomaly detection method is suggested in this paper, which can identify abnormal network traffic by analyzing its major and minor components. By using the kernel trick, the model can handle the non-linear nature of network traffic. A simplified version is also presented for faster processing, which only considers the major component. The effectiveness of the proposed method is confirmed through experimental results. 
4142	414278	Robust Multi-Network Clustering via Joint Cross-Domain Cluster Alignment	Network clustering is a widely researched problem that has gained significant attention in recent years. While most existing methods focus on clustering nodes within a single network, there are many applications where multiple related networks exist. These networks may be constructed from different domains and have instances that are related to each other. In this paper, a robust algorithm called MCA is proposed for multi-network clustering. MCA considers cross-domain relationships between instances and has several advantages over single network clustering methods. It can detect associations between clusters from different domains and achieves more consistent results by leveraging the duality between clustering individual networks and inferring cross-network cluster alignment. MCA also provides a more robust solution to noise and errors. Extensive experiments on real and synthetic networks show the effectiveness and efficiency of MCA. 
4142	414246	A Local Algorithm for Structure-Preserving Graph Cut	In recent times, large amounts of graph data are being generated in various real-world applications such as social networks, co-authorship networks, and road traffic networks. However, most existing graph mining techniques only focus on the vertices and edges, using a first-order Markov chain as the underlying model. This limits their ability to explore high-order network structures, which are crucial in many domains such as bank customer PII networks and financial transaction networks. To address this, a new algorithm called High-Order Structure-Preserving LOcal Cut (HOSPLOC) is proposed, which can efficiently identify a structure-rich subgraph without exploring the entire graph. This algorithm is based on the concept of high-order conductance and a high-order random walk. Results from experiments on synthetic and real graphs demonstrate the effectiveness and efficiency of HOSPLOC.
4143	414329	Using FCA to suggest refactorings to correct design defects	Design defects are mistakes in the design of software that make it difficult to maintain. Detecting and correcting these defects is important for creating high-quality software. Modern techniques can accurately identify design defects, but correcting them is still a manual process that is prone to errors. To address this issue, formal concept analysis (FCA) has been used as a framework for refactoring, specifically redistributing class members. A new approach is proposed that combines metrics and FCA to remove defects in object-oriented programs. A case study of a specific defect, called the Blob, in the Azureus project is used to demonstrate the effectiveness of this approach. 
4143	414314	Relational concept analysis: mining concept lattices from multi-relational data	Knowledge discovery from data (KDD) is concerned with processing complex data, particularly from relational databases and the Web of Data. To effectively analyze this data, there is a need for Entity-Relationship and rdf compliant data mining tools. This is where relational concept analysis (rca) comes in, which extends formal concept analysis (fca) to handle multi-relational datasets with different types of individuals, attributes, and relationships. Through an iterative process, rca constructs concept lattices for each object type and abstracts the relationships between objects into attributes similar to those used in description logics. The article discusses important aspects of rca, such as how the data description evolves during iterations and when the process terminates. Implementations of rca are described and its applications to software and knowledge engineering problems are listed.
4144	41440	Dealing with Disappearance of an Actor Set in Social Networks	Social networks are complex structures made up of entities and connections. When a node or group of nodes disappears from the network, it can disrupt the flow of information and cause the network to disconnect. The aim of this paper is to build upon previous research on managing node disappearances by addressing the disappearance of a group of nodes. The proposed approach focuses on the role of the group in maintaining network connectivity and restoring information flow. The group is categorized into three classes based on its relationship to the network, and the approach involves adding new links and finding substitutes to maintain the network's quality. Unlike other methods, this approach considers information flow as a key factor in identifying potential links and substitutes. The solution is validated through experiments and shows improvements in response time and the number of added links.
4144	414430	Predicting a Social Network Structure Once a Node Is Deleted	This article discusses the dynamic nature of social networks and proposes a method for predicting changes in the network when an entity disappears. The focus is on the role of individual nodes within the network, which is determined by the number of interactions they have with other nodes. The two roles considered are the leader, with a high degree centrality, and the mediator, with a high betweenness centrality. By understanding the importance of these roles, the proposed method aims to accurately predict the new structure of a social network after an entity has disappeared. 
4145	414521	An efficient implementation for the 0-1 multi-objective Knapsack problem	This paper introduces a new approach for solving 0-1 multi-objective knapsack problems using dynamic programming. The approach utilizes dominance relations to eliminate partial solutions that cannot lead to new nondominated criterion vectors. This results in a more efficient method compared to existing methods, as shown through numerical experiments on different types of instances. The approach also outperforms other exact methods and has been tested for the first time in the three-objective case. 
4145	414562	A practical efficient fptas for the 0-1 multi-objective knapsack problem	The authors of this work introduce a new approach to solving the approximation version of the 0-1 multiobjective knapsack problem. Their methodology is based on general techniques, making it applicable to other problems. Through extensive numerical experiments, they demonstrate the effectiveness of their method in terms of CPU time and size of solved instances. They also provide insights into the reasons for its success, and compare it to an exact method. This research contributes to the practical application of fptas in solving optimization problems.
4146	41463	Hereditary Domination in Graphs: Characterization with Forbidden Induced Subgraphs	The leaf graph of a connected graph is created by adding a new vertex with degree one to each noncutting vertex. It has been proven that if a connected graph is not dominated by any of its induced paths, then it is dominated by a connected induced subgraph whose leaf graph is also an induced subgraph of the original graph. This means that the minimal graphs not dominated by any induced subgraph isomorphic to a certain class of graphs will be cycles or leaf graphs of graphs not in that class. If the class of graphs is closed under the operation of taking connected induced subgraphs, then the hereditarily dominated graphs can be identified by the following forbidden induced subgraphs: leaf graphs of connected graphs that are not in the class, but all of their connected induced subgraphs are, and the cycle of length equal to the shortest path not in the class. This solves a problem that has been open since the 1980s. A different method has been used by Bacsó to solve the case of induced-hereditary classes.
4146	414620	Strong branchwidth and local transversals	Chordal graphs can be represented as edge intersection graphs of subtrees in a tree with a maximum vertex degree of 3. A new graph invariant, related to branchwidth, is introduced and a theorem is presented that characterizes the existence of representations where each edge is in fewer subtrees than the clique number of the graph. This leads to a sufficient condition for chordal graphs to have a branchwidth smaller than their clique number.
4147	414748	Approximation complexity of min-max (regret) versions of shortest path, spanning tree, and knapsack	This paper explores the approximation of min-max (regret) versions of common problems such as shortest path, minimum spanning tree, and knapsack. The authors establish fully polynomial-time approximation schemes for these problems when the number of scenarios is limited, using relationships between multi-objective and min-max optimization. They also propose a scheme for min-max regret shortest path using dynamic programming and trimming techniques. However, they prove that min-max regret knapsack is not possible to approximate. For an unbounded number of scenarios, the problems become strongly NP-hard and the authors provide non-approximability results for min-max (regret) versions of shortest path and spanning tree. 
4147	414723	Approximating min-max (regret) versions of some polynomial problems	The article discusses the lack of research on the approximation of min-max and min-max regret versions of combinatorial optimization problems. However, a general approximation scheme is proposed for a limited number of scenarios, which can be applied to certain polynomial problems such as shortest path and minimum spanning tree. This results in improved running times compared to previous schemes presented in literature. Overall, the article highlights the need for further investigation into the approximation of these versions of combinatorial optimization problems.
4148	414823	A size-sensitive inequality for cross-intersecting families.	The content discusses the concept of cross-intersecting families A and B, where all subsets of A and B have at least one common element. Pyber's theorem states that for n2k, the cardinality of A and B is at most (n1k12). The present paper strengthens this theorem by proving that for n1k1+niki+1, the cardinality of A and B is at most (n1k1+niki+1)(n1k1nik1). This inequality is optimal. Additionally, the paper presents a shorter proof of Pyber's theorem and another proof of an inequality by Frankl and Tokushige (1992) without the need for computation.
4148	414827	A note on universal and canonically coloured sequences	Kleitman and Kwiatkowski found that the minimum length of a t-universal sequence, containing all permutations of an alphabet with t symbols, is (1 − o(1))t2. A related problem is addressed, where an r-colouring of the sequence X is canonical if the same symbols have the same colour. It is proven that for a fixed t, the shortest sequence over an alphabet of size t, where every r-colouring contains a t-universal and canonically coloured subsequence, has a length of at most $cr^{\lfloor\frac{t}{2}\rfloor}$, with a constant c independent of r. This is the best possible result up to a multiplicative constant. 
4149	41496	Constructions of almost difference families	This paper discusses almost difference families (ADFs), which are a generalized version of almost difference sets (ADSs). The paper presents various methods for constructing ADFs and identifies several infinite classes of ADFs. These results expand upon previously known difference families and demonstrate the usefulness of ADFs in creating partially balanced incomplete block designs. These designs have practical applications in combinatorial and statistical problems. 
4149	414937	Linear Codes From Some 2-Designs	The paper discusses a new method of constructing linear codes over GF(q) using specific types of 2-designs, such as almost difference sets and semibent functions. This approach has potential applications in secret sharing and authentication schemes, as well as in consumer electronics, communication, and data storage systems. The codes obtained from this method have a few weights and have been proven to be optimal in two families. Additionally, the paper also presents a coding-theory approach to characterizing highly nonlinear Boolean functions. This research builds upon previous work on using incidence matrices of t-designs as generator matrices for linear codes.
4150	415012	Adaptive Mctf Based On Correlation Noise Model For Snr Scalable Video Coding	This paper presents a new method for scalable video coding called subband adaptive motion compensated temporal filtering (MCTF). It also introduces a revised model for quantization in this technique. Hierarchical MCTF is commonly used in scalable video coding to utilize temporal correlation between frames. However, the strength of this correlation varies with the level of temporal transform and different spatial frequencies in a frame. The proposed method adjusts the strength of motion compensation based on the correlation and noise characteristics of subbands, while also adjusting the quantization step according to the MCTF structure. This adaptive approach improves the coding performance of scalable video coding by maximizing the use of temporal correlation while limiting the propagation of reconstruction noise.
4150	415068	Compressive Sensing Based Soft Video Broadcast Using Spatial and Temporal Sparsity.	Video broadcasting over wireless networks has become increasingly popular, but the traditional digital framework can struggle with diverse channel conditions, known as the "cliff effect". To address this, a new scheme has been proposed that utilizes nonlocal sparsity and hierarchical group of pictures (GOP) structure for compressive sensing-based soft video broadcasting. This approach minimizes bandwidth consumption and generates equally important measurements, making it ideal for video broadcasting. At the encoder side, block-wise compressive sensing is used to generate measurement data, which is then sent over an OFDM channel. The hierarchical GOP structure and nonlocal sparsity are utilized at the decoder side to improve decoding quality. Experimental results show that this scheme outperforms traditional methods with up to 8 dB coding gain in certain channel conditions.
4151	415158	Directional filtering transform for image/intra-frame compression.	Traditional transforms have been adapted to have a directional component, resulting in different outcomes depending on the order of the transformations. This paper examines the effects of transform orders on coding gain in an anisotropic image model. It is found that the transform orders have minimal impact on coding gain when fully decomposed, but in practical compression schemes where high-pass bands are not fully decomposed, different orders can affect coding performance. To address this, an adaptive transform order is proposed, which evenly distributes prediction modes and takes into account interblock and intrablock correlations. Experimental results in H.264 intraframe coding show the effectiveness of this approach.
4151	415119	Highly parallel image coding for many cores	The paper introduces a pure line-by-line image coding scheme (LBC) for many-core computers that can compress one HD image using tens or hundreds of cores simultaneously. LBC codes the input image line by line, dividing each line into short segments that are independently coded. Experimental results demonstrate that LBC can achieve encoding speeds up to 14 times faster than parallel H.264 intra-frame coding using 16 cores. The proposed scheme also utilizes efficient intra predictions to take advantage of correlations between neighboring lines, resulting in comparable or better coding performance than H.264 high profile at middle bit rates and up to 10dB better performance at high bit rates.
4152	415224	Swift: A Hybrid Digital-Analog Scheme for Low-Delay Transmission of Mobile Stereo Video	Efficient and robust wireless stereo video delivery is crucial for mobile 3D applications. Existing solutions either have high coding efficiency but are not robust to channel variations, or have the opposite characteristics. In this paper, a hybrid digital-analog (HDA) solution is proposed to combine the advantages of both approaches. One frame in each stereo pair is digitally encoded for basic quality, while the other is analogly processed to take advantage of good channels. A zigzag coding structure is designed to explore intra-view and inter-view correlations, and a reference selection mechanism is used to improve coding efficiency. The system also addresses optimal power and bandwidth allocation between digital and analog streams. A trace-driven evaluation on a software-defined radio platform shows that the proposed system, named Swift, outperforms an omniscient digital scheme or can achieve comparable performance with 2x power savings. Subjective quality assessments also demonstrate that Swift provides better visual quality than a straightforward HDA extension of SoftCast.
4152	4152147	Robust Linear Video Transmission Over Rayleigh Fading Channel	This research focuses on improving the quality of video transmission over a channel with statistical channel state information. The authors suggest discarding low-priority data to prioritize high-priority data and minimize errors. They propose an optimization problem to allocate power and bandwidth resources, and develop an iterative algorithm to approximate the solution. A simpler one-pass two-step fast algorithm is also proposed. Simulations show that their system outperforms existing methods, with a PSNR gain between 4.0 dB and 7.5 dB.
4153	41538	Channel-adaptive resource allocation for scalable video transmission over 3G wireless network	The paper discusses the challenges of transmitting scalable video over 3G wireless networks and proposes resource allocation strategies to optimize video quality and minimize power consumption. The proposed schemes take into account the varying wireless channel and network conditions, and use a combination of unequal error protection and automatic repeat request to adapt to these conditions. The authors also propose a power-minimized bit allocation scheme for mobile devices. Simulation results using a progressive fine granularity scalability video codec demonstrate the effectiveness of the proposed schemes in improving video quality under different network conditions. 
4153	415368	Resource allocation with adaptive QoS for multimedia transmission over W-CDMA channels	.This paper discusses the challenges of resource allocation and rate adaptation for multiple types of media transmitted over a W-CDMA channel with adaptive quality of service (QoS) support. To address this, the authors propose an architecture that combines link layer and application layer controls. This includes a model to estimate the fading channel, a hybrid delay-constrained automatic repeat request (ARQ) and unequal error protection (UEP) mechanism, and a resource allocation scheme that considers varying media characteristics and changing bit error rate (BER) conditions. Simulation results show the success of this approach.
4154	415421	Log-domain wave filters	The article presents a method for designing log-domain wave filters, which are used to simulate passive LC ladder prototype filters with low sensitivity. The technique involves transposing the signal flow graph of the linear domain to the log-domain using complementary operators to maintain linear operation. The effectiveness of this approach is demonstrated through simulations of a fifth-order low-pass and fourth-order bandpass log-domain wave filter using HSPICE. These filters are suitable for low-voltage and high-frequency applications.
4154	415411	Modular log-domain filters realized using wave port terminators	This paper introduces log-domain wave filters that mimic passive LC ladder prototype filters. These filters are created using wave equivalents of the reactive elements found in the prototype circuit. A new log-domain wave port terminator is used to obtain the wave equivalent circuits. The wave equivalent of a capacitor is used as a building block for creating high-order filters. Other reactive elements can be easily obtained by using the building block and inverters. This allows for modular high-order filter configurations. An example of a third-order elliptic low-pass filter is presented, and its performance is validated through simulation.
4155	415524	Model checking system software with CMC	Complex systems often have errors that are difficult to detect, especially when they involve specific situations and complicated sequences of events. Traditional testing methods are not always effective in finding these errors. In recent years, formal verification techniques have become more popular as they can check for errors in all possible system behaviors. However, these techniques require creating an abstract model of the system, which can be unreliable and miss implementation errors. CMC is a framework that directly runs software code without the need for an abstract model, making it more reliable and effective. When used to model check the AODV routing protocol and the IP Fragmentation module in the Linux TCP/IPv4 stack, CMC was able to find numerous bugs and verify correctness.
4155	41559	Using model checking to find serious file system errors	This article discusses the use of model checking to detect critical errors in file systems. Model checking is a formal verification technique that thoroughly explores a system's state space to uncover corner-case errors. File systems are particularly suitable for this approach due to the severity of their errors, which can result in data loss and corruption. Traditional testing methods are not practical for file systems as they require an exponential number of test cases. The article introduces FiSC, a system for model checking file systems, and its successful application to popular file systems, including ext3, JFS, ReiserFS, and XFS. A total of 33 serious bugs were found and quickly addressed with patches. These bugs were found to cause the destruction of metadata and entire directories, including the root directory.
4156	415649	On-the-fly auditing of business processes	Information systems that support business processes can be complex, making it difficult to ensure that business rules are being followed. To simplify this task, a separate system called a monitor can be designed. The monitor collects events from the business processes and checks if the rules are being followed. A business rule language (BRL) is needed to verify the rules over finite histories. This BRL can express many common business rules and has two important properties: future stability and past stability. To handle the increasing history, the monitor uses abstractions of the history and a labeled transition system. This system can be transformed into a colored Petri net for easier verification. 
4156	415627	A simple but formal semantics for XML manipulation languages	XML is a commonly used format for storing documents on the web. As documents can be seen as semi-structured databases, it is important to be able to query and transform them. XPath, XQuery, and XSLT are three manipulation languages specifically designed for XML. Each of these languages has a sublanguage called MiXPath, MiXQuery, and MiXSLT, respectively, which have a simpler syntax and semantics. These languages have different expressive powers and can be used for various purposes such as query optimization and studying the relationship between XML and generic database queries. MiXPath, MiXQuery, and MiXSLT are useful for both educational and research purposes, and their properties can be applied to XPath, XQuery, and XSLT.
4157	415775	A Measurement-Based Simulation Study of Processor Co-allocation in Multicluster Systems	The article discusses the challenges and potential benefits of using multiple clusters in systems such as the Distributed ASCI Supercomputer. While the availability of processors in multiple clusters can be advantageous for applications, the slow network connections between clusters can lead to degraded performance for single-application multicluster execution. Scheduling policies for these systems also need to consider the additional restrictions of fitting each component of a job into separate clusters. The paper presents a measurement study of the runtime and communication time for two applications on single clusters and multicluster systems, as well as simulations of different multicluster scheduling policies. It concludes that restricted forms of co-allocation in multiclusters can improve performance compared to not allowing co-allocation at all.
4157	415744	Performance analysis of dynamic workflow scheduling in multicluster grids	This paper examines the performance of different dynamic workflow scheduling policies in multicluster grids. These grids are used by scientists to run complex mixtures of applications, but the dynamic nature of grid workflow scheduling can lead to poor or unpredictable performance. The paper introduces a taxonomy of scheduling policies based on the amount of dynamic information used, and evaluates seven policies through simulations and experiments on a real multicluster grid. The results show that there is no single policy that performs well in all scenarios, and that limitations of grid cluster head-nodes can affect performance. The authors suggest using task throttling to prevent head-node overload while maintaining good performance for communication-intensive workflows.
4158	415876	GRENCHMARK: A Framework for Analyzing, Testing, and Comparing Grids	Grid computing is becoming increasingly popular for aggregating and sharing diverse resources. However, in order for grid development to be successful, it is important to demonstrate that grids can reliably support real applications and to create benchmarks to measure this support. Traditional benchmarks are not always suitable for grid environments, so a middle-ground approach is needed. This is where GRENCHMARK comes in - a framework for generating and submitting synthetic workloads that accurately represent the applications used in modern grids. With over 35 synthetic and real applications, GRENCHMARK is versatile and can be used for grid system analysis, functionality testing, and comparison of different grid settings. The results obtained with GRENCHMARK in the DAS multi-cluster grid show its effectiveness.
4158	41585	DGSim: Comparing Grid Resource Management Architectures through Trace-Based Simulation	The integration of a global computing infrastructure for scientific purposes, known as grid computing, still requires significant advancements in grid resource management. This pressure for progress is heightened by the rapid development of single, large clusters which are seen as an alternative to grids. However, these advancements cannot be achieved without the use of appropriate tools, such as simulation environments. Current grid simulation environments lack important features in workload and system modeling, as well as research productivity features. To address these issues, the authors have designed and implemented DGSim, a framework for simulating grid resource management architectures. DGSim introduces concepts such as grid evolution and job selection policy, and improves the understanding of grid inter-operation, dynamics, and workload modeling. Two real-life examples demonstrate how DGSim can be used to compare different grid resource management architectures.
4159	41594	A Graph-based model for context-aware recommendation using implicit feedback data	Recommender systems have been successful in dealing with information overload, but they are mostly designed for scenarios where explicit feedback is available. This can be limiting in situations where only implicit feedback is present. Additionally, most existing methods only consider user and item dimensions, ignoring other important contextual information such as time and location. To address these limitations, the authors propose a graph-based recommendation framework that incorporates contextual information into the recommendation process. This framework, called Multi-Layer Context Graph (MLCG), models the interactions between users and items and includes a variety of contextual information. Two novel ranking methods, Context-aware Personalized Random Walk (CPRW) and Semantic Path-based Random Walk (SPRW), are developed based on MLCG. The authors demonstrate the effectiveness of their approach through experiments on two real-world datasets. 
4159	41597	Personalized Recommendation On Multi-Layer Context Graph	Recommender systems have been successful in addressing information overload by providing personalized recommendations to users. However, most existing approaches only consider user and item dimensions and ignore other contextual information like time and location. To address this limitation, a Multi-Layer Context Graph (MLCG) model is proposed in this paper. It incorporates various contextual factors and captures the interactions between users and items to improve the recommendation process. Additionally, a new ranking algorithm based on Personalized PageRank is introduced for MLCG, which takes into account users' preferences and current situations. Experiments on two real-world datasets show the effectiveness of this approach. 
4160	416021	Performance evaluation of a DySER FPGA prototype system spanning the compiler, microarchitecture, and hardware implementation	Specialization and accelerators are being suggested as a solution to the slowdown of Dennard scaling. One such accelerator, called DySER, uses a co-designed compiler and microarchitecture to dynamically synthesize large functional units that match program regions. This paper presents a full prototype implementation of DySER integrated into the OpenSPARC processor, a co-designed compiler in LLVM, and a performance evaluation on an FPGA system running Ubuntu Linux and full applications. The results show that DySER provides energy efficient speedups with a 6X performance improvement and minimal overhead. The DySER compiler is effective at extracting computationally intensive regular and irregular code, but struggles with non-computationally intense irregular code. The authors also highlight the need for open-source baseline processors and declarative tools for designing ISA-exposed accelerators more efficiently.
4160	416013	Dynamically Specialized Datapaths for energy efficient computing	The efficiency of logic devices is decreasing in successive generations due to technology limitations. To continue improving performance without increasing power, we propose using Dynamically Specialized Datapaths in general purpose programmable processors. This involves identifying phases in application execution and creating specialized datapaths for these phases using a circuit-switched network. A 64-functional-unit DySER block can be integrated with a processor pipeline and achieve similar performance as a specialized hardware module for each phase. When integrated with a dual-issue out-of-order processor, two DySER blocks can provide a 2.1X speedup and 40% energy reduction. This approach can cover 12% to 100% of the dynamically executed instruction stream and achieve a 60% energy reduction without sacrificing performance. 
4161	416127	A Cross-Domain Recommendation Model for Cyber-Physical Systems	Cyber-physical systems (CPS) are intelligent systems that interact with other systems through information and physical interfaces. With an increased reliance on CPS, there has been a significant collection of human-centric data, leading to information overload in various domains. Recommender systems in CPS, which provide information recommendations based on historical ratings from a single domain, face the challenge of data sparsity. To address this issue, researchers have proposed recommendation models that transfer knowledge across multiple domains, assuming a common rating pattern. However, in real-world scenarios, domains may not necessarily share the same rating pattern, and the diversity among domains may outweigh the benefits of a common pattern, resulting in decreased performance. To overcome this, a new cross-domain recommendation model is proposed in this paper, which learns both a common rating pattern and domain-specific patterns to improve performance. Experiment results on real-world datasets demonstrate the superiority of this model compared to existing methods for cross-domain recommendation in CPS.
4161	416149	Zero-Shot Embedding For Unseen Entities In Knowledge Graph	Knowledge graph (KG) embedding is a technique used to learn the hidden meanings of entities and relationships in a knowledge graph. However, current methods are limited to completing existing KGs and cannot identify relationships involving new or "Out-of-KG" entities. To address this, a new model called JointE is proposed, which combines KG and entity description embedding to add new relations with Out-of-KG entities. The effectiveness of JointE is evaluated through zero-shot learning for entity prediction, and it outperforms other state-of-the-art approaches on benchmark datasets. The source code for JointE is publicly available for further use.
4162	416247	The Use of Lexical Context in Question Answering for Spanish	This paper discusses a prototype created by the Language Technologies Laboratory at INAOE for evaluating Spanish monolingual question-answering (QA) at CLEF 2004. The prototype uses context and lexicons to identify potential answers to factoid questions, and employs pattern recognition for definition questions. The paper describes the methods used at various stages of the system and the prototype's architecture. The results of this approach are also presented and analyzed.
4162	416224	Question answering for spanish supported by lexical context annotation	The Language Technologies Laboratory at INAOE created a prototype for Spanish monolingual question-answering (QA) evaluation at CLEF 2004. Their approach uses context and pattern recognition to identify potential answers to factoid and definition questions. The paper details the methods used and the prototype's architecture, as well as the results achieved.
4163	4163114	NLEL-MAAT at CLEF-ResPubliQA.	The report discusses the participation of NLE Lab in the QA@CLEF-2009 competition. The lab used the JIRS passage retrieval system, which is focused on redundancy, to find answers to questions in a large document collection. The passages were ranked based on the number, length, and position of question n-grams. The system performed well in monolingual English, but had lower results in French. This is attributed to the varying styles of questions in different languages. Overall, the report highlights the effectiveness of the JIRS system in retrieving relevant passages for question answering tasks.
4163	4163133	NLEL-MAAT at ResPubliQA	The report describes the work done by NLE Lab for the QA@CLEF-2009 competition. They utilized the JIRS passage retrieval system, which relies on redundancy to find the answer to a question in a large document collection. The retrieved passages were then ranked based on the number, length, and position of the question n-gram structures found in them. The team achieved the best results in monolingual English but struggled in French, possibly due to variations in the question style between languages. 
4164	416415	The Use of Lexical Context in Question Answering for Spanish	The Language Technologies Laboratory at INAOE created a prototype for evaluating Spanish monolingual question-answering (QA) at the CLEF 2004 competition. Their approach focuses on using context at a lexical level to find potential answers to factoid questions, and also utilizes pattern recognition to identify potential answers to definition questions. This paper details the methods used at various stages of the system and the prototype's architecture for QA. The results of this approach are presented and analyzed.
4164	416455	The role of lexical features in question answering for spanish	This paper presents a prototype developed by the Language Technologies Laboratory at INAOE for the Spanish monolingual QA evaluation at CLEF 2005. The system uses a methodology that adapts to the type of question being asked (factoid or definition). For factoid questions, the system relies on lexical features to identify potential answers, while for definition questions, a pattern recognition method is used. The paper discusses the various methods used in the system, with a focus on those for factoid questions, and presents the results of the evaluation.
4165	4165121	A Layered Lattice Coding Scheme for a Class of Three User Gaussian Interference Channels	The paper focuses on a specific type of communication system called three user Gaussian interference channels. It proposes a new method called layered lattice coding, which uses lattice codes to align interference at each receiver. This approach is found to have multiple degrees of freedom and achieve higher rates compared to the traditional Han-Kobayashi coding scheme. The paper suggests that layered lattice coding can improve the performance of communication systems using Gaussian interference channels.
4165	416542	Sum capacity of K user Gaussian degraded interference channels	This paper presents a new family of genie-MAC outer bounds for K-user Gaussian interference channels. These bounds are inspired by existing bounding mechanisms but differ in their formulation and application. The idea is to create multiple access channels with genie receivers that can decode a subset of the interference channel's messages. The sum capacity of each genie receiver serves as an outer bound for the subset's sum of rates. These bounds are then used to derive closed-form sum-capacity results for the class of K-user Gaussian degraded interference channels. It is shown that the sum-capacity can be achieved using a successive interference cancellation scheme, which generalizes a known result for two-user channels to K-user channels. 
4166	416610	Using the Representation in a Neural Network''s Hidden Layer for Task-Specific Focus of Attention	In this paper, the concept of focusing attention on important features in real-world tasks is discussed. A mechanism called saliency map is proposed, which uses a computed expectation of input contents to identify important regions in the input. This map can then be used to emphasize crucial features and downplay less relevant ones. The effectiveness of this method is demonstrated in a robotics task and its applicability is also shown in a non-visual domain. The paper provides details about the architecture and algorithm used, as well as empirical results. Overall, the saliency map is shown to be a useful tool for achieving task-specific focus of attention. 
4166	416655	Using a Saliency Map for Active Spatial Selective Attention: Implementation & Initial Results	The ability to focus attention on important parts of a scene is crucial for good performance in vision-based tasks. This paper introduces a simple method of achieving spatial selective attention through the use of a saliency map, which indicates which regions of the input retina are important for the task. The saliency map is created through predictive autoencoding and has been demonstrated to improve performance on tasks with distracting features in the input. The motivation for this work is the task of vision-based road following, where extraneous features can interfere with task performance. This method can be extended and applied to other tasks, but for the specific task of road following, it has shown significant success in reducing distractions.
4167	416773	Proofs as computations in linear logic	Uniform proof and resolution are fundamental concepts in the proof-theoretic characterization of logic programming. The class of Abstract Logic Programming Languages effectively captures these ideas for a wide range of logical systems. In logic programming, the structure of formulas, such as Horn clauses and hereditary Harrop formulas, is crucial in differentiating between programming and theorem proving. This paper presents an extension of hereditary Harrop formulas and a corresponding logical system, which serve as the foundations for a logic programming language. The study is based on Forum, a presentation of higher-order linear logic using uniform proofs. A subset of its formulas has been identified as suitable for representing various programming paradigms.
4167	416772	Automated protocol verification in linear logic	In this paper, the authors explore the use of a bottom-up evaluation strategy for validating authentication protocols in a first order fragment of linear logic. They use multi-conclusion clauses to represent agent behavior and adopt the Dolev-Yao intruder model and related assumptions. Their approach also utilizes universal quantification for expressing nonces. This method is effective in verifying properties with minimality conditions and can handle infinite-state systems without limitations on parallel runs. The approach can also be used to detect attacks and prove protocol correctness. The authors present preliminary experiments using this approach on the ffgg protocol by Millen, which is challenging due to parallel attacks.
4168	416846	Recovery Of Correlated Sparse Signals Using Adaptive Backtracking Matching Pursuit	Distributed compressive sensing is a technique used to reconstruct multiple signals from a small number of measurements. By using a modified Compressive Sensing (CS) algorithm called Modified Basis Pursuit (Mod-BP), which utilizes Partially Known Support (PKS), the remaining signals in a joint-sparse signal ensemble can be reconstructed if one signal is known apriori. However, Mod-BP is slow in converging, making it unsuitable for practical applications such as video frame reconstruction. To improve the recovery performance at a shorter time, Carrillo et al proposed using PKS in iterative greedy algorithms. However, these algorithms are blind to incorrect atoms present in the PKS, which is common in video frames. To address this, the authors introduce Adaptive Backtracking Matching Pursuit (AdBMP), which effectively utilizes the PKS to reconstruct sparse signals. Experimental results show that AdBMP outperforms existing PKS based iterative greedy algorithms in terms of reconstruction accuracy.
4168	416812	Replacing K-Svd With Sgk: Dictionary Training For Sparse Representation Of Images	Sparse representation using a trained dictionary has advantages over standard parametric bases, and a new algorithm called SGK has been proposed as an alternative to the popular K-SVD algorithm. SGK has been shown to have faster execution speed and better performance in constrained sparse coding through synthetic data experiments. However, its effectiveness on real data has not been demonstrated. This article compares SGK and K-SVD by training dictionaries on a face image database and using them for image recovery tasks such as inpainting and denoising. A technique to prevent noise from affecting the dictionary is also introduced.
4169	416939	Passive Source Localization Using Time Differences of Arrival and Gain Ratios of Arrival	This paper investigates the use of gain ratios of arrival (GROAs) and time differences of arrival (TDOAs) to improve the accuracy of source localization. The GROAs are defined as the ratio of the received signal amplitudes at a reference sensor to other sensors, and are dependent on the distance between the source and receivers. The paper utilizes a Gaussian random signal model to derive the Cramer-Rao lower bound (CRLB) for source location estimation using GROAs and TDOAs. The results show that the improvement from GROAs increases with the factor c/omegao, where c is the signal propagation speed and omegao is the signal bandwidth. An algebraic closed-form solution for source location using GROAs and TDOAs is also developed and shown to reach the CRLB accuracy under the Gaussian data model. Numerical simulations support the theoretical findings.
4169	416925	Adaptive time-delay estimation in nonstationary signal and/or noise power environments	A model is proposed to improve the performance of estimating the arrival time difference of a bandlimited random signal received by two spatially separated sensors in a changing environment. The model includes two adaptive units: a filter to compensate for time shift between the two receiver channels and a gain control for Wiener filtering. The filter coefficients and gain are simultaneously adjusted using modifications from the stochastic mean-square-error gradient in the traditional adaptive least-mean-square time-delay estimation (LMSTDE) method. The proposed system is compared with the traditional technique and theoretical results show that it can better handle changing signal and noise power, resulting in improved convergence behavior of the delay estimate. Simulation results are provided to support the effectiveness of the model.
4170	417065	Depth map enhancement using adaptive steering Kernel regression based on distance transform	This paper introduces a new method for improving noisy depth maps using adaptive steering kernel regression based on distance transform. This approach considers spatial and photometric properties of pixel data to filter out noise more effectively. By refining the steering kernel according to local region structures, flat and textured areas, the method is able to reduce noise in depth maps. The process involves creating distance transform maps from the depth map and its corresponding color image, and modifying the steering kernel using a newly-designed weighing function. This function expands the kernel in flat areas and shrinks it in textured areas towards local edges in the depth map. Experimental results show that this method performs better than other techniques for enhancing depth maps.
4170	417050	An Adaptive Focal Connectivity Algorithm for Multifocus Fusion	Multifocus fusion is a process that combines focal information from multiple input images to create one all-in-focus image. A new multifocus fusion algorithm has been developed that can be used for any type of fusion. Focally connected regions, which are areas within an image that are in focus, are identified and fused together using a partition synthesis method. The resulting image contains information from all focal planes and accurately represents the scene. The effectiveness of the algorithm has been compared to other fusion techniques, and it has been found to produce sharper and more seamless results. Multiple examples are provided to demonstrate the success of this multifocus fusion method.
4171	417169	Camera handoff with adaptive resource management for multi-camera multi-object tracking	Camera handoff is a critical aspect of multi-camera surveillance systems, as it enables the continuous tracking and consistent labeling of objects of interest. While existing algorithms focus on data association, there are still many unanswered questions in developing an efficient camera handoff method. This paper proposes a trackability measure that evaluates tracking effectiveness, allowing for timely and optimal camera handoff selection. The measure considers resolution, distance to the edge of the camera's field of view, and occlusion. The algorithm also addresses the decrease in frame rate in real-time tracking systems by using an adaptive resource management mechanism to allocate resources to multiple objects with different priorities. Experimental results show a 20% increase in overall tracking rate compared to previous methods.
4171	417111	Optical flow-based real-time object tracking using non-prior training active feature model	The paper introduces a feature-based object tracking algorithm that uses optical flow and the non-prior training (NPT) active feature model (AFM) framework. The algorithm has three steps: localization of the object, prediction and correction of its position using spatio-temporal information, and restoration of occlusion using NPT-AFM. It is able to track both rigid and deformable objects and is robust against sudden motion and complicated backgrounds. The AFM allows for stable tracking of occluded objects and removes the need for an offline preprocessing step. The algorithm is also able to track partially occluded objects with fewer feature points, making it suitable for real-time tracking. Experiments on various video clips and datasets show the effectiveness of the proposed algorithm.
4172	417270	A Polynomial Approach To Nonlinear State Feedback Stabilization Of Saturated Linear Systems	This paper discusses the problem of stabilizing saturated linear systems using nonlinear control laws. The proposed approach uses a rational formulation of the control action, with a parameter sigma that is determined by solving an implicit equation based on the system state. By utilizing a sum-of-squares formulation and imposing a polynomial dependence on sigma, constructive solutions for local exponential and global asymptotic stabilization are presented. This approach offers a more efficient and effective method for controlling these systems compared to previous approaches.
4172	417242	Safety verification for distributed parameter systems using barrier functionals.	The safety verification problem for distributed parameter systems described by partial differential equations (PDEs) involves checking if the solutions of the PDEs meet certain constraints at a specific time. A new approach using barrier functionals is proposed, which are functionals of the dependent and independent variables. By satisfying two integral inequalities, the barrier functional ensures that the system's solutions do not enter an unsafe set. This method does not require finite-dimensional approximations, making it suitable for PDEs. Additionally, for PDEs with polynomial data, the associated integral inequalities are solved using semi-definite programming (SDP) and a quadratic representation method. Examples are provided to demonstrate the effectiveness of this approach.
4173	417366	An Augmented Reality System for Epidural Anesthesia (AREA): Prepuncture Identification of Vertebrae.	Our proposed system uses augmented reality technology to assist in spinal needle insertion for epidural anesthesia. It relies on a trinocular camera to track an ultrasound transducer and generate a panoramic image of the lumbar spine. This image is then used to automatically identify the lumbar levels and overlay them onto a live camera view of the patient's back. We conducted validation tests to ensure the accuracy of panorama generation, lumbar level identification, overall system accuracy, and its ability to handle changes in the spine's curvature. Results from 17 subjects showed that our system can achieve an error within the acceptable range for epidural anesthesia, making it a feasible and effective tool for this procedure.
4173	417357	Towards real-time, tracker-less 3D ultrasound guidance for spine anaesthesia	Epidural needle insertions and facet joint injections are important procedures for spine anaesthesia, but their deep location and proximity to sensitive anatomy present challenges for safe insertion. The use of ultrasound as a guiding modality has shown promise, but is not yet widely used due to difficulties in image interpretation, limited view of internal anatomy, and costly tracking hardware. To address these issues, a novel guidance system has been proposed that continuously aligns a statistical model of the lumbar spine on a live 3D ultrasound stream without the need for additional tracking hardware. An in vivo study on 12 volunteers showed that the system can accurately locate the epidural space and facet joints at a volume rate of 0.5 Hz with an accuracy of 3 and 7 mm, respectively. This system has potential to assist physicians in quickly and safely finding the target structure for spine anaesthesia.
4174	41746	Language-independent multi-document text summarization with document-specific word associations.	Automatic text summarization is the process of creating a condensed version of a document or set of documents. The paper proposes a new method for generating summaries in multiple languages, using word associations specific to the given document(s). This statistical approach is effective in selecting sentences for the summary. The method has been tested in various languages and has shown to outperform other methods.
4174	41748	Privacy Preservation by k-Anonymization of Weighted Social Networks	The privacy preserving analysis of social networks aims to understand the network's behavior while protecting the privacy of its individuals. This study focuses on anonymizing weighted graphs, which are important for social network analysis, but also pose challenges for privacy preservation. The proposed method ensures k-anonymity of nodes against attacks where the adversary has information about the network's structure, including edge weights. The study also considers preventing identity, edge, and edge weight disclosure. The method is efficient and has been tested on real world datasets for privacy and utility.
4175	41757	Reducing VM Startup Time and Storage Costs by VM Image Content Consolidation.	Elastic cloud applications require fast virtual machine (VM) startup in order to handle increased workload. While previous studies have looked into VM startup time in clouds, the impact of VM image (VMI) disk size and its contents is not well understood. To address this gap, a detailed study was conducted on Amazon EC2. Based on the findings, a new method was developed for optimizing the size and contents of VMIs. This approach was tested with an open-source Platform-as-a-Service runtime, resulting in a significant reduction in disk size, faster startup time, and lower storage costs compared to unmodified VMIs.
4175	417568	Kangaroo: A Tenant-Centric Software-Defined Cloud Infrastructure	Cloud applications often use virtual machines (VMs) from providers as needed. However, the current interface for acquiring VMs is limiting for users, with minimal control over VM size and placement. This often leads to VMs being underutilized and has performance implications, resulting in higher costs for users. To address these issues, this work introduces Kangaroo, an Open Stack-based virtual infrastructure provider, and IPOPsm, a virtual networking switch. They use nested virtualization and a networking overlay to improve performance and reduce costs. Skippy, a virtual infrastructure API, is also designed and implemented to program Kangaroo. Benchmarks show that Kangaroo has significantly better performance and cost compared to Amazon EC2, and its unified API allows for easy migration between different cloud providers without downtime or code modification.
4176	417619	Modeling Language Evolution	The article presents a model for how languages evolve within a society. It explains that under certain conditions, the languages used by individuals within a society will eventually converge on a common language. The article also delves into a few specific scenarios that demonstrate this convergence.
4176	41762	On The Complexity Of Quantifier Elimination - The Structural Approach	This paper explores the complexity of quantifier elimination in the elementary theory of real numbers with real constants. The researchers use a new model of computation that deals with real number inputs, and show that this model can be used to prove the existence of NP(R)-complete problems. They also introduce a new complexity class, PAT(R), to describe the complexity of decision problems for quantified formulae and compare it to existing complexity classes. Through parallel computations, the researchers are able to show that the polynomial hierarchy over the reals is strictly contained within PAT(R).
4177	417722	Quantitative Driven Optimization of a Time Warp Kernel.	The pending event set in a Parallel Discrete Event Simulation (PDES) is a collection of events available for execution. In a Time Warp synchronized simulation engine, these events are aggressively scheduled without strictly enforcing causal relations. This approach aims to process events in a way that implicitly maintains their causal order without incurring the overhead of strict enforcement. However, on a shared memory platform, scheduling events in their Least TimeStamp First (LTSF) order can lead to contention and negatively impact performance. To address this, profile data from Discrete-Event Simulation (DES) models can be used to optimize the simulation kernel. By grouping events based on this profile data, performance can be improved. However, the size of event groups must be carefully considered as smaller groups can improve performance while larger groups may lead to more frequent causal violations and slower simulation. 
4177	417733	Software control systems for parallel simulation	Parallel simulations using optimistic synchronization strategies, such as Time Warp, prioritize parallelism and minimizing synchronization costs over global synchronization. However, this can lead to thrashing, similar to virtual memory. The challenge lies in properly configuring the Time Warp simulator for optimal performance and avoiding thrashing. To address this, online feedback control systems are used to adjust input parameters in the simulation kernel. However, designing and implementing effective feedback control systems can be difficult and can add overhead to the performance gains. This is further complicated when trying to build a simulation kernel that can efficiently run multiple applications. To address this, a control-centric architecture is introduced, using concepts from control theory to design hierarchically-distributed run-time control systems for Time Warp based parallel simulation.
4178	417870	Generalized Posynomial Performance Modeling	The paper introduces a new method for automatically generating posynomial symbolic expressions to describe the performance of analog integrated circuits. These expressions are based on SPICE simulation data and have high accuracy at the device level. The method solves a non-convex optimization problem without local minima to determine the coefficient and exponent sets for the posynomial expression. This approach can be applied to both linear and nonlinear circuits and allows for the automatic generation of a sizing model that accurately describes the analog circuit. This eliminates the need for hand-crafted analytic models, saving time and effort. Experimental results demonstrate the effectiveness of this approach.
4178	417832	An efficient optimization-based technique to generate posynomial performance models for analog integrated circuits	This paper introduces a new method for creating posynomial response surface models for linear and nonlinear performance parameters of analog integrated circuits. These models allow for efficient use of geometric programming techniques in circuit sizing and optimization. The method automates the generation process, avoiding the time-consuming and inaccurate process of manual model creation. It involves fitting posynomial model templates to data from SPICE simulations and includes measures for evaluating the accuracy of the generated models. Experimental results demonstrate the improved accuracy of this approach.
4179	417913	Summarization of scientific documents by detecting common facts in citations	Scientific articles require more effort to read due to the need to search and go through many citations. To address this issue, a citation-guided method is proposed for summarizing multiple scientific papers. It is observed that citations in one paragraph or section often discuss a common fact, represented by a set of noun phrases. A multi-document summarization system is designed based on detecting these common facts. A challenge is the use of different terms for the same fact in citations, which is addressed by using a term association discovering algorithm. This allows for clustering of citations based on common facts, which are then used to retrieve relevant sentences and create a summary. Experiments show that this method outperforms three baseline methods in terms of ROUGE metric. The system also expands citations through the identification of common facts in the scientific literature.
4179	417968	Summarising News with Texts and Pictures	The increasing amount of information has made effective multi-document summarization techniques essential. Previous methods have focused on text only, but their summaries lack readability and are not widely used. To address this issue, a new approach is proposed that incorporates relevant pictures into news document summaries. A unified semantic link network is constructed, and a mutual reinforcement network method is used to calculate the saliency scores of concepts, sentences, and pictures. An Integer Linear Programming model is then used to select key sentences and pictures. Experiments demonstrate that this approach produces more comprehensible and readable summaries.
4180	41808	On effective flip-chip routing via pseudo single redistribution layer	The use of flip-chip design in power distribution has led to the use of redistribution layer (RDL) for interconnection. However, RDL can become congested, making it difficult to complete routing on a single layer. A previous algorithm using two layers of RDLs was proposed, but it often required more than one layer of routing area. To address this issue, a pseudo single-layer concept was adopted, along with heuristics and observations to reduce constraints. This approach resulted in 100% routability and was compared to manual and commercial methods, showing its effectiveness in a real industrial case.
4180	418067	ACER: An Agglomerative Clustering Based Electrode Addressing and Routing Algorithm for Pin-Constrained EWOD Chips	The limited number of control pins and routing resources on pin-constrained electrowetting-ondielectric (EWOD) biochips poses a challenge for performing complex bio-chemical operations. Previous solutions focused on reducing the number of pins but this often led to designs that were difficult to route. In contrast, the proposed algorithm, called ACER, prioritizes routability by using a clustering approach and integer linear programming for escape routing. This approach leads to a reduction in the required control pins by an average of 13% and a 68% decrease in wirelength compared to previous state-of-the-art methods. Therefore, ACER effectively addresses both pin merging and routing issues in broadcast addressing framework.
4181	418118	Sidelines: An Algorithm for Increasing Diversity in News and Opinion Aggregators	Aggregators use votes and links to select and display a smaller selection of news and opinion pieces from a larger pool of content. However, relying solely on the most popular items may not accurately represent the diversity of opinions and topics present. To address this, three diversity metrics - inclusion, non-alienation, and proportional representation - are defined. The Sidelines algorithm is then introduced as a way to increase diversity in result sets by suppressing a voter's preferences after a preferred item is selected. In comparison to popular collections, Sidelines showed increased inclusion and decreased alienation. In an online experiment, readers were more likely to encounter challenging views with Sidelines. This can help create more diverse news and opinion aggregators for users.
4181	418133	When (ish) is My Bus?: User-centered Visualizations of Uncertainty in Everyday, Mobile Predictive Systems.	Users often rely on realtime predictions for everyday tasks such as taking the bus, but may not fully understand that these predictions are not completely certain. Existing uncertainty visualizations may not align with user needs or their usual way of thinking about probability. To address this, the authors propose a novel mobile interface and visualization for transit predictions on phones, using discrete outcomes. They conducted a literature review, a survey of users of a popular transit app, and an iterative design process to identify specific requirements for visualizing uncertainty in transit prediction. They present several potential visualizations, including a unique discrete representation called quantile dotplots, which was found to improve confidence and reduce estimation variance in a controlled experiment.
4182	418217	A graph-search framework for associating gene identifiers with documents.	The process of curating a model organism database involves finding the identifiers for each gene mentioned in an article. A semi-automated approach is used, where the article is associated with a list of potential gene identifiers. Different methods for solving this geneId ranking problem are compared, including a graph-based method and a learning method for re-ranking the output. The results show that the performance of named entity recognition (NER) systems can vary when used with a "soft dictionary" of gene synonyms. The graph-based approach outperforms individual NER systems and can be further improved through learning. It is also noted that the effectiveness of a NER system for geneId finding cannot be accurately measured by its entity-level performance. The best approach for geneId ranking is to combine multiple NER systems, using easily-available resources.
4182	418236	Automatically Exploring Hypotheses About Fault Prediction: A Comparative Study Of Inductive Logic Programming Methods	This study evaluates inductive logic programming (ILP) methods for predicting fault density in C++ classes. These methods search a large space of hypotheses based on an abstract logical representation of the software, eliminating the need for manual prediction metrics. Two ILP systems, FOIL and FLIPPER, are compared and it is found that FLIPPER generally performs better. Two extensions are proposed for FLIPPER, including a user-directed bias and the use of "counting clauses" to improve performance. The study also explores the use of ILP techniques for automatically generating Boolean indicators and numeric metrics from the calling tree representation.
4183	418377	Apprenticeship learning via inverse reinforcement learning	This article discusses learning in a Markov decision process using an expert's demonstration instead of a given reward function. This approach is useful in complex tasks like driving, where it is difficult to specify a reward function. The expert's goal is to maximize a reward function expressed as a linear combination of known features. The proposed algorithm uses "inverse reinforcement learning" to recover the unknown reward function and learns the task demonstrated by the expert. This algorithm is shown to converge in a few iterations and produce a policy that performs similarly to the expert, based on the expert's unknown reward function.
4183	418372	Space-indexed dynamic programming: learning to follow trajectories	This paper discusses the task of accurately following a trajectory in a vehicle, such as a car or helicopter. Dynamic programming algorithms like Differential Dynamic Programming and Policy Search by Dynamic Programming are efficient in computing non-stationary policies for this task. However, their time-indexed policies may not correspond well to our position along the trajectory and uncertainty over states can hinder their effectiveness. To address these issues, the paper proposes a space-indexed dynamic programming method. This involves rewriting the dynamical system in terms of a spatial index variable and using it to derive space-indexed versions of DDP and PSDP algorithms. The effectiveness of these algorithms is demonstrated through simulations and real-life control tasks.
4184	418415	Deterministic automata simulation, universality and minimality	Finite automata have become a popular model in theoretical physics, particularly in studying the distinction between endophysical and exophysical perception. These studies often involve Moore experiments, which have shown that it is impossible to determine the initial state of an automaton. This has led to the suggestion of a discrete model for Heisenberg uncertainty. However, the classical theory of finite automata, which includes initial states, is not suitable for this purpose. This paper focuses on studying finite deterministic automata without initial states, using an extension of the Myhill-Nerode technique. The goal is to define and analyze the complexity of different types of simulations between automata. The paper also presents the construction of minimal automata, which are proven to be unique up to an isomorphism. All of these findings are based on using "automata responses" to simple experiments and do not consider any information about the internal workings of the automata.
4184	418437	Unary automatic graphs: An algorithmic perspective	This paper explores the properties of infinite graphs created through a natural unfolding process applied to finite graphs. These unfolded graphs have finite degree and can be described by finite automata over the unary alphabet. The study investigates the algorithmic properties of these graphs, specifically determining if a given node is part of an infinite component, if two nodes are reachable from each other, and if the graph is connected. The paper presents polynomial-time algorithms for each question, with a constant time algorithm for the first question and a uniform automaton for the second question. This improves on previous research that used non-elementary or non-uniform algorithms. 
4185	418567	Framework for Active Clustering With Ensembles	Clustering methods are useful for organizing large collections of video and images, particularly when it comes to identifying and grouping faces. A new semisupervised approach combines ensemble clustering and active learning to improve accuracy. The user is asked to provide soft link constraints for pairs of faces that are difficult to match. This approach was evaluated on various types of data, including blurry videos, images of women with makeup, and photographs of twins. The results show that this method is more robust to noise and more accurate than other approaches. This new clustering algorithm is also more effective and efficient than current methods.
4185	418585	On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs	The proposed model is a multi-scale GAN that can automatically generate realistic context and background pixels from a single input face mask. It does not require any user supervision and is able to generate realistic images even for a large number of missing pixels. The model is made up of a cascaded network of GAN blocks, each responsible for generating missing pixels at a specific resolution. The resulting full face image is made photo-realistic through the use of various loss functions. Extensive experiments show that the model is effective in generating context and background pixels for various facial poses, expressions, and lighting conditions. It is also compared to other popular face inpainting and swapping models in terms of visual quality, realism, and identity preservation. The model's cascaded pipeline is also compared to the progressive growing of GANs and its potential use as a data augmentation module for training CNNs is explored.
4186	4186125	Quantifying and Verifying Reachability for Access Controlled Networks	This paper discusses the importance of quantifying and querying network reachability for security monitoring, auditing, and network management purposes. While previous attempts have been made to model network reachability, there have not been feasible solutions for computing it. The paper proposes a suite of algorithms for quantifying reachability based on network configurations, such as Access Control Lists (ACLs), and also presents solutions for querying network reachability. The proposed network reachability model takes into account various aspects of network protocols and features, and the algorithms have been implemented in a tool called Quarnet. Experiments on a university network have shown that the offline computation of reachability matrices takes a few hours, and online processing of a reachability query takes an average of 0.075 seconds.
4186	418676	Privacy-Preserving Quantification of Cross-Domain Network Reachability	Network reachability is a crucial aspect in understanding the behavior of a network and detecting security policy violations. However, quantifying network reachability across multiple administrative domains is a challenging task due to the privacy concerns surrounding individual domain policies. In this paper, a cross-domain privacy-preserving protocol is proposed for accurately determining network reachability while ensuring the privacy of Access Control List (ACL) rules. The protocol has been implemented and evaluated on both real and synthetic ACLs, showing efficient processing and comparison times and low communication costs. This protocol provides a solution for quantifying network reachability along a network path spanning multiple administrative domains.
4187	418748	Memetic cooperative models for the tool switching problem.	This work focuses on using cooperative memetic algorithms, which combine different optimization strategies, to solve the tool switching problem (ToSP). The ToSP is a difficult optimization problem in the field of flexible manufacturing and has been previously tackled using various algorithms. The authors present a proof-of-concept of their cooperative memetic model and conduct experiments to compare its performance with individual memetic algorithms and local search approaches. They find that the cooperative model consistently outperforms the individual components and also conduct a sensitivity analysis to better understand the relationship between the different elements of the model. 
4187	418722	A memetic cooperative optimization schema and its application to the tool switching problem	This paper presents a generic cooperative optimization schema where multiple agents with different optimization techniques work together to solve a problem. The approach is applied to the Tool Switching Problem (ToSP), a challenging optimization problem in flexible manufacturing. The agents can use a variety of optimization methods, such as local search and population-based methods, resulting in a multilevel hybridization. Extensive experiments were conducted, comparing various algorithms and instances, and the results show that some meta-cooperative instances outperform others, including a previously successful memetic algorithm for ToSP. This highlights the effectiveness of the proposed cooperative optimization approach.
4188	418818	Short Paper: Limitations of Key Escrow in Identity-Based Schemes in Ad Hoc Networks	Identity-based cryptography (IBC) has gained attention for its potential to secure ad hoc networks. This study focuses on the role of the Trust Authority (TA) as a key escrow, which is a characteristic of all IBC schemes. The unique role of key escrow in ad hoc networks is explored and compared to other networks. Adversary models for dishonest TAs in ad hoc networks, including a new model where TAs use spy nodes to record and report communications, are introduced. Analytical results demonstrate that in most ad hoc network scenarios, the TA can be prevented from functioning as a key escrow.
4188	418813	Key revocation for identity-based schemes in mobile ad hoc networks	Identity-based cryptographic (IBC) schemes have gained attention for securing mobile ad hoc networks (MANETs) due to their efficient key management properties. However, existing schemes lack mechanisms for key revocation and renewal. To address this issue, the authors propose the first key revocation and renewal mechanisms specifically designed for MANETs. The revocation scheme involves nodes monitoring and securely propagating observations of other nodes within communication range. If a minimum number of nodes accuse a node, its public key is revoked. To enable key renewal, a modified format for ID-based public keys is introduced. This scheme is efficient as it uses pre-shared keys and limits message propagation to a specified neighborhood.
4189	418934	W-privacy: understanding , and  inference channels in multi-camera surveillance video	Surveillance systems record massive amounts of video every day, containing valuable information that can be used for various purposes. However, the potential privacy loss caused by public access to this video is a concern. Privacy solutions must consider the different ways in which personal information can be inferred from the video, such as location, time, and activities, in addition to direct identifiers like faces. This paper presents a privacy loss model that takes into account identity leakage through multiple inference channels in video data. The model combines identity leakage and sensitive information to calculate the overall privacy loss, and experimental results show its effectiveness.
4189	41899	Adaptive transformation for robust privacy protection in video surveillance	The issue of privacy in video surveillance systems is a major concern, as it often leads to unmonitored areas and potential security threats. Existing privacy protection methods rely on accurate region of interest detectors, but these are not always reliable, resulting in breaches of privacy. This paper proposes a method that uses adaptive data transformation, including selective obfuscation and global operations, to provide robust privacy even with unreliable detectors. Additionally, the method addresses both explicit and implicit channels of privacy leakage. Experimental results show that this method has significantly less distortion and near-zero privacy loss compared to previous global transformation methods. 
4190	419084	Scalable 3D video of dynamic scenes	This paper introduces a scalable 3D video framework that captures and renders dynamic scenes. The system uses multiple 3D video bricks, each consisting of a projector, two grayscale cameras, and a color camera. Structured light with complementary patterns is used to acquire texture images and pattern-augmented views simultaneously. Depth maps are extracted from the pattern images using space-time stereo, and a view-independent, point-based 3D data structure is created. This allows for effective enforcement of photo consistency and removal of outliers, resulting in high-quality rendering using EWA volume splatting. The framework also allows for easy editing of 3D video, as demonstrated through compositing techniques and spatio-temporal effects.
4190	4190255	Low-cost telepresence for collaborative virtual environments.	The authors have developed a new, cost-effective method for visual communication and telepresence in a CAVE-like environment. This method uses 2D stereo-based video avatars and combines various efficient algorithms to create a realistic real-time representation of a remote user in a spatially immersive display. The system is designed to be easily integrated into existing projection systems with minimal hardware modifications and low cost. By using infrared-based image segmentation, the system can simultaneously acquire and project images without a static background. It utilizes two color cameras and two additional black-and-white cameras for segmentation in the near-infrared spectrum, and does not require special optics. The stereo image stream is compressed, transmitted over a network, and displayed as a frame-sequential stereo texture in the virtual environment. 
4191	419151	Hole filling of a 3D model by flipping signs of a signed distance field in adaptive resolution.	The use of range finders to observe objects often results in holes and gaps in the model, making it unsuitable for certain applications. A new method is proposed to fill these gaps using a Signed Distance Field (SDF) that stores distances from a voxel to the nearest point on the mesh model. By minimizing the area of the interpolating surface, a smooth and continuous surface can be generated. Unlike other methods, this approach does not require boundary conditions and can handle high curvature holes. It has been successfully tested on both synthetic and real objects.
4191	419121	Calibration of a 3D endoscopic system based on active stereo method for shape measurement of biological tissues and specimen.	Accurate measurement of lesion size and shape is crucial for improving diagnostic accuracy in endoscopic medical treatment. A system is being developed to measure the shapes and sizes of living tissue using an active stereo method with a micro pattern projector attached to a normal endoscope. This requires estimating the intrinsic and extrinsic parameters of the endoscopic camera and pattern projector for 3D reconstruction, which is challenging. To simplify the calibration process, a simultaneous estimation method for both parameters is proposed in this paper. An intuitive user interface has also been developed for easy operation. The system was successfully used to measure the shape of human soft palate tissue and a biological specimen.
4192	419219	Aggregating E-commerce Search Results from Heterogeneous Sources via Hierarchical Reinforcement Learning	The paper discusses the challenge of aggregating search results from different sources in an E-commerce environment. Unlike traditional web search, this task requires dynamically deciding which source should be presented on each page. It also addresses the difficulty of ranking items from different sources due to differences in relevance scores. The authors propose a hierarchical approach to address these issues, with a high-level task for source selection and a low-level task for item presentation. Both tasks are formulated as sequential decision problems and learned through reinforcement learning. The proposed model shows significant improvements in search performance and user satisfaction.
4192	419237	PatentMiner: topic-driven patent analysis and mining	Patenting is crucial for safeguarding a company's core business concepts and proprietary technologies. By analyzing a large volume of patent data, potential competitive or collaborative relationships among companies in certain areas can be uncovered, providing valuable insights for developing strategies related to intellectual property, research and development, and marketing. A topic-driven patent analysis and mining system is presented in this paper, which focuses on studying the heterogeneous patent network derived from the patent database. This network comprises various objects such as companies, inventors, and technical content that evolve over time. A dynamic probabilistic model is proposed to characterize the topical evolution of these objects within the patent network. This model is used to develop several patent analytics tools that can aid in IP and R&D strategy planning, including a network co-ranking method, competitor evolution analysis algorithm, and a search results summary method. The effectiveness of these techniques is demonstrated through evaluation on a real-world patent database.
4193	41930	Extract interaction detection methods from the biological literature.	Efforts have been made to extract protein-protein interactions from the literature, but not much has been done on extracting their detection methods. This is important because different methods may affect the reliability of the reported interactions. However, the variety of method mentions makes automatic extraction difficult. To address this, a Correlated Method-Word (CMW) model was developed to extract detection methods by considering the correlation between methods and related words. The model was tested on a corpus of 5319 documents and showed promising results, outperforming previous methods. This demonstrates the model's ability to overcome challenges posed by diverse method mentions and capture the correlation between methods and related words.
4193	419375	Exploiting and integrating rich features for biological literature classification.	Efficient features are crucial in automated text classification, especially in handling large amounts of data. In the bioscience field, there are numerous features used to describe biological structures and terminologies. Incorporating domain dependent features can greatly enhance classification performance. This paper focuses on the issue of effectively selecting and integrating different types of features to improve biological literature classification. The proposed approach, TF*ML, combines lower level domain independent "string features" with higher level domain dependent "semantic template features" to achieve better results. Compared to previous methods, TF*ML showed a significant improvement in AUC and F-Score, surpassing the best performance from BioCreAtIvE 2006. It is important to properly integrate different types of features to avoid overfitting and improve performance in literature classification.
4194	419442	Breaking the Synaptic Dogma: Evolving a Neuro-inspired Developmental Network	The majority of artificial neural networks do not change within a learning environment, in contrast to biological networks which have time-dependent morphology and can change through interactions. To address this, a new type of neural network has been designed that models different components of biological neurons and allows for changes in response to the environment. This network begins as a small randomly generated network and evolves through the use of seven programs that simulate various aspects of biological neurons. The goal is to demonstrate that this type of network can learn through experience and the authors discuss their ongoing research in using this network to learn how to play checkers.
4194	419437	Evolution of Cartesian Genetic Programs for Development of Learning Neural Architecture	This paper discusses the use of evolutionary approaches in artificial neural networks to incorporate biological plausibility and improve their learning abilities. The model allows for the growth and change of neural components, such as neurons and synapses, to better mimic natural neurological systems. The researchers used a form of genetic programming, known as Cartesian genetic programming (CGP), to obtain suitable computational functions for these components. The model was tested on a single agent learning problem and a more challenging competitive environment with two antagonistic agents, showing promising results in both scenarios. The study highlights the potential of incorporating genetic programming in artificial neural networks to improve their performance and better replicate natural neural systems.
4195	419559	Chosen Ciphertext Security via Point Obfuscation.	This paper presents two new constructions for chosen ciphertext secure public key encryption (PKE) from general assumptions. The constructions make use of an obfuscator for point functions with multi-bit output (MBPF obfuscators) that satisfies a security property called AIND in the presence of hard-to-invert auxiliary input. The first construction is based on a chosen plaintext secure PKE scheme and an MBPF obfuscator satisfying AIND with computationally hard-to-invert auxiliary input, while the second construction is based on a lossy encryption scheme and an MBPF obfuscator satisfying AIND with statistically hard-to-invert auxiliary input. These constructions demonstrate the strength of AIND security in relation to other security notions for MBPF obfuscators. Additionally, the paper shows that a lossy encryption scheme can be constructed from a point obfuscator that satisfies certain properties, leading to a CCA secure PKE scheme constructed solely from a re-randomizable and composable point obfuscator. The authors believe that these results provide a connection between the seemingly isolated cryptographic primitives of CCA secure PKE and program obfuscators.
4195	419553	Simple CCA-Secure Public Key Encryption from Any Non-Malleable Identity-Based Encryption	This paper introduces a new method for creating public key encryption (PKE) that is secure against chosen ciphertext attacks (CCA) using identity-based encryption (IBE). The method involves encrypting a message and a random value under a specific identity using the encryption algorithm of an IBE scheme that is non-malleable and has a one-way function. While this method requires stronger security for the underlying IBE scheme compared to previous methods, it offers a simple and novel approach for constructing PKE. The paper also presents a new proof technique for the security of the proposed scheme, which can handle certain decryption queries that cannot be addressed using traditional methods.
4196	419637	Analyzing and Modeling Router---Level Internet Topology	The Internet topology has been studied and it shows that the connectivity of nodes follows a power-law distribution. However, this does not fully determine the network structure, especially when studying routing control. This paper examines the structures of router-level topologies in actual ISP networks, revealing a high level of clustering. This is not accounted for in existing modeling approaches. To address this, a new realistic modeling method is proposed, where new nodes are likely to connect to the nearest nodes and new links are added based on node utilization. This method produces similar metrics to actual ISP topologies while maintaining a power-law degree distribution. 
4196	41969	Location-aware utility-based routing for store-carry-forward message delivery	Store-carry-forward routing faces the challenge of reducing message copies without increasing delivery delay. To address this issue, utility-based routing schemes have been proposed, where a node's utility indicates its proximity to the message's destination. However, these schemes are not effective in sparse areas where relay nodes have limited encounters with other nodes. To solve this, a location-aware utility-based routing scheme is proposed, combining utility-based routing with probabilistic forwarding based on node utility and density at contact locations. Simulations show that this scheme performs well in terms of reducing copies and delivery delay. 
4197	419729	Botnet spam campaigns can be long lasting: evidence, implications, and analysis	The accurate identification of spam campaigns launched by large numbers of bots in a botnet is crucial in order to generate effective spam campaign signatures and defeat spamming botnets. The traditional approach of clustering spam with the same label, such as a URL, can be easily bypassed by techniques like obfuscating URLs. In this paper, the authors conduct a comprehensive study of content-agnostic characteristics of spam campaigns, such as duration and source-network distribution of spammers, to determine their effectiveness in assisting label-based clustering methods. They analyze a five-month trace of seven URL-based botnet spam campaigns and find that these campaigns have longer durations than previously reported. This has implications for spam campaign signature generation and the authors also uncover new findings about workload distribution, sending patterns, and coordination among spamming machines. 
4197	419722	Accurate Real-time Identification of IP Prefix Hijacking	This article discusses new techniques for detecting IP prefix hijacking attacks in real time. These attacks can disrupt network services and allow for malicious activities without revealing the attacker's identity. By combining analysis of BGP routing updates with data from the network's data plane, the accuracy of detection is significantly improved. The use of data plane information, specifically edge network fingerprints, helps to identify and differentiate suspicious incidents of IP hijacking. This system has been tested with multiple real-time BGP feeds and has shown to effectively distinguish between legitimate routing changes and actual attacks. Furthermore, the article also highlights the correlation between the identified hijacked prefixes and spam emails, confirming the accuracy of the proposed techniques.
4198	4198141	DaVinci: dynamically adaptive virtual networks for a customized internet	This paper discusses the benefits of running multiple virtual networks with different performance objectives on a shared substrate. The traditional approach of statically dividing resources between virtual networks is often inefficient, while dynamic resource allocation can be unstable. The paper proposes a solution using optimization theory, where each substrate link periodically reallocates bandwidth shares between its virtual links and each virtual network independently maximizes its own performance objective. Numerical experiments with various types of traffic show that this approach is efficient and adaptable. This allows for more efficient and flexible management compared to a single, compromise design.
4198	419846	The "Platform as a service" model for networking	Decoupling infrastructure management from service management has the potential to drive innovation, introduce new business models, and simplify the process of running services. This concept, already prevalent in the computing world, is now being considered for networking. Most discussions have focused on network virtualization, where a virtual network of multiple virtual routers is created on top of a shared physical infrastructure. However, this paper suggests a different approach, where service providers are presented with the abstraction of a single router, allowing them to focus solely on their service without worrying about managing a virtual network. The challenges of mapping multiple abstract routers from different parties to a distributed and shared physical infrastructure are also discussed. 
4199	419931	Space-code bloom filter for efficient traffic flow measurement	Per-flow traffic measurement is crucial for various purposes such as usage tracking, traffic management, and detecting anomalies. However, existing methods either rely on inaccurate random sampling or only account for large flows. This paper introduces a new technique, using a data structure called Space Code Bloom Filter (SCBF), for measuring per-flow traffic accurately and efficiently at high speeds. SCBF utilizes a Maximum Likelihood Estimation (MLE) approach to estimate the size of each flow in a multiset. By adjusting parameters, SCBF allows for a balance between accuracy and complexity. It also introduces a new concept called blind streaming, contributing to data streaming. The performance of SCBF was evaluated on real-world data and showed promising results with low storage and computational requirements.
4199	419919	Robust traffic matrix estimation with imperfect information: making use of multiple data sources	Traffic matrices are essential for network capacity planning and traffic engineering, but accurately estimating them has been a challenging research problem. Previous methods relied on either SNMP link loads or sampled NetFlow records, but a new approach has been designed that combines both sources to improve accuracy. This is made possible by the widespread availability of SNMP and NetFlow in operational IP networks. Additionally, this research has led to the discovery that SNMP link loads and sampled NetFlow records can act as "error correction codes" for each other, allowing for the identification and removal of dirty data. Experiments on real data from a large ISP show significant improvements in accuracy, even with a small fraction of NetFlow data. The use of routing changes as an a priori further improves estimation accuracy. This is the first comprehensive solution that utilizes multiple readily available data sources, providing valuable insights on the effectiveness of combining flow and link load measurements. 
41100	4110068	Maximally permissive controlled system synthesis for non-determinism and modal logic.	In this paper, the authors propose a new technique for controlled system synthesis on non-deterministic automata. This technique involves restricting the behavior of an uncontrolled system to satisfy a given logical expression, while adhering to the rules of supervisory control. The authors use a requirement formalism that extends Hennessy-Milner logic with modalities from Gödel-Löb logic, allowing for a broad range of control requirements to be expressed. The paper contributes to the field of control synthesis by achieving maximal permissiveness in a non-deterministic context and addressing controllability through partial bisimulation. The authors provide a well-defined and complete derivation of the synthesis result, supported by computer-verified proofs. An algorithmic form of the synthesis method is also presented, with an analysis of its computational complexity. The paper demonstrates the effectiveness of the proposed technique in two industrial case studies and discusses its scalability.
41100	4110042	Precongruence Formats with Lookahead through Modal Decomposition.	Bloom, Fokkink & van Glabbeek (2004) proposed a way to break down Hennessy-Milner logic formulas based on structural operational semantics specifications. According to their method, a term in the corresponding process algebra meets a Hennessy-Milner formula if its subterms satisfy specific formulas obtained through decomposition. This approach was used to establish congruence formats within structural operational semantics. The authors also demonstrated how this framework can be expanded to include bounded lookahead in the premises, and used this extension to derive a congruence format for the partial trace preorder.
41101	4110153	Characteristic matrices of compound operations of coverings and their relationships with rough sets.	This paper discusses the use of matrices in representing generalized rough sets, specifically focusing on coverings. While matrices have been used in other types of rough sets, they have not been extensively studied in connection with coverings. The paper defines three composition operations of coverings and studies their characteristic matrices, as well as their relationships with covering approximation operators. The paper also introduces a new matrix representation for neighborhoods called the type-2 characteristic matrix. The importance of knowledge fusion and decomposition is highlighted, and the representable properties of covering approximation operators are explored. The paper concludes that matrices have potential for studying covering-based rough sets in data mining and machine learning.
41101	4110146	Geometric Lattice Structure of Covering-Based Rough Sets through Matroids.	Covering-based rough set theory is a useful tool for dealing with vague or uncertain information in information systems. In this paper, the authors construct three geometric lattice structures for covering-based rough sets using matroids and explore their properties. They also establish a correspondence between these structures and transversal matroids. Furthermore, they present conditions for certain types of covering upper approximation operators to be closure operators of matroids, and use closure axioms to obtain two additional geometric lattice structures. Finally, the authors investigate the relationships among these structures and explore core concepts such as reducible elements. This work presents a unique perspective on studying covering-based rough sets through the use of geometric lattices.
41102	4110264	Characterization of the electromagnetic side channel in frequency domain	This article proposes a new method for identifying and analyzing the electromagnetic (EM) leakage of electronic devices. The approach focuses on identifying the frequencies that leak the most information and uses cryptanalysis techniques to estimate security risks. Two approaches are introduced: an empirical one and one based on information theory. These techniques are low cost, automatic, and fast, and can be performed with an oscilloscope and software. The article also mentions TEMPEST evaluations, which require specialized equipment and are time-consuming. The proposed approach does not replace regulatory TEMPEST evaluations, but can identify leakage with high confidence. An example is provided to demonstrate the effectiveness of this approach in recovering a key from a keyboard at a distance of 5 meters.
41102	4110263	A Silicon-Level Countermeasure Against Fault Sensitivity Analysis and Its Evaluation	This paper introduces a new countermeasure against fault sensitivity analysis (FSA) using configurable delay blocks (CDBs). FSA is a type of fault attack that targets the relationship between fault sensitivity (FS) and secret information in cryptographic modules. Previous studies have shown that FSA can bypass conventional countermeasures against differential fault analysis (DFA). The proposed countermeasure can effectively defend against both DFA and FSA attacks by utilizing CDBs as a time base and incorporating Li's countermeasure concept to remove FS-secret data dependency. The postmanufacture configuration of CDBs allows for reduced overhead in operating frequency. An implementation of this countermeasure in an application-specific integrated circuit is also presented, along with an investigation of its hardware overhead and a demonstration of its effectiveness through an experiment.
41103	411032	Aligning codebooks for near duplicate image detection	Detecting near duplicate images in large databases, such as those found on social networks, digital investigation archives, and surveillance systems, is crucial for various image forensics applications. To address this, hashing techniques are commonly used to index large quantities of images and detect copies from different archives. Recently, the Bags of Visual Features paradigm has been enhanced by incorporating multiple descriptors, such as Bags of Visual Phrases, to take advantage of the coherence between different feature spaces. This paper proposes further improvements to this approach by considering coherence not only during image representation, but also during codebook generation. A new image database with over 3,300 images and 500 scenes with real near duplicates is also introduced for benchmarking purposes. Additionally, a method for compressing the image representation for storage is suggested. Experimental results show that the proposed near duplicate retrieval technique surpasses the original Bags of Visual Phrases approach.
41103	4110364	A robust forensic hash component for image alignment	The distribution of digital images through various platforms has led to a need for systems that can protect the images from malicious manipulation during transmission. A key issue in this context is the authentication of the received image, which is typically done by identifying any tampered regions. This requires aligning the received image with the original one using information from a forensic hash. The paper proposes a robust alignment method that utilizes a specific component of the image hash based on the Bag of Visual Words concept. This signature is attached to the image before transmission and analyzed at the destination to recover any geometric transformations applied to the image. Experiments demonstrate that the proposed method outperforms existing techniques.
41104	41104110	Using General Impressions to Analyze Discovered Classification Rules	Data mining is a crucial aspect of information retrieval, but one of its main challenges is evaluating the subjective interestingness of discovered rules. Previous research has shown that in real-life applications, a large number of rules are generated but only a few are useful or interesting to the user. This is due to the difficulty of manually analyzing a large number of rules to identify the interesting ones. To address this issue, a technique has been proposed that compares the discovered rules against a type of existing knowledge called general impressions. These general impressions are specified using a representation language, and algorithms are used to analyze the rules. The results of this analysis help identify which rules conform to the general impressions and which ones are unexpected, and therefore considered interesting. 
41104	4110453	Finding interesting patterns using user expectations	, making it suitable for a variety of applicationsThe interestingness problem in knowledge discovery (or data mining) is a major concern as it is easy to discover a large number of patterns in a database, most of which are useless or uninteresting. To tackle this issue, a technique called the user-expectation method is proposed. It involves the user providing their expected patterns, which are then matched against the discovered patterns using a fuzzy matching technique. This allows for the ranking of patterns based on their level of interest to the user. The proposed technique is versatile and can be used for various purposes, such as confirming the user's knowledge and identifying unexpected patterns, which are considered interesting.
41105	4110565	Coordination guided reinforcement learning	In this paper, the authors propose a new approach to reinforcement learning (RL) for multi-agent problems. The idea is to use expert coordination knowledge, in the form of constraints, to guide the learning process. These constraints are used to restrict the joint action space and direct exploration towards more promising states, resulting in a faster learning rate. The authors introduce a two-level RL system that incorporates these coordination constraints for online applications. This approach allows for knowledge sharing between constraints and features, and has been tested on a soccer game and a real-time strategy game, showing improved learning rates compared to a single-level approach.
41105	4110590	Distributed relational temporal difference learning	The use of relational representations in large decision-making processes, particularly in multi-agent problems, has the potential for quick and effective generalization of learned knowledge. This paper introduces a method called relational temporal difference learning for distributed systems with dynamic communication links between agents. By using partially bound relational features and a message passing scheme, agents can learn and generalize from each other. This approach can also be applied to distributed reinforcement learning with value functions. Experiments in soccer and real-time strategy games with dynamic communication demonstrate improved goal achievement and reduced parameter learning compared to existing methods.
41106	41106118	Biasing mutations in cooperative coevolution	Coevolution is a process in which species evolve in response to each other, with their genetic changes being reciprocal. One type of coevolution is cooperative coevolution, where species work together to solve problems. In this type of coevolution, an individual's fitness is determined by how well it collaborates with individuals from another species. Cooperative revolutionary algorithms (CCEAs) are an extension of evolutionary algorithms, but with a focus on cooperation. In this paper, the authors explore the use of biased mutations in CCEAs and compare them to conventional CCEAs with binary representations. Their experiments show that biased mutations can improve the performance of CCEAs, especially when using high orders of binary representations.
41106	4110665	A Multi-Modal Agent Based Mobile Route Advisory System for Public Transport Network	The transport network in Hong Kong is complex and challenging to navigate, especially for e-tourism tourists and mobile workforces. Finding the most cost-effective route can be time-consuming. To address this issue, a conceptual model and multi-agent information system (MAIS) have been proposed. The MAIS utilizes intelligent search agents and a knowledge-basket approach to enhance route finding. Backend information agents gather updates from transportation companies, and a sub-system for reservation of hired transportation services is also included. A prototype of the route advisory system has been built with multiple user interface views on different mobile platforms. Benchmark results have shown the effectiveness of this approach.
41107	41107135	Joint Server and Network Energy Saving in Data Centers for Latency-Sensitive Applications	The paper discusses the challenges of achieving energy proportionality in data centers that support latency-sensitive applications. Previous studies have focused on either making servers energy proportional or reducing network power consumption for applications with lower latency requirements. The proposed solution, called EPRONS, aims to minimize overall data center power consumption by trading off network slack for additional computation time. A linear programming model is used to consolidate search queries and background flows to a minimal subnet, turning off unused switches and links without violating application deadlines. Servers use a power saving technique based on Dynamic Voltage and Frequency Scaling (DVFS) and can request additional network resources if needed. Experimental results show significant power savings of up to 31.25% in the data center.
41107	4110771	E-AHRW: An Energy-Efficient Adaptive Hash Scheduler for Stream Processing on Multi-core Servers	In this paper, we focus on video transcoding on a multi-core server and aim to minimize processing time while maintaining good video quality in an energy-efficient manner. Existing scheduling schemes have limitations due to ineffective use of the multi-core architecture and undifferentiated energy consumption. We propose a new scheduler, E-AHRW, based on MC^2 (memory access, core/cache topology, and transcoding format cost) which identifies key factors affecting transcoding performance. E-AHRW adaptively adjusts the hashing decision based on real-time weighted queue length of each processing unit, achieving stream locality and load balancing at both stream and packet level. We also introduce a hash-tree scheduler to further reduce computation cost and improve load balancing. Evaluation on an Intel Xeon server shows that E-AHRW improves throughput, energy efficiency, and video quality due to better load balancing, lower L2 cache miss rate, and minimal scheduling overhead.
41108	4110849	Designing participation in agile ridesharing with mobile social software	In order for sustainability initiatives to be effective, they often require participation at the local community level. This paper examines the use of technology, specifically mobile social software and interface design, to facilitate carpooling as a means of increasing participation. Convenience, ease of use, and adaptability to individual circumstances were found to be crucial in designing a successful carpooling system. While global technology platforms may have limited reach if they do not cater to local needs, a localized approach focusing on designing technology to support and grow mobile social ridesharing networks is proposed. This paper contributes to the understanding of Human-Computer Interaction (HCI) in the context of designing participation.
41108	4110836	Getting to the nub of neighbourhood interaction	Designing technologies to facilitate communication within local communities is a complex task. This research examines two different approaches for achieving this goal: assisting a community organization in creating their own technology, and implementing a digital noticeboard in the physical community space. Both methods aim to promote communication among community members. However, challenges arise in terms of adapting the built environment for public use and obtaining relevant information from the community. The article delves into these challenges and highlights the importance of considering the needs and preferences of the community when designing communication technologies for local communities.
41109	4110943	Representing Guard Dependencies in Dataflow Execution Traces	The use of heterogeneous parallel systems is becoming increasingly popular in computing. However, one of the main challenges is how to fully utilize the available computational power when porting existing programs or developing new ones. Various design space exploration methods have been developed, but determining the feasible design space for dynamic dataflow programs is still a challenge. This paper proposes a new methodology that uses homotopy theoretic methods to define the design space through a serial execution. It also introduces the concept of dependencies graph with the addition of two new types of dependencies and a 3-tuple notation to represent them. 
41109	4110957	Methods to explore design space for MPEG RMC codec specifications	The MPEG Reconfigurable Media Coding (RMC) standard allows for the creation of media processing specifications that are abstract and independent from the implementation platform, but can still be easily implemented on specific targets. This is achieved through the use of an asynchronous dataflow model and a specification language. While this approach offers many advantages, such as improved performance and energy efficiency, there are challenges in mapping these specifications onto concurrent and heterogeneous platforms. This paper discusses the current research and developments in addressing these challenges and optimizing the design of dataflow program implementations for applications such as image and video coding. Various heuristics are explored for problems like mapping, scheduling, and buffer minimization in order to achieve optimal system performance.
41110	4111031	Why is it so difficult for a robot to pass through a doorway using ultrasonic sensors?	The article discusses the challenges of controlling a robot to perform complex tasks, such as navigating through a narrow doorway. The authors argue that while high-level goals are typically used to describe these tasks, details must be provided in order to effectively control the robot. They demonstrate this difficulty using real data from a robot, and propose an architecture based on reconfigurable objects that contain domain knowledge and information about the robot's sensors and actuators. This approach allows for the transformation of high-level goals into specific commands at the time of execution. The authors provide an example of how their approach can be used to successfully complete the task of navigating through a narrow doorway.
41110	411108	Sensor explication: knowledge-based robotic plan execution through logical objects.	The article discusses the challenges of translating high-level goals into specific commands for controlling a real robot. A new concept called "sensor explication" is proposed to make these details explicit in order to generate primitive commands. The proposed approach involves using reconfigurable objects with domain knowledge and information about available sensors and actuators. This allows for the transformation from high-level goals to primitive commands to be performed at execution time. The effectiveness of this approach is demonstrated through experiments involving a real mobile robot performing a complex task of navigating through an unknown and narrow doorway using sonic range data. The results show how the integration and utilization of sensors and actuators can improve the robustness of plan execution.
41111	4111167	Discovery and Evaluation of Aggregate Usage Profiles for Web Personalization	Web usage mining is a technique that can be combined with other personalization approaches, such as collaborative filtering, to overcome limitations such as subjective user ratings, scalability, and poor performance with high-dimensional and sparse data. However, simply discovering patterns from usage data is not enough for effective personalization. The key is to derive actionable “aggregate usage profiles” from these patterns. This paper presents and evaluates two clustering techniques for discovering overlapping aggregate profiles that can be used by recommender systems for real-time personalization. The results show that these techniques can achieve effective personalization early on in a user's visit to a website, using only anonymous clickstream data and without explicit user input or detailed knowledge about them.
41111	4111140	Data mining for web personalization	This chapter discusses the process of Web personalization and how it involves various stages of a data mining cycle. This includes collecting and processing data, discovering patterns, and using this knowledge in real-time to personalize the user's experience on the Web. The chapter also explores the use of multiple data sources and different data mining techniques, such as clustering, association rule discovery, and probabilistic models, to improve the effectiveness of personalization. Additionally, the chapter discusses the concept of hybrid data mining frameworks that combine data from various sources to create more efficient personalization solutions.
41112	4111254	Analysis of time-delay estimation in reverberant environments.	Room reverberation poses a significant challenge for creating reliable microphone-based source localization systems. To address this issue, researchers have turned to statistical room acoustics and analyzed the effectiveness of GCC-based methods for time-delay estimation. One notable finding is that the PHAT time-delay estimator is proven to be the most optimal among a group of cross-correlation based time-delay estimators. This analysis sheds light on the potential for improving the accuracy and robustness of microphone-based source localization systems.
41112	41112202	A Binary Tree for Probability Learning in Eye Detection	The paper proposes a solution to the eye detection and localization problem using a statistical model based object detection framework. A binary tree representation is used to capture the underlying statistical structure of the objects, allowing for a coarse-to-fine description of the local statistics. The tree is built in a top-down manner, with subsets of features with low conditional independence identified through k-means clustering. The more dependent features are grouped together in the tree, and the distribution of the object is learned using a Gaussian mixture model in the independent component analysis subspace. The use of a tree structure allows for recursive learning using Bayesian criterion. Experiments show that the proposed method outperforms the Viisage system in terms of detection accuracy and localization ability.
41113	4111311	A fast handoff scheme for IP over Bluetooth	The use of multiple electronic devices such as cell phones, PDAs, and laptops has become common in daily life. The popularity of Bluetooth, a wireless technology that allows communication between different devices, has been growing due to its ability to connect incompatible devices. One application of Bluetooth, called BLUEPAC, allows users to access public networks like the Internet. However, frequent handoffs between base stations can cause connection losses and hurt network performance. To address this issue, a new scheme has been proposed that involves a base station controller assisting a mobile device in establishing a connection with a new base station. Simulation results have shown that this scheme reduces handoff delay and has a minimal impact on the base station.
41113	4111390	Friend-safe evasion attack: An adversarial example that is correctly recognized by a friendly classifier.	Deep neural networks (DNNs) are being used in various services such as image recognition, intrusion detection, and pattern analysis. However, the emergence of adversarial examples, which are slightly altered data that can cause incorrect classification, pose a significant threat to the security of DNNs. In some cases, an adversarial example can be useful, such as deceiving an enemy classifier on the battlefield. To address this, a friend-safe adversarial example is proposed where the friendly machine can correctly classify it. This is achieved by minimizing the probability of incorrect classification by the friendly machine and maximizing the probability of correct classification by the adversary. The proposed method has been tested on MNIST and CIFAR10 datasets, showing a 100% success rate and small distortion. Furthermore, potential applications of this method in covert channels and mixed battlefield scenarios are suggested.
41114	4111457	Assessing the computational benefits of AREA-oriented DAG-scheduling	The AREAOriented scheduling (AO-scheduling) paradigm was developed to improve the performance of modern computing platforms, which are "task hungry" and benefit from having many tasks available for allocation to processors. It is a weaker but more robust successor to IC-scheduling, which is not achievable for many DAGs. AO-scheduling coincides with IC-scheduling for optimal DAGs and can be achieved for all DAGs, but the computational complexity of optimal AO-scheduling is not yet known. This paper uses simulation experiments to compare AO-scheduling with other heuristics and shows that it can enhance the efficiency of task-hungry platforms, depending on processor availability and DAG structure.
41114	4111436	Extending IC-Scheduling via the Sweep Algorithm	The article discusses the development of a scheduling theory for computations with intertask dependencies in Internet-based computing. The goal is to create schedules that allow for efficient use of clients' resources and minimize the chance of a computation stalling. Simulation studies have shown that this approach can accelerate computation over the Internet. The theory involves breaking down a computation into smaller "building-block" dags and analyzing the scheduling dependencies between them. The paper introduces the "Sweep Algorithm" which allows for scheduling of non-connected building blocks and interleaving the execution of subdags with no dependencies. This allows for optimal schedules for previously unschedulable dags. Examples are provided, including a component of a dag from a functional MRI application.
41115	4111532	Sparse Representations For Classification Of High Dimensional Multi-Sensor Geospatial Data	Modern geospatial sensing techniques capture a large amount of information, resulting in high dimensional feature spaces. This includes hyper-spectral imagery, which records the reflectance/radiance of hundreds of contiguous spectral bands, and full-waveform LiDAR, which records the entire waveform of the return laser pulse. However, despite the high dimensionality, the underlying information can often be represented in a lower dimensional subspace, making it sparsely represented in an appropriate dictionary. Sparse representation based classification has been successfully used for face recognition tasks, but its effectiveness for geospatial image analysis tasks has been limited. To improve performance, researchers have developed a subspace learning preprocessing method that effectively reduces the dimensionality of geospatial data, making it more suitable for sparse representation based classification.
41115	4111553	Ensemble Multiple Kernel Active Learning For Classification of Multisource Remote Sensing Data	Remote sensing data analysis often requires incorporating features from multiple sources to obtain diverse information. However, this can be challenging as it requires a large amount of labeled data to train robust supervised classifiers. This paper proposes an ensemble multiple kernel active learning (EnsembleMKL-AL) framework to address this issue. This framework incorporates different features from hyperspectral imagery and LiDAR data and uses an ensemble of probabilistic multiple kernel classifiers in an active learning system. A decision fusion strategy is also implemented to make a final decision based on the outputs. The results from experiments show that this approach is effective in a multisource environment and using ensemble classifiers and a large number of relevant features can further improve its performance. 
41116	4111618	Cooperative mobile guards in grids	The article discusses the concept of weakly cooperative mobile guards on grids. These guards are allowed to move along vertical and horizontal segments and can see points on the same segment or on crossing segments. The authors propose a quadratic time algorithm for finding the minimum weakly cooperative guard set in polygon-bounded and simple grids. They also present a faster algorithm for horizontally or vertically unobstructed grids. The article then explores the use of weakly cooperative mobile guards in complete rectangular grids with obstacles, showing that a minimum of k+2 guards can cover the grid as long as both dimensions are larger than the number of obstacles. The authors also prove that the problem of finding the minimum weakly cooperative guard set is NP-hard for grids with at most three crossing segments, making the minimum k-periscope guard problem for 2D grids also NP-hard.
41116	411162	Position discovery for a system of bouncing robots	This article discusses a problem in which a group of anonymous mobile robots are deployed on either a ring or a line segment. The robots must detect the presence and initial positions of all other robots, but can only do so by bouncing off of each other or the endpoints of the segment. They have no control over their movements and must rely on a clock to observe the times of their bounces. The article presents optimal position detection algorithms for feasible configurations on both the ring and segment, and provides characterizations of all infeasible configurations. For the ring, it is shown that all configurations with different initial directions are feasible, while for the segment, only symmetric configurations are infeasible. The article also proves the optimality of the proposed algorithms.
41117	41117159	High-Level Estimation Techniques For Usage In Hardware/Software Co-Design	High-level estimation techniques play a crucial role in design decisions such as hardware/software partitioning and design space explorations. Finding the right balance between accuracy and computation time is essential for the feasibility of these techniques. The paper introduces high-level estimation techniques for hardware effort and hardware/software communication time, which provide quick and sufficiently accurate results. It also demonstrates how these techniques can be used to address conflicting design goals, such as performance and hardware effort constraints. The proposed solution is a cost function for hardware/software partitioning that allows for dynamic weighting of its components. Experiments show that combining these estimation techniques leads to more effective hardware/software implementations compared to approaches that only consider single constraints.
41117	41117167	A hardware/software partitioner using a dynamically determined granularity	Computer aided hardware/software partitioning is a crucial aspect of hardware/software co-design. Traditional methods have a fixed granularity, meaning the size of the partitioning objects is predetermined. However, a new approach has been developed that allows for dynamic determination of partitioning granularity. This enables the optimization process to better adapt to application properties and changes in optimization results. Experiments using simulated annealing optimization have shown faster convergence and improved adaptability to cost function variations compared to previous methods with fixed granularity. This new approach has the potential to greatly improve the efficiency and effectiveness of hardware/software partitioning.
41118	4111853	A scheduling algorithm for optimization and early planning in high-level synthesis	The increasing complexity of embedded and programmable systems has made manual mapping of applications onto these systems a tedious task. This has led to the use of high-level synthesis in design flows. In this study, the focus is on data flow graph (DFG) scheduling, which is a crucial part of high-level synthesis. The authors propose an algorithm that schedules a chain of operations with data dependencies, using a technique from computational geometry. This allows for flexibility in optimization objectives and has shown promising results in terms of finding optimal solutions and generating latencies comparable to the optimal solution. The algorithm also allows for the incorporation of different optimization goals, as demonstrated in various examples. 
41118	4111811	A Fast Heuristic Algorithm for Multidomain Clock Skew Scheduling	Clock skew scheduling (CSS) is a technique used to minimize the clock period in sequential components by assigning a dedicated clock delay to each. However, multidomain CSS (MDCSS) groups sequential components into clusters called clock domains, each with a uniform clock delay. This is achieved using deskew buffers with programmable phase shifts. With increasing process variations, it becomes difficult to create precise clock delays for all sequential elements, making MDCSS attractive. A fast algorithm is presented in this paper to determine the minimum number of clock domains needed for MDCSS, as the exact solution becomes impractical for more than three domains. The algorithm is efficient, solving the problem in an average of 14.7 minutes for large benchmark circuits, while a commercial solver takes over 5 hours. The results show that the algorithm yields the optimal solution for most benchmarks.
41119	4111934	Using Sample Selection To Improve Accuracy And Simplicity Of Rules Extracted From Neural Networks For Credit Scoring Applications	This paper presents a method for selecting samples in credit scoring using an ensemble of neural networks. The ensemble identifies outliers by checking the classification accuracy of the networks on the original training data. These outliers are then removed from the dataset, resulting in a simpler and more accurate neural network. The remaining data is used to train and prune another network, and a rule extraction algorithm is applied to generate concise and comprehensible rules. Experimental results show that this approach results in higher predictive accuracy and simpler neural networks compared to using the original dataset. 
41119	4111926	Greedy rule generation from discrete data and its use in neural network rule extraction.	The GRG algorithm is a new method for generating classification rules from data sets with discrete attributes. It is a "greedy" algorithm that searches for the best rule at each iteration, taking into account factors such as the number of samples and subspaces covered, as well as the number of attributes in the rule. This method is used to extract rules from trained and pruned neural networks for solving classification problems. The rules are extracted using a standard approach, and experiments show that the GRG method produces accurate and concise rule sets. It has also been successfully applied to medical data sets with discrete attributes.
41120	411208	Feature-aligned T-meshes	High-order and regularly sampled surface representations are preferred over general meshes as they are more efficient and simplify geometric modeling and processing algorithms. Recent algorithms convert general meshes to regularly sampled form, but feature alignment makes it difficult to get coarse regular patch partitions. This paper suggests using quadrilateral T-meshes, where the intersection of faces may not be the whole edge or vertex, but a part of an edge. This allows for a smaller number of patches and vertices in a base domain while still maintaining feature alignment. T-meshes also have the advantages of quadrangulations, such as high-order representations and easy packing of geometric data into textures, while supporting different types of discretizations for physical simulation.
41120	411205	Computing Discrete Shape Operators On General Meshes	Discrete curvature and shape operators are vital in various applications such as simulating deformable 2D objects, variational modeling, and processing geometric data. These operators provide complete information about the directional curvatures at a point and are commonly used in representing objects using meshes. Currently, there are different approaches for formulating these operators, ranging from accurate but computationally expensive methods in engineering to efficient but less accurate methods in computer graphics. This study proposes a simple and efficient formulation for the shape operator on general meshes using normals. It has been tested and proven to produce consistent results for different types of meshes and mesh refinement.
41121	4112110	Interactive Editing of Large Point Clouds	This paper introduces a new data structure for real-time visualization and interactive editing of large point clouds. This structure allows for efficient rendering and handling of very large data sets through out-of-core storage. Unlike previous approaches, it also allows for dynamic operations such as insertion, deletion, and modification of points with minimal impact on performance. This enables real-time local editing of large models while maintaining a multi-resolution representation for visualization. The structure is demonstrated through a prototypical editing system that provides both real-time local editing tools and a two-resolution scripting mode for larger changes. The system is evaluated on synthetic and real-world examples up to 63GB in size.
41121	4112151	The Randomized Sample Tree: A Data Structure for Interactive Walk-Throughs in Externally Stored Virtual Environments	The authors propose a new data structure for rendering complex virtual environments, which can be navigated interactively and can handle large scenes that cannot fit into main memory. The approach involves using a sampling technique and building a sample tree during preprocessing, which only takes up linear space. During rendering, the tree is traversed and polygons stored in visited nodes are rendered. The system has been implemented and tested, showing comparable image quality to traditional methods regardless of scene topology. This allows for rendering of more complex scenes by instantiating objects. 
41122	4112235	Segment-based tetrahedral meshing and rendering	This article discusses a framework for handling large unstructured tetrahedral meshes in a more efficient manner. The framework is based on a multi-resolution model made up of segments, each containing thousands of tetrahedra. These segments can be adapted in real time to different viewing and classification parameters. Dependencies between segments are stored in a hierarchical graph to ensure consistency. The concept of multi-triangulations is extended to tetrahedral meshes and the segments are stored in a compressed format for efficiency. This framework allows for more interactive visualization of large meshes on standard computers.
41122	4112233	Real time compression of triangle mesh connectivity	This paper introduces a new compressed representation for triangle mesh connectivity and presents fast local compression and decompression algorithms suitable for real-time applications. The compression rates achieved are comparable to the best global compression algorithms. This has significant advantages for various applications, such as compressing meshes before storage or transmission, and a simple decompression algorithm allows for potential hardware implementation, potentially improving rendering speed for pipelined graphics hardware.
41123	4112364	On the Methodology for Calculating SFN Gain in Digital Broadcast Systems	The Single-Frequency Network (SFN) mode is an alternative to the traditional Multi-Frequency Network (MFN) for broadcast networks. SFN uses the same frequency for all transmitters, resulting in better frequency reuse and improved quality of service. However, not all areas within the service area will benefit from SFN, as some may experience degraded quality due to SFN echoes. This paper presents a methodology for calculating the SFN gain, which is a measure of potential gain or interference in SFN. The gain is investigated for a DVB-H network and can be used for coverage planning in future broadcast networks.
41123	4112317	Proposed framework for evaluating quality of experience in a mobile, testbed-oriented living lab setting	The paper introduces a framework for evaluating Quality of Experience (QoE) in a mobile, testbed-oriented Living Lab environment. This aligns with the trend towards user-centric approaches in innovation research and aims to bridge the gap between technical parameters and human experience. QoE is viewed as a multi-dimensional concept that should be studied from an interdisciplinary perspective. Previous approaches for evaluating perceived QoE have been limited and fail to capture subjective user experiences. The proposed distributed architecture monitors network Quality of Service (QoS), context information, and user experience in real-life settings, allowing for the evaluation of all relevant QoE dimensions in a mobile context.
41124	4112411	Throughput optimization of wireless LANs by surrogate model based cognitive decision making	The growth of wireless networks and limited availability of the electromagnetic spectrum have led to increased interference and decreased quality of service for users. In response, a new cognitive decision engine has been proposed to optimize the throughput of IEEE 802.11 links in the presence of interference. This engine uses a model to predict throughput based on the current state of the network, allowing it to find the best configuration for controllable parameters. In testing, the cognitive decision engine outperformed a scenario without its use, improving performance by over 100%. This solution shows promise for improving wireless network performance in the face of interference.
41124	4112475	A cognitive QoS management framework for WLANs.	This paper discusses the challenges faced by wireless networks due to their rapid growth and limited spectrum, leading to increased interference and performance constraints. To address this, a framework is proposed that uses cognitive radio techniques to manage quality of service (QoS) in wireless LANs. The framework includes a cognitive decision engine that uses radio environment maps to optimize QoS parameters such as throughput. A new physical model is developed to predict and improve the throughput of wireless terminals, which was experimentally verified. The framework was tested in both stationary and time-variant interference scenarios, resulting in significant throughput gains of up to 344%. 
41125	4112530	Transmitter Design for Uplink MIMO Systems With Antenna Correlation	The article discusses the study of uplink transmission in multiple-input multiple-output (MIMO) systems with antenna correlation. The focus is on schemes that require only channel covariance information at the transmitter (CCIT), which is cheaper than full channel state information at the transmitter (CSIT). The study starts with mutual information analysis and finds that a simple CCIT-based scheme, known as statistical water-filling (SWF), can perform similarly to the optimal full CSIT-based scheme in MIMO systems with more receive antennas than transmit antennas. The implementation of SWF in coded systems is then explored, with the use of an iterative linear minimum mean squared error (LMMSE) receiver and an extrinsic information transfer (EXIT) chart type curve matching technique. Simulation results show that the proposed scheme outperforms the conventional equal power transmission. It is also found to be efficient in multi-user uplink MIMO systems with distributed channel information.
41125	4112523	Spatially Coupled Ldpc Coding And Linear Precoding For Mimo Systems	This paper introduces a transmission scheme for a MIMO quasi-static fading channel with imperfect channel state information at the transmitter. The scheme utilizes a precoder structure and spatial coupling to improve performance. An analytical evaluation method based on EXIT functions is developed for precoder design. The paper also discusses an area property that suggests spatial coupling can help with uncertainty in CSIT, making it easier to design LDPC codes. Numerical results demonstrate the effectiveness of the proposed scheme in MIMO fading channels with imperfect CSIT.
41126	4112658	Distances in benzenoid systems: further developments	This note discusses new findings on distances in benzenoids, including an algorithm that can compute the Wiener index of a benzenoid system in O(n) time. It also introduces a dismantling scheme for benzenoid systems, derived through breadth-first search on their dual graphs. The note also addresses the clustering problem for sets of atoms in benzenoid systems, proposing an efficient implementation of the k-means clustering algorithm. 
41126	4112612	On Distances In Benzenoid Systems	The molecular graph G of a benzenoid hydrocarbon can be embedded into the Cartesian product of three trees, T-1, T-2, and T-3, based on the directions of the hexagonal grid. Each vertex in G is associated with an ordered triplet (v(1), v(2), v(3)), where v(i) is a vertex in T-i. The distance between two vertices in G can then be calculated as the sum of the respective tree-distances between their associated triplets. This labeling process can be done in O(n) time. This result can be used to develop an efficient algorithm for computing the diameter of G in O(Pt) time.
41127	4112791	Local Color Vector Binary Patterns From Multichannel Face Images for Face Recognition.	This paper introduces a new face descriptor called local color vector binary patterns (LCVBPs), which utilizes color information for face recognition. LCVBP is composed of two patterns, color norm and color angular, which capture texture patterns and spatial interactions among spectral-band images. By combining these features, LCVBP achieves high performance on five public databases and outperforms other existing face descriptors. The proposed method is also compared to state-of-the-art techniques, further demonstrating its effectiveness for challenging face images.
41127	411273	Face Feature Weighted Fusion Based on Fuzzy Membership Degree for Video Face Recognition	This paper presents a novel method for video face recognition (FR) that aims to improve accuracy by combining multiple face features from a sequence of video frames. The effectiveness of this weighted feature fusion approach is supported by a theoretical upper bound for recognition error. To determine the optimal weights for feature fusion, the authors introduce a new solution based on fuzzy membership function and quality measurement. The proposed method is evaluated on four public video databases and shows promising results for real-world FR applications. It is also noted that the method is easy to implement. 
41128	4112879	Granular Computing: A Rough Set Approach	Information granule calculi form the basis of granular computing, defined by constructs such as information granules, inclusion and closeness relations, and operations on these granules. As different information sources may have varying interpretations of granule languages, rough inclusion and closeness are used instead of equality. Examples of basic constructs are provided, and the creation of more complex information granules is described through terms. The synthesis problem of robust terms, which satisfy a given specification to a satisfactory degree, is discussed. A method for synthesizing information granules using decomposition of specifications is also presented. These issues are particularly relevant for applications involving spatial reasoning, knowledge discovery, and data mining.
41128	4112854	Information Granules: Towards Foundations Of Granular Computing	This article introduces the concept of granular computing and its basic notions, including information granule syntax and semantics, as well as inclusion and closeness relations. Two types of operations, transformation and approximation, are defined for information granules in different information sources. These operations are used to construct more complex granules from input information granules. The process of constructing information granules is described by terms. The article also discusses the problem of synthesizing robust terms, which are descriptions of information granules that meet a given specification. This is an important problem for the practical applications of granular computing, such as spatial reasoning and knowledge discovery. 
41129	4112920	Extended cubes: enhancing the cube attack by extracting low-degree non-linear equations	This paper introduces an improved version of the original cube attack, which can extract simple low-degree equations in addition to linear ones. This extended cube attack can be used in cryptosystems where the original attack may fail due to a lack of sufficient linear equations. The paper also presents a side channel cube attack on the PRESENT block cipher using the Hamming weight leakage model, which is a more relaxed assumption compared to previous work. The extended cube attack reduces the time and data complexity compared to previous attacks, making it applicable to both the 80-bit and 128-bit key variants of PRESENT.
41129	4112955	Fault analysis of the KATAN family of block ciphers	The paper investigates the vulnerability of the KATAN family of block ciphers to differential fault attacks. This family consists of three variants with different block sizes and a key length of 80 bits. The researchers assume a single-bit fault injection model where an attacker can corrupt a single random bit of the cipher's internal state, which can be repeated. They analyze the distributions of low-degree polynomial equations to determine suitable rounds for fault injections. Then, they demonstrate how to identify the specific faulty bits within the internal state by precomputing difference characteristics. The attack on KATAN32 requires 259 computations and about 115 fault injections, while for KATAN48 and KATAN64, it requires 255 computations and 211/278 fault injections respectively.
41130	41130101	Automatic Preview Frame Selection for Online Videos	In this paper, the authors discuss the importance of the preview frame in online videos and the challenges of automatically selecting the best frame. They propose a classification approach that considers three types of features - informativeness, attention, and aesthetics - in order to characterize each frame. Due to the imbalanced training data, they utilize random forests and identify visually similar frames to increase the number of positive training samples. The proposed method is evaluated on a set of news videos and shows promising results. The authors also analyze the contribution of each visual feature for future studies.
41130	41130268	Pairwise constraints based multiview features fusion for scene classification	The paper discusses a new method for scene classification that takes into account multiple features from different views to construct a low-dimensional subspace. This method considers both intraclass and interclass geometries to preserve discriminability and also incorporates user labeling to further improve performance. Experiments on real-world datasets show that this method outperforms existing methods in scene classification and can also be applied to other classification problems, such as shape classification. 
41131	4113132	Longitudinal study of a building-scale RFID ecosystem	RFID deployments are increasing in popularity in both industrial and consumer settings. However, there are challenges in managing RFID data streams and dealing with limitations in reader accuracy and coverage. Pervasive computing also presents additional issues related to user acceptance and system utility. A four-week study of a building-scale RFID deployment with 47 readers and 300 tags was conducted to better understand these challenges. The study found that the deployment produced manageable amounts of data, but with significant differences among participants and objects. Tag detection rates were low and varied among different types of tags, participants, and objects. Users need guidance for effective tag placement and are more likely to wear tags with compelling applications. Probabilistic modeling and inference techniques can help overcome data gaps and errors, but may add computational and storage overhead.41132
41131	4113120	Physical Access Control for Captured RFID Data	RFID security is a constantly evolving field with many techniques being developed to prevent unauthorized cloning and reading attacks. However, there has been limited focus on the issue of privacy for RFID data once it has been captured and stored by authorized systems. The authors address this problem by discussing peer-to-peer privacy for personal RFID data. They assume a system with trusted owners and administrators and propose an access control policy, called Physical Access Control (PAC), which limits peers' access to information about each other. PAC offers a high level of privacy and also provides a database view that enhances users' memory of places, objects, and people. This article is part of a special issue on security and privacy and presents PAC as a natural and intuitive approach to balancing privacy and utility in peer-to-peer privacy.
41132	41132133	Random sampling in residual graphs	This article discusses a new algorithm for finding the maximum flow value in an undirected graph with n vertices and m edges. The algorithm takes Õ(m+nv) time and is based on randomly sampling edges from the residual graph and finding augmenting paths. Special sampling probabilities are assigned to the edges in Õ(m) time, making the algorithm simple to implement. This approach allows for a more efficient way of finding the maximum flow in a graph compared to traditional methods. 
41132	4113234	Minimum cuts in near-linear time	A new algorithm has been developed that significantly improves the time complexity for solving the minimum cut problem on undirected graphs. By using a combination of "semiduality" and random sampling techniques, the algorithm can find a minimum cut in a graph with m edges and n vertices in O(m log3 n) time with high probability. A simpler version of the algorithm is also provided, which can find all minimum cuts in the same time complexity and has an optimal parallelization method. This is a significant improvement over the previous best time bound of O(n2 log3 n). Additionally, the algorithm has applications in determining the number of near-minimum cuts in a graph and a new data structure for efficiently representing them.
41133	41133164	Co-Utility: Self-Enforcing protocols for the mutual benefit of participants.	Protocols are rules that govern how agents interact with each other in society, and those based on mutually beneficial cooperation are particularly beneficial as they improve societal welfare without the need for a central authority. In this study, the concept of co-utility is introduced as a framework for cooperation between rational agents, where the best strategy for each agent is to help another agent achieve their best outcome. The study focuses on self-enforcing protocols in game-theoretic terms and develops the concept of co-utile protocols, exploring when co-utility can arise. The case of anonymous query submission to a web search engine is used to illustrate the design of co-utile protocols, with theoretical analysis and empirical results showing how co-utility can make cooperation self-enforcing and improve agents' welfare.
41133	41133101	Predictive protocol for the scalable identification of RFID tags through collaborative readers	RFID technology has greatly impacted the manufacturing industry, but there are still concerns about scalability, security, and privacy. The sharing of identification information among multiple parties is a particular issue, especially with the increase in outsourcing. To address this problem, a private and scalable protocol for RFID collaborative readers has been proposed. This protocol uses "next reader predictor" and "previous reader predictor" to efficiently identify tags and has a Markov-based predictor implementation. This protocol also helps in securely sharing identification information among multiple parties, which is crucial in supply chain management. Experimental results have shown that this protocol outperforms previous approaches. 
41134	4113460	Meta-level patterns for interactive knowledge capture	The current tools for acquiring knowledge have limited understanding of how users input knowledge and how it is used, resulting in a lack of assistance in organizing knowledge authoring tasks. Users must compensate for this by keeping track of their past mistakes, current status, potential new problems, and possible courses of action on their own. This paper introduces a new extension to existing knowledge acquisition tools that organizes past interactions using declarative meta-level patterns and improves suggestions based on these interactions. The focus is on assessing confidence in suggestions, suggesting how to perform knowledge authoring actions based on successful past actions, and monitoring changes in the environment to suggest modifications to the knowledge base. A preliminary study shows that this approach can reduce incorrect suggestions and prevent user mistakes, leading to improved problem solving results.
41134	411343	Memory based meta-level reasoning for interactive knowledge capture	Current knowledge acquisition tools lack the ability to track a user's process and progress when entering new knowledge, forcing users to keep track of this themselves. To address this issue, a new extension has been developed that tracks past problem solving episodes and relates them to user-entered knowledge. This allows for an assessment of the current status and progress of both the knowledge and the problem solving process, providing assistance to the user based on this assessment. This approach has been applied in the development of an intelligent assistant for decision making tasks, which has shown to improve the user's understanding of progress and guide the knowledge authoring process for better problem solving.
41135	4113538	RMS-TM: a comprehensive benchmark suite for transactional memory systems	Transactional Memory (TM) is a proposed solution for easier parallel programming on Chip Multiprocessors (CMPs) by replacing traditional lock synchronization constructs. However, the evaluation of TM designs is still lacking realistic applications and comparison against locks. The paper introduces RMS-TM, a benchmark suite consisting of seven real-world applications from the RMS domain, addressing current TM research issues and providing a mix of short and long transactions with different characteristics. This makes it a useful tool for evaluating TM designs, especially on high core counts. The evaluation with STM and HTM systems shows that RMS-TM is also scalable, making it a valuable addition to current TM benchmarks.
41135	41135117	Hybrid Transactional Memory with Pessimistic Concurrency Control	Transactional Memory (TM) aims to simplify the creation and use of shared-memory data structures in parallel software. However, many TM systems use writer-locks to protect data being modified, which can lead to inconsistent results when combined with non-transactional accesses. One solution is Pessimistic Concurrency Control, but it comes with a performance penalty due to reader-writer locking. This paper proposes a hybrid TM design that reduces this performance overhead while still offering full TM functionality, using a single code path, and avoiding the privatization problem. However, this hybrid approach can result in the loss of important properties such as starvation freedom. To address this, the paper introduces Directory Reservations, a low-cost mechanism that improves upon existing solutions for Hardware TM.
41136	4113682	A simple P-complete problem and its language-theoretic representations	The article introduces a new version of the Circuit Value Problem where each gate follows the NOR function and has one of its inputs connected to the previous gate. This problem is still considered P-complete and can be represented as a simple language over a two-letter alphabet using language equations. Three different grammars, including a conjunctive grammar, a Boolean grammar, and an LL(1) Boolean grammar, are created to represent this problem. Additionally, the problem can also be represented by a trellis automaton with 11 states and a linear conjunctive grammar with 20 rules. 
41136	4113630	One-Nonterminal Conjunctive Grammars over a Unary Alphabet	Conjunctive grammars over an alphabet Σ={a} are studied, specifically those with a unique nonterminal symbol. These grammars can be represented as equations X=ϕ(X) over sets of natural numbers, using union, intersection, and addition. It is possible to transform a grammar with multiple nonterminals into one with a single nonterminal, with a slight modification of the language. This results in a compressed membership problem for one-nonterminal conjunctive grammars over {a} that is EXPTIME-complete. The same problem for context-free grammars can be solved in NLOGSPACE, but becomes NP-complete if the grammar is compressed. The equivalence problem for these grammars is shown to be co-r.e.-complete, and both finiteness and co-finiteness are r.e.-complete. Equivalence to a fixed unary language with a regular positional notation can be decided. 
41137	4113769	Combining linear-time temporal logic with constructiveness and paraconsistency	This paper introduces two new versions of linear-time temporal logic, called IB[l] and PB[l], which are extensions of intuitionistic logic and Nelson's paraconsistent logic. These logics are designed to handle not only temporal reasoning, but also constructive and paraconsistent reasoning. The time domain in these logics is bounded by a fixed positive integer, but they are still able to derive most of the typical temporal axioms of LTL. The paper also proves completeness, cut-elimination, normalization, and decidability theorems for these logics, as well as providing sound and complete display calculi. This work serves as a theoretical basis for representing various forms of reasoning in computer science. 
41137	4113748	Inconsistency-Tolerant Bunched Implications	Paraconsistent logical systems are able to handle uncertainty and inconsistency better than non-paraconsistent systems. The logic BI, which is used for resource-sensitive reasoning, is useful for formalizing such reasoning. In this paper, a paraconsistent extension called PBI is studied, which combines both inconsistency-tolerant and resource-sensitive reasoning. A sequent calculus called SPBI is introduced for PBI, and theorems for cut-elimination and decidability are proven. Additionally, an extension of the Grothendieck topological semantics is introduced for PBI and a completeness theorem is proven with respect to this semantics. Overall, this paper presents a new paraconsistent logic that is able to effectively handle both inconsistency and resource-sensitivity.
41138	4113822	Protection of Data and Delegated Keys in Digital Distribution	Cryptography is an effective way to protect digital information from unauthorized access. However, in open networks, if an encrypted message is withdrawn by an unauthorized user, an additional mechanism is needed to convert it into a form that can only be accessed by an authorized user. This can be achieved through a proxy cryptosystem, using techniques from the ElGamal or RSA cryptosystems. A blind decryption protocol can also be used to protect privacy and ensure secure information delivery. However, there is a risk of an "oracle problem" where a decrypting person can freely compute exponentiation for any message selected by a requesting person. To address this, a transformable signature can be used, and this paper presents another solution to prevent the abuse of blind decryption protocols in information provider systems.
41138	4113867	How to Utilize the Transformability of Digital Signatures for Solving the Oracle Problem	Transformability is a feature of a digital signature that allows one valid signature to be transformed into another valid signature within the same signature scheme. While this property is typically seen as undesirable because it can lead to forgery, it can actually be beneficial in solving the oracle problem in certain cryptographic protocols. The oracle problem involves an entity receiving a message from an adversary and returning a value based on a specified procedure, potentially revealing useful information to the adversary. To prevent this, a transformable digital signature can be used in the blind decoding scheme based on the ElGamal cryptosystem. This offers users perfect untraceability, and has implications for other schemes such as blind signatures and divertible zero-knowledge interactive proofs.
41139	4113960	Icis 2007 Panel Report: Bridging Service Computing And Service Management: How Mis Contributes To Service Orientation	Service computing is a rapidly growing field in enterprise computing, as organizations strive for agility and to meet changing business needs. Major corporations are implementing initiatives to restructure their IT systems through service computing, creating new challenges and research questions. A framework is needed to align technology and management in this area. The International Conference on Information Systems 2007 held a panel on Bridging Service Computing and Service Management, where panelists discussed the importance of MIS taking a leadership role in this research. The paper highlights the viewpoints of each panelist and presents a joint perspective on bridging service computing and service management.
41139	4113933	A framework for transformation from conceptual to logical workflow models	Workflow systems are commonly used to automate business processes. To ensure efficient and accurate automation, both conceptual and logical workflow models are necessary. Conceptual models define the general business process requirements, while logical models specify the technology-specific requirements and software modules. However, transforming conceptual models into logical models can be challenging and error-prone. This paper proposes a formal approach for this transformation process, including a procedure for mapping dependencies and transforming patterns. Additionally, a validation procedure is developed to ensure consistency between the two models. Business process ontologies are used to describe both models, and a prototype system is implemented to demonstrate the feasibility of this approach. 
41140	4114015	Weighted tree automata and weighted logics	This article introduces a weighted monadic second order logic for trees using a commutative semiring as weights. It is shown that a restricted version of this logic can characterize the class of formal tree series accepted by weighted bottom-up finite state tree automata. The restriction can be lifted if the semiring is locally finite. This extends previous results for tree languages and also expands recent findings on formal power series on words to formal tree series. 
41140	4114059	A Weighted MSO Logic with Storage Behaviour and Its Büchi-Elgot-Trakhtenbrot Theorem.	A new weighted MSO-logic is introduced that allows for one outermost existential quantification over storage type behaviors. The weight structures used in this logic include semirings, bounded lattices, and average or discounted cost computations. Each formula is interpreted over finite words and translated into elements in the weight structure. It is proven that this logic is equivalent to weighted automata with storage, and it also satisfies the Buchi-Elgot-Trakhtenbrot Theorem for weighted iterated pushdown languages. The logic is decidable for bounded lattices, as long as their infimum can be computed.
41141	4114146	Analysing GeoPath diversity and improving routing performance in optical networks	This paper discusses the increasing vulnerability of telecommunication networks to cascading and regional-correlated challenges, such as natural disasters and intentional attacks. The authors propose a network vulnerability identification mechanism and a new graph resilience metric called cTGGD, which incorporates geographical diversity to measure the resiliency of optical fibre networks. They also present two heuristics for solving the path geodiverse problem and propose the GeoDivRP routing protocol, which outperforms OSPF when faced with area-based challenges. The paper also analyses potential attacker strategies and demonstrates the effectiveness of restoration plans. 
41141	4114171	Geodiverse routing with path delay and skew requirement under area‐based challenges	This article discusses the growing vulnerability of communication networks to both natural disasters and intentional attacks, and how these can lead to cascading and regionally correlated challenges. To address this issue, the authors propose extending the GeoDivRP routing protocol to consider delay-skew requirements in the face of these challenges. They present a flow-diverse minimum-cost routing multicommodity flow problem and a nonlinear delay-skew optimization problem to balance between delay and traffic skew on paths. The effectiveness of this approach is demonstrated through implementation in ns-3 and compared to other routing protocols. This solution allows for satisfying traffic demands while also guaranteeing delay-skew constraints in telecommunication networks. 
41142	4114234	Summarizing transactional databases with overlapped hyperrectangles	Transactional data, which refers to data that records individual transactions, are found everywhere. To analyze these databases, various methods have been proposed, such as frequent itemset mining and co-clustering. This research presents a new problem of succinctly summarizing transactional databases by linking the database's structure to numerous frequent itemsets. The problem is formulated as a set covering problem using overlapped hyperrectangles, which has been proven to be NP-hard and has a connection to the compact representation of a directed bipartite graph. An approximation algorithm, Hyper, is developed to solve this problem in polynomial time with a logarithmic approximation ratio. A pruning strategy is also proposed to speed up the algorithm's processing, and an efficient algorithm, Hyper+, is introduced to further summarize the hyperrectangles by allowing false positive conditions. The generated hyperrectangles can be properly visualized and have been proven effective and efficient in summarizing both real and synthetic datasets. 
41142	4114261	Overlapping Matrix Pattern Visualization: A Hypergraph Approach	This study focuses on a visual data mining problem where researchers aim to find the best way to display overlapping submatrices of interest in a data matrix and their relationships. The problem is converted to the hypergraph ordering problem, which is a more generalized version of the traditional minimal linear arrangement problem. The researchers prove the NP-hardness of this problem and propose an iterative algorithm that utilizes a graph ordering algorithm to solve it. This algorithm ensures convergence to a local minimum and has been tested on publicly available datasets, showing its effectiveness and efficiency.
41143	4114373	Design and implementation of a new 3-DOF electromagnetic micropositioner utilizing flexure mechanism	The paper introduces a compact micropositioner with three degrees-of-freedom (DOFs) and large travel ranges. It utilizes a monolithic parallel flexure mechanism with electromagnetic actuators and optical sensors for precise 3-DOF motion. The system is controlled by an adaptive sliding-mode controller that consists of a sliding mode controller, adaptive law, and force allocation. Experimental results show that the system has satisfactory stiffness and precision.
41143	41143135	A New Design Of 3-Dof Flexure-Mechanism Positioner With Electromagnetic Technology	This paper presents a novel, compact and precise positioning system with three degrees of freedom (DOF) and submicrometer-scale precision. The system uses a monolithic parallel flexure mechanism with built-in electromagnetic actuators and eddy-current sensors to achieve 3-DOF motion. It has a travel range of 1mmx1mm and a position resolution of 300 nm. To improve stability, an adaptive sliding-mode controller is proposed. The system's stiffness and precision are demonstrated through theoretical analysis and experimental results. Overall, this system offers a compact and precise solution for positioning with a wide range of applications.
41144	4114431	Prediction-as-a-Service for Meme Popularity	Memes are ideas, behaviors, or styles that spread among people in a community. Recent research has shown that memes follow epidemic patterns on microblogs and their popularity can be predicted. Some models consider the detailed structure of community graphs to accurately predict meme spread, while others sacrifice accuracy for efficiency by assuming equal connectivity within a community. This paper proposes a prediction service that combines the benefits of both approaches. It considers community graph properties without the complexity of their structural details, resulting in more efficient online consumption. The service identifies memes that have the potential for widespread coverage by comparing their virality to a community threshold. It also takes into account sampling biases to make reliable thresholds and accurate predictions, as demonstrated in experiments.
41144	4114456	Service Selection Model for Multimedia Service Workflow	In SOA, quality of service (QoS) is an important factor in selecting services for a workflow. Different attributes such as availability, reliability, performance, and reputation are used as criteria for QoS. However, in the domain of multimedia services, traditional QoS models may not accurately capture the unique characteristics of these services. For instance, video streaming may not require the fastest delivery network, but it does have specific data rate requirements. To address this issue, a new model is proposed in this paper for multimedia service selection. The model is supported by both theory and simulation studies, which demonstrate that considering quality requirements is crucial for selecting the best multimedia service.
41145	411455	Mobile data gathering with Wireless Energy Replenishment in rechargeable sensor networks	The advancement of wireless energy transfer technology has allowed for the charging of sensor batteries in a wireless sensor network (WSN), enabling perpetual operation of the network. This has opened up new possibilities for designing sensor network protocols. However, the variability of energy recharging rates in wireless rechargeable sensor networks poses a challenge in achieving optimal data gathering strategies. To address this, a joint framework of wireless energy replenishment and mobile data gathering is proposed in this paper. The framework considers different sources of energy consumption and the time-varying nature of energy replenishment. The proposed distributed algorithm includes subalgorithms for each sensor node and the mobile collector, and is evaluated through numerical simulations. The results demonstrate the convergence of the algorithm and the impact of utility weight on network performance.
41145	4114532	Joint Subcarrier Pairing and Power Allocation in OFDMA Cooperative Relay Networks	This paper presents a new approach called the Joint Subcarrier Pairing and Power Allocation (JS2PA) scheme for cooperative relay networks using Intelligent Water Drop (IWD) optimization. The scheme comprises of a subcarrier pairing and selection algorithm and a power allocation algorithm. The problem is formulated as a mixed integer programming problem to maximize network utility while satisfying power and fairness constraints. The proposed scheme is solved using a subcarrier pairing algorithm based on Hungarian method and a power allocation algorithm using the IWD method. Simulation results show that the JS2PA scheme outperforms existing methods in terms of convergence and network utility. 
41146	4114631	Simple dynamics for plurality consensus	The Plurality Consensus process is studied in a communication network where n anonymous agents support an initial opinion and can revise it based on a random sample of neighbors. The goal is for all agents to converge to the stable configuration where the majority color is supported. The initial color configuration has a large bias, with the plurality color being supported by a higher number of nodes. The process is based on a basic model with a clique network and a majority rule update (3-majority dynamics) where each agent looks at the colors of three random neighbors. The convergence time is proven to be Θklog n for various values of k and n, which is a linear relationship. It is shown that looking at more than three random neighbors does not significantly speed up the process, with samples of polylogarithmic size only having a polylogarithmic impact on the speed. This is in contrast to the median process, which has a much faster convergence time. 
41146	4114654	Stabilizing Consensus with Many Opinions	The distributed consensus problem involves a complete communication network with n nodes, each holding an opinion from a set Σ. The goal is for all or almost all nodes to reach a consensus opinion that is valid (one of the initial opinions) even in the presence of a malicious adversary. The 3-majority dynamics involves each node pulling opinions from three random neighbors and adopting the majority one. If the number of valid opinions (k) is less than or equal to nα, where α is a positive constant, the 3-majority dynamics can converge in a polynomial time with high probability, even with an adversary affecting a limited number of nodes. This is an improvement over previous protocols which only worked for |Σ| = 2 and did not handle adversarial errors. 
41147	4114743	Greedily Improving Our Own Centrality in A Network	The closeness and betweenness centralities are commonly used to measure the importance of a vertex in a complex network. High values of these centralities can have positive effects on the vertex. This paper focuses on the problem of increasing a vertex's centrality by creating a limited number of new edges connected to it. It is proven that this problem is difficult to solve efficiently unless the P=NP conjecture is true. A greedy approximation algorithm with a nearly optimal ratio is proposed and tested on simulated and real networks.
41147	4114731	Greedily Improving Our Own Closeness Centrality in a Network.	The closeness centrality is a commonly used measure to determine the importance of a vertex in a complex network. This paper focuses on the optimization problem of increasing a vertex's centrality by adding a limited number of new edges. The study covers both undirected and directed graphs and proves that this optimization problem cannot be solved efficiently unless P &equals; NP. A greedy approximation algorithm with a high accuracy ratio is proposed and tested on various graphs and networks to evaluate its performance. 
41148	4114814	Packing cycles in undirected graphs	The paper discusses the problem of finding the largest collection of edge-disjoint cycles in an undirected graph, known as CYCLE PACKING, which has practical applications in computational biology. The complexity and approximability of this problem is studied, and it is shown to be APX-hard but can be approximated within O(log n) by using a simple greedy approach. The authors also provide a non-trivial example where the greedy approach achieves a ratio of Ω(√logn/(loglogn)). For graphs with a certain level of sparsity, an approximation ratio of 2t/3 can be achieved in polynomial time. A linear programming relaxation for the problem is also discussed.
41148	4114852	Fast primal-dual distributed algorithms for scheduling and matching problems	This paper presents efficient distributed algorithms for solving scheduling and matching problems with approximation guarantees. The algorithms have a poly-logarithmic number of communication rounds and are within a constant factor of the optimal solution. The scheduling problem involves a bipartite graph with computing agents and resources, where jobs with specific lengths and profits need to be completed within a given time-window. The goal is to maximize profit while ensuring resource usage does not exceed one at any given time. The algorithm is then adapted for the weighted b-matching problem, with a randomized algorithm achieving an approximation guarantee of $${\frac{1}{6+\epsilon}}$$ and a deterministic algorithm for weighted matching with the same guarantee. These algorithms are the first of their kind to have poly-logarithmic running time and are based on primal-dual algorithms. 
41149	4114946	Texture analysis using feature-based pairwise interaction maps	Pairwise pixel interactions have been shown to be effective in both feature-based and model-based texture analysis. The feature-based interaction map (FBIM) approach has been successfully applied in various applications and has been discussed in different aspects, but there is no complete description of the method available. This paper offers a comprehensive overview of the FBIM method, including all major algorithms and systematic experimental studies to showcase its capabilities. This serves as an up-to-date survey of the approach and its potential in texture analysis.
41149	4114921	Pattern Orientation and Texture Symmetry	 a richer variety of anisotropic structures.Human texture perception is influenced by fundamental properties like symmetry and directionality. The study explores a new method for assessing directionality through texture symmetry. The method uses a spatial gray-level difference feature to generate an interaction symmetry map, which reflects both short and long-range pixel interactions. This map can accurately assess pattern orientation by identifying the characteristic axes of symmetry. The results of the study show that texture symmetry is closely related to perceived orientation. This method is compared to the traditional filtering approach and is found to be more comprehensive as it considers all components of anisotropy. Applications for this research include flow analysis, image database retrieval, and texture classification. 
41150	4115045	Parallel scheduling for cyber-physical systems: Analysis and case study on a self-driving car	The increasing complexity of software for Cyber-Physical Systems (CPS) has led developers to turn to multi-core processors and parallel programming models like OpenNP to ensure timely execution. In this paper, the authors propose a real-time scheduling method for parallel tasks on multi-core processors, specifically in CPS like self-driving cars. They extend the fork-join parallel task model and develop a task stretch* transform to efficiently schedule tasks with varying numbers of parallel threads. The proposed algorithm, implemented on Linux/RK and Boss (winner of the 2007 DARPA Urban Challenge), shows promising results with a resource augmentation bound of 3.73, meaning tasks can be completed faster on fewer processors. The scheme is evaluated on Boss through its driving quality, including curvature and velocity profiles.
41150	4115037	Scalable QoS-based resource allocation in hierarchical networked environment	The paper examines the issue of allocating bandwidth to multiple traffic flows in a large-scale network. It uses the Q-RAM model, where each flow is assigned a utility based on its allocated bandwidth. The aim is to maximize the overall utility of the network. However, the problem is challenging due to the NP-hard nature and the need to select paths between source-destination pairs. To address this, the paper proposes a hierarchical decomposition approach, dividing the network into subnets and making resource allocation decisions on a subnet-by-subnet basis. A distributed transaction scheme ensures consistency across subnets. The scheme is shown to be scalable and effective through both analytical and experimental evidence. 
41151	411517	Deadline-Based Differentiation in P2P Streaming	The paper discusses the benefits of splitting a P2P video distribution into multiple media flows with varying priorities. This approach allows for the development of flexible and adaptive streaming systems, suitable for both VoD and TV. It also minimizes network resource usage by discarding low-priority flows when receivers do not have enough resources. The paper focuses on chunk-based video distribution in unstructured meshes, using a push strategy and a deadline-based scheduling algorithm to prioritize flows and avoid sending duplicate chunks. The results of experiments demonstrate improved streaming performance and PSNR measures compared to strict priority and single stream distribution.
41151	411510	Deadline scheduling in the Linux kernel	In recent years, there has been a growing interest in using Linux for real-time systems, particularly in industrial control. Its simple design and open-source license make it reliable and high-performing, with the ability to be modified according to user needs. However, Linux was not originally designed for real-time use, leading to issues such as unpredictable delays and limited real-time scheduling support. To address this, a real-time scheduler based on the Resource Reservation paradigm has been developed and included in the Linux kernel. This paper discusses the development process, challenges faced, implementation details, and API for programmers. Experimental results on real hardware are also presented.
41152	41152224	Feature deduction and ensemble design of intrusion detection systems	This study aims to identify important input features for building a computationally efficient and effective intrusion detection system (IDS). Two feature selection algorithms, Bayesian networks (BN) and Classification and Regression Trees (CART), were evaluated, as well as an ensemble of the two. Results showed that significant input feature selection is crucial for designing a lightweight and effective IDS for real-world systems. The study also proposes a hybrid architecture for combining different feature selection algorithms in IDS. Overall, the study highlights the importance of feature selection in creating a successful IDS for detecting intrusion and misuse patterns.
41152	41152121	Matrix Factorization Approach for Feature Deduction and Design of Intrusion Detection Systems	Intrusion Detection Systems (IDS) analyze various data features to identify potential intrusion or misuse patterns. However, some of these features may be unnecessary or have little impact on the detection process. This study aims to determine which input features are most important in creating a computationally efficient and effective IDS. The research proposes a new matrix factorization method for feature deduction and IDS design. Experiment results show that this approach is successful in reducing the number of features while maintaining the system's efficiency. 
41153	4115349	Evolutionary Design of Intrusion Detection Programs	Intrusion detection is the process of monitoring a computer system or network for signs of unauthorized access or attacks. This paper proposes the development of an Intrusion Detection Program (IDP) that uses genetic programming techniques to detect known attack patterns. The IDP acts as a last line of defense, working alongside other preventive measures. Three genetic programming techniques - Linear Genetic Programming (LGP), Multi-Expression Programming (MEP), and Gene Expression Programming (GEP) - were evaluated for designing the IDP. Results showed that genetic programming techniques can be effective in creating lightweight and accurate IDPs, making them a viable alternative to traditional intrusion detection systems based on machine learning. 
41153	4115322	Genetic programming approach for fault modeling of electronic hardware	This paper introduces two genetic programming (GP) approaches for monitoring the performance of electronic circuits and systems in real-time. The stressor-susceptibility interaction model is used for reliability modeling, where a circuit or system is considered failed if the stressor surpasses the susceptibility limits. Validated stressor vectors can be obtained through measurements or sensors and are then inputted into the GP models after preprocessing. The results of the GP models are compared to those of neural networks trained with the backpropagation algorithm. The experiment results are also compared to the actual failure model values, demonstrating the effectiveness of the GP method for fault monitoring. This study suggests that GP could be a valuable tool for future performance monitoring systems.
41154	4115456	CompChall: Addressing Password Guessing Attacks	Passwords are a common way to authenticate users, but they also make them vulnerable to dictionary attacks. These attacks come in two forms: offline, which requires eavesdropping on communication, and online, which can be done by anyone. While public key cryptography can protect against offline attacks, there is no effective solution for online attacks. To address this issue, a new authentication protocol called CompChall is proposed. It uses one-way hash functions and a challenge-response system to prevent online dictionary attacks. The system is designed to be easy for legitimate users, but difficult for adversaries attempting to launch multiple login requests. The protocol is also stateless, making it less susceptible to DoS attacks.
41154	4115434	A Heuristic Reputation Based System to Detect Spam activities in a Social Networking Platform, HRSSSNP	The rise of social networking has greatly changed the way people interact, with many positive effects. However, it has also brought about serious threats, as a significant number of crimes and malicious activities originate from these platforms. About one-third of all internet crimes are initiated through social networking websites. Spam messages are also a major concern, causing unnecessary traffic and potentially impacting the user base of these platforms. To address these issues, a new approach has been proposed which uses a social network as a weighted graph, with each node representing a user and their interactions with others stored in a localized data-set. The weights on the edges indicate the trust relationship between profiles and can be used to detect compromised nodes engaging in spam or malicious activities.
41155	4115533	Toward a distributed benchmarking tool for biometrics	Computer science research is constantly evolving, making it necessary to efficiently test new algorithms on a significant benchmark. This requires a powerful computation capability, which can be achieved through distributed computing in research laboratories. In the field of biometric systems, numerous computations on various data are needed, typically images. To simplify this task, the paper proposes a software-based solution that uses a server to distribute computation tasks among available clients. The results of experiments using this software demonstrate its effectiveness.
41155	411554	Unconstrained keystroke dynamics authentication with shared secret	Keystroke dynamics authentication systems have gained popularity due to their user acceptance and cost-effectiveness. However, previous methods have required a large amount of data for enrollment, making them impractical for industrial use. In this study, a new method using Support Vector Machine (SVM) learning is proposed, which only requires a few samples per user for enrollment and can authenticate users through a shared secret. The method was tested on a database of 100 users and compared to six other methods. Results showed that the proposed method outperformed the others in an industrial context with a lower Equal Error Rate and faster computation time.
41156	4115660	A signal-flow-graph approach to on-line gradient calculation.	The article discusses how signal flow graphs (SFGs) can effectively represent nonlinear dynamic adaptive systems, such as dynamic recurrent neural networks. These systems are described as a general connection of simple components, similar to an electrical circuit. While SFGs are commonly used for qualitative descriptions in the neural network community, this article demonstrates their potential for rigorous representation and computational purposes. The article presents a method for computing the system's output or cost function with respect to its parameters, using the SFG representation theory and its properties. This method can be applied to various types of dynamic systems, including feedforward, time-delay, and recurrent neural networks. The article also highlights the importance of on-line learning for real applications, such as digital signal processing and system identification and control.
41156	4115635	A General Approach to Gradient Based Learning in Multirate Systems and Neural Networks	Signal-Flow-Graphs (SFGs) are an effective way to represent non-linear dynamic adaptive systems, including dynamic recurrent neural networks. These graphs describe complex systems as a connection of simple components, similar to electrical circuits. While popular in the neural network community, they are often used for qualitative descriptions rather than rigorous representation. Recently, an algorithm was proposed to estimate the derivative of the output with respect to an internal parameter for discrete-time systems. This approach has now been extended to multirate digital systems, commonly used today. The new method allows for gradient-based learning of general multirate circuits, including new "multirate" neural networks.
41157	4115725	Improving manual reviews in function-centered engineering of embedded systems using a dedicated review model	Model-based engineering of embedded systems requires manual validation activities, such as reviews and inspections, to ensure that the system meets the intended goals of stakeholders. Changes in stakeholder intentions often require revisions of previously developed and documented engineering artifacts, which are not always properly documented or consistently incorporated. Manual reviews are typically used in industry to ensure that stakeholder intentions are adequately considered in these artifacts. This article introduces a review model specifically for behavioral requirements and functional design specifications, which has been shown to significantly increase the effectiveness and efficiency of manual reviews in controlled experiments. The use of this review model also leads to more confident decisions and is perceived as more supportive by reviewers. 
41157	4115756	Dynamic Consistency Checking of Domain Requirements in Product Line Engineering	The domain requirements specification (DRS) for a product line includes the common and variable requirements for all products in that line. However, due to the variability of the product line, the DRS may contain conflicting requirements. To ensure consistency, the DRS must incorporate information about variability and avoid including contradictory requirements in the same product. In this paper, a new technique is presented for checking the consistency of dynamic properties in the DRS using model checking. The correctness of this technique is proven, and it has been applied to an industrial example. 
41158	4115891	Analysis And Optimization Of Message Acceptance Filter Configurations For Controller Area Network (Can)	Due to cost pressures, processors used in automotive ECUs have limited resources such as low clock speeds and limited memory. CAN is used to connect these ECUs, but the broadcast nature of CAN can cause unnecessary processing load on receiving nodes. To reduce this load, hardware filters can be used to filter out irrelevant messages based on message IDs. However, selecting the best filter configuration to minimize load is a complex problem and is NP-complete. This paper proposes using Simulated Annealing to find near-optimal filter configurations, specifically for cases where there are more desired messages than available filters.
41158	4115853	On Extensible Networks for Embedded Systems	When creating a distributed computing-system, the communication networks play a crucial role in its performance. While minimizing bandwidth-consumption is a common approach, other important objectives such as maintainability, extensibility, and robustness are often overlooked in the literature. This work provides a design-methodology that effectively balances these conflicting objectives. It involves using heuristics to create an initial network configuration and then using optimization strategies to refine it, ensuring that the network can handle future communication demands in addition to the current ones. Results from an automotive case-study demonstrate that this approach can significantly improve extensibility (up to 44%) with only a small decrease in bandwidth efficiency (1%). 
41159	4115958	Design of a novel finger haptic interface for contact and orientation display.	The article discusses a new portable device that allows for haptic interaction with virtual environments. It is a lightweight fingertip interface that provides cutaneous feedback and displays contact and non-contact transitions in immersive virtual environments. The device is mounted on a kinesthetic haptic interface and has a described kinematics, mechanical design, and control system. Tests have shown that the device performs well in tasks involving shape exploration and is comparable to a kinesthetic haptic interface. This new device has the potential to enhance the experience of virtual reality by providing both cutaneous and kinesthetic feedback.
41159	4115963	Reactive robot system using a haptic interface: an active interaction to transfer skills from the robot to unskilled persons	This paper discusses the reactive robot system (RRS), a new approach to human-robot interactions that utilizes haptic interfaces to transfer skills from robots to unskilled individuals. The RRS has two levels of interaction: the first level follows pre-defined dynamic rules, while the second level adds a degree of intelligence and adapts to user preferences. The paper focuses on the implementation of the second level and its ability to assist users during the autonomous stage of learning. Experiments were conducted, specifically with Japanese handwriting, and the results showed that the RRS was able to provide assistance without hindering user performance.
41160	411609	GyroPen: Gyroscopes for Pen-Input With Mobile Phones	GyroPen is a method that uses standard built-in sensors in smartphones to reconstruct the motion path of a pen-like interaction. It focuses on the angular trajectory of the phone's corner touching a surface, using data from the gyroscopes and accelerometers. This eliminates the need for accurate 3-D position estimation, which can be challenging with low-cost accelerometers. The method is connected to a handwriting recognition system and has been proven effective in two experiments, with novice participants achieving an average time of 37 seconds to write their first word and experienced users writing at a speed of 3-4 seconds per English word with an 18% character error rate. This suggests that GyroPen has potential for text entry.
41160	4116019	Measuring the Objectness of Image Windows	The authors introduce a new objectness measure that quantifies the likelihood of an image window containing an object. This measure is trained to distinguish well-defined objects from background elements and incorporates multiple image cues, including a new cue for measuring closed boundaries. Experiments on a dataset show the new cue outperforming a state-of-the-art saliency measure and the combined objectness measure outperforming individual cues. The measure can also be used to improve object detection by reducing the number of windows evaluated and reducing false positives. It has potential applications in weakly supervised learning, pixelwise segmentation, and object tracking. The computation of objectness is efficient, taking only 4 seconds per image.
41161	4116165	We Don’t Need No Bounding-Boxes: Training Object Class Detectors Using Only Human Verification	Training object class detectors usually requires a large set of images with manually drawn bounding-box annotations, which is time consuming. A new approach is proposed where annotators only need to verify automatically generated bounding-boxes. The process involves re-training the detector, re-localizing objects, and human verification. The verification signal is used to improve re-training and reduce the search space for re-localization. This method is different from traditional weakly supervised methods. Experiments on PASCAL VOC 2007 show that this approach leads to rapid production of high-quality annotations, comparable to fully supervised training. Additionally, it reduces total annotation time by 6-9 times due to the quick verification task.
41161	4116125	Efficient Object Annotation via Speaking and Pointing.	Deep neural networks have greatly improved visual recognition, but they require large annotated datasets which can be time-consuming to create. Currently, datasets are annotated in two stages: determining the presence of object classes in images and marking their spatial extent. This article proposes using speech and mouse inputs to speed up this process. Annotators can now indicate object class presence through speech, and simultaneously draw bounding boxes and provide class labels with their mouse and speech. This approach is faster and more efficient than relying on mouse inputs alone, resulting in high-quality annotations at a significantly faster rate. Experiments on COCO and ILSVRC datasets show that this approach is 2.3-14.9 times faster than existing methods and annotating class labels at the same time as bounding boxes comes at no additional cost. Overall, this method is 1.9 times faster than the traditional two-stage approach. 
41162	411626	Software Development Effort Estimation Using Fuzzy Logic - A Survey	Effort estimation is a crucial aspect of project management, especially in software development. It involves predicting the amount of time and resources required to complete a project. This estimation is typically done at the beginning of the project, when the problem to be solved is still unknown. However, accurately estimating effort is a challenging task and can greatly impact the success of a project. To address this issue, fuzzy logic, which mimics the imprecise decision-making processes of the human brain, has been used in some software estimation models. This survey aims to analyze the use of fuzzy logic in existing models and provide a comprehensive review of software estimation techniques used in both industry and literature, including their strengths and weaknesses.
41162	4116292	A real-time path planning approach without the computation of Cspace obstacles	The concept of shrinking a robot to a point and expanding obstacles in the workspace to create Configuration Space (Cspace) obstacles has been widely used in robot path planning. However, it is a difficult task when dealing with non-polygonal objects or allowing for rotation, especially in 3D environments. To address this issue, a new real-time approach that does not require the calculation of Cspace obstacles has been proposed. This approach uses inequality and optimization techniques and has been shown to be effective through simulations. It is the first of its kind in the robotic community and provides a more efficient path planning solution.
41163	41163279	Capture, Management, and Utilization of Lifecycle Information for Learning Resources	The lifecycle of learning resources involves various processes such as creation, usage, provision, and reuse. To ensure reusability, learning resources often need to be adapted for different contexts, resulting in multiple re-authoring processes. These processes generate different types of information that can be useful for future retrieval, authoring, and use of learning resources. This paper examines the lifecycle of learning resources and the information that is generated, and proposes a distributed architecture to capture, process, manage, and utilize this information in a generic manner. The framework has been implemented in three steps and initial evaluations have shown promising results. 
41163	41163151	Context-aware Communication Services: A Framework for Building Enhanced IP Telephony Services	Communication plays a crucial role in our daily lives and various devices allow us to communicate anytime and anywhere. However, this constant availability can also be a drawback as callers expect to always reach the person they are trying to contact. To address this issue, the ability to restrict availability for communication has become a necessary feature, with users demanding efficient filtering mechanisms to control incoming calls based on their current context. This paper explores the use of context information to enhance existing SIP call control and Call Processing Language (CPL) services, creating more user-centric communication services. These solutions have been implemented and evaluated using location information as the primary source of context. Different indoor location systems have been assessed, and two service types have been chosen for evaluation as representatives of the various service creation approaches in a SIP environment.
41164	4116416	Predicting The Existence Of Design Patterns Based On Semantics And Metrics	The identification of design patterns is an important aspect of the reengineering process, as it provides valuable information for designers. However, existing pattern detection methods often have limitations, such as only detecting exact pattern instances or lacking guidelines on which pattern to search for first. To address these issues, a new approach is proposed which involves a preliminary "sniffing" step to detect potential patterns and rank them based on their similarity to design elements. This approach uses design metrics to evaluate the structure and meaning of different patterns. By optimizing the pattern detection process, this approach aims to improve the understanding of existing designs and facilitate code and design improvements. 
41164	4116442	Evaluation Of An Automated Multi-Phase Approach For Patterns Discovery	Design patterns are a valuable tool for designers, as they provide reusable solutions to common design problems and can lead to higher quality designs and faster development. However, understanding and choosing the right patterns for a specific application can be challenging. One way to benefit from design patterns is to assist inexperienced designers in identifying patterns during the design process. This approach allows for variations in the design, but not all variations are acceptable. Some may result in non-optimal instantiations, known as "spoiled patterns," which need to be detected and transformed. A new approach called MAPeD (Multi-phase Approach for Pattern Discovery) uses XML information retrieval to identify and assist with design/spoiled pattern detection. The approach has been evaluated and shown to have a 9.98% improvement in precision compared to other approaches.
41165	411655	On being cool: exploring interaction design for teenagers	This paper discusses a series of studies that examined the concept of 'cool' in relation to designing interactive products for teenagers. The studies confirmed a three-dimensional model of cool, which includes having, doing, and being cool. The methods used in the studies were effective in exploring this model and providing insights for product design. The paper also presents key aspects for designing cool products and provides an example of their application. It highlights the complexity of cool as a design space and offers insight on how to avoid designing products that are considered uncool. Overall, the studies demonstrate the relevance and importance of understanding cool in designing interactive products for teenagers.
41165	4116512	CASAM: collaborative human-machine annotation of multimedia	The CASAM multimedia annotation system allows for collaboration between a human annotator and automated components. This allows for efficient use of each party's unique skills, with the system focusing on areas where automation is most effective and the user working on areas where their skills are needed. The system's reasoning is influenced by the user's annotations and the user can also modify and direct the system's work. The system provides a window for the user to view the current state of annotation and generates requests for information to improve the annotation process. This system aims to improve the speed and quality of annotations, especially in time-constrained situations. The paper describes the prototype system, techniques used for analysis and reasoning, and presents results of evaluations with media professionals.
41166	4116611	Exploiting Local Knowledge to Enhance Energy-Efficient Geographic Routing	Geographic routing is a popular method for routing information in large wireless sensor networks. It uses a greedy forwarding strategy where a sensor node chooses the most promising neighbor closer to the destination. Energy efficiency is crucial in these networks due to limited battery life. In this paper, a new algorithm called Locally-Optimal Source Routing (LOSR) is introduced. Unlike existing algorithms, LOSR considers all nodes in the neighborhood to find a locally optimal path towards the next hop. Source routing is then used to guide data packets along this path. Simulation results demonstrate that LOSR outperforms existing solutions in terms of energy efficiency in various network scenarios.
41166	4116621	Energy-Efficient face routing on the virtual spanner	Geographic routing protocols are commonly used in sensor networks and have two modes of operation: greedy routing and face routing. Face routing requires a planar graph, which means some links may not be used. This paper introduces a new localized scheme that creates a virtual planar spanner to enable efficient face routing without eliminating any links. This spanner is easy to build and uses only local information, making it scalable for large networks. It is used as a routing anchor for face mode, while real nodes are used for routing. Simulation results show that this scheme outperforms other energy-efficient protocols for various network densities and energy models.
41167	411679	Robust stability of interval bidirectional associative memory neural network with time delays.	The paper introduces a new model, called the interval dynamic BAM (IDBAM) model, which combines the conventional bidirectional associative memory (BAM) neural network with signal transmission delay. The IDBAM model is able to study the effects of deviations in network parameters and external perturbations on the network's behavior. The paper presents several different Lyapunov functionals and uses the Razumikhin technique to derive sufficient conditions for unique equilibrium and robust stability of the IDBAM model. The paper also extends its investigation to the time-varying delay case and derives robust stability criteria for BAM with perturbations of time-varying delays. The analysis method used in the paper can consider various types of activation functions, making it applicable to a wide range of BAM neural networks. The authors believe that the results obtained in this paper have significant implications for the design and application of BAM neural networks.
41167	4116719	Stability Analysis For Delayed Cellular Neural Networks Based On Linear Matrix Inequality Approach	The article discusses the conditions for the asymptotic stability of cellular neural networks with time delay. The authors use the Lyapunov-Krasovskii stability theory and the linear matrix inequality approach to derive these conditions, and show that they can be used to refine and generalize existing results. The stability criteria obtained are delay-independent, less conservative and restrictive, and provide a more comprehensive set of criteria for determining the stability of delayed cellular neural networks. This research expands on previous work in the field and offers a more accurate and flexible approach to analyzing the stability of these networks.
41168	4116838	Visualizing lifelog data for different interaction platforms	The use of interactive platforms and devices in people's daily lives is rapidly increasing, with new applications and services constantly emerging. This has led to the need for considering affordances and contexts for these devices when designing visualization and interaction strategies for lifelogging. As the activity becomes more ubiquitous, we will be interacting with a diverse set of devices. In this paper, the authors describe their project where they created interactive visualizations and usage scenarios for lifelogging on three different devices: smartphones, tablets, and desktops. Each device was utilized to maximize its unique interaction capabilities, resulting in three distinct lifelog data usage scenarios.
41168	4116851	Designing novel applications inspired by emerging media technologies	Human-Computer Interaction is an important field that helps design computer systems by gathering information on end-users and their usage context. However, little is known about designing for a completely new application without any existing user base or practice. While the current HCI methodology focuses on understanding user needs and establishing requirements, this can limit the exploration of new and innovative design ideas. This paper argues that instead of starting with defining user requirements, designers should focus on leveraging emerging technologies and design knowledge to create novel applications. The authors propose a pragmatic design methodology based on their experience in designing applications inspired by emerging media technologies. 
41169	41169147	Extraction of events and temporal expressions from clinical narratives.	This paper discusses the task of event and timex extraction from clinical narratives, specifically in the context of the i2b2 2012 challenge. While previous approaches have used multi-class classifiers to identify event types, this paper introduces a sentence-level inference strategy that considers the relationships between events. The authors also propose novel features, such as clinical descriptors from medical ontologies, to improve the accuracy of the extraction process. For timex extraction, the authors adapt a state-of-the-art system and develop rules to complement it. Overall, the system achieved a high F1 score for both event and timex extraction. The paper also includes a detailed error analysis and suggests ways to further improve accuracy.
41169	41169199	Joint inference for event timeline construction	This paper discusses creating a timeline of events from a news article. A new method is proposed, using time intervals to represent the temporal structure. An algorithm is then introduced that combines local classifiers and global constraints to optimize the temporal structure. The paper also explores incorporating event coreference knowledge to enhance the system's performance. Results from experiments show that the joint inference model outperformed local classifiers by 9.2% in F1. It is also suggested that accurate event coreference can greatly improve the overall system for constructing event timelines.
41170	4117027	Towards situated speech understanding: visual context priming of language models	Fuse is a situated spoken language understanding system that combines visual context and speech interpretation to accurately identify objects in a scene. By fusing knowledge of visual semantics and the specific contents of a scene, the system can anticipate various ways a person might describe an object and use this information to improve speech recognition. A dynamic visual attention mechanism is also used to focus processing on likely objects in the scene as spoken utterances are processed. This integration of visual context in speech recognition has been shown to significantly improve accuracy in evaluations and has potential applications in mobile and assistive technologies. 
41170	4117039	A visual context-aware multimodal system for spoken language processing	Psycholinguistic experiments have revealed that visual context can influence the processing of speech in real-time, and vice versa. In response to these findings, a multimodal system has been developed that integrates visual context to improve speech recognition. This system uses a "show-and-tell" technique to acquire a grammar and visually grounded lexicon, using camera images paired with verbal descriptions of objects. When presented with a new scene, the system generates a dynamic language model and uses visual attention to guide the search for the most likely word sequences in spoken language.
41171	4117161	Model-driven multidimensional modeling of secure data warehouses	Data warehouses, multidimensional databases, and online analytical processing applications are crucial for companies to make informed decisions based on historical data. It is important for these systems to have strong security and confidentiality measures in place from the beginning of a project. However, current proposals for conceptual multidimensional modeling do not consider security as a key element and do not allow for specifying confidentiality constraints. To address this issue, the authors propose an Access Control and Audit (ACA) model for conceptual multidimensional modeling and extend the Unified Modeling Language (UML) with this model. The use of the Object Security Constraint Language (OSCL) ensures that these constraints are not arbitrarily applied. This approach is also aligned with the Model-Driven Architecture, Model-Driven Security, and Model-Driven Data Warehouse, making it compatible with recent technologies.
41171	411710	Access control and audit model for the multidimensional modeling of data warehouses	Data Warehouses (DWs) contain sensitive data, making it crucial to implement security measures from the early stages of the design process. Traditional access control models for relational databases are not suitable for DWs. Instead, security and audit rules must be based on the multidimensional (MD) modeling used in DW design. However, current approaches for conceptual modeling do not allow for the specification of security and confidentiality constraints. In this paper, the authors propose an Access Control and Audit (ACA) model for DWs that integrates security rules into the conceptual MD modeling. This model defines authorization rules for users and objects and assigns sensitive information rules and authorization roles to MD model elements. Additionally, audit rules are included to analyze user behavior. The ACA model is integrated into the Unified Modeling Language (UML) to allow for the design of secure MD models. A health care case study is used to demonstrate the effectiveness of this approach.
41172	41172229	CQA-ENV: An Integrated Environment for the Continuous Quality Assessment of Software Artifacts	The quality of software artefacts is a growing concern for software development organizations. The quality of the final software product is heavily influenced by the quality of the models produced throughout the development process. With the emergence of Model Driven Development, there is a need for a more generic and flexible methodology for assessing the quality of software artefacts. In this paper, the authors propose an integrated environment called "CQA-ENV" which includes a methodology for continuous quality assessment and a set of tools to support it. This environment can be used by companies offering quality assessment services, as well as software development organizations looking to perform their own evaluations. 
41172	4117290	Improving Quality of Business Process Models	Business process improvement is crucial for enhancing organizational performance. It is most effective when implemented during the design stage of the process lifecycle, as this prevents errors from propagating to later stages. To improve business process models, a proposal of steps based on measurement activities on conceptual models (such as measurement, evaluation, and redesign) is recommended. These steps aim to increase the quality of the models, specifically in terms of understandability and modifiability. They have been successfully applied to a real hospital business process model, resulting in a higher-quality model. The use of these measurement activities is supported by expert opinions and modeling guidelines, demonstrating their practical utility for business process improvement.
41173	4117339	Improving contextual advertising matching by using Wikipedia thesaurus knowledge	Contextual advertising is a common form of online advertising that involves placing relevant ads within the content of a web page to improve user experience and increase ad clicks. However, traditional keyword matching techniques often struggle with this type of advertising due to issues such as multiple meanings for keywords and a lack of relevant semantics. To address these problems, a new approach that utilizes Wikipedia thesaurus knowledge has been introduced. This approach involves mapping each page to a keyword vector and using two additional feature vectors based on Wikipedia concepts and categories to determine relevant ads. Results from experiments show that this approach outperforms traditional methods and can significantly improve ad selection.
41173	4117319	Position-wise contextual advertising: Placing relevant ads at appropriate positions of a web page.	Web advertising has become a crucial marketing channel, utilizing the Internet to promote products and services. Contextual advertising, a prevalent form of web advertising, strategically places relevant ads on web pages to enhance the user experience and increase ad clicks. However, existing techniques often overlook the impact of ad placement on the page, resulting in unsatisfactory ad relevance. This paper introduces a new approach to this problem, considering both global and local context relevance in selecting and placing ads. Moreover, the use of Wikipedia knowledge improves the accuracy of context relevance measures. The effectiveness of this approach is demonstrated through evaluation using real ads and web pages.
41174	4117467	On-line scheduling with precedence constraints	 for this problem (see [2] ). Hence, to prove lower bounds on the competitive ratio of randomized algorithms, it is enough to find a probability distribution on the input sequences such that the expected competitive ratio of any deterministic algorithm is at least c . To prove the lower bounds for the related machines case, we use the same probability distribution as in [4] . In the restricted assignment case, we use a new probability distribution. Consider the following probability distribution: for some constant p > 0, each job j will arrive with probability p / m and have running time w j = m / p . We define the distribution such that jobs with running time m / p can be processed on any machine, while jobs with running time less than m / p can be processed on a subset of machines. We show that the expected competitive ratio of any deterministic algorithm for this distribution is Ω ( log m) .
41174	4117444	Robust algorithms for preemptive scheduling	Preemptive scheduling problems on parallel machines are well-studied problems with the goal of minimizing the makespan. These problems involve assigning a set of jobs to be executed on a set of machines, where each job can be split into parts and assigned to time slots on the machines. In this context, robust algorithms that can adapt to changing input have been investigated. A migration factor, which measures the maximum amount of rescheduling allowed when a new job is added, plays a key role in these algorithms. It has been shown that for identical machines, a strongly optimal algorithm with a migration factor of 1-1/m exists, resulting in optimal solutions for both makespan and lp norms. However, for uniformly related machines or identical machines with restricted assignment, no optimal algorithm with a constant migration factor exists. In the case of two machines, it is still possible to maintain an optimal schedule with a small migration factor for two uniformly related machines and two identical machines with restricted assignment.
41175	4117543	WHOIS Lost in Translation: (Mis)Understanding Domain Name Expiration and Re-Registration.	Internet domain names have an expiration date and can be claimed by a new owner if not renewed. However, there is limited understanding of how often and how quickly these domain names are re-registered after expiration, leading to potential bias in studies. While registration data is available in Whois databases, scalability issues and data ambiguities make studying re-registrations difficult. By focusing on domains that are about to be deleted, researchers were able to track 7.4 million domains over 10 months and found that re-registrations can happen soon after expiration, particularly for older domains. The complexity of Whois data can also pose challenges for both operational and research communities.
41175	4117544	Thou Shalt Not Depend on Me: Analysing the Use of Outdated JavaScript Libraries on the Web.	Web developers often use third-party JavaScript libraries, such as jQuery, to improve the functionality of their websites. However, if these libraries are not properly maintained, they can create vulnerabilities that can be exploited by attackers. A study of over 133,000 websites found that 37% of them include at least one library with a known vulnerability, and the lag time between a library's newest release and its implementation on a website can be years. The study also found that websites often include libraries in ad hoc and transitive ways, which can result in multiple versions of the same library being loaded simultaneously. This highlights the need for better approaches to dependency management and code maintenance in order to improve the security of the Web. 
41176	4117623	AccessMiner: using system-centric models for malware protection	System call models are commonly used to analyze the behavior of programs. They are used by intrusion detection systems and for access control. With the rise of malware, system calls have been proposed as a way to distinguish between benign and malicious processes. However, most models are based on specific behaviors of individual applications and may not generalize well when exposed to diverse, real-world applications. Previous studies have also been limited in their data collection and only evaluated a small set of programs. To address these limitations, a new system-centric approach is proposed, which models the general interactions between programs and the operating system. This approach shows promising results in detecting malware while minimizing false positives.
41176	4117640	A large-scale analysis of the security of embedded firmwares	This paper discusses the growing importance of security in embedded systems, which are prevalent in our society. Despite the reputation of being insecure, there is still a lack of understanding and tools to support this claim. The authors present the first large-scale analysis of firmware images, unpacking 32,000 images and analyzing 1.7 million files. This analysis revealed 38 previously unknown vulnerabilities in over 693 firmware images, impacting at least 140,000 devices accessible over the Internet. The authors also highlight the benefits of analyzing multiple devices and linking data with other large-scale datasets. They plan to provide their analysis as a web service to improve our understanding of embedded device security.
41177	41177101	Static analysis for detecting taint-style vulnerabilities in web applications	The number of web applications has increased significantly in recent years, leading to an increase in security vulnerabilities. Manual code reviews are time-consuming and costly, making the need for automated solutions clear. This paper proposes using static source code analysis, specifically flow-sensitive, interprocedural, and context-sensitive data flow analysis, to identify vulnerable points in a program. In addition, they use a precise alias analysis to target the unique reference semantics found in scripting languages. The paper also discusses an iterative two-phase algorithm for resolving file inclusions, enhancing the accuracy and number of vulnerability reports. The techniques presented can be applied to various types of vulnerabilities, and have been implemented in Pixy, a tool for detecting cross-site scripting and SQL injection vulnerabilities in PHP programs. Testing on popular open-source web applications revealed hundreds of previously unknown vulnerabilities, demonstrating the effectiveness of the techniques for security audits.
41177	4117785	Precise alias analysis for static detection of web application vulnerabilities	In recent years, the use and importance of web applications have grown significantly, leading to an increase in security vulnerabilities. Manual code reviews are time-consuming and costly, making the need for automated solutions clear. This paper focuses on addressing the issue of vulnerable web applications through static source code analysis. A new, precise alias analysis is presented for scripting languages, along with a two-phase algorithm for accurate resolution of file inclusions. These techniques have been integrated into Pixy, a static analysis tool for detecting cross-site scripting vulnerabilities in PHP scripts. By analyzing three web applications, 106 vulnerabilities were discovered, demonstrating the efficiency and effectiveness of these techniques for conducting security audits.
41178	4117812	Analysis of Minimum Distances in High-Dimensional Musical Spaces	The proposed method aims to improve music search engines and recommended systems by automatically measuring content-based music similarity. It uses features extracted from unlabeled audio data and a distance threshold to efficiently solve retrieval tasks. The method is compatible with locality-sensitive hashing, resulting in significantly faster retrieval times. The method's performance was evaluated on three different music similarity tasks, achieving near-perfect results in two tasks and 75% precision at 70% recall in the third task. The approach is efficient for large music collections and can be implemented with different databases.
41178	4117811	Song Intersection by Approximate Nearest Neighbor Search	The article discusses new methods for calculating similarities between songs using intersections of audio pieces. This approach is useful for identifying derivative works in large song databases. The authors have developed an algorithm called audio shingles, based on locality-sensitive hashing, which efficiently identifies approximate nearest neighbors in a high-dimensional feature space. This approach is demonstrated in a retrieval experiment for derivative works, comparing both exact and approximate methods. The results show that the approximate method is significantly faster and does not compromise accuracy.
41179	4117943	A Perceptual Subspace Method For Sinusoidal Speech And Audio Modeling	The problem of representing a signal segment as a sum of damped sinusoidal components is important in various fields like speech and audio processing. Typically, subspace-based methods are used to estimate the model parameters, taking advantage of the shift-invariance property. However, these techniques do not consider the relevance of the model components in terms of perception. This paper proposes a combination of subspace-based methods with a perceptual distortion measure to extract perceptually relevant components. Experiments using audio signals show that this algorithm significantly improves signal quality compared to traditional subspace-based methods, based on both objective and subjective evaluations.
41179	4117949	A perceptual model for sinusoidal audio coding based on spectral integration	Psychoacoustical models have been widely used in audio coding for many years. With the recent application of parametric coding to general audio, there is a need for a psychoacoustical model specifically designed for sinusoidal modeling. This paper introduces a new model that predicts masked thresholds for sinusoidal distortions, based on signal detection theory and incorporating new insights about auditory masking. This model is beneficial for optimization algorithms and has been shown to be preferred over the ISO MPEG-1 Layer I-II recommended model in listening tests. It results in a 20% reduction in the number of sinusoids needed to represent signals at a given quality level. 
41180	4118056	Three novel opportunistic scheduling algorithms in CoMP-CSB scenario	Coordinated scheduling/beamforming (CSB) is a technique within the coordinated multi-point (CoMP) transmission that has gained attention for its potential to reduce inter-cell interference and increase cell-edge throughput. However, current scheduling algorithms for CSB are complex and have high overhead. In this paper, three new opportunistic scheduling algorithms are proposed that consider both the intended channel of the scheduled user and the orthogonality between the intended channel and interference channels from nearby cells. These algorithms aim to exploit multi-user diversity and mitigate interference. Simulation results show that all three algorithms improve signal to interference plus noise ratio (SINR) and achieve higher throughputs and utilities compared to existing algorithms. Algorithm 2 has the best performance, but requires more complexity and overhead than Algorithms 1 and 2. Optimal parameters for all three algorithms are also provided to balance performance and complexity. 
41180	4118095	Network synchronization for dense small cell networks	The development of 5G wireless networks is still in its early stages, but it is widely agreed that key technologies such as millimeter waves, massive MIMO, small cells, and new radio interfaces will play a crucial role. Among these, small cell networks are seen as a practical solution for achieving the promise of ubiquitous mobile broadband. However, synchronization between small cells is essential for realizing the benefits of this technology and is a challenging issue. Existing synchronization techniques such as GNSS and IEEE 1588 have limitations in small cell deployment scenarios. Therefore, a radio-interface-based solution is being studied as a practical alternative for synchronization in dense small cell networks, with a focus on LTE technology.
41181	4118145	Efficient algorithms for computing a class of subsethood and similarity measures for interval type-2 fuzzy sets	Subsethood and similarity measures are important concepts in fuzzy set theory, specifically for type-1 and interval type-2 fuzzy sets. Rickard et al. have defined a measure for IT2 FS subsethood, based on Kosko's T1 FS subsethood measure and the Representation Theorem. Nguyen and Kreinovich have also extended the Jaccard similarity measure for T1 FSs to create a measure for IT2 FS similarity. The authors also propose efficient algorithms for computing these measures and simulations show that they outperform existing algorithms. These measures and algorithms have practical applications in fields such as pattern recognition and decision making.
41181	411813	A vector similarity measure for linguistic approximation: Interval type-2 and type-1 fuzzy sets	Fuzzy logic is often used in computing with words (CWW), where input words are represented by interval type-2 fuzzy sets (IT2 FSs). The CWW engine's output is also an IT2 FS, which needs to be mapped to a linguistic label for better understanding. To find the most similar linguistic label, a vector similarity measure (VSM) is proposed, which considers both shape and proximity between two IT2 FSs. A comparative study shows that the VSM provides more accurate results compared to other existing similarity measures for IT2 FSs. Furthermore, the VSM can also be applied to type-1 FSs, which are a special case of IT2 FSs with no uncertainty.
41182	4118295	Advanced computing with words using syllogistic reasoning and arithmetic operations on linguistic belief structures	This paper presents solutions to an Advanced Computing with Words problem, which is equivalent to one of Zadeh's challenge problems on linguistic probabilities. The problem is interpreted using a syllogism based on the entailment principle, resulting in two linguistic belief structures. These structures are then combined using addition to obtain a belief structure for the variable of interest. Pessimistic and optimistic probabilities can be inferred from this belief structure using Linguistic Weighted Averages and compatibility measures. Vocabularies for linguistic attributes and probabilities are chosen and modeled using interval type-2 fuzzy sets. The resulting probabilities are translated into words from the vocabulary for easier understanding by humans.
41182	4118245	Intelligent systems for decision support	This research focuses on multi-criteria decision-making (MCDM) in the presence of linguistic uncertainties. This is important because linguistic information is often a crucial input in decision-making and is often uncertain, posing a risk to the decision-making process. MCDM can be divided into two categories: multi-attribute decision-making (MADM), which selects the best alternative from a group of candidates based on multiple criteria, and multi-objective decision-making (MODM), which optimizes conflicting objectives. The dissertation implements Perceptual Computer, an architecture for computing with words, for both categories. For MADM, novel weighted averages are proposed to aggregate diverse and uncertain information from criteria inputs. For MODM, two approaches for extracting IF-THEN rules are proposed: linguistic summarization and knowledge mining. Applications for all techniques are demonstrated in the dissertation.
41183	4118360	Performance Evaluation of WMN-GA System for Node Placement in WMNs Considering Exponential and Weibull Distribution of Mesh Clients and Different Selection and Mutation Operators	This paper evaluates the performance of the WMN-GA system for the node placement problem in wireless mesh networks (WMNs). The study considers two types of distributions for mesh clients and various selection and mutation operators. A population size of 64 and 200 generations are used for evaluation. The performance is measured using the giant component and number of covered users metrics. Results indicate that the WMN-GA system performs better for Exponential distribution of mesh clients. 
41183	41183224	Effects of Selection Operators for Mesh Router Placement in Wireless Mesh Networks Considering Weibull Distribution of Mesh Clients	This paper evaluates the performance of a WMN-GA system for solving the node placement problem in WMNs. The evaluation considers a Weibull Distribution of mesh clients and different selection operators, with a population size of 64 and 200 generations. Two metrics, giant component and number of covered users, are used for evaluation. The simulation results show that for small and medium grid sizes and high density of mesh clients, WMN-GA with Linear Ranking performs better. For larger grid sizes and low density of mesh clients, WMN-GA with Exponential Ranking is more effective. Overall, the study demonstrates the effectiveness of WMN-GA in solving the node placement problem in WMNs.
41184	4118459	A Fuzzy-Based Approach for Classifying Students' Emotional States in Online Collaborative Work	Emotion awareness has become an important aspect in collaborative work in academia, enterprises, and organizations that use group work. With the widespread use of ICT's, collaboration can mostly be done through communication channels like forums and social networks. The emotional state of users during collaborative activities, such as learning or project work, can greatly impact their performance and outcomes. Therefore, it is crucial to monitor and use this emotional information to provide feedback and support. In this paper, a fuzzy approach is proposed to process data collected from communication channels and classify the emotional states of users. This is achieved by using a fuzzy-based emotive classification system, which is fed with data from the ANEW dictionary. The proposed system has been evaluated using real data from collaborative learning courses in academia.
41184	4118434	Using Bi-clustering Algorithm for Analyzing Online Users Activity in a Virtual Campus	Data mining algorithms are useful for processing large data sets and extracting relevant information and knowledge. They are particularly important for analyzing data collected from user activity in online applications. One type of data analysis is mining log files from online applications, which can provide insights into user behavior and inform the design process for better usability and adaptation to user preferences. The research discussed in this paper focuses on a virtual campus where thousands of students and tutors use online applications for learning and teaching. The log files from this virtual campus are complex and require efficient and intelligent processing and analysis. The authors present a new algorithm, abi-clustering, for processing these log files and extracting useful information such as navigation patterns and time parameters. This information can be beneficial for both users and designers of the virtual campus to improve the online learning experience.
41185	41185138	A Fuzzy-Based Cluster-Head Selection System forWSNs Considering Different Parameters	Cluster formation and cluster head selection are crucial factors in sensor network applications, as they greatly impact the communication energy consumption within the network. However, selecting an appropriate cluster head can be challenging in different environments with varying characteristics. To address this issue, a power reduction algorithm using fuzzy logic and node movement has been proposed. Unlike previous systems, this algorithm takes into account 4 linguistic parameters: Remaining Power of Sensor, Degree of Number of Neighbor Nodes, Distance from Cluster Centroid, and Sensor Speed. By considering the node's speed, the algorithm can predict if it will leave the cluster, aiding in cluster-head decision making. Simulation results demonstrate the effectiveness of this approach in selecting optimal cluster heads.
41185	41185315	A Fuzzy-Based Cluster-Head Selection System for WSNs: A Comparison Study for Static and Mobile Sensors	Cluster formation and selecting the cluster head are crucial in sensor network applications and can greatly impact the network's energy usage. However, choosing the cluster head can be challenging in different environments with varying characteristics. To address this issue, a power reduction algorithm was proposed using fuzzy logic and node movement. The algorithm considers four input linguistic parameters (remaining power, number of neighbor nodes, distance from cluster centroid, and sensor speed) to make cluster head decisions. The system was evaluated for both static and moving sensors, taking into account the sensor's speed to predict if it will leave the cluster. Simulation results demonstrate the effectiveness of the proposed system in selecting cluster heads. 
41186	41186173	Model driven middleware: A new paradigm for developing distributed real-time and embedded systems	Distributed real-time and embedded (DRE) systems are crucial in various domains, including avionics, telecommunications, tele-medicine, and defense applications. These systems are increasingly interconnected through networks, creating systems of systems. One of the main challenges for DRE systems is meeting a diverse set of quality of service (QoS) requirements, such as predictable latency, scalability, and security, in real-time. Managing long lifecycles is also a significant challenge, as DRE systems often rely on commercial-off-the-shelf (COTS) technology that may change over time. To address these challenges, a new software paradigm called Model Driven Middleware (MDM) combines model-based development techniques with QoS-enabled component middleware. The CoSMIC toolsuite, which uses MDM, helps developers and integrators throughout the lifecycle of DRE systems by facilitating component partitioning, software configuration validation, QoS assurance, and adaptation to changing technology. 
41186	41186200	A Platform-Independent Component Modeling Language for Distributed Real-Time and Embedded Systems	This paper discusses the use of domain-specific modeling languages (DSMLs) in developing and implementing distributed real-time and embedded systems, specifically those using QoS-enabled component middleware. It introduces the Platform-Independent Component Modeling Language (PICML) which allows for the definition of component interfaces, QoS parameters, and software building rules, and also generates descriptor files for system deployment. The paper then applies PICML to a UAV application in an emergency response system, demonstrating how it can address common challenges in developing component-based DRE systems. The results show that PICML, with its design and deployment validation capabilities, can improve the effectiveness of using QoS-enabled component middleware in the DRE system domain by reducing errors.
41187	4118740	SAT solving for argument filterings	This paper discusses the use of a propositional encoding for lexicographic path orders in dependency pairs, making it possible to use SAT solvers for analyzing the termination of term rewrite systems. The paper focuses on two key issues: finding a lexicographic path order and an argument filtering process to orient inequalities, and how the choice of argument filtering affects the set of inequalities that need to be oriented. The proposed method has been implemented in the termination prover AProVE, and experiments have shown significant improvements in both speed and termination proving capabilities.
41187	4118722	Proving Termination Using Recursive Path Orders and SAT Solving	The Recursive Path Order with Status (RPO) is a way of determining if a term rewrite system is terminating. It combines a multiset path order and a lexicographic path order, taking into account the order of arguments in the comparison. By using a propositional encoding, RPO can be applied with SAT solvers to determine termination. This encoding was also combined with an existing encoding for argument filters to use RPO in the dependency pair framework. AProVE, a termination prover, implemented these techniques and experiments showed significant improvement in RPO-implementation performance. 
41188	4118862	Single-Carrier Incremental Relaying with Joint Tx/Rx FDE	The proposed scheme is an incremental relaying method that utilizes joint Tx/Rx frequency-domain equalization (FDE) for single-carrier transmission. If a source node successfully sends a packet to a relay node but not to the destination node, the source and relay nodes cooperate for retransmission. This is possible because they share channel state information (CSI) with the destination node. The joint Tx/Rx FDE is performed by optimizing the weights among the three nodes based on the minimum mean square error (MMSE) criterion and a total transmit power constraint. Computer simulations show that this scheme is effective.
41188	4118872	Single-Carrier Hybrid ARQ Using Joint Transmit/Receive MMSE-FDE	HARQ is essential in achieving higher throughput performance in the next generation of high-speed wireless packet access systems. This involves using Chase combining (CC), where the same coded packet is transmitted until it is successfully received, and the received packets are combined for time-diversity gain. A recent proposal suggested using joint transmit/receive frequency-domain equalization (FDE) based on the minimum mean square error (MMSE) criterion for single-carrier (SC) signal transmission in a frequency-selective channel. This paper focuses on SC-HARQ using joint transmit/receive MMSE-FDE and presents a suboptimal set of transmit and receive FDE weights for packet combining. Computer simulations show that this approach offers better throughput compared to conventional receive MMSE-FDE.
41189	4118912	Exploring Affiliation Network Models As A Collaborative Filtering Mechanism In E-Learning	This article discusses the use of data from online interactions between learners and tutors to aid in decision making for future learning activities. Specifically, the article explores the use of an affiliation network model, blockmodeling, and m-slices to determine the configuration of topics and learner groups. The results of a case study showed that these techniques can be used to filter participants, rearrange topics, and dynamically change the structure of a course. This approach can be seen as a type of collaborative filtering based on social network structure. 
41189	4118931	Empirical assessment of a collaborative filtering algorithm based on OWA operators	Collaborative filtering algorithms use user ratings to generate recommendations based on common interests. These algorithms typically use a weighted averaging scheme to predict ratings and determine recommendations. However, a new approach that considers group consensus in the aggregation process is proposed in this paper. The approach uses OWA operators with different properties and has been empirically tested on a dataset. The results show that this approach can better capture the "word-of-mouth" philosophy of collaborative filtering systems, especially for high ratings. This algorithm can be considered a generalization of the traditional Pearson formula based algorithm and can be used to fit the behavior of collaborative filtering systems to specific databases. 
41190	4119018	The ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics	The ACL Anthology is an online database containing conference and journal papers related to natural language processing and computational linguistics. While its main purpose is to provide a reference for research, it can also be utilized as a platform for further study. A new reference corpus, the ACL Anthology Reference Corpus (ACL ARC), has been created from the ACL Anthology and is available for use by researchers. This corpus was developed through collaboration with several research groups and is intended to be a standard testbed for experiments in scholarly document processing. The ultimate goal is to make the corpus widely accessible and encourage its use in bibliographic and bibliometric research.
41190	4119094	RevisionBank: A Resource for Revision-based Multi-document Summarization and Evaluation	Multi-document summaries created through extracting sentences often have issues with coherence, such as dangling anaphora, topic shifts, and incorrect chronological ordering. To address this, researchers are working on an automated revision process. This paper introduces the RevisionBank, a collection of 240 manually revised multi-document summaries that aim to improve coherence. The revisions were done by linguistic students using a set of predefined revision operations. The paper discusses the development of a taxonomy for cohesion problems and corresponding revision techniques, as well as the annotation schema for the corpus. The taxonomy and corpus can be used for studying revision-based multi-document summarization and evaluating summaries.
41191	41191110	Analysis and control of power grasping	The article discusses the issue of grasping objects with a robotic hand and the limitations of controlling interactions due to potential defects in the fingers. It highlights the importance of coordinating such defective systems to still perform effectively, as it is common in many manipulation operations. The paper then focuses on optimizing contact forces in power grasps through establishing a basis of grasp forces and developing a control algorithm that minimizes the risk of slippage and maintains bounded contact forces. This allows for the asymptotic convergence to an optimal grasp force configuration. The strategies and analysis presented in the paper aim to improve the effectiveness of robotic hand grasping in various manipulation tasks.
41191	4119156	Postural Hand Synergies during Environmental Constraint Exploitation.	Humans have a remarkable ability to use the shape of objects and environmental constraints to grasp and manipulate them with stability and dexterity. This is achieved through a variety of kinematic strategies, which can be described as a synergistic behavior involving a reduced set of commonly used patterns. This study investigated this hypothesis by analyzing hand postures of six subjects as they grasped objects from a flat surface. The results showed the presence of at least two postural synergies in all conditions, with minimal impact from tactile impairment. These findings have implications for robotics, as they demonstrate the potential for incorporating human-like behaviors in robot design and control. 
41192	4119211	Probabilistic signature based generalized framework for differential fault analysis of stream ciphers.	Differential Fault Attack (DFA) involves injecting faults at random locations and times in order to identify the exact location and timing of the fault. This paper introduces a new approach to DFA using probabilistic signatures and Maximum Likelihood methods. This technique is able to identify faults in a variety of encryption algorithms, including the Grain family, MICKEY 2.0, and Trivium. It also provides improved fault attacks for all versions of the Grain family and MICKEY 2.0, and is able to handle cases where certain bits of the keystream are missing. The paper also presents a solution to the previously unsolved problem of identifying faults in random time for Grain 128a, and reduces the required number of faults for MICKEY 2.0 by 60%.
41192	4119258	Improved differential fault attack on MICKEY 2.0.	This paper discusses differential fault attacks (DFA) on the stream cipher MICKEY 2.0, which is part of the eStream hardware profile. The paper presents different scenarios where an adversary can induce faults in the cipher's internal state and shows that by injecting a certain number of faults and performing computations, it is possible to recover the entire internal state of MICKEY. The number of faults required varies depending on the scenario, with the highest number being \(2^{18.4}\) faults for the case where the fault affects multiple neighboring bits. The paper also suggests that using SAT solvers can reduce the number of required faults to \(2^{14.7}\) for single-bit faults and \(2^{16.06}\) for multiple-bit faults.
41193	4119365	HAUCA Curves for the Evaluation of Biomarker Pilot Studies with Small Sample Sizes and Large Numbers of Features.	Biomarker studies aim to find a combination of measured attributes to help diagnose a specific disease. These attributes are often obtained through high-throughput technologies like next generation sequencing, resulting in a large number of potential biomarkers. However, with a small sample size of only 24 patients, it is difficult to accurately identify specific biomarkers using purely statistical methods. Despite this limitation, researchers can still determine if there are more biomarkers strongly correlated with the disease compared to what would be expected by chance. This can be done using a method based on the area under the ROC curve (AUC). The authors also provide estimations for sample sizes in follow-up studies that can successfully identify concrete biomarkers and build disease classifiers. The method can also be adapted to other performance measures besides AUC.
41193	4119326	Constructing a fuzzy controller from data	Fuzzy control at the executive level is an approximating method for control functions, using imprecise input-output tuples represented by fuzzy sets. The imprecision is measured by similarity relations induced by transformations of the canonical distance function between real numbers. To create a fuzzy controller, typical input-output tuples must be identified and a concept of similarity using transformed distance must be established. Various fuzzy clustering algorithms exist that follow this approach, by identifying prototypes and assigning fuzzy sets based on transformed distance. This paper discusses the application of these techniques to construct a fuzzy controller from data and introduces specialized clustering algorithms designed for this purpose.
41194	4119448	Neuro-fuzzy control based on the NEFCON-model: recent developments	 optimizationFuzzy systems are widely used in various industries and scientific fields, but designing and optimizing them can be time-consuming. To address this issue, algorithms have been developed to automatically construct and optimize fuzzy systems. One popular approach is combining fuzzy systems with learning techniques from neural networks, known as neuro-fuzzy systems. This paper discusses the NEFCON-Model, which uses reinforcement learning to optimize a Mamdani-style fuzzy controller rule base in real-time. The paper also describes methods for determining a fuzzy error measure for a dynamic system and presents implementations and an application example. These implementations are available for non-commercial use. The paper highlights the benefits of using hybrid neuro-fuzzy systems for system optimization.
41194	4119446	A fuzzy neural network learning fuzzy control rules and membership functions by fuzzy error backpropagation	The fuzzy neural network is a type of neural network specifically designed for control tasks. Its three-layered architecture resembles that of a fuzzy controller, and uses fuzzy sets as its weights. The network employs a special learning algorithm, called fuzzy error backpropagation, which is inspired by the standard BP-procedure for multivariable neural networks. This extended version can also learn fuzzy-if-then rules by reducing the number of nodes in the hidden layer. Unlike traditional neural networks that learn from examples, the fuzzy neural network learns by evaluating a specific fuzzy error measure. This approach allows for more precise and efficient learning in control tasks.
41195	4119566	Supporting Knowledge Sharing In Heterogeneous Social Network Thematic Groups	Thematic groups are becoming increasingly popular in online social networks (OSNs), as users collaborate and share ideas within these groups. Personal software agents can assist users who belong to multiple groups within different OSNs by facilitating relationships and cooperation. These software agents are able to gather detailed information about users and their interests, which can be used to match them with relevant groups. This article presents a multi-agent framework that focuses on the role of software agents and the use of a shared dictionary for each group. Personal agents are associated with individual users to share knowledge related to a specific theme, while group agents support their respective groups by interacting with personal agents and managing group membership. The common dictionary of each group is a key element in enabling knowledge sharing and interoperability between personal and group agents. Each user agent can personalize their own dictionary and contribute to the group's dictionary by selecting relevant categories.
41195	4119535	A Hybrid Model for Ranking Cloud Services.	In this paper, the focus is on the negotiation of a Cloud Service Agreement in an inter-cloud scenario. The roles of customers, providers, brokers, and auditors are discussed, along with the importance of trust in their relationships. A hybrid model for ranking cloud services is proposed, which takes into account both QoS measurements from independent brokers and certifications from auditors. The model also incorporates a reputation system, where cloud users can provide feedback and recommendations based on their experiences. This allows for a more comprehensive evaluation of cloud services and involves the community of users in the inter-cloud environment.
41196	411965	A fault-tolerant self-organizing flocking approach for UAV aerial survey.	In this paper, the authors focus on the issue of self-organization for a group of multirotor UAVs during a monitoring mission. This type of mission involves collecting pertinent data from a specific area and sending it back to a Base Station. The paper delves into the challenges of coordinating the movements and tasks of multiple UAVs to efficiently complete the mission. The authors propose a solution using decentralized control algorithms that allow the UAVs to communicate and coordinate with each other without relying on a central controller. The effectiveness of this approach is demonstrated through simulation results.
41196	4119612	An economic model for resource management in a Grid-based content distribution network	This paper introduces a Content Distribution Grid (CDG), which is a federated system of Content Distribution Networks (CDNs) that share computational resources. The CDG aims to improve the quality of service for users by allowing CDNs to share resources based on a community policy. This policy follows an offer/demand competitive model, where CDNs can purchase resources from each other using virtual or real money. An economic model is developed to guide this resource exchange, and a multi-agent system is proposed as the software architecture for the CDG. This approach is beneficial for CDNs belonging to different organizations, as it allows them to maximize their own system's performance while still meeting the needs of other CDNs. 
41197	4119765	Augmenting a Web Server with QoS by Means of an Aspect-Oriented Architecture	This paper discusses the use of aspect-orientation to handle Quality of Service (QoS) on a web server, specifically Jigsaw. By using this approach, QoS concerns can be separated from communication-related concerns and improvements can be made to the web server's processing of incoming requests. This is achieved by associating requests with priorities and implementing checks on resource usage and tasks. The suggested aspects are connected at compile-time to existing classes, keeping the QoS-enforcing code separate from the web server modules. Overall, this approach allows for more effective handling of QoS parameters on a web server.
41197	4119711	Design And Evaluation Of A High-Level Grid Communication Infrastructure	This paper discusses the challenges of developing a distributed application in a Grid context, which often requires significant effort and skills to adapt to Grid-specific communication technologies. To address this issue, the authors propose HiC, a high-level communication infrastructure for Java technology. HiC hides the complexities of data transfer mechanisms and allows for easy integration with new technologies without impacting the application. It consists of two layers, with the upper layer providing simple and powerful abstractions for remote services and the lower layer providing access to specific data-transfer technologies. The performance of HiC has been evaluated and found to be acceptable, making it a useful tool for transparent distributed object allocation. The paper also proposes a standard interfacing architecture to minimize dependencies on HiC for user software. Overall, HiC synergizes well with the ReD reflective framework and alleviates developers from the concerns of retargeting and implementing intermediaries between the application and HiC. 
41198	411987	Reconstructive explanation: explanation as complex problem solving	Existing explanation facilities are better suited for knowledge engineers conducting system maintenance rather than end-users. This is because the current methods simply provide a trace of the problem-solving steps. However, a more effective approach involves reorganizing the reasoning process and incorporating additional information to support the result. This highlights the complexity of explanation as a problem-solving process that relies not only on the reasoning but also on domain knowledge. A new computational model of explanation is introduced, which is argued to be more effective than traditional methods. This model takes into account the need for additional knowledge and results in significant improvements in the explanation process.
41198	4119815	Recognition-Based Diagnostic Reasoning	The ability to diagnose faults effectively relies on recognizing patterns in observed data that have been previously seen and interpreted correctly. This can lead to more efficient analysis by avoiding costly and detailed examinations of causal relationships between faults and data. Recognition-based reasoning involves focused search strategies to extract relevant information from a knowledge base with many prototypes for potential faults. This paper presents a model and implementation of recognition-based reasoning for solving diagnostic problems in pediatric cardiology. The model first analyzes patient data to generate a limited set of potential diseases, then uses specific knowledge to refine these choices. This model has been successfully tested against real hospital cases and performs similarly to expert physicians.
41199	4119963	Global Linear Complexity Analysis Of Filter Keystream Generators	The article introduces a new algorithm for determining lower bounds on the global linear complexity of PN-sequences that have been filtered nonlinearly. The algorithm is designed to be efficient and can be implemented in both software and hardware using bitwise logic operations. It can be applied to any type of filter generator, making it versatile. The focus of the algorithm is to achieve large lower bounds, which demonstrate the exponential growth of the global linear complexity for these types of sequences. 
41199	4119936	On the Linear Complexity of Nonlinear Filtered PN-sequences	The article discusses a method for analyzing the linear complexity of PN-sequences that have been nonlinearly filtered. The method provides a lower bound for the linear complexity and offers an algorithm to improve it. This applies to any nonlinear function with a dominant term and any maximal-length LFSR. The approach is based on binary strings rather than determinants in a finite field, building on the root presence test by Rueppel. This method provides a more efficient way to analyze the linear complexity of nonlinearly filtered PN-sequences.
41200	412007	Foundations for Knowledge-Based Programs using ES	Reiter introduced a way to handle knowledge-based Golog programs with sensing, where program execution can be based on tests that reference the agent's knowledge. This approach allows reasoning about knowledge to be reduced to classical reasoning from an initial first-order theory. However, this method has limitations, such as only considering knowledge about the current state and not allowing quantifying-in formulas. This is due to the use of Reiter's version of the situation calculus. A new situation calculus developed by Lakemeyer and Levesque offers a solution to these limitations by allowing for an expansion of Reiter's foundations and removing the mentioned restrictions.
41200	4120097	Decidable reasoning in a logic of limited belief with introspection and unknown individuals	SL is a logic of belief that stands out for its clear meaning and computational efficiency. However, it has limitations such as not being able to handle beliefs about beliefs and assuming known identities for individuals. In this paper, the authors propose extensions to SL that address these shortcomings. They demonstrate that these extensions allow for determining beliefs in fully introspective knowledge bases and accommodating unknown individuals in a decidable manner. This makes SL a more versatile and powerful tool for reasoning about beliefs.
41201	412015	Firing Fuzzy Rules with Measure Type Inputs	This article discusses the issue of determining the satisfaction level of a fuzzy systems rule base when the antecedent condition is expressed using a normal fuzzy set and the input information is also in the form of a fuzzy set. The authors explore different approaches for calculating this satisfaction level and outline the necessary requirements for any formulation used. They then introduce the concept of a measure and demonstrate how it can be applied to uncertain values associated with a variable. The requirements for determining satisfaction level using a measure are also discussed, with some examples provided. Finally, the authors show how this method can be used for fuzzy rules with probabilistic inputs.
41201	41201268	Jeffrey's rule of conditioning with various forms for uncertainty	The article discusses the extension of Jeffrey's rule of conditioning, which helps determine the probability of an event based on information about conditional probabilities of that event. In the original paradigm, uncertainty about the conditioning variable is expressed as a probability distribution, but the authors allow for alternative formulations of this uncertainty. This includes using a measure or a Dempster-Shafer belief structure. The article also touches on scenarios where the underlying distribution is unknown, and the decision maker's subjective attitude towards uncertainty is taken into account. Overall, the authors aim to improve the applicability of Jeffrey's rule by considering different expressions of uncertainty about the conditioning variable.
41202	4120259	Visual Protection of HEVC Video by Selective Encryption of CABAC Binstrings	This paper introduces a method for protecting the newly developed video codec HEVC through selective encryption of binstrings in a format-compliant manner. The approach differs from that of H.264/AVC and addresses challenges such as non-dyadic encryption space and adaptive context. An algorithm is proposed for converting non-dyadic spaces to dyadic, ensuring that the encrypted bitstream remains format-compliant and maintains the same bit-rate. The method requires minimal processing power and is suitable for a variety of applications, providing protection for contour, motion, and texture information. Experimental evaluation and security analysis support the effectiveness of this method for content protection. 
41202	4120235	FAST PROTECTION OF H.264/AVC BY SELECTIVE ENCRYPTION OF CABAC FOR I & P FRAMES	This paper presents a new way to protect copyrighted multimedia content by combining selective encryption (SE) and compression using the H.264/AVC video codec. The selective encryption is applied to the context-based adaptive binary arithmetic coding (CABAC) module of the codec by converting it into an encryption cipher. This allows for encryption without affecting the coding efficiency or increasing the bit rate. The proposed method is suitable for real-time multimedia streaming and playback on handheld devices due to its low processing power. The effectiveness of the algorithm is evaluated on nine different benchmark video sequences with varying motion, texture, and objects. 
41203	4120386	Minimum parent-offspring recombination haplotype inference in pedigrees	The paper discusses the problem of haplotype inference under the Mendelian law of inheritance on pedigree genotype data. The minimum recombination principle is used to determine the most likely haplotype configurations with the least number of recombinations. The authors propose a variation of the Minimum Recombination Haplotype Inference (MRHI) problem, called the k-MRHI problem, which adds the constraint that the number of recombinations in each parent-offspring pair is at most k. They present an efficient algorithm using dynamic programming to solve the k-MRHI problem and also propose an algorithm to find the root of the pedigree tree and reduce the time complexity. Experiments on real and simulated data show that their algorithm outperforms other popular algorithms in terms of both speed and accuracy.
41203	4120315	Efficient inference control for range SUM queries on statistical data bases	The paper discusses the use of Auditing, an inference control technique, which is considered to be better than previous techniques. Auditing involves logging all answered queries and using this information to determine if a new query could lead to compromise. However, the complexity of time and storage may make it impractical for large SDBs. The focus of the study is on SUM queries and the assumption is made that statistical information is only available for subsets of records within a certain range. With the right data structure, the time and storage complexity for checking a new range query can be reduced to O(n) time and storage, or O(t log n) time with O(n2) storage for t new range queries and n records in the SDB.
41204	4120412	Computational Argument as a Diagnostic Tool: The role of reliability.	Formal and computational models of argument are useful tools for teaching in complex fields like law, public policy, and science. These areas rely heavily on open-ended arguments, but students may not have been taught a structured approach to constructing and analyzing arguments. Computational models can act as argument tutors, guiding students through the process using an explicit model. However, it is important to ensure that these models can be easily understood and evaluated, which is currently being studied in the LARGO tutoring system. Ongoing research is being conducted to determine the effectiveness of argument diagrams produced by this system.
41204	4120469	Evaluating an Intelligent Tutoring System for Making Legal Arguments with Hypotheticals	Argumentation is a process that helps deal with the uncertainty and lack of definition in certain domains. In these ill-defined domains, the concept of "correctness" for an argument cannot be formally defined or verified due to open-ended concepts and potential disagreements among experts. Previous research has shown the benefits of using graphical representations in learning argumentation skills. LARGO (Legal Argument Graph Observer) is a system that supports law students in representing legal interpretations with hypotheticals using diagrams. A pilot study and a mandatory study both showed that LARGO improved the learning outcomes for students, especially those with lower aptitude. However, engagement with the argument diagrams is crucial for their effectiveness. In conclusion, graphical representations can be more effective than text in intelligent tutoring systems for argumentation, but engagement is key.
41205	4120534	Re-evaluating LARGO in the Classroom: Are Diagrams Better Than Text for Teaching Argumentation Skills?	Diagramming is a helpful way to teach argumentation skills in complex subjects, but it is challenging for an intelligent tutoring system (ITS) to provide feedback without a clear evaluation process. LARGO is an ITS designed for legal argumentation that offers advice by identifying patterns in students' diagrams. A study was conducted using LARGO in a first-year law class, but it did not show significant improvement compared to a text-based note-taking tool. This may be due to low use of the graphical tools and different motivations. However, there is potential for LARGO to benefit lower-aptitude students if they engage more with the system. 
41205	4120532	"How Else Should It Work?" A Grounded Theory of Pre-College Students' Understanding of Computing Devices.	This article discusses the importance of students being able to identify computing technology in their environment and how this task has become more challenging as technology becomes more pervasive and invisible. The authors present a grounded theory based on interviews with pre-college students in Germany, which highlights the three levels of capability that students use to differentiate technical devices. They also address the need for this issue to be addressed as technology continues to advance, and suggest some potential ways to do so.
41206	4120676	Does supporting multiple student strategies in intelligent tutoring systems lead to better learning?	Intelligent Tutoring Systems (ITS) are challenging to construct due to the concept of strategy freedom, where students have the ability to use multiple approaches to solve a problem. However, a study was conducted in two US middle schools with 57 students in grades 7 and 8 to determine if this freedom leads to more robust learning. Three versions of an ITS for solving linear equations were developed, varying in the amount of strategy freedom. The results showed that students' algebra skills improved, but there was no difference in learning gain and motivation between the different versions. This suggests that a small amount of strategy freedom is beneficial in early algebra learning, but students tend to stick to a standard strategy and its minor variations rather than exploring alternative strategies. This study validates the complexity of ITS architectures.
41206	4120683	An Interview Reflection on “Intelligent Tutoring Goes to School in the Big City”	This article discusses a study from 1997 that showed the positive impact of an intelligent tutoring system (ITS) on high school students' learning in algebra. The study aimed to demonstrate that ITSs, which incorporate cognitive science research, can have a significant effect in real school settings. The study used various assessments, including standardized tests and measures of complex problem-solving, and found that students in classes with the ITS performed better than those in regular classes. This study was one of the first to evaluate the use of ITSs in high schools, and since then, more studies have shown the benefits of using ITS-enhanced curricula. The article also suggests that cognitive task analysis is crucial for the success of ITSs and that further research is needed to determine the most effective way to use ITSs for open-ended problem-solving. Despite the recent focus on MOOCs, the field of Artificial Intelligence in Education has been making significant progress with a widespread impact on students' learning.
41207	4120715	Quadratic factorization heuristics for copositive programming	Copositive optimization problems are a type of conic programming where the objective is to optimize linear forms over the copositive cone, subject to linear constraints. This class of problems is NP-hard and can be used to formulate quadratic programs with linear constraints, even when some variables are binary. Most methods for solving these problems aim to approximate the copositive cone from within, but a new method has been proposed that approximates the cone from outside by using the dual problem. This approach involves considering descent directions in the completely positive cone and solving regularized strictly convex subproblems. By replacing the completely positive cone with a nonnegative cone, the problem can be solved using linearization techniques, resulting in promising numerical results on various test problems.
41207	4120751	Think co(mpletely)positive ! Matrix properties, examples and a clustered bibliography on copositive optimization	Copositive optimization is a growing field of research with various practical applications, including solving difficult problems in engineering and combinatorial optimization. It belongs to the category of conic programming, specifically involving the cone of completely positive symmetric matrices (which can be factored into a rectangular matrix with no negative entries). The dual cone, which consists of copositive matrices, is also discussed. The paper delves into the algebraic properties of these cones and provides examples that illustrate the differences between copositive and semidefinite optimization. It also introduces a construction method for non-attainable solutions and attempts to organize the scattered literature in this area.
41208	4120890	Fast population game dynamics for dominant sets and other quadratic optimization problems	The authors propose a new population game dynamics method for finding dominant sets in graphs, inspired by infection and immunization processes. This method has linear time and space complexity and is shown to have increasing population payoff along any non-constant trajectory. Dominant sets are proven to be stable points for this dynamics under the assumption of symmetric affinities. This approach is applicable to a wide range of quadratic optimization problems in computer vision. Experimental results show that this method is significantly faster and equally accurate compared to standard algorithms.
41208	4120813	Payoff-monotonic game dynamics and the maximum clique problem.	The use of evolutionary game-theoretic models, specifically the replicator equations, has been successful in solving the maximum clique and related problems. This is due to the ability to formulate the problem as a standard quadratic program and exploit the dynamical properties of these models, which possess a Lyapunov function under a symmetry assumption. This letter expands on previous work by introducing a wider family of game-dynamic equations, known as payoff-monotonic dynamics, which share the same properties as replicator equations. These dynamics have been shown to be faster and more accurate than replicator equations in simulations on various benchmark graphs. However, they struggle with escaping from poor local solutions. To address this issue, a subclass of these dynamics, used to model behavior through imitation processes, has been studied and a class of annealed imitation heuristics has been proposed. These heuristics involve varying a parameter during the imitation optimization process to avoid inefficient solutions. Experiments show that this approach is effective in avoiding poor local optima and outperforms other algorithms for maximum clique.
41209	4120944	Kernel-level scheduling for the nano-threads programming model	 ses/threads, and then exploit concurrency by managing the parallel execution of these processes.Multiprocessor systems are popular for low and high-end servers, performing diverse tasks like number crunching, simulations, databases, and web servers. Due to the variety of workloads, system utilization, throughput, and execution time are crucial performance metrics. This paper presents efficient kernel scheduling policies and a new kernel-user interface to support parallel execution in different workloads. It includes user level threads to exploit parallelism and a two-level scheduling policy to coordinate resources with the number of threads generated by applications. The performance of the proposed policies is compared to the gang scheduling policy of IRIX 6.4 on a Silicon Graphics Origin2000, showing significant improvements in workload and application execution times, as well as cache performance. However, parallel applications on multiprogramming systems often suffer from performance degradation due to interferences between programs contending for resources. This is because these applications are designed assuming a dedicated system, and manage parallel execution by setting up the desired number of processes/threads.
41209	4120925	Scheduling Algorithms With Bus Bandwidth Considerations For Smps	The bus that connects processors to memory is a major bottleneck in SMPs, but software and scheduling policies for these systems typically focus on memory optimizations rather than addressing bus bandwidth limitations. However, experiments have shown that bus saturation can significantly slow down applications. To address this issue, two new scheduling policies are introduced that consider the bus bandwidth consumption of applications. These policies use performance monitoring counters to organize jobs and improve bus bandwidth utilization without saturating the bus. The scheduler is effective for applications with varying bandwidth requirements and has been tuned to handle bursts of high bus bandwidth consumption. Compared to the standard Linux scheduler, the new policies can improve system throughput by up to 68%.
41210	4121058	On the longest common rigid subsequence problem	The longest common subsequence problem (LCS) and closest substring problem (CSP) are both used to identify common patterns in strings and have been extensively studied. LCS has been proven to be not solvable in polynomial time within a certain ratio, while CSP is known to be NP-hard but has a possible solution through a PTAS. The paper focuses on the longest common rigid subsequence problem (LCRS), which is similar to LCS and CSP and is often used in finding motifs in biological sequences. The paper proves LCRS to be Max-SNP hard and provides an exact algorithm with a relatively fast average running time.
41210	4121025	Computing similarity between RNA structures	RNA molecules are made up of a sequence of nucleotides and can form secondary and tertiary structures through base-pair bonds. This paper explores the concept of similarity between RNA structures, considering the primary, secondary, and tertiary structures. The maximization version of this problem is Max SNP-hard and cannot be approximated within a ratio of 2logn in polynomial time, unless NPDTIME[2polylogn]. The paper presents an algorithm for practical use, which produces an optimal solution when one RNA has a secondary structure. An approximation algorithm is also shown.
41211	4121113	Augmenting a Conceptual Model with Geospatiotemporal Annotations	Many real-world applications require organizing data based on space and time, but existing conceptual models do not explicitly capture these spatial and temporal semantics. As a result, database designers must ad hoc discover and implement these concepts. To address this issue, the authors propose an annotation-based approach that allows designers to first focus on non-temporal and non-geospatial aspects before adding geospatiotemporal annotations. This approach provides a higher level of abstraction and extends the semantics of conventional conceptual models. The authors argue that this approach is both expressive and easy to understand and implement, balancing expressiveness and simplicity in conceptual modeling. 
41211	4121151	DISTIL: A Design Support Environment for Conceptual Modeling of Spatio-temporal Requirements	DISTIL is a web-based conceptual modeling prototype system that helps capture the meaning of spatio-temporal data. This approach involves two steps: first, creating a conventional conceptual model of the application without considering spatial aspects, and then annotating the schema with spatio-temporal semantics. This allows a database development team to easily capture and validate their spatio-temporal data needs. DISTIL's annotation-based approach is simple to implement, meets ontology and cognition-based requirements, and can be seamlessly integrated into existing database design methods. 
41212	41212109	Discovering Architectures from Running Systems	A common challenge for software developers is ensuring that a system matches its intended architectural design. To address this issue, a technique called DiscoTect uses runtime observations to create an architectural view of the system. This involves creating mappings that translate low-level system events into higher-level architectural operations, using a formal language called DiscoSTEP. Two case studies demonstrate the effectiveness of DiscoTect in verifying conformance to an existing architectural specification. This approach is useful for legacy systems and provides a practical solution for dynamically checking system consistency. 
41212	4121215	Efficient Decision-Making under Uncertainty for Proactive Self-Adaptation	Proactive latency-aware adaptation is a method for improving self-adaptive systems by considering both current and anticipated adaptation needs, as well as the latency of tactics. This involves solving an optimization problem to select the best adaptation path over a finite look-ahead horizon. Markov decision processes (MDP) are used to address this problem, but constructing the MDP can be complex due to potential interactions between tactics, the system, and the environment. To eliminate the run-time overhead of constructing the MDP, a new approach is presented where most of the MDP is built offline using formal specification. At run time, the MDP is solved through stochastic dynamic programming and incorporates the stochastic environment model. Experimental results show that this approach reduces adaptation decision time significantly compared to traditional probabilistic model checking methods.
41213	412138	Data intensive production systems: the DIPS approach	The paper discusses the design of the DIPS system and its main contributions. These include the use of special data structures and a matching algorithm to efficiently identify when rule conditions are satisfied for execution. The paper also proposes a concurrent execution strategy that outperforms traditional sequential algorithms. A correct and serializable execution based on locking is described. The matching technique used in DIPS is also fully parallelizable, making it suitable for parallel computing environments.
41213	4121354	Implementing large production systems in a DBMS environment: concepts and algorithms	The use of rule based reasoning in future database applications, especially in engineering, manufacturing, and communications, has been widely acknowledged. This paper focuses on ways to effectively store and manipulate large rule bases using relational database management systems. The authors present a matching algorithm to efficiently identify relevant rules and propose concurrent execution strategies that outperform sequential execution methods. The method is also fully parallelizable, making it suitable for use in parallel computing environments. This approach has the potential to greatly improve the performance of rule based reasoning in database applications. 
41214	4121432	On the L(h, k)-labeling of co-comparability graphs and circular-arc graphs	The article discusses the L(h, k)-labeling problem, which involves assigning integer labels to vertices in a graph, with the constraint that adjacent vertices must be at least h units apart and vertices at a distance of 2 or less must be at least k units apart. This problem is NP-complete, so the focus is on finding efficient solutions for special classes of graphs, specifically co-comparability, interval, and circular-arc graphs. The article presents the first algorithm for labeling these types of graphs with a bounded number of colors, and also shows an improvement for the special case of interval graphs with k = 1. 
41214	4121416	On the L(h, k)-labeling of co-comparability graphs	An L(h, k)-labeling is a way of assigning labels to the vertices of a graph such that adjacent vertices have labels that are at least h apart, while vertices that are close together have labels that are at least k apart. The goal is to find the smallest possible label used in this labeling. This problem is NP-complete, so researchers have focused on finding efficient solutions for certain types of graphs, such as co-comparability graphs, interval graphs, and unit-interval graphs. This paper presents the first upper bounds on the L(h, k)-number for these types of graphs, with a specific focus on co-comparability graphs. This result is an improvement on previous approximations for interval graphs when k = 1. 
41215	4121536	The IS risk analysis based on a business model	As technology becomes increasingly essential for organizations, the risk of disruptions to operations due to IS failure has become a major concern. However, traditional methods of IS risk analysis do not accurately assess the impact of disruption on the organization's objectives. Quantitative methods do not consider the loss of operations, while qualitative methods are subjective. This study proposes a business model-based approach to IS risk analysis that takes into account the importance of different business functions and the necessity level of various assets. This method adds an organizational investigation stage and utilizes methodologies such as paired comparison and asset dependency diagrams. 
41215	4121515	The Role of Environmental Information Disclosure Systems and Their Impact on Firm Performance	This paper investigates the impact of environmental information disclosure systems (EIDs) on financial performance in the chemical industry, using the U.S. Toxic Release Inventory (TRI) as a case study. The study found that firms with better environmental performance also showed greater cost competitiveness, challenging the belief that environmental efforts are always a burden. Instead, the study suggests that these activities can lead to improved production efficiency, resource management, and financial sustainability. This highlights the potential of EIDs as strategic tools for green information systems, and calls for further research on their impact. Overall, the study emphasizes the importance of incorporating environmental considerations into business operations for both financial success and sustainability.
41216	41216125	Soft-Decision Decoding of Linear Block Codes Using Preprocessing and Diversification	Order-w reprocessing is a soft-decision decoding method for binary block codes where up to w bits are systematically flipped on the most reliable basis. This method is improved by incorporating preprocessing rules and using a second basis for practical signal-to-noise ratios (SNRs). The resulting multibasis order-w reprocessing scheme has comparable complexity to order-w reprocessing but achieves better performance, particularly at high SNRs. This approach also accurately characterizes the performance of the Chase and generalized minimum distance decoding algorithms. The proposed algorithm successfully decodes the (192,96) Reed-Solomon concatenated code and the (256,147) extended BCH code with a block-error rate of 10-5 and a computational cost that is affordable.
41216	4121627	Soft-decision decoding of linear block codes using efficient iterative G-space encodings	The paper presents a sub-optimal maximum-likelihood decoding algorithm for linear block codes. This algorithm uses reliability information about each bit in the received word to find the most reliable "basis" information bits. It then iteratively flips bits in this set, constructing the corresponding codeword and determining its likelihood. The search of "test error patterns" on the basis set is conducted in a way that only one bit differs from a previously stored pattern, making the decoding process more efficient. The proposed decoding scheme has polynomial space complexity and a heuristic criterion is also described to approximate the optimal set of test error patterns. Simulation results show that this approach is efficient for codes with large lengths, especially under low SNR conditions.
41217	4121763	A novel Gaussianized vector representation for natural scene categorization	This paper introduces a new method for representing scene images using a Gaussianized vector approach. The process involves encoding images as an ensemble of orderless bag of features and then using a global Gaussian Mixture Model to randomly distribute the features into Gaussian components. The parameters of the distribution are determined by the posteriors of the features on the Gaussian components. The resulting supervectors, which are a compact representation of each scene image, are shown to follow a standard normal distribution. Experiments using this representation demonstrate better performance in scene categorization compared to the traditional bag-of-features approach.
41217	4121735	Novel Gaussianized vector representation for improved natural scene categorization	We propose a new way to represent scene images by using a Gaussianized vector representation. This is done through an unsupervised approach where each image is first encoded as a bag of features. A global Gaussian Mixture Model (GMM) is then used to randomly distribute these features into different Gaussian components. The posteriors of the features on these components are used as parameters of a multinomial distribution. The normalized means of the features in each component are then concatenated to form a supervector, which serves as a compact representation for each scene image. Our experiments show that this representation outperforms the commonly used histogram representation in scene categorization tasks. This paper is an extended version of our award-winning work at the 2008 International Conference on Pattern Recognition. 
41218	4121819	A Fast and Flexible Clustering Algorithm Using Binary Discretization	The paper introduces a new clustering algorithm, BOOL, for multivariate data. It can detect arbitrarily shaped clusters and is noise tolerant. The algorithm works in two steps - first, the data is discretized and represented as binary words, and then clusters are constructed by agglomerating smaller clusters using this representation. This step is done efficiently by sorting the binary representations. Experiments show that BOOL is faster than K-means and two other state-of-the-art algorithms for detecting non-convex clusters. The algorithm is also robust to changes in parameters, thanks to the hierarchical structure of clusters introduced by improving the accuracy of the discretization. 
41218	412181	Semi-supervised learning on closed set lattices	In this article, the authors propose a new method for semi-supervised learning using closed set lattices, which have previously been used for frequent pattern mining in Formal Concept Analysis (FCA). The proposed algorithm, SELF, is able to handle mixed-type data containing both discrete and continuous variables, which is a limitation for many existing learning algorithms. SELF constructs a closed set lattice from both labeled and unlabeled data, using FCA and discretizing continuous variables, and then learns classification rules by finding maximal clusters on the lattice. Additionally, SELF can weight these rules using the lattice, providing a preference order for class labels. Experimental results on UCI datasets show that SELF performs competitively in both classification and ranking compared to other learning algorithms.
41219	4121948	A game-theoretic intrusion detection model for mobile ad hoc networks	This paper addresses the issue of improving the effectiveness of intrusion detection systems (IDS) in ad hoc networks by proposing a unified framework. The framework aims to balance resource consumption among nodes, catch and punish misbehaving leaders, and maximize the probability of detection for elected leaders. This is achieved by electing a cost-efficient leader, using a cooperative game-theoretic model to analyze checker interactions, and formulating a game between the leader and intruder. The proposed framework is shown to increase the overall lifetime of the cluster, reduce false positives, and effectively execute the detection service. Empirical results are provided to support the solutions.
41219	4121968	A Mechanism Design-Based Multi-Leader Election Scheme for Intrusion Detection in MANET	This paper discusses the election of multiple leaders for intrusion detection in mobile ad hoc networks. The presence of selfish nodes can disrupt the balance of resource consumption and shorten the lifetime of nodes. To address this issue, a mechanism design theory-based solution is proposed, which elects leaders based on their remaining resources and provides them with incentives in the form of reputation. This incentivizes nodes to participate honestly and balance resource consumption among all nodes in the network. The proposed scheme, based on the Vickrey, Clarke, and Groves model, is shown to effectively prolong the lifetime of intrusion detection systems in MANETs and balance resource consumption through simulation results. 
41220	4122089	The failure and recovery problem for replicated databases	A replicated database is a distributed database that stores data redundantly at multiple sites to improve system reliability. However, managing replicated data can be difficult as sites fail and recover. A replicated data algorithm has two parts: a read-write discipline and a concurrency control algorithm. The read-write discipline ensures logical conflicts are addressed, while the concurrency control algorithm manages physical conflicts. This paper presents a theory, based on serializability, for proving the correctness of replicated data algorithms. Three specific algorithms are discussed: Gifford's “quorum consensus” algorithm, Eager and Sevcik's “missing writes” algorithm, and Computer Corporation of America's “available copies” algorithm. 
41220	4122057	Query processing in a system for distributed databases (SDD-1)	This paper discusses the optimization techniques used in the SDD-1 distributed database system for relational queries. These queries are written in a high-level procedural language called Datalanguage and are translated into a relational calculus form called an envelope. The paper focuses on the optimization of envelopes, which are processed in two phases. The first phase involves executing relational operations at various sites to reduce the database to a subset relevant to the query. The second phase transmits this reduction to a designated site for local execution. The key optimization problem is efficiently performing the reduction phase, and the paper presents the semijoin operator as the principal reduction operator and an algorithm for constructing a cost-effective program of semijoins for a given envelope and database.
41221	4122146	Huddle: automatically generating interfaces for systems of multiple connected appliances	Huddle is a new system designed to make using connected appliances easier in both homes and workplaces. These systems, such as home theaters and presentation rooms, can be challenging to use because users must figure out how to split their tasks into smaller ones for each appliance and find the specific functions of each appliance to complete their tasks. Huddle simplifies this process by automatically generating interfaces for multiple appliances based on the content flow within the system. This allows users to easily complete their tasks without having to navigate through multiple interfaces. Huddle aims to make using connected appliances a more seamless and user-friendly experience.
41221	4122149	SHIFTR: a user-directed, link-based system for ad hoc sensemaking of large heterogeneous data collections	The authors have developed a new method and system, called Shiftr, to help users organize and understand large amounts of diverse information. Drawing on theories of categorization and cognitive psychology, Shiftr is designed for flexible sensemaking, adapting to users' changing goals and mental representations of concepts. Using a Belief Propagation algorithm from graph mining, Shiftr can efficiently cluster information of different types and requires minimal human labeling. It also supports the use of positive and negative examples. The usefulness of Shiftr is demonstrated through different scenarios, including one using a dataset with over 1.7 million author-paper relationships.
41222	4122282	Similarity searching in medical image databases	The proposed method is designed to handle approximate searching in medical image databases. The method uses attributed relational graphs to represent image content, including both expected and unexpected objects. The method is based on the assumption that a fixed number of labeled objects are common in all images, while there may also be a variable number of unlabeled objects. Queries can be answered by example, such as finding images similar to a specific X-ray. The images are indexed using R-trees and the method has desirable properties such as approximate search, no false dismissals, and fast search speeds. This makes it a suitable option for handling large databases.
41222	41222306	A novel optimization approach to efficiently process aggregate similarity queries in metric access methods	A similarity query involves searching a dataset for elements that are either within a certain distance from a specified center or the k nearest ones to the center. Different algorithms have been developed for efficient execution of similarity queries. However, there are also aggregate similarity queries, where multiple centers are given and the desired elements must be similar to all of them. These queries are useful for tasks like relevance feedback. The authors present a new algorithm, called Metric Aggregate Similarity Search (MASS), which can handle such queries on Metric Access Methods (MAM). MASS has several advantages, including its ability to work with any MAM, guaranteeing no false-dismissals, and handling any number of query centers. Experiments have shown that MASS performs well on both real and synthetic data, and can scale to large numbers of elements and high-dimensional datasets. 
41223	412231	Testing software in age of data privacy: a balancing act	Database-centric applications (DCAs) are commonly used in enterprise computing and require complex databases. Testing of these applications is often outsourced to reduce costs and improve quality. However, when proprietary DCAs are released, their databases cannot be shared with test centers due to data privacy laws. This results in testing being performed with anonymized data, leading to lower test coverage and fewer discovered faults. To address this issue, a new approach is proposed that combines program analysis with a data privacy framework designed specifically for software testing. This allows organizations to balance privacy concerns with the needs of testing. A tool has been developed and applied to Java DCAs, showing that by anonymizing data based on their impact on the application, test coverage can be maintained at a higher level.
41223	4122329	Is Data Privacy Always Good for Software Testing?	Database-centric applications (DCAs) are widely used in enterprise computing and rely on complex databases. To cut costs and improve quality, testing of DCAs is often outsourced to test centers. However, when proprietary DCAs are released, the accompanying databases must also be made available to test engineers in order to test with real data. This is crucial since fake data lacks important connections and can lead to lower code coverage and fewer bugs being uncovered. However, due to data privacy laws, organizations are unable to share sensitive data with test centers. As a result, testing is often done with fake data, resulting in lower quality DCAs. To address this issue, researchers have proposed an approach that uses program analysis to selectively apply a popular data anonymization algorithm called k-anonymity. This ensures that sensitive data is protected while still allowing for effective testing. However, it has been found that for small values of k, test coverage can drop significantly, making it challenging to achieve good quality when testing DCAs with data privacy measures in place. 
41224	4122431	Sixth international workshop on traceability in emerging forms of software engineering: (TEFSE 2011)	TEFSE 2011 is a workshop that will gather researchers and practitioners to address the difficulties of maintaining traceability for various forms of software engineering artifacts. The main goal of this 6th edition is to expand on the previous work done by the traceability research community and tackle the ongoing traceability challenges. The workshop will be a collaborative event where participants will discuss key problems related to software artifact traceability and propose potential solutions. Furthermore, it aims to highlight the significance of preserving traceability information during software development, improve collaboration between academia and industry, and facilitate the transfer of technology. 
41224	4122497	Grand challenges, benchmarks, and TraceLab: developing infrastructure for the software traceability research community.	Implementing successful and cost-effective traceability is a complex task that has become a major focus of research. This research has addressed a variety of issues, from understanding the needs of traceability users in industry to more technical and quantitative studies. However, progress is hindered by the time and effort required to establish a traceability research environment and compare new results to existing benchmarks. The Center of Excellence for Software Traceability (CoEST) is working on defining the Grand Challenges of Traceability, creating benchmarks, and developing TraceLab, a visual environment for conducting traceability experiments. These efforts aim to improve and expand traceability research capabilities in a scalable and efficient manner.
41225	41225108	Efficient Online Learning For Optimizing Value Of Information: Theory And Application To Interactive Troubleshooting	The optimal value of information problem involves selecting a set of tests at minimal cost to make the best decision based on observed outcomes. Existing algorithms are either unreliable or have high run times and assume a known distribution of test outcomes, which isn't always the case. To address these issues, a sampling-based online learning framework is proposed. This includes a dynamic hypothesis enumeration strategy for efficient information gathering and a progressive learning algorithm for unknown parameters. The approach has been proven to identify near-optimal decisions with high probability and has been successfully applied in a real-world troubleshooting application, resulting in high-quality decisions at low cost.
41225	4122591	Information Directed Sampling and Bandits with Heteroscedastic Noise.	The stochastic bandit problem involves maximizing an unknown function through noisy function evaluations. Existing approaches, such as upper confidence bound algorithms (UCB) and Thompson Sampling, assume independent and uniformly bound noise. This work considers heteroscedastic noise, where the noise distribution can depend on the evaluation point. A new frequentist regret framework is introduced to address shortcomings of existing approaches. This framework is used to create a frequentist version of Information Directed Sampling (IDS) which minimizes a surrogate of the regret-information ratio. Generalizations of concentration inequalities for online least squares regression are used to formulate several variants of IDS for different response functions. Empirical results show that these methods can outperform UCB and Thompson Sampling, even in the case of homoscedastic noise. 
41226	4122632	Trains of thought: generating information maps	In a world where information is abundant, it can be challenging to make sense of all the different pieces of knowledge. This is where metro maps come in - a methodology for creating structured summaries of information. These maps serve as a guide to navigate and explore complex stories and relationships among different pieces of information. The algorithm used to generate these maps ensures that important information is included while also showing the development of the story. The process of creating these maps is efficient and can be adjusted by users to align with their interests. Studies have shown that metro maps are effective in helping users acquire knowledge in an efficient manner.
41226	4122640	Metro maps of science	The increasing number of scientific publications makes it difficult for researchers to keep up with the ever-evolving literature. This is especially challenging for new investigators and those trying to explore multiple fields. To address this issue, the authors propose metrics of influence, coverage, and connectivity for scientific literature and use them to create structured summaries called "metro maps." These maps visually represent the relationships between papers and help researchers efficiently acquire new knowledge. Pilot studies show that map users have better precision and recall scores and find more important papers while performing fewer searches. 
41227	4122754	The low power energy aware processing (LEAP)embedded networked sensor system	In order to meet the demands of critical environmental monitoring applications, embedded networked sensor systems now require complex and high-power sensor devices, as well as high-performance computing and communication capabilities. To address these needs while maintaining low energy consumption, a new multiprocessor node architecture, called Low Power Energy Aware Processing (LEAP), has been developed. This architecture includes energy monitoring and power control scheduling for all subsystems, including sensors. A distributed node testbed has also been created to demonstrate the effectiveness of the LEAP architecture in meeting both performance and energy efficiency objectives for a wide range of applications. 
41227	4122737	etop: sensor network application energy profiling on the LEAP2 platform	The demand for embedded networked sensor (ENS) systems for critical environmental monitoring has increased, requiring complex and high power consuming devices. This also calls for high performance computing and communication. To meet these demands while maintaining low energy operation, a new multiprocessor node architecture called Low Power Energy Aware Processing (LEAP) has been developed. It includes energy monitoring and power control scheduling for all subsystems, including sensors. The second generation of LEAP, LEAP2, has even higher resolution energy monitoring and the ability to do energy profiling per process and application through a dedicated ASIC. A custom monitoring application called etop will be demonstrated, showcasing this profiling capability. 
41228	4122847	A metric conceptual space algebra	The use of cognitive modeling is important for designing spatial information systems that can effectively work with human users. Building concept representations using geometric and topological structures allows for semantic similarity and combination operations. Spatial concepts, which are closely connected to the physical world, are especially well-suited for this approach. Despite its advantages, the use of conceptual spaces is limited due to a lack of a comprehensive algebra. To address this, a metric conceptual space algebra is proposed, using convex polytopes to represent conceptual regions and incorporating context as a key element. This algebra is applied to a proof-of-concept spatial information system.
41228	4122828	Conceptual Space Markup Language (CSML): Towards the Cognitive Semantic Web	CSML is a semantic markup language specifically designed for sharing and publishing conceptual spaces, which are geometric structures that represent semantics at the conceptual level. It is a useful tool for describing semantics that cannot be accurately captured by traditional ontology languages used in the Semantic Web. CSML allows for the measurement of semantic similarity between concepts and the combination of concepts without shared properties, which are common cognitive tasks for humans. These operations are difficult for tools that rely on set-theoretic and syllogistic reasoning with symbolic ontologies, but can be easily modeled using conceptual spaces. This paper explains the design of CSML, introduces its key components, and provides examples of its application.
41229	412297	Facing Fault Management as It Is, Aiming for What You Would Like It to Be	Telecommunication systems are designed with redundancy and complexity to ensure reliability and high-quality service. This requires advanced tools for fault identification and management. To reduce the number of alarm events and accurately identify underlying faults, monitoring, filtering, and masking techniques are used. Fault management is a challenging task due to uncertainty in presenting symptoms. The paper discusses two approaches for fault management: rule discovery and Bayesian Belief Networks (BBNs). The former aims to reduce symptoms and provide better diagnostic assistance, while the latter uses intelligent techniques. The paper suggests that the research and development of both approaches can complement each other in efficient fault management.
41229	4122956	Discovering Rules For Fault Management	The Internet revolution relies on global telecommunication systems that were originally designed for voice traffic but now provide the necessary bandwidth for Internet traffic. These systems have built-in redundancy and complexity to ensure reliability and quality of service. To manage potential faults, complex fault identification and management systems are used, which involve monitoring, filtering, and masking alarm events. The ultimate goal is to determine and present the actual fault. In the meantime, methods such as computer-assisted human discovery and human-assisted computer discovery are being developed to reduce the number of symptoms and aid in diagnosis. These techniques are important in moving towards automated fault identification. 
41230	4123020	Improving Relative Transfer Function Estimates Using Second-Order Cone Programming.	This paper discusses how to estimate the Relative Transfer Function (RTF) between microphones using incomplete initial measurements. The initial measurement is only known for a few frequency bins and is completed by finding its sparsest representation in the time domain. This is done by solving a Second-Order Cone Program (SOCP), with free parameters representing the distance of the completed RTF from the initial estimate. These parameters are chosen based on the theoretical performance of the initial estimate. Experiments with real-world data show that this method greatly improves the accuracy of the RTF, particularly in situations with low signal-to-noise ratios.
41230	412303	Informed generalized sidelobe canceler utilizing sparsity of speech signals	The report introduces a new version of the generalized sidelobe canceler that relies on a set of prepared relative transfer functions (RTFs) for different potential positions of a target source in a specific area. The main challenge is to choose the correct RTF when the target's position is unknown and there are interfering sources. The proposed method uses an lp-norm, with p ≤ 1, measured at the blocking matrix output in the frequency domain to select the RTF. Experiments demonstrate that this approach outperforms other methods in scenarios where the target and interferer signals are speech signals.
41231	41231117	MuiCSer: A Process Framework for Multi-disciplinary User-Centred Software Engineering Processes	The paper introduces MuiCSer, a conceptual process framework for Multi-disciplinary User-centred Software Engineering (UCSE) processes that combines principles from software engineering and user-centred design to improve user experience. It aims to provide a common understanding of important components and activities in UCSE processes and serves as a reference for future research. The framework is demonstrated through the (re)design of a system and has been useful in studying the role of multi-disciplinary team members and process coverage of existing artefact transformation tools. 
41231	4123156	Task modeling for ambient intelligent environments: design support for situated task executions	The design of interactive systems for ambient intelligent environments is challenging due to the variety of devices and the user's environment. Traditional task-centered interface design is not suitable for these environments, so a new approach is needed. In this paper, the authors propose a task-centered approach using visualization and simulation to design interaction mechanisms. They focus on three key concepts - situated task allocations, user interface distributions, and visualization of context influences - to support this approach. The designer should consider the impact of context changes on task execution and be able to define task possibilities while considering environmental constraints. A tool is introduced to aid in this process through visualization and simulation. 
41232	4123288	Investigating The Influence Of Communication And Input Devices On Collaboration In Virtual Environments	Collaborative virtual environments (CVEs) are gaining popularity for both recreational and professional purposes, making it essential to understand the factors that affect collaboration between users. A study was conducted using a puzzle solving task in a basic interactive virtual environment to investigate the impact of voice communication and the use of different input/output devices. The results showed that voice communication plays a crucial role in dividing tasks and improving collaboration in CVEs. However, the use of different devices did not significantly affect collaboration, indicating that the availability of different options does not hinder the acceptance of CVEs. This suggests that there is no need to mandate a specific device for CVEs, particularly for home users.
41232	4123219	Influence of network delay and jitter on cooperation in multiplayer games	The article discusses the impact of transmission delays on modern multiplayer games, which can be caused by physical limitations or network overload. While studies have been conducted on competitive games, this paper focuses on how network delay affects cooperative games, where players must interact with shared objects and obstacles. User tests were conducted using Little Big Planet 2, a cooperative game, and it was found that delays over 100 ms negatively affect player performance and perception of network quality. Jitter also affects performance, but is not perceived as disruptive by players.
41233	4123315	Constructing Authorization Systems Using Assurance Management Framework	The model-driven approach has gained attention in developing secure software and systems, and developers are now utilizing it in the early stages of the software development life cycle (SDLC). However, security concerns are often overlooked due to the lack of proper mechanisms and tools. To address this issue, the authors propose a multilayered SDLC based on an assurance management framework (AMF) that focuses on developing authorization systems. The AMF enables the creation of a formal security model, specification and verification of security policies, generation of security codes, and rigorous testing. The authors also share their experience in implementing a role-based authorization system using this approach.
41233	4123365	Security-Enhanced OSGi Service Environments	Today's home and local-area networks are made up of a variety of personal devices, network devices, and services. Due to the frequent handling of private and sensitive information in these environments, it is crucial to have proper access control measures in place. The open services gateway initiative (OSGi) was developed to address this issue, but its current authorization mechanism is not robust enough for dynamic OSGi environments. This paper proposes the use of a role-based access control (RBAC) approach in OSGi environments as a more systematic and secure solution. The authors demonstrate how their proposed framework can enhance existing access control modules and achieve important RBAC features. A proof-of-concept prototype is also described, using the open source OSGi framework Knopflerfish, to showcase the feasibility of this approach.
41234	4123415	State-aware Network Access Management for Software-Defined Networks.	OpenFlow is a popular technique used in Software-Defined Networks (SDNs) that allows for greater programmability, control, and flexibility in managing network flows. However, its focus on simplicity and efficiency means that it lacks the ability for stateful forwarding in the data plane and limited access to connection-level information. To address these limitations, a new framework called STATEMON has been proposed, which adds global state-awareness to OpenFlow. This allows for better access control in SDNs and enables the implementation of stateful network applications such as firewalls and port knocking. Evaluations of STATEMON show minimal message exchanges and manageable overhead, making it a practical and feasible solution for SDNs.
41234	4123489	Managing heterogeneous network environments using an extensible policy framework	Security policy management is crucial for organizations that rely on computer networks and the Internet for their daily operations. As organizations grow and their networks become larger and more diverse, it becomes increasingly difficult to effectively deploy and enforce security policies. To address this challenge, a policy framework called Chameleos-x has been developed. This framework is designed to consistently enforce security policies across different systems, such as operating systems, firewalls, and intrusion detection systems. The paper focuses on the design and architecture of Chameleos-x and demonstrates its effectiveness in helping organizations implement security policies in diverse environments. The feasibility of the approach is also demonstrated through experimentation.
41235	41235328	Adaptive thermal management for portable system batteries by forced convection cooling	The lifespan of a battery is greatly affected by its operating conditions, especially the temperature. Higher temperatures can significantly accelerate the aging process of batteries. To address this issue, a forced convection cooling technique is introduced for batteries used in portable systems. However, there is a tradeoff between the battery's service time and cycle life when using a cooling fan, as the fan's power consumption can decrease the battery's charge capacity. This paper presents the first solution for the adaptive thermal management problem in portable systems, using a hierarchical algorithm that combines reinforcement learning and dynamic programming. This approach aims to find the optimal balance between fan speed and battery temperature to prolong the battery's lifespan.
41235	41235353	Battery-aware power management based on Markovian decision processes	The paper focuses on maximizing the use of battery power in portable electronic systems while adhering to constraints on latency and loss rate. A stochastic model of a power-managed system is presented, incorporating the current rate-capacity characteristic and relaxation-induced recovery of rechargeable battery cells. The dynamic power management problem is then solved using a linear programming approach, resulting in a 17% increase in energy delivery per unit weight of battery cells compared to existing heuristic methods. This demonstrates the effectiveness of the proposed method in optimizing battery usage. 
41236	4123621	Non-Negative Group Sparsity with Subspace Note Modelling for Polyphonic Transcription.	Automatic music transcription (AMT) involves breaking down a piece of music into pitch and time components. This is typically done by using non-negative matrix factorisation (NMF) methods to decompose the spectrogram of the music. However, using only one atom to represent each note may not accurately capture the changing spectrum of a note over time. Previous studies have tried to address this by using different atoms for different parts of a note or by using large dictionaries. In this paper, the use of subspace modelling and group sparsity is explored to better capture the variability in note spectra. This approach leads to improved AMT results when applied to a generic harmonic subspace dictionary.
41236	4123616	Masked non-negative matrix factorization for eire detection using weakly labeled data.	The use of acoustic monitoring for bird species has become increasingly important in signal processing. However, many available bird sound datasets only have coarse weak labels rather than exact timestamps for bird calls. Traditional Non-negative Matrix Factorization (NMF) models are not well-suited for dealing with weakly labeled data. To address this issue, a novel Masked NMF approach is proposed for bird detection. This method uses a binary mask during dictionary extraction to control which parts of the dictionary are used to reconstruct the data. It outperforms conventional NMF approaches and is currently the best non-deep learning method for bird detection on weakly labeled data. The proposed Masked NMF also achieved the best result among non-deep learning methods in the recent Bird Audio Detection Challenge.
41237	412376	Building detection from mobile imagery using informative SIFT descriptors	This article proposes a new method for reliable outdoor object detection on mobile phone imagery using off-the-shelf devices. The method, called the 'Informative Descriptor Approach', uses SIFT features (i-SIFT descriptors) and an attentive matching approach to improve upon traditional SIFT descriptor based keypoint matching. In the off-line learning stage, standard SIFT responses are evaluated using an information theoretic quality criterion and a decision tree is used to map SIFT descriptors to entropy values. The authors argue that the proposed i-SIFT method has advantages over standard SIFT encoding in terms of performance complexity and demonstrate its effectiveness in a mobile vision experiment on the MPG-20 reference database. 
41237	4123734	Urban Object Recognition From Informative Local Features	Autonomous mobile agents need object recognition to interpret and locate complex scenes. In urban environments, recognizing buildings is important for robot systems to navigate and use visual feedback for self-localization. A new method, called the informative local features approach, uses an information theoretic saliency measure and a decision tree to quickly identify important features for recognition. This approach is robust against occlusion and noise, and can handle changes in scale and illumination. Tests on a publicly available object database show that this method is efficient and can even outperform the popular SIFT feature approach. Building recognition has many practical applications, such as mobile mapping, unmanned vehicle navigation, and car driver assistance.
41238	4123856	Validating Adequacy and Suitability of Business-IT Alignment Criteria in an Inter-Enterprise Maturity Model 202	The alignment of business requirements with information technology is a crucial issue in enterprise computing. While there are existing criteria to measure alignment within a single enterprise, there is a lack of research on identifying or rethinking these criteria in inter-enterprise settings. In these settings, alignment is influenced by economic processes rather than centralized decision-making. To address this, researchers have developed a maturity model for business-IT alignment in inter-enterprise settings. This paper discusses the multi-method approach used to validate the criteria included in the model, including a focus group session and case study. The results and implications for the model are also presented.
41238	4123846	Value-based requirements engineering for value webs	Since the 1980s, requirements engineering for information systems has utilized techniques from Information Engineering, such as business goal analysis and process modeling. These techniques have recently been supplemented with portfolio management, which aids in decision-making for IT projects. However, in today's interconnected world, these techniques face challenges in accurately specifying requirements for IT systems used in value webs. To address this issue, the authors propose coupling IE and portfolio management with value-based RE techniques at the business network level. They provide an example to illustrate how these techniques can work together.
41239	4123916	Relevance and problem choice in design science	The idea of rigor versus relevance is based on the misconception that rigor involves linear technology transfer and positivistic science. However, historical insights from the history of science and technology show that this is not the case and that technology is not transferred linearly from research to practice. Both technical and social sciences share similarities in terms of practical problem solving and research focus. Relevance is also context-dependent and depends on the goals of stakeholders. This paper highlights the importance of applicability in design science, and discusses the need to consider the context and practical conditions in research. It also discusses the relevance of these insights for information systems design science and its implications for research methods, strategy, and knowledge transfer.
41239	4123977	Technical action research as a validation method in information systems design science	The current approach to combining action research and design science involves identifying a problem in an organization, using an artifact to address the problem, and reflecting on the process. This is done in hopes of finding a balance between practical relevance and scientific rigor. However, this paper proposes an alternative approach, starting with the artifact and then testing it in real-life problems. This aims to bridge the gap between idealized designs and the complexities of real-world situations. The paper discusses the role of idealization in design science and suggests a method called technical action research (TAR) to bridge the gap. TAR involves the researcher taking on three roles: artifact developer, artifact investigator, and client helper. This approach is compared to other uses of action research in design science and to canonical action research.
41240	4124041	Scalable hardware monitors to protect network processors from data plane attacks	Modern router hardware in computer networks relies on programmable network processors for various packet forwarding operations, making them vulnerable to attacks launched through the data plane without access to the control interface. Previous research has shown that a single malformed UDP packet can take over a vulnerable network processor and cause a devastating denial-of-service attack. To defend against these attacks, hardware monitoring systems can be used to track processor core operations and detect any deviations from expected behavior, triggering reset and recovery actions. This paper introduces a Scalable Hardware Monitoring Grid that allows for the dynamic sharing of monitoring resources among multiple processor cores. The system has been tested on large network processors with numerous cores and a multicore prototype has been implemented on an FPGA platform.
41240	412402	Securing Network Processors with High-Performance Hardware Monitors	The Internet is an essential tool in daily life and its reliability is crucial. With the rise of attacks on the Internet, it is important to protect it with the same level of security as the devices it connects. Recent developments in network router architecture have made them more vulnerable to software attacks. These routers use software to determine their protocol processing functions, making them susceptible to attacks through data packets. To combat this, a hardware monitor can be used to ensure the correct functioning of the network processor. This monitor can detect and prevent malicious behavior, while still allowing regular network traffic to pass through uninterrupted. This study presents advancements in hardware monitoring for network processors, including a low-overhead monitor architecture that effectively prevents stack smashing attacks without affecting data throughput. The authors also provide an evaluation of monitor design parameters to optimize its effectiveness.
41241	4124118	Reactive scheduling in a job shop where jobs arrive over time	The paper discusses the dynamic job shop scheduling problem (DJSSP) with job release dates, which is common in production systems. Unlike the classical job shop scheduling problem (CJSSP), where all jobs are known at the beginning, DJSSP deals with continuously arriving jobs with unknown attributes. The proposed heuristic approach uses reactive scheduling to solve this problem by breaking it down into sub problems. These sub problems are solved using priority scheduling and scheduling rules (SRs) are used to prioritize the system elements. The paper also proposes an approach using gene expression programming (GEP) to construct efficient SRs for DJSSP and compares them with rules constructed by other methods on various measures of performance.
41241	4124156	An effective hybrid particle swarm optimization algorithm for multi-objective flexible job-shop scheduling problem	The flexible job-shop scheduling problem (FJSP) is a more complex version of the traditional job-shop scheduling problem. While existing optimization algorithms work well for mono-objective FJSP, they struggle with the more challenging multi-objective FJSP with conflicting goals. To address this, a hybrid approach using both particle swarm optimization (PSO) and tabu search (TS) algorithms is proposed. PSO combines local and global search methods for efficient searching, while TS is specifically designed for combinatorial optimization problems. The hybrid approach proves to be effective in solving multi-objective FJSP, especially for larger problems. 
41242	412425	Effective Information Spreading in Social Networks	Traditional data management techniques are no longer sufficient in the face of the emerging Big Data paradigm. The increasing amount of data related to social interactions between users requires advanced analysis strategies. The heterogeneity and speed of this data also necessitate the creation of new data storage and management tools. This paper presents a framework specifically designed to analyze user searches while connected to a social network, with the goal of identifying influential users who can spread their influence throughout the network. Understanding user preferences is crucial in various fields such as viral marketing, tourism promotion, and food education.
41242	4124248	Evaluating the Influence of User Searches on Neighbors	The rise of Big Data has rendered traditional data management techniques inadequate in many real-life situations. With the vast amount of data available on user suggestions and searches, advanced analysis strategies are needed to effectively utilize this information. Additionally, the heterogeneity and high speed of this data require the development of new data storage and management tools. This paper presents a proposal for analyzing how user searches and suggestions impact their social environment, with the goal of identifying influential users who can spread their influence throughout the network. This information is crucial in various scenarios such as viral marketing, tourism promotion, and food education.
41243	4124352	Detecting broad phonemic class boundaries from greek speech in noise environments	This study evaluates the performance of an automatic segmentation method for Greek speech signals into phonemic classes. The method was tested on clear speech and speech with various types of noise. The results showed high accuracy of 76.1% for clear speech without over-segmentation. However, there was a slight decrease in accuracy (4%) when the speech was distorted with wideband noise. This suggests that the method is effective for segmenting speech signals, but may be affected by certain types of noise. Overall, the results were promising and demonstrate the potential of this approach for automatic speech segmentation.
41243	4124323	Vergina: A Modern Greek Speech Database for Speech Synthesis	The Vergina speech database is a resource for developing speech synthesis systems for the Modern Greek language. It was created through a recording campaign using a text corpus of 5 million words from various sources. The selected utterances cover a wide range of topics and styles, making it suitable for different applications. The database contains approximately 3,000 phonetically balanced Modern Greek utterances, recorded in an audio studio. Annotation was done using task-specific tools and manual corrections, ensuring high quality. This database is a valuable tool for research and development in the field of speech synthesis for the Greek language. 
41244	4124459	Phone duration modeling: overview of techniques and performance optimization via feature selection in the context of emotional speech	The accurate modeling of prosody, including phone duration, is crucial for creating high-quality synthetic speech that sounds natural. This study evaluates different modeling techniques, such as decision trees and linear regression, that have been successfully used for various tasks. Feature selection techniques, such as RReliefF and Correlation-based Feature Selection, are also applied to improve performance. The study uses a Modern Greek emotional speech database and found that feature selection significantly improves the accuracy of phone duration prediction, with the M5p trees model achieving the best results. This highlights the importance of feature selection in phone duration modeling for emotional speech.
41244	4124422	Two-stage phone duration modelling with feature construction and feature vector extension for the needs of speech synthesis	The proposed two-stage phone duration modelling scheme aims to enhance the prosody modelling in speech synthesis systems. In the first stage, independent feature constructors (FCs) extract linguistic features from text. In the second stage, a phone duration model (PDM) operates on an extended feature vector, obtained by appending the phone duration predictions from the FCs. Experiments on American-English and Modern Greek databases showed that this two-stage scheme outperforms the best individual predictor and a two-stage scheme without the extended feature vector. The mean absolute error and root mean square error were reduced by 3.9% and 3.9% on the American-English database and by 4.8% and 4.6% on the Modern Greek database. 
41245	4124582	Learning verb complements for modern greek: Balancing the noisy dataset	The article discusses the challenges of automatically learning to identify verb complements from natural language corpora without the use of complex linguistic resources. This often leads to noise in the data and an imbalanced set of class-labelled feature-value vectors, with one class heavily underrepresented. To address this, the authors propose balancing the learning data by applying one-sided sampling to the training corpus. They also introduce the use of the value difference metric, which is better suited for nominal attributes, in this process. The effectiveness of this approach is tested on various learning algorithms, including Bayesian learners and decision trees, resulting in a 22% improvement in performance. The authors also present a new variation of Bayesian belief networks, called COr-BBN, which further improves the results. Overall, their approach achieves a 73.7% f-measure for the complement class using just a phrase chunker and basic morphological information for preprocessing.
41245	412455	Bayesian feature construction	This paper discusses the issue of improving classification performance through methods other than enhancing the ability of Machine Learning algorithms to create accurate models. The proposed approach involves creating more features based on Bayesian networks, which can reveal hidden aspects of the data domain. This feature construction algorithm is able to represent conditional independence assumptions and project relational attributes that may not be obvious to the classifier in their original format. The results show a significant increase in classification performance across different machine learning domains, as demonstrated by various classifiers using data from the UCI ML repository and the Artificial Intelligence group.
41246	4124635	Ontology fusion in HLA-based collaborative product development	This paper introduces a new method for combining ontologies in HLA-based collaborative product development systems, with the goal of promoting mutual understanding among different systems. The approach involves three steps: mapping, alignment, and merging of ontologies. It utilizes an axiom-based deduction strategy and takes into account heavily weighted ontologies. This approach is able to identify all explicit and derived inter-ontology relationships, and can even find implicit equivalent relationships. By improving the efficiency of preparation and reducing the workload for platform adjustments, this approach has the potential to enhance the applicability and flexibility of collaborative development systems.
41246	41246109	Ontology maintenance in high level architecture federation development and execution process	This paper presents an approach to dynamically maintaining the federation collaboration ontology in the High Level Architecture (HLA) development and execution process. The main goal is to provide guidelines for using and organizing the ontology to meet different requirements. The proposed approach includes two algorithms, one for federate joining and one for resigning, and uses an axiom-based deduction strategy. It also takes into account heavy-weighted ontologies and can identify all explicit and derived inter-ontology relations. A case study demonstrates the potential of this approach to improve efficiency, reduce workload, and enhance the flexibility of HLA based collaborative product development systems.
41247	4124770	Coordination of Free/Libre Open Source Software Development	This paper examines the coordination mechanisms used in free/libre open source software (FLOSS) development and compares them to those used in mainstream software development. Using coordination theory, the study analyzed data from three successful FLOSS projects and found similarities in the management of task-task dependencies, but differences in how task-actor dependencies are handled. While proprietary software development typically involves a designated developer for each piece of code, FLOSS projects often rely on "self-assignment" by volunteers. The paper discusses the potential transferability of these practices to mainstream software development and suggests areas for further research.
41247	4124774	Coordination practices within FLOSS development teams: The bug fixing process	FLOSS, or Free/Libre Open Source Software, is created by teams of developers from around the world who primarily communicate through email and bulletin boards. These teams have found ways to benefit from the advantages and overcome the challenges of working in a distributed environment. However, there is limited research on the work practices of FLOSS teams. This study focuses on the bug-fixing process, a key aspect of FLOSS project success. By analyzing messages in the bug tracking system of four projects, the researchers identify the tasks completed, coordination mechanisms used, and the roles of both the development team and the wider FLOSS community. More research is needed in this area.
41248	4124818	Decentralized, connectivity-preserving, and cost-effective structured overlay maintenance	This paper discusses the maintenance of structured overlays in decentralized peer-to-peer (P2P) systems, taking into account potential system and network failures. The authors propose a specific set of requirements for overlay maintenance protocols, including decentralization, maintaining connectivity, and convergence to the desired structure. They also present a complete protocol with proof that meets these requirements. This protocol addresses challenges related to decentralization and concurrency in the system and overcomes limitations of existing maintenance protocols, such as the need for a centralized bootstrap system, known stabilization time, and large local membership lists. 
41248	4124842	Compact routing in power-law graphs	The authors modified the compact routing scheme by Thorup and Zwick to better suit power-law graphs. They used the theory of unweighted random power-law graphs with fixed expected degree sequence by Aiello, Chung, and Lu to analyze their adapted scheme. This is the first theoretical bound for a compact routing scheme that is specific to the parameter of the power-law graph model. They proved that for stretch 3, expected sizes of O(nγ log n) bits are sufficient, and all routing tables can be constructed at once in expected time O(n1 + γ log n). The scheme is a labeled scheme with a stretch-5 handshaking step and uses addresses and message headers with O(log n log log n) bits. Simulations on real-world and synthetic graphs showed the effectiveness of the scheme. They also adapted the approximate distance oracle by Thorup and Zwick for stretch 3 and obtained a new upper bound for space and preprocessing. 
41249	4124955	Matroid Online Bipartite Matching and Vertex Cover.	The Adwords and Online Bipartite Matching problems have gained attention in recent years due to their relevance to Internet advertising. New models and extensions have been developed to address practical issues, and this paper proposes a new generalization based on matroids. This new setting is potentially useful in both theory and practice due to the rich structures and expressive power of matroids. The paper presents algorithms for Matroid Online Bipartite Matching that achieve the same performance as the classical version of the problems. These algorithms are $1-1/e$-competitive and are applicable to both the small bid assumption and the random arrival model. A key aspect of these algorithms is a primal-dual waterfilling procedure that accommodates for matroid constraints. This paper also introduces techniques for handling submodularity in online problems, which may be of interest for future research.
41249	4124971	Scalable influence maximization for prevalent viral marketing in large-scale social networks	Influence maximization is the task of finding a small group of influential nodes in a social network that can spread influence to a large number of other nodes. This is important for viral marketing in online social networks. Previous solutions, such as the greedy algorithm, have limitations in scalability and performance. In this paper, a new heuristic algorithm is proposed that is easily scalable and has a tunable parameter for balancing between running time and influence spread. Extensive simulations on real-world and synthetic networks show that this algorithm outperforms other scalable solutions, with increases in influence spread of 100% to 260%. This makes it the best option for influence maximization in large-scale networks.
41250	4125045	A wireless mesh network testbed in rural mountain areas	A wireless mesh network testbed has been created in two rural mountain areas to address the digital divide. The testbed consists of 22 nodes installed on power poles and is connected to a control server at Niigata University through a gateway node at the local government office. Each node has two wireless LAN interfaces and an access point. Throughput and packet transmission success rates are measured in the testbed, and technical challenges in managing and operating the network in rural areas are being investigated. This testbed aims to provide disaster-tolerant and reliable broadband access to bridge the digital gap in rural areas.
41250	4125018	Performance of Wireless Mesh Networks with Three Sector Antenna	Wireless Mesh Networks (WMN) are an important technology that rely on mobile ad hoc networks (MANETs). In early 2010, a three sector antenna system was proposed for WMN and its performance was analyzed. In this paper, a real-world three sector antenna system is introduced in the WMN test bed at Niigata University, which uses IEEE 802.11 wireless LAN and supports multi-interface and multi-channel. Channel assignment is performed and two network patterns are obtained. Single-flow and multi-flow throughput experiments are conducted using the three sector antenna system, and its performance and characteristics are discussed. The test bed node has both three sector and omni directional antennas, and a comparison between the two is made. The results show that the three sector antenna system has significantly better throughput performance in one-hop experiments. 
41251	4125149	PhotoMap: using spontaneously taken images of public maps for pedestrian navigation tasks on mobile devices	Public maps are commonly found in mid- to large-sized cities, as well as in parks and near hiking trails. These maps serve as a useful tool for both tourists and locals to orient themselves and find information about unfamiliar places. They offer advantages over mobile maps from services like Google Maps Mobile or Nokia Maps, as they often include local landmarks and sights that are not shown on digital maps. These maps, known as 'You are here' (YAH) maps, are designed for specific purposes such as zoo or hiking maps, and are aesthetically pleasing. A new technique called PhotoMap uses images of these YAH maps, taken with a GPS-enhanced mobile camera phone, as background maps for on-the-fly navigation tasks. The challenge is to accurately georeference the image to support pedestrian navigation, and a study has been conducted to determine the suitability of various public maps for this task. The feasibility of using these georeferenced photos for navigation on GPS-enabled devices is also evaluated.
41251	4125141	Innovative Retail Laboratory Investigating Future Shopping Technologies	The Innovative Retail Laboratory (IRL) is a research lab run in collaboration between the German Research Center for Artificial Intelligence (DFKI) and GLOBUS SB-Warenhaus Holding. It focuses on developing and testing new technologies and concepts for intelligent shopping, such as virtual assistants for dieting and allergies, digital sommeliers, and personalized cross and up selling. These technologies are tested to see if they are suitable for everyday use and beneficial for customers. The lab also explores new ways of customer interaction, including personalized shopping assistants, "talking" products, and intelligent shopping carts. Additionally, IRL looks at the relationship between stores and customers, starting with personalized shopping preparation and offers at home and continuing with advice and information about purchased goods. 
41252	4125245	Automatic classification of reading disorders in a single word reading test	The traditional method for evaluating reading disorders in clinical practice relies on perceptual assessment. To address this issue, the use of automatic speech processing techniques is proposed to classify reading disorders. A study was conducted with 38 children suspected of having a reading disorder, using a German standard test. Each child's reading errors and duration were recorded and compared to age-dependent limits for diagnosis. Results showed that 30 of the 38 children were confirmed to have a reading disorder. This paper presents the findings on using automatic evaluation for a single word reading test, achieving a recognition rate of 78.9% for detecting reading error limits and 97.4% for classifying reading disorders. 
41252	4125230	An automatic version of a reading disorder test	A new system has been developed to automatically diagnose reading disorders using a speech recognition engine and a module for prosodic analysis. The system has eight subtests with a recognition accuracy of at least 95&percnt; in each one. The results of these subtests are combined to determine if a child has a reading disorder, with the system correctly identifying 98.3&percnt; of children in the final classification stage. This system will aid in the clinical evaluation of reading disorders in the future.
41253	4125323	Efficient shared memory with minimal hardware support	Shared memory is a popular approach for developing parallel programs, as it is seen as more user-friendly compared to message passing. This model can be implemented through hardware, software, or a combination of both. However, it faces the challenge of maintaining cache coherence, which is crucial for efficient performance. Hardware-coherent multiprocessors have proven to be more efficient than distributed shared-memory (DSM) emulations on message-passing hardware. This paper proposes an alternative option, a network or multiprocessor system that offers a global physical address space and allows for non-coherent memory accesses without interrupting remote processors. This option, called NCC-NUMA, has been tested through simulation and shown to have comparable performance to fully hardware-coherent designs, and outperforms traditional DSM systems. 
41253	4125355	High performance software coherence for current and future architectures	Shared memory is a popular way to program large-scale parallel computing systems, but it requires a coherence mechanism to ensure that processors do not use outdated data. There are two main implementation options for this: distributed shared memory on networks of workstations or tightly coupled distributed shared memory multiprocessors. However, performance varies greatly between these two options. An intermediate solution, using memory-mapped network interfaces without cache coherence, can provide similar performance benefits at a lower cost. This approach is supported by a software coherence protocol and has been studied through simulations. The results suggest that this is a more cost-effective approach than pure distributed shared memory or hardware cache coherence for large-scale shared-memory multiprocessing.
41254	4125428	Lifted cover facets of the 0-1 knapsack polytope with GUB constraints	This paper discusses the use of facet-defining inequalities from minimal covers as effective cutting planes in 0-1 integer programming algorithms. The authors build upon previous work by Balas and Zemel by providing a set of inequalities that can determine the lifting coefficients for facet-defining inequalities in the 0-1 knapsack polytope, regardless of the order in which variables are lifted. They also extend this result to address the 0-1 knapsack problem with generalized upper bound constraints. These findings can improve the efficiency and accuracy of algorithms used to solve 0-1 integer programming problems.
41254	4125447	Cutting Planes for Multistage Stochastic Integer Programs	This paper discusses the issue of generating cutting planes for multistage stochastic integer programs. The authors propose a general approach that combines valid inequalities for different scenarios to generate these cutting planes. They demonstrate the effectiveness of this method by applying it to a stochastic version of a dynamic knapsack problem and stochastic lot-sizing problems. Computational results show that these new inequalities are highly effective when used in a branch-and-cut algorithm. Overall, this approach provides a promising solution for improving the efficiency of solving multistage stochastic integer programs.
41255	4125580	Decomposition-Based Recursive Least Squares Identification Methods For Multivariate Pseudo-Linear Systems Using The Multi-Innovation	This paper explores the estimation of parameters in multivariate pseudo-linear autoregressive systems. A decomposition-based recursive generalised least squares algorithm is proposed to estimate the parameters by dividing the system into two subsystems. To enhance the accuracy of parameter estimation, a decomposition-based multi-innovation recursive generalised least squares algorithm is also developed using the multi-innovation theory. Simulation results demonstrate the effectiveness of these algorithms.
41255	4125533	Auxiliary Model-Based Recursive Generalized Least Squares Algorithm for Multivariate Output-Error Autoregressive Systems Using the Data Filtering	The paper discusses the issue of estimating parameters in multivariate output-error autoregressive systems. A filtering-based auxiliary model recursive generalized least squares algorithm is proposed, which involves filtering input-output data and deriving two identification models – one for system parameters and one for noise parameters. This algorithm is more efficient and accurate compared to existing methods. An example is provided to demonstrate its effectiveness.
41256	4125628	CLIFT: a Cross-Layer InFormation Tool to perform cross-layer analysis based on real physical traces	Network simulations often rely on approximated models rather than real physical traces, which can make it difficult to accurately analyze the impact of link layer reliability schemes on transport layer performance. In order to address this issue, the authors propose a software called CLIFT that translates real physical events from a given trace into a format that can be used in a network simulator like $ns$-2. This allows for more realistic cross-layer analysis, particularly in the transport layer. The software is specifically designed for 4G satellite communications scenarios and provides metrics for consistent cross-layer analysis. The paper discusses the internal mechanisms and benefits of using CLIFT in simulations.
41256	4125643	On the Impact of Link Layer Retransmissions on TCP for Aeronautical Communications.	This article examines the effect of link layer retransmissions on TCP performance in aeronautical communications. The architecture of aeronautical networks, which is characterized by significant channel access delays, is described. Different retransmission strategies at the link and transport layers are also explained. The authors use a worst-case scenario to demonstrate the advantages of implementing Automatic Repeat Request (ARQ) at the link layer in reducing transmission delay. The trade-off between fast data transmission and efficient use of satellite capacity is evaluated by adjusting link layer parameters. This study highlights the importance of considering link layer retransmissions in optimizing TCP performance in aeronautical communications.
41257	4125759	Task scheduling for heterogeneous reconfigurable computers	This article discusses the problem of executing a changing set of tasks on a reconfigurable system, which includes a processor and a reconfigurable device. The scheduling of tasks is managed by a scheduler, which can allocate them to either the processor or the device. A placer is responsible for placing tasks on the device at runtime, using a database of equivalent implementations. However, if there is not enough space on the device, the scheduler may have to preempt a running task or run it on the software. This article presents an implementation of a placer module and explores task preemption, both of which are part of an operating system for a reconfigurable system in development.
41257	4125768	Optimal Free-Space Management and Routing-Conscious Dynamic Placement for Reconfigurable Devices	This article discusses algorithmic results related to allocating resources on computational hardware devices that have partial reconfigurability. By using computational geometry techniques, the authors have developed a method that efficiently maintains the free and occupied space of a set of rectangular modules. This method has a time complexity of O(n log n), which is an improvement over previous approaches that required O(n2) time. Additionally, the authors demonstrate that this approach is optimal by showing a matching lower bound of Omega (n log n). Furthermore, the article discusses a method for finding an optimal feasible communication-conscious placement with a time complexity of Theta(n log n). Both of these algorithms are easy to implement and have shown positive results in experiments.
41258	4125848	Robust blind watermarking of point-sampled geometry	This article discusses the use of digital watermarking to protect the copyright of 3-D models. However, traditional methods cannot be applied to point clouds, as the connectivity information may change due to attacks. To address this issue, the article proposes a new blind watermarking mechanism that uses a cluster tree to embed and extract watermarks from 3-D point-sampled geometry. This technique is robust against various attacks, such as affine transformations, reordering, cropping, simplification, and noise addition. It can also be applied to 3-D meshes and achieves high hiding capacity with low distortion. The time complexity is estimated to be O(n log n), where n is the number of 3-D points.
41258	412581	Receiver-based loss tolerance method for 3D progressive streaming	Efficient streaming of detailed 3D models over lossy networks has been a challenge due to unpredictable packet loss, which can cause connectivity issues and distortions in decompressed meshes. Previous research has proposed methods to handle transmission loss, but they come with additional costs and distortions. In this paper, the problem is addressed from a receiver's perspective, with a novel loss tolerance scheme that uses constraints during compression and a prediction method for handling loss on the client side. The algorithm does not require data retransmission or extra protection bits. Mesh refinement data is streamed separately on reliable and unreliable networks to reduce delay and achieve satisfactory decompression results. Experimental results show that the proposed solution is efficient, with quick decompression and low distortions. The prediction technique also produces good approximations with controlled error propagation.
41259	4125918	Evaluation of Three Parametric Models for Estimating Directional Thermal Radiation from Simulation, Airborne, and Satellite Data.	This research evaluated the performance of three published directional thermal radiation models (RL, BRDF, Vinnikov) for correcting thermal radiation anisotropy for land surface temperature (LST) measurements. Results showed that the Vinnikov model performed the best at canopy scale, particularly for erectophile canopy and low leaf area index (LAI). At pixel scale, all three models had a stable effect, but the Vinnikov model was most effective for correcting LST in savannas, grasslands, and areas with low LAI. The Vinnikov model was then calibrated for different land cover types and was found to significantly improve the accuracy of LST measurements, reducing the RMSE by 0.89 K. This study highlights the importance of an appropriate model for correcting thermal radiation anisotropy in order to improve the accuracy of LST measurements.
41259	4125928	Scale issues in remote sensing: a review on analysis, processing and modeling.	Quantitative remote sensing has brought about a growing concern for scale issues among scientists. The scale discrepancy between data sources and models has made it challenging to interpret data and apply models effectively. As a result, scaling remotely sensed information at different levels has become a major focus in the field. This paper aims to address the scale issues in terms of analysis, processing, and modeling, providing technical support for dealing with these challenges. The definition of scale and related terminology is presented, followed by a discussion on the causes and effects of scale on measurements, retrieval models, and products. The paper also explores ways to describe the scale threshold and domain, and compares various scaling methods, particularly up-scaling methods, in detail.
41260	4126013	Transform-space view: performing spatial join in the transform space using original-space indexes	Spatial joins are a common technique used in databases to find pairs of objects that have a specific spatial relationship. To improve the efficiency of these joins, indexes are often used. Original-space indexes, like the R-tree, are complex to optimize because they deal with the extents of objects. Transform-space indexes, on the other hand, only deal with points and are simpler to optimize. However, they cannot be applied to original-space indexes. To address this issue, a new concept called the transform-space view is proposed, which allows an original-space index to be interpreted as a transform-space index without any modifications. This leads to the development of the transform-space view join algorithm, which outperforms existing algorithms in terms of buffer size, disk accesses, and wall clock time. This algorithm has the potential to be applied in other spatial query processing algorithms in the transform space. 
41260	4126046	Query optimization in a memory-resident domain relational calculus database system	The paper discusses techniques for optimizing queries in memory-resident database systems, which differ from those used in conventional disk-resident systems. The authors address several aspects of query optimization in these systems, including developing a CPU-intensive cost model, optimization strategies for main-memory query processing, and insights into join algorithms and access structures that take advantage of the data's memory residency. They also examine the impact of the operating system's scheduling algorithm on the memory-residency assumption. Their research shows that a significant cost in processing queries in these systems is due to evaluating predicates. They present results from performance measurements using the Office-by-Example (OBE) tool, which demonstrate the effectiveness of their techniques. The paper highlights the need for further study on query optimization in memory-resident database systems and provides practical solutions for addressing these challenges.
41261	4126196	Extracting and mining protein-protein interaction network from biomedical literature	SPIE-DM is a biomedical data mining system designed to extract and analyze protein-protein interactions from biomedical literature, specifically from MedLine. The system consists of two phases: phase 1 focuses on extracting the interactions using a scalable and portable method called SPIE, resulting in a scale-free network graph. Phase 2 uses a clustering algorithm called SFCluster to mine the network and identify potential protein complexes, which are important for studying protein functionality. This algorithm considers the characteristics of scale-free networks and utilizes local density and neighborhood functions to identify meaningful clusters at different density levels. Experiments on 1600 chromatin proteins demonstrate the effectiveness of SPIE-DM for extracting and mining data from biomedical literature databases. 
41261	4126161	Continuous query processing in data streams using duality of data and queries	TelegraphCQ and other recent data stream systems utilize the duality between data and queries in their query processing methods, classifying them as either data-initiative or query-initiative depending on whether they are initiated by selecting a data element or a query. However, these systems have not fully taken advantage of this property, using the two dual methods separately for continuous and ad-hoc queries. The authors propose an integrated approach, using spatial join as a powerful tool to optimize continuous query processing. They present a new perspective of transforming the continuous query problem into a multi-dimensional spatial join problem and introduce a new algorithm, Spatial Join CQ, which outperforms previous algorithms by up to 36 times for simple selection queries and 7 times for sliding window join queries.
41262	4126244	Speaker Diarization for Conference Room: The UPC RT07s Evaluation System	In this paper, the authors discuss the UPC speaker diarization system used in the NIST Rich Transcription Evaluation. The system is based on the ICSI RT06s system and utilizes agglomerative clustering with a modified Bayesian Criterion measure. This is the first time UPC has participated in the evaluation and their goal was to create a baseline system for future research in diarization. Prior to diarization, the system includes a Speech/Non-Speech detection module and a Wiener Filtering module. The speech parameterization also includes a Frequency Filtering method. The authors also made some changes to the complexity selection algorithm and introduced a new post-processing technique for the shortest clusters. 
41262	4126240	Dialogue management in an automatic meteorological information system	This paper discusses a new automatic meteorological information system that offers real-time data from automatic sensors through a user-friendly voice interface. The system also includes an automatic warning service for weather updates. The goal is to make meteorological information more accessible, personalized, and user-friendly through the use of natural language processing, dialogue management, and mobile technology. The paper outlines the system's functionalities and architecture, with a focus on the dialogue manager. Key objectives of the system include providing accurate information, designing a user-friendly interface, and guiding the user through the dialogue process. 
41263	4126356	Person Tracking in Smart Rooms using Dynamic Programming and Adaptive Subspace Learning	The article discusses a new vision system for tracking a single person in a smart room using synchronized, calibrated stationary cameras. The system has three components: initialization, tracking, and drift detection. The main innovation is an adaptive tracking mechanism that uses subspace learning to track the person's appearance in two-dimensional views. This approach includes a "forgetting" mechanism to reduce drifting and replaces the previous mean-shift tracking method. The system also includes a robust initialization component using face detection and spatio-temporal dynamic programming. Overall, this system outperforms previous systems in tracking a presenter in data from the CHIL project.
41263	4126327	Joint face and head tracking inside multi-camera smart rooms	The paper presents a new system for detecting and tracking human movement in a "smart room" using multiple synchronized cameras. The system is specifically designed for tracking a lecturer's head and face in 3D space in front of an audience. It utilizes a statistical appearance model and two novel components to improve tracking accuracy and handle head pose variation. The system achieves excellent tracking precision and outperforms four other tracking schemes. It also performs well in detecting frontal and near-frontal faces in the camera views. The system is evaluated on a database of human interactions in a lecture scenario and shows promising results.
41264	4126414	Intelligent virtual agents in collaborative scenarios	The success of interactive games and virtual communities depends on the ability of intelligent virtual agents (IVAs) to meet user expectations and display believable behaviors. In scenarios where users and IVAs interact as a group, it is important to have believable group dynamics. To address this, a model was developed based on theories of group dynamics from social psychology. The model was implemented in a computer game where users work with a group of IVAs to complete tasks. An evaluation experiment showed that the model had a positive impact on users' social engagement, specifically in terms of trust and identification with the group.
41264	4126437	AgriVillage: A Game to Foster Awareness of the Environmental Impact of Agriculture	The importance of agriculture in society cannot be denied, but it also has a negative impact on the environment. In order to raise awareness about this issue, a game was developed that allows players to experience the effects of different agricultural styles on the environment. The game focuses on three main issues: the use of fertilizers in water sources, deforestation and its impact on weather, and the need to balance economic and environmental perspectives for sustainable food production. The game also includes non-player characters, resembling living entities, to show the direct impact of player actions on the environment. It was implemented in a 3-D virtual world platform and a study showed that it successfully increased players' knowledge and awareness of agriculture and its environmental impact.
41265	4126558	A logic-based axiomatic model of bargaining	This paper presents a mathematical model for analyzing bargaining situations. It uses propositional logic to describe the situation and total pre-orders to represent the preferences of the bargainers. The authors propose a solution to n-person bargaining problems based on the concept of minimal simultaneous concessions. This solution is unique and can be characterized by five logical axioms. This framework offers a basic solution to multi-person, multi-issue bargaining problems in discrete domains. It can also be applied to continuous bargaining problems by discretizing them. In this case, the solution is equivalent to the well-known Kalai-Smorodinsky solution.
41265	4126557	Reasoning about Bargaining Situations	 on the maxmin rule and show that it is uniquely character- ized by four axioms and three assumptions.The paper presents a logical framework for analyzing bargaining solutions. It describes a bargaining situation using propositional logic and quantifies bargainers' preferences based on the logical structure of the situation. The proposed solution to n-person bargaining problems uses the maxmin rule and is uniquely characterized by four axioms and three assumptions. These axioms and assumptions are represented in logical statements and have game-theoretic counterparts. The aim of the paper is to develop a logical theory of bargaining in a cooperative model by representing negotiation items in propositional logic and quantifying preferences according to the logical structure. The solution proposed is based on the maxmin rule and is characterized by four axioms and three assumptions. 
41266	41266105	Per-survivor processing Viterbi decoder for Bluetooth applications	.A new noncoherent sequence detection receiver has been developed for Bluetooth systems, based on Rimoldi's decomposition of the transmit signal. Software simulations have demonstrated its power efficiency. In this paper, a hardware implementation of the decoder for the receiver is presented. A low-complexity Viterbi decoder is described, which uses per-survivor processing in a two-state trellis. The prototype decoder is implemented in an FPGA and its bit-error-rate performance is compared to that of the software simulations. This shows the effectiveness of the hardware implementation and its potential for use in Bluetooth systems.
41266	41266199	Performance Comparison of Bluetooth LDI, Modified LDI, and NSD Receivers.	The article discusses two recently proposed Bluetooth receiver designs, the modified limiter-discriminator detector with integrate and dump filtering (LDI) and noncoherent sequence detection (NSD), which have been shown to improve system performance in terms of bit-error rate (BER) and packet-error rate (PER) compared to the conventional LDI receiver. The study presents a more comprehensive performance comparison for these receivers, taking into account factors such as device distribution, channel propagation, data traffic, scheduling, automatic repeat request (ARQ), and baseband packet selection. The results show that incorporating these new receivers into Bluetooth devices can significantly enhance data-rate and quality-of-service performance, as measured by end-to-end delay and throughput.
41267	412670	The MMCPP/GE/c Queue	The article discusses the probability distribution of queue length in a multi-server, single queue system with a generalized exponential service time distribution and a Markov modulated compound Poisson arrival process. This type of arrival process, which is commonly used in ATM networks, involves bulk arrivals with geometrically distributed batch sizes that are modulated by a Markovian arrival phase process. The result, obtained using the method of spectral expansion, is exact and applies to both infinite and finite capacity queues. The Laplace transform of the interdeparture time probability density function is also derived. This analysis could be useful for modeling networks of switching nodes with correlated and bursty internal arrival processes.
41267	4126717	Sojourn Time Distributions in Modulated G-Queues with Batch Processing	Quantiles on response times are important for measuring quality of service in computer networks and logistical systems. We have derived explicit expressions for the time domain of response time probability distribution in a modulated, batched G-queue. This complex queue can model characteristics seen in modern computer systems and telecommunications traffic, like burstiness and failures. However, previous studies have not obtained sojourn time distributions for this type of queue. Our research simplifies a previous result for the Laplace transform, which can then be inverted to give a mixture of exponential and Erlang distributions. We have also developed an algorithm to generate these functions for different model parameters, and have applied it to various problems to demonstrate the diverse distribution functions that can be obtained.
41268	4126884	A knowledge-based framework for multimedia adaptation	The delivery of personalized multimedia content over the internet has the potential to revolutionize future multimedia applications and is a key focus in ongoing MPEG-7 and MPEG-21 standardization efforts. This technology will allow for automatic preparation of digital content according to the user's device capabilities, network conditions, and content preferences. However, this will require dealing with a variety of end user devices, media formats, and metadata. To address these challenges, a knowledge-based approach is proposed, utilizing standardized Semantic Web Services technology to express tool capabilities and execution semantics. This approach has been successfully implemented and evaluated in an ISO/IEC MPEG Core Experiment and is currently being evaluated by the standardization body. 
41268	4126875	Knowledge-based multimedia adaptation for ubiquitous multimedia consumption	The article discusses the importance and challenges of intelligent, server-side adaptation of multimedia resources. This is due to the constant emergence of new mobile devices with different display formats and network capabilities, as well as the availability of metadata annotations in standards such as MPEG-7 and MPEG-21. To address this, an intelligent multimedia adaptation server must integrate external tools and algorithms to perform a sequence of adaptation operations on the original resource before sending it to the client. The article presents the results of an ISO/IEC MPEG Core Experiment on using Semantic Web Services to describe the semantics of adaptation services and construct multi-step adaptation sequences. This approach introduces declarative, knowledge-based technology to the multimedia community and expands the application of Semantic Web Services in the area of general semantic service descriptions and automated program construction.
41269	4126930	Cost-Efficient Development of Virtual Sales Assistants	The concept of a virtual sales assistant represents the idea of an interactive online tool that mimics the role of a human salesperson in physical stores. These assistants aim to enhance the shopping experience for users and add a social and emotional aspect to e-commerce platforms. While many promising pilot applications have been researched, not many have been implemented commercially. This highlights the gap between advanced technology and practical profitability for e-commerce sites. Drawing from industry experience, the authors propose key factors for developing efficient and persuasive virtual sales assistants, and offer a framework for creating conversational sales recommender systems. The paper also shares insights from real-world applications.
41269	4126938	Persuasive online-selling in quality and taste domains	The e-commerce market for high-quality products such as wine and cigars is rapidly growing, requiring a balance of attractive web presentation and in-depth product knowledge for online shops. Recommender systems can enhance the shopping experience for users by providing personalized recommendations. The Advisor Suite framework is a conversational recommender system that uses knowledge-based techniques to bridge the gap between consumer preferences and technical details of the product domain. This paper introduces a framework for classifying the key elements that contribute to creating a compelling online shopping experience with recommender systems. It also presents results from a long-term evaluation of the framework in the Cuban cigar market.
41270	4127012	Towards next generation coordination infrastructures.	Coordination infrastructures are essential in designing multiagent systems. With the rise of agent technology, many different coordination infrastructures have been developed. This paper aims to review the current state of coordination infrastructures and identify areas for future research. The authors suggest that the next generation of coordination infrastructures should focus on becoming more socially aware, providing decision support for agents, and increasing openness for decentralized design and execution. Additionally, the paper highlights promising approaches and research areas to address these challenges. Overall, the authors stress the importance of developing advanced coordination infrastructures to improve the effectiveness and efficiency of multiagent systems. 
41270	4127019	Off-line synthesis of evolutionarily stable normative systems.	Normative systems are commonly used in multi-agent systems to coordinate interdependent activities. However, a key issue is how to synthesize norms that will effectively coordinate agents and be complied with. Existing literature focuses on the online synthesis of a single norm that can be modeled as a game in advance. This work introduces a framework for the offline synthesis of norms that coordinate agents in multiple, unknown situations. This framework uses evolutionary game theory and simulations to automatically identify potential conflict situations and select evolutionarily stable norms to coordinate agents. An empirical evaluation in a simulated traffic domain demonstrates the effectiveness of this approach. 
41271	4127136	Ddrp: An Efficient Data-Driven Routing Protocol For Wireless Sensor Networks With Mobile Sinks	The addition of mobile sinks to a wireless sensor network can greatly improve its performance. However, this can also lead to unexpected changes in network topology, resulting in excessive protocol overhead and offsetting the benefits of using mobile sinks. To address this issue, a data-driven routing protocol (DDRP) is proposed. The objective of DDRP is to reduce protocol overhead for data gathering in wireless sensor networks with mobile sinks. This is achieved by using the broadcast feature of the wireless medium for route learning, where data packets carry additional information about the distance to the target mobile sink. Simulation results show that DDRP can significantly reduce protocol overhead and increase network lifetime while maintaining a high packet delivery ratio.
41271	4127120	Meeting Position Aware Routing for Query-Based Mobile Enabled Wireless Sensor Network	This paper proposes meeting position aware routing (MPAR) for query-based mobile enabled wireless sensor networks. MPAR utilizes dense sensor nodes and a mobile sink that moves through the sensing field to gather information. The unique feature of MPAR is its ability to predict the position of the mobile sink at the time it meets a data packet, allowing for more efficient delivery. Comparative analysis and simulations show that MPAR outperforms existing routing protocols in terms of end-to-end delivery latency, energy consumption, and robustness to changes in the mobile sink's velocity. 
41272	4127261	Iteration-free PDL with storing, recovering and parallel composition: a complete axiomatization.	This article discusses the axiomatization and completeness of PRSPDL0, which is a variant of PDL that does not use iterations and allows for parallel composition. The program operation of parallel composition is not originally definable in the language of PDL, but it can be defined by introducing propositional quantifiers. Instead of using axioms, the article proposes using a non-traditional rule of proof to define the program operation in the expanded language. The proof of the Truth Lemma is supported by large programs.
41272	4127223	A new proof of completeness for a relative modal logic with composition and intersection	This paper addresses the completeness issue of RMLCI, a restriction of propositional dynamic logic with intersection. The problem with RMLCI is that the operation of intersection cannot be defined using modal operators. The authors use the concept of mosaics to provide a new proof of a previously established theorem regarding the completeness of RMLCI. This theorem states that the proof system for RMLCI is complete with respect to the class of all RMLCI models. The authors' proof relies on the use of a new notion of modal depth, which allows for a more direct and elegant approach to the completeness proof.
41273	4127357	Adaptive Rsu Re-Routing Under Delay Constraint In Multi-Hop Vehicular Networks	Vehicular wireless networks (VWNs) use multi-hop communication between vehicles and roadside units (RSUs). However, this can result in longer packet delivery delays. To address this issue, this paper suggests an adaptive RSU re-routing strategy that changes the associated RSU based on delay or throughput constraints. This allows for better selection of packet size, balancing the trade-off between delay and throughput to meet the needs of various applications in multi-hop VWNs. Performance results demonstrate the effectiveness of this approach in meeting the diverse requirements of applications in VWNs.
41273	4127345	Reducing Multi-hop Wireless Delay for High-Priority Applications in Vehicular Wireless Networks	Vehicular wireless networks (VWNs) provide both high-priority safety and low-priority non-safety applications. In these networks, high-priority applications require low delay and high bandwidth for urgent information. During heavy traffic, reducing the multi-hop wireless delay is crucial for urgent information. A blocking strategy is proposed in this paper to preempt the transmission of low-priority vehicles and decrease the number of contending vehicles. This strategy greatly reduces the multi-hop wireless delay for high-priority vehicles, even in heavy traffic. A mathematical model is also proposed to evaluate the performance of the strategy and compare it to existing strategies. Results show that the proposed strategy significantly reduces the multi-hop wireless delay.
41274	4127487	On the expected number of failures detected by subdomain testing and random testing	The effectiveness of subdomain testing and random testing strategies can be evaluated using the E-measure, which measures the expected number of failures detected. This provides valuable information about the fault detecting capabilities of these strategies. Our analysis considers the general case of overlapping subdomains, rather than just disjoint subdomains, and introduces new conditions for determining the efficacy of subdomain testing compared to random testing. Additionally, we establish connections between the E-measure and the previously used P-measure, and use these connections to characterize subdomain testing based on the P-measure. 
41274	4127420	On the Analysis of Subdomain Testing Strategies	Weyuker and Jeng studied the factors that impact the effectiveness of partition testing and compared it to random testing. Chen and Yu built upon their findings. This paper expands on the analysis by considering subdomain testing, where subdomains can overlap. The researchers present results for a specific scenario and propose a technique to extend these findings to more general cases. They believe that this approach can aid in better understanding the behavior of subdomain testing.
41275	4127522	An ordering of secondary task display attributes	This paper discusses a study that challenges the established display design guidelines for focal images when they are used as a secondary task in a dual-task situation. The study proposes a new ordering guideline for secondary task image attributes based on human cognitive ability to extract information. This is necessary because the effectiveness of an image in conveying meaning decreases when it is shifted from a focal to a secondary task. The ordering of secondary task attributes also varies depending on the level of degradation in the primary task. These findings highlight the need for alternate guidelines for displaying images in a dual-task setting.
41275	4127567	An evaluation of information visualization in attention-limited environments	This article discusses the importance of designing information visualizations that can be accessed quickly and effectively while individuals are busy with primary tasks. The researchers aim to understand how to create visualizations that can be viewed as secondary information and how well people can interpret them while multitasking. The study evaluated the impact of factors such as visual density, presence time, and type of secondary task on individuals' ability to continue with their primary task and complete secondary tasks related to the visualization. The results suggest that when given enough time and a clear secondary task, individuals can effectively interpret visualizations without being too distracted from their primary task.
41276	412763	STOC: extending oracle to support spatiotemporal data management	This paper discusses the development of STOC (Spatio-Temporal Object Cartridge), an extension of Oracle that allows for practical management of spatiotemporal data. Previous research has primarily focused on modeling, indexing, and query processing, with little attention given to implementation. STOC is developed as a PL/SQL package and can be integrated into Oracle, providing users with access to spatiotemporal data types and functions using standard SQL. The paper provides an overview of STOC's features, discusses its architecture and implementation, and presents a case study demonstrating its capabilities. This extension offers a valuable tool for managing spatiotemporal data in various applications.
41276	4127651	OSTM: A Spatiotemporal Extension to Oracle	The focus of most research on spatiotemporal databases is on data model and query processing, with little attention given to the actual implementation of these systems. This paper introduces OSTM, a spatiotemporal database module that builds upon the Oracle9i platform. By utilizing the extensibility of Oracle's Object-Relational DBMS, OSTM offers specialized data types and functions for spatiotemporal data. The paper includes an overview of the spatiotemporal data model and details on how OSTM is implemented. OSTM also supports traditional SQL language, making it a practical solution for managing spatiotemporal data.
41277	4127747	Generating semantic-based trajectories for indoor moving objects	This paper introduces a new method for creating trajectories of indoor moving objects based on semantic information. Traditional methods for collecting this data require expensive positioning equipment and complex environment setups. To address these issues, the paper proposes using virtual positioning equipment to simulate the indoor environment. The semantic-based approach takes into account the type of moving objects, their relationship with locations, and the distribution of trajectories. This method is more realistic and provides useful data for indoor data management analysis. The paper also presents a tool for generating these trajectories and conducts a case study to demonstrate its effectiveness. The results show that the tool can generate semantic-based trajectories for different indoor scenarios.
41277	4127720	A flexible simulation environment for flash-aware algorithms	Flash-DBSim is a simulation environment designed for evaluating the performance of flash-aware algorithms. It provides a virtual flash disk that can be configured for use with upper systems like file systems and DBMS. This allows for easy evaluation of algorithms on different types of flash disks. Flash-DBSim also offers a prototyping environment for algorithms within the flash disk, such as those for garbage collection and wear-leveling. The paper gives an overview of Flash-DBSim's features and architecture, and presents a case study demonstrating its capabilities. Overall, Flash-DBSim is a flexible and useful tool for evaluating flash-aware algorithms.
41278	4127854	Tighter low-rank approximation via sampling the leveraged element	The authors propose a new randomized algorithm for computing a low-rank approximation to a given matrix. Their method involves biased sampling based on leverage scores, then weighted alternating minimization to minimize error. This approach can leverage input sparsity and produce approximations in spectral norm. It requires O(nnz(M)+nκ2r5/ε2) computations, with a better dependence on error ε compared to existing methods. The algorithm is also parallelizable and has two interesting extensions: a method for computing a low-rank approximation to the product of two matrices, and an improved algorithm for distributed PCA with smaller communication complexity. 
41278	4127849	Solving a Mixture of Many Random Linear Equations by Tensor Decomposition and Alternating Minimization.	The problem of solving mixed random linear equations with $k$ components, also known as the noiseless setting of mixed linear regression, is discussed. The objective is to estimate multiple linear models from mixed samples when the labels are not observed. A tractable algorithm is proposed with a guaranteed exact solution, given certain conditions, and a sample complexity that is linear in the dimension and polynomial in $k$. Previous approaches have had either exponential or super-linear dependence on these parameters. The algorithm combines tensor decomposition and alternating minimization, and its analysis shows that the initialization provided by the tensor method allows the alternating minimization to converge to the global optimum at a linear rate. 
41279	4127926	Sums of squares of polynomials and their applications	A multivariate polynomial can be expressed as a sum of squares (SOS) if it can be written as a sum of squares of other polynomials. This condition can be used to test for polynomial nonnegativity, as a polynomial that is SOS will always be greater than or equal to 0 for all values of x. The decomposition of a polynomial into a sum of squares has been extensively studied in pure and applied mathematics, with recent developments in computational implications. Semidefinite programming (SDP) has emerged as an efficient approach for computing SOS decompositions, with the potential for practical applications in areas such as real algebraic geometry. This talk will provide an overview of this research area, discussing the basics of SDP and how it can be used to compute SOS decompositions, certify results, and obtain structured infeasibility certificates for real solutions of polynomial equations and inequalities. Examples from various domains will be used to illustrate the methods.
41279	4127936	SOS methods for semi-algebraic games and optimization	Semialgebraic computations are essential in the analysis and design of hybrid dynamical systems, as they involve manipulating sets and logical conditions defined by polynomial inequalities in real variables. However, these computations can be a bottleneck in the design process due to the complexity of the tasks involved, such as reachability analysis and verification. Current methods for semialgebraic computations are often limited in their practical performance, which can be attributed to the purely algebraic nature of these methods and the insistence on exact solutions. To overcome these limitations, there is a growing interest in developing efficient techniques for restricted classes of semialgebraic problems. The SOS approach, which combines symbolic and numerical techniques, is one such method that has shown promise. It involves computing sum of squares (SOS) decompositions for multivariate polynomials using semidefinite programming and can be extended to provide infeasibility certificates for polynomial equations and inequalities. This approach unifies and generalizes many existing methods and has been successfully applied to various problems, including zero-sum two-person games with an infinite number of pure strategies. While the focus of this approach is on hybrid dynamical systems, it has also shown potential in other domains such as dynamical systems and geometric theorem proving.
41280	41280184	Reconstructing binary polygonal objects from projections: a statistical view	Tomography is a common technique used to create images of objects by measuring the transmission of waves or particles through them. In many cases, the most important aspects of these images are their geometric features. However, traditional methods of reconstructing these images, such as filtered back-projection, can produce inaccurate results when the data is sparse and noisy. In this paper, a new framework is presented that uses statistical methods to extract specific geometric features directly from the noisy data. This approach involves formulating the reconstruction problem as a parameter estimation problem, with the vertices of binary polygons being the defining parameters. By using maximum likelihood or minimum description length criteria, the optimal solution can be found, although it may be challenging due to the nonlinearity of the problem. To address this issue, a novel method for initializing the solution is proposed, using estimated moments of the object obtained from the noisy data.
41280	41280120	Tomographic reconstruction of polygons from knot location and chord length measurements	This work focuses on developing algorithms to reconstruct binary polygonal objects using sparse and noisy tomographic-based data. Traditional methods for reconstructing geometric objects from projection data often face challenges with nonlinearity. To address this, the authors first examine the reconstruction problem based on measurements of knot locations, similar to multitarget radar tracking. They then propose a sequential hypothesis-testing algorithm that solves a series of linear estimation problems by generating data association hypotheses. This approach is made more efficient by incorporating constraints from the tomography problem. The resulting estimates are of good quality and can be further improved by combining them with a fully nonlinear inversion. 
41281	4128110	Lower Bounds on the Minus Domination and k-Subdomination Numbers	A minus dominating function is a three-valued function defined on the vertices of a graph. The function's values must be -1, 0, or 1, and the sum of its values in any closed neighborhood must be at least one. This means that for every vertex, the sum of its value and the values of its adjacent vertices must be at least one. The weight of a minus function is the sum of its values over all vertices. The minus domination number of a graph is the minimum weight of a minus dominating function. This paper establishes lower bounds for the minus domination of bipartite graphs, proving a conjecture and providing a lower bound for a related graph property.
41281	4128123	On the parameterized complexity of pooling design.	Pooling design is a useful tool for reducing the number of tests in DNA library screening, a crucial step in obtaining high-quality DNA libraries for studying gene functions. The three main problems in pooling design involve determining if a given binary matrix is d-separable, (d) over bar -separable, or d-disjunct, and are all known to be coNP-complete. While d is typically small compared to the matrix size, research has shown that there are no efficient algorithms for solving these problems when d is small. This article discusses the parameterized complexity of these problems, showing that they are co-W[2]-complete. This means that a deterministic algorithm with a running time of f(d) x (mn)(O(1)) cannot be expected for these problems.
41282	4128233	Capacity Bounds for a Class of Interference Relay Channels	The article investigates the capacity of a specific type of Interference Relay Channels (IRC), called the Injective Semideterministic IRC. This type of IRC has a relay that can only observe one of the sources. The authors develop a new outer bound and two inner bounds for this type of IRC, using various cooperative strategies and interference decoding techniques. The outer bound is an extension of previous work, while the inner bounds contain existing results. The main result is the determination of the capacity region of the Gaussian class of IRCs, with a fixed number of bits per dimension and a constant gap. The proof involves using different cooperative strategies in different signal-to-noise ratio regimes, highlighting the complexity of the Gaussian IRC. This also shows that a single coding scheme for the Gaussian relay and interference channel may not be optimal, even for achieving capacity within a constant gap.
41282	4128246	Constant-gap results and cooperative strategies for a class of Interference Relay Channels	The capacity of Interference Relay Channels (IRC) is studied and new bounds are derived based on existing cooperative strategies and proper interference decoding. The main finding is that three different cooperative strategies are needed to achieve a constant gap to the capacity for each signal-to-noise ratio (SNR) regime of the Gaussian IRC. This is in contrast to the standard Gaussian RC (Relay Channel) where multiple cooperative strategies can yield constant-gap results in all regimes. This highlights the complexity of the IRC and the need for multiple strategies to achieve optimal performance.
41283	4128321	Scalable visibility color map construction in spatial databases	Advances in 3D modeling allow us to answer queries about the best location for a new billboard or the hotel room with the best view despite obstacles. This requires measuring the visibility of the target object from various viewpoints in a dataspace. To address this problem, a visibility color map (VCM) is proposed as a surface color map where each viewpoint is assigned a color value representing the visibility of the target. However, computing the visibility for every viewpoint is expensive, so an efficient approach is proposed that takes advantage of human vision and also offers approximations to reduce computational overhead. The effectiveness and efficiency of this approach are demonstrated through experiments on real 2D and 3D datasets.
41283	4128330	GAMPS: compressing multi sensor data by grouping and amplitude scaling	The problem of efficiently storing and reconstructing multiple sensor signals with a given maximum error is common in applications that collect large amounts of data. To address this, the GAMPS framework uses techniques such as grouping correlated signals, scaling their amplitudes, and maintaining an index for efficient querying. These techniques result in a polynomial time approximation with a maximum error of α ε and using at most β times the optimal memory. Experiments on real-world sensor data show that GAMPS effectively reduces space usage without compromising search and query quality.
41284	4128433	Practical, Real-time Centralized Control for CDN-based Live Video Delivery	This year, live video delivery is expected to reach its peak at 50 Tbps. This growing popularity is changing the landscape of Internet video delivery. Content delivery networks (CDNs) must meet users' expectations for fast join times, high quality, and minimal buffering, while also keeping their own costs low and addressing issues in real-time. However, challenges such as latency, loss, and varied workloads make this task difficult. A study found that a centralized controller could improve user experience, but CDNs have avoided this approach due to the challenge of handling failures quickly. To address this, a new approach called VDN has been introduced that uses a centralized algorithm for live video optimization. VDN allows CDN operators to have real-time control over video delivery, despite challenges from the wider network. This approach has shown to increase average bitrate by 1.7 times and decrease costs by 2 times in different scenarios.
41284	412847	Understanding internet video viewing behavior in the wild	The popularity of online video has grown significantly in recent years and it is predicted that it will make up more than half of internet traffic in the near future. However, this increase in video viewership has put a strain on the Content Delivery Network (CDN) infrastructure. To address this issue, a study was conducted to analyze video viewing behaviors and patterns using data from two large internet video providers. The results showed that these behaviors have a significant impact on the design and provision of the CDN infrastructure. This study aims to provide a deeper understanding of how the infrastructure should be designed to handle the growing video workload. 
41285	41285275	2-layered metadata service model in grid environment	Data intensive tasks have become increasingly important in the grid environment, with data sets reaching hundreds of terabytes and soon petabytes. However, organizing the storage devices in a geographically distributed manner to support collaborative operations on this data presents a challenge. Performance is crucial for such applications, but the varying network conditions make it difficult to ensure consistent service quality. To address this issue, a 2-layered metadata service model has been proposed and implemented in the ChinaGrid Supporting Platform (CGSP), a grid middleware for the ChinaGrid project. This model takes into account the distribution of users and aims to provide a platform for efficient grid data management.
41285	4128570	Replica based distributed metadata management in grid environment	Metadata management is a crucial aspect of data grid technology, with the goal of achieving high efficiency and availability. This paper introduces a Replication Based Metadata Management System (RBMMS) implemented in the Global Distributed Storage System (GDSS). To meet these goals, RBMMS utilizes a strongly connected graph to describe the structure and relationships of replicas. This graph is used to update information and discover replicas during the addition and removal process. Additionally, a cache module is implemented in RBMMS to enhance the performance of metadata access. The evaluation of RBMMS shows that it successfully achieves high efficiency and availability in managing metadata.
41286	4128659	Mammoth: Gearing Hadoop Towards Memory-Intensive MapReduce Applications	The MapReduce platform is commonly used for processing and analyzing large amounts of data, but it requires well-configured hardware in order to work effectively. However, our survey has shown that many small and medium-sized enterprises do not have suitable hardware configurations for this task, especially in terms of memory constraints. In response to this issue, we have developed Mammoth, a new MapReduce system that utilizes global memory management to improve performance. This includes a rule-based heuristic for memory allocation and revocation, a multi-threaded execution engine, and techniques such as sequential disk accessing and multi-cache. Our experiments have shown that Mammoth can significantly reduce job execution time and improve performance even in memory-constrained systems, making it a promising solution for large-scale data processing and analysis.
41286	4128646	Communication-driven scheduling for virtual clusters in cloud	Cloud computing is becoming increasingly popular as an alternative to local clusters for both academic and commercial users due to its high flexibility and cost-effectiveness. However, running tightly-coupled parallel applications in a cloud environment can result in significant performance degradation, particularly when the system is overcommitted. This is due to the current Virtual Machine Monitors (VMMs) being unable to effectively coordinate the Virtual CPUs (VCPUs) that host these applications. In this paper, the authors propose a communication-driven scheduling approach for virtual clusters that can mitigate this performance degradation. Their approach involves a communication-driven VM scheduling algorithm that is integrated into the Xen VMM scheduler, resulting in improved performance for tightly-coupled parallel applications compared to existing approaches. Experiments conducted on a real cluster environment demonstrate the effectiveness of their solution.
41287	4128774	Energy-Efficient Data Gathering for Road-Side Sensor Networks Ensuring Reliability and Fault-Tolerance	The paper discusses a novel tree-based data gathering scheme for road side sensor networks. This application is popular for traffic monitoring, but is challenging due to the required delay sensitivity and reliability, as well as limited sensor resources. The proposed scheme utilizes the strip-like structure of the road network and creates virtual blocks with one active node in each. Efficient scheduling ensures both coverage and power savings, while a tree maintenance module handles sensor node joining and leaving events to maintain network connectivity. Simulation results show minimal overhead for tree maintenance in terms of delay and control message communication.
41287	4128728	Convergecast tree management from arbitrary node failure in sensor network.	Efficiency and reliability of convergecast in sensor networks rely on proper data accumulation to the sink. A tree structure, with the sink as the root, is the ideal topology for data gathering. However, resource-constrained sensor nodes are prone to sudden crashes, requiring prompt tree repairs. The proposed scheme creates a data gathering tree rooted at the sink, with each node maintaining the shortest hop-count to reduce routing delay. During tree construction, nodes collect extra neighborhood information for efficient tree repairs in case of node failures. When a node fails, affected nodes in its vicinity use control messages to fix the parent, resulting in minimal repairing delay and successful message delivery. Simulations show minimal data loss and no redundancy in the presence of node crashes.
41288	4128815	Shape of an arbitrary finite point set in IR2	The article discusses the study of the shape of a finite point set in two-dimensional space, where the points are not arranged in a regular grid. The shape of a connected point set is represented by a directed graph that links the boundary points. The authors argue that a proper boundary definition should have certain characteristics, such as regulating scale and being consistent with the definition of connected objects. They propose the use of the a-graph, which is based on a generalization of the convex hull, as a shape descriptor and boundary definition. The computational aspects of the a-graph have been studied, but not its potential use in these contexts. The authors prove that the a-graph meets the desired criteria and also establish a relationship between the a-graph and the opening scale space in mathematical morphology.
41288	4128891	Parameterized feasible boundaries in gradient vector fields	Segmenting images with multiple objects is difficult using only local information. A model-based approach is more effective, using a geometrical model to extract object boundaries by tuning its parameters. This is achieved by an objective function that considers both image information and the shape of the template. Previous methods mainly use gradient magnitude and smoothness measures, but a new approach using directional gradient information from smoothed derivatives is proposed. This method can accurately locate object boundaries, even when there are conflicting objects nearby. A new smoothness objective is also introduced to ensure physical feasibility. The method is tested on artificial and real medical images, showing great success in complex images. 
41289	4128978	User-oriented healthcare support system based on symbiotic computing	The article describes a proposed healthcare support system that utilizes multi-agent technology and ubiquitous computing in order to provide personalized and efficient healthcare assistance. The system collects and analyzes various data, such as vital signs and physical location, to provide useful information to the user about their health. It is designed to be user-oriented and based on the concept of symbiotic computing. The article details the design and initial prototype implementation of the system. 
41289	4128992	An effective inference method using sensor data for symbiotic healthcare support system	This paper discusses a symbiotic healthcare support system in a ubiquitous computing environment. The system uses various types of information, such as vital signs, location data, and multimedia data, to provide effective and user-oriented health information. The paper focuses on the use of this information for real-time service provisioning. However, due to the diverse nature of this data, existing inference mechanisms are not able to handle it in real-time. To address this issue, the paper proposes an effective inference mechanism that combines sensor data and specialized healthcare knowledge. This allows for the provision of real-time healthcare information and services.
41290	4129097	Equality sets of prefix morphisms and regular star languages	We explore the concept of equality sets of prefix morphisms, defined as sets of words that have the same output when applied to two different prefix morphisms. These sets are proven to have a surprising connection to the families of regular star languages and languages formed by applying a projection function to the output of two prefix morphisms. The prefix morphisms must fulfill certain criteria, including not having the output of one letter as a prefix of another. This result sheds light on the relationship between these two families of languages.
41290	4129062	Equality sets for recursively enumerable languages	The article discusses a type of language called shifted equality sets, which are defined as a set of words where a specific letter and two nonerasing morphisms have equal outputs. The authors focus on a family of languages formed by coding these shifted equality sets. They prove various properties for this family and demonstrate that every recursively enumerable language can be represented as a projection of a shifted equality set. This means that the language can be obtained by deleting certain letters from the shifted equality set. The authors conclude that recursively enumerable star languages are the same as the projections of equality sets.
41291	4129116	MSO definable string transductions and two-way finite-state transducers	The study of string-to-string functions, also known as string transductions, has been extended to include those that can be defined by monadic second-order (MSO) logic. It has been shown that these transductions can be realized by deterministic two-way finite-state transducers. This means that the equivalence of two MSO definable string transductions can be decided. However, in the case of nondeterministic transductions, it has been found that they are incomparable to those realized by nondeterministic two-way finite-state transducers. This has led to the search for another machine model, and it has been discovered that both classes of MSO definable string transductions can be characterized by Hennie machines, which are two-way finite-state transducers with certain restrictions.
41291	4129124	XML transformation by tree-walking transducers with invisible pebbles	The pebble tree automaton and pebble tree transducer have been improved by allowing an unlimited number of "invisible" pebbles. This enhancement allows them to recognize regular tree languages and validate generalized DTD's, making them useful for finding matches of MSO definable n-ary patterns. These devices also have a navigational aspect that resembles XPath and can perform MSO definable tests on their configurations. The addition of invisible pebbles can also reduce the time complexity for certain conjunctive queries. These advancements make the pebble tree automaton and pebble tree transducer useful for modeling the recursion mechanism of XSLT. 
41292	412929	RIM: Robust Intersection Management for Connected Autonomous Vehicles	The use of intelligent transportation systems can greatly improve the flow of intersections for Connected Autonomous Vehicles (CAVs). However, the current Intersection Manager (IM) is vulnerable to model mismatches and external disturbances, resulting in a need for a large safety buffer and lower speeds for CAVs, ultimately reducing the intersection's throughput. To address this issue, a new space and time-aware technique called RIM is proposed, where the IM assigns a safe Time of Arrival (TOA) and Velocity of Arrival (VOA) to approaching CAVs, allowing them to determine and track an optimal trajectory and compensate for model mismatches and disturbances. Results from experiments show that RIM can significantly reduce position error and achieve 2.7X better throughput compared to other techniques.
41292	4129251	Efficient Code Assignment Techniques for Local Memory on Software Managed Multicores.	The challenge of scaling the memory hierarchy in multicore processors is addressed by Software Managed Multicore (SMM) architectures, which eliminate caches and use local scratchpad memory instead. However, managing code and data between global and local memory is crucial for efficient execution of large applications on SMM architectures. This article proposes a cost calculation graph and two heuristics, CMSM and CMSM_advanced, to efficiently manage code on SMM architectures. Experimental results show that the correct management cost model and optimized code mapping can significantly improve performance compared to previous approaches. Additionally, the code management overhead on SMM cores is found to be less than the memory latency of cache-based systems.
41293	412936	Adaptive Scratch Pad Memory Management for Dynamic Behavior of Multimedia Applications	Runtime memory access traces can be used in addition to compiler optimizations to reduce energy consumption in memory hierarchy. This is especially useful for multimedia applications that have dynamic and irregular memory access patterns. These types of applications are difficult to optimize with static compiler techniques because their behavior is unknown until runtime and can change during computation. To address this issue, a combined approach of software (compiler and operating system) and hardware (data access record table) techniques is proposed. This approach utilizes a runtime scratch pad memory manager in the operating system to adapt the scratch pad data layout to the input data pattern, resulting in more efficient scratch pad utilization and a reduction in the amount of main memory accesses. Experimental results show that this approach is effective in reducing energy consumption compared to existing compiler and hardware techniques.
41293	4129313	Memory access optimization in compilation for coarse-grained reconfigurable architectures	Coarse-grained reconfigurable architectures (CGRAs) offer high performance and power efficiency by simplifying hardware and shifting complexity to application mapping. However, a major challenge arises in data mapping due to the use of multibank local memory, where a row of processing elements (PEs) share memory access. To address this, a hardware arbiter is used to manage memory requests from the PEs. But a fundamental restriction remains as a bank cannot be accessed by two different PEs simultaneously. To overcome this, the proposed solution is to map application operations and data into memory banks to avoid conflicts, and use a compiler optimization to exploit data reuse for better performance. Experimental results show that this approach can improve performance by up to 53&percnt; compared to a memory-unaware scheduler.
41294	412941	PEICS: towards HCI patterns into engineering of interactive systems	Despite significant efforts in research, there is still a lack of standardized description and organization for HCI patterns. This creates challenges for developers in identifying and applying the relevant patterns to solve problems in specific contexts. In order to effectively utilize HCI patterns in the development of interactive computer systems, they need to be integrated into a model-based UI development process. The Pattern-driven Engineering of Interactive Computer Systems (PEICS) workshop aims to bring together different research approaches to advance the field. This includes contributions in areas such as semantics, formalization, languages, support, research directions, and tools. 
41294	4129470	Formal pattern specifications to facilitate semi-automated user interface generation	This paper discusses the potential benefits of using formal HCI pattern specifications to aid in the semi-automated generation of user interfaces for interactive applications. It reviews established techniques in model-based UI development and proposes a combination of model-based and pattern-oriented methods to enable automated UI generation. The authors also introduce a pattern definition approach and provide examples from the domain of interactive knowledge sharing applications. This framework has the potential to streamline the UI development process and improve the usability of interactive applications.
41295	4129531	Software extension and integration with type classes	In software engineering and programming-language design, being able to extend and integrate software modules without altering existing source code is a crucial challenge. This article explores this issue at the language level, specifically with the use of type classes in the functional programming language Haskell. By comparing it with other approaches, the authors demonstrate that type classes offer a strong framework for addressing software extension and integration problems. However, they also note some limitations of type classes in this context.
41295	4129566	Programming with Patterns	The article discusses the implementation of language support for object-oriented programming with design patterns. This allows for easier and more traceable implementation of designs using design patterns. The essential language constructs used are nested classes and a form of superposition for class structures. The experimental programming language discussed is based on a compilation to Eiffel and a library covering the 23 GoF patterns has been developed. To capture reuse schemes for patterns, a new form of abstraction is introduced. The article also highlights the challenges in supporting design patterns in programming and how the proposed model addresses them. 
41296	4129626	Encouraging knowledge exchange in discussion forums by market-oriented mechanisms	Discussion forums are an effective way to share knowledge on the internet, but they often struggle with getting users to participate. In this paper, a market-based approach is proposed to incentivize users to engage in knowledge exchange. Unlike other market-based solutions, this approach takes into consideration the unique characteristics of the limited resource – the effort to create information. The paper also introduces a method to continuously improve the system's performance by using user behavior statistics to reduce response time for satisfactory answers.
41296	4129662	Comparing Learning Methods	This article discusses two experiments that were conducted to study how an automatic system learns rules from an artificial environment. The performance of the system was compared to that of humans in similar conditions. The first experiment observed the system's performance over time as it learned the rules by observing positive and negative results. The second experiment involved seventy students playing a game in the same artificial environment, with the objective of gaining points by figuring out the unknown rules. The learning curves of both humans and the automatic system were compared, and the results were used to analyze the similarities and differences between their learning processes. The article also discusses the relevance of these experiments in assessing the system's ability to pass the Turing test.
41297	4129735	Present and Future Challenges Concerning DoS-attacks against PSAPs in VoIP Networks	The integration of voice over IP (VoIP) telephony networks and traditional public switched telephony networks (PSTNs) has made it possible for emergency calls from VoIP peers to reach public service answering points (PSAPs). However, this connection also brings potential vulnerabilities to PSAPs, making them targets for more advanced denial of service (DoS) attacks. This paper discusses the current and future architecture of PSAPs and presents the results of measurements taken at a PSAP regarding the detection of attacks. It also offers potential solutions and evaluates their effectiveness.
41297	4129746	Graph based Metrics for Intrusion Response Measures in Computer Networks	This contribution introduces a graph-based approach for modeling attacks and response measures in computer networks. The model uses certain properties of the graphs to quantify response metrics commonly used by network security officers. This allows for the practical assessment of response measures after application and estimation of their effectiveness beforehand. The model is similar to those used in software reliability analysis and is designed to represent network properties with scalable granularity. Several examples demonstrate the applicability of the model and the resulting metric values. Overall, this approach offers a comprehensive and efficient way to evaluate and select appropriate response measures against attacks in computer networks. 
41298	4129867	Triangle algebras: A formal logic approach to interval-valued residuated lattices	The paper introduces triangle algebras, a type of residuated lattices with approximation operators and a third point u. These algebras are shown to represent interval-valued residuated lattices (IVRLs). A system of many-valued logic called triangle logic (TL) is also presented, which captures the tautologies of IVRLs. The use of closed intervals as truth values in L is expressed through a set of logical axioms using triangle algebras. This work is an important step towards formally representing residuated t-norm based logics on the lattice of closed intervals of [0,1], similar to how formal fuzzy logics are represented on the unit interval.
41298	4129853	Filters of residuated lattices and triangle algebras	Filters are an essential concept in the theory of residuated lattices and other algebraic structures used in formal fuzzy logic. They are used to define congruence relations and can be classified into different types, such as Boolean filters and prime filters. This paper explores the relationships between various filters of residuated lattices and triangle algebras, which are used to represent interval-valued residuated lattices. Understanding these filters and their connections can provide valuable insights into the properties and structure of these algebraic structures in formal fuzzy logic.
41299	41299154	Swarm intelligence supported e-remanufacturing	E-Remanufacturing, a process for recovering and refurbishing used products, has become a popular approach for product recovery management. To increase efficiency in this process, various techniques have been utilized. One of these techniques is swarm intelligence (SI), which takes inspiration from the behavior of swarms in nature. SI has shown success in other fields and is now gaining attention from researchers in the remanufacturing industry. This paper provides an overview of SI methods that have been applied in e-remanufacturing. 
41299	4129973	Can ant algorithms make automated guided vehicle system more intelligent?	An automated guided vehicle (AGV) system is a group of driver-less vehicles used in manufacturing to transport goods and materials between workstations and storage locations. In the field of soft computing, ant algorithms are population-based methods inspired by the behavior of real ant colonies. Over the past 20 years, ant algorithms have been successful in solving many optimization problems. This article explores the potential of using ant algorithms to solve design and control problems in AGV systems. By utilizing the strengths of ant algorithms, the authors hope to offer alternative solutions to traditional AGV system challenges and suggest new avenues for research in this area.
41300	4130014	Adaptive Neuro Fuzzy Inference System, Neural Network and Support Vector Machine for Caller Behavior Classification	This paper discusses the development and implementation of a classification system for caller behavior in Interactive Voice Response systems, specifically for a pay beneficiary application. The system uses Adaptive Neuro-Fuzzy Inference System, Feed forward Artificial Neural Network, and Support Vector Machine classifiers, achieving exceptional results with the ANN classifiers being the preferred models. The ANN classifiers achieved 100% classification accuracy for 'Say account', 'Say amount', and 'Select beneficiary' unseen data, and 95.42% accuracy for 'Say confirmation' unseen data. This classification system has the potential to improve the effectiveness of automated self-service applications. 
41300	4130018	Genetic Algorithms, Neural Networks, Fuzzy Inference System, Support Vector Machines for Call Performance Classification	This paper discusses the development of a classification system for Interactive Voice Response systems used in pay beneficiary applications. The use of Fuzzy Inference Systems, Multi-Layer Perceptron, Support Vector Machine, and ensemble of classifiers was explored and compared based on accuracy, sensitivity, and specificity metrics. The goal was to accurately classify caller behavior within these systems. The results showed that the ensemble of classifiers had the highest accuracy of 99.17%, making it the preferred solution. The paper highlights the importance of accurate classification in determining caller behavior and the potential benefits for corporations in utilizing such a system.
41301	4130142	Scaling up Link Prediction with Ensembles.	A network with $n$ nodes can have O(n2) possible links, making it difficult to evaluate all pairwise possibilities in a meaningful way, especially for large networks. Most link prediction methods are designed for a subset of links rather than the entire network due to computational complexity. However, it is important to perform an exhaustive search over the entire network in practice. This paper proposes an ensemble-based approach to scale up link prediction, breaking it down into smaller subproblems that can be solved using latent factor models. This approach has been shown to be effective and scalable in experiments on large networks.
41301	4130157	Towards an efficient snapshot approach for virtual machines in clouds.	High-availability of virtual machines is crucial for a successful cloud computing environment, as system maintenance, malicious attacks, and hardware and software failures can compromise cloud services. Existing VM snapshot methods have performance issues such as long downtime and I/O performance degradation during a live snapshot. To address these issues, the iROW system was proposed, which uses the qemu-kvm virtual block device driver. iROW employs techniques such as a bitmap-based index method, an index-free approach for VM state data, separating VM state data from disk image data, and a free page detection system to reduce snapshot creation time and disk space usage. Experimental results show that iROW outperforms qcow2 in terms of disk and state snapshot creation and disk I/O performance. 
41302	4130240	Spray and wait: an efficient routing scheme for intermittently connected mobile networks	Intermittently connected mobile networks are sparse wireless networks where there is not always a complete path from the source to the destination. These networks are used in various real-life situations such as wildlife tracking, military communication, and inter-planetary communication. Traditional routing schemes do not work well in these networks, so researchers have suggested using flooding-based routing schemes. However, these schemes have high energy consumption and contention issues, leading to delays. To address this, a new routing scheme called Spray and Wait has been introduced, which sends multiple copies of a message and waits for one to reach the destination. This scheme outperforms existing ones in terms of delivery delay and transmissions per message. It is also scalable and easy to implement and optimize for specific performance goals.
41302	413023	Efficient routing in intermittently connected mobile networks: the single-copy case	Intermittently connected mobile networks are wireless networks where there is not always a complete path from the source to the destination. Conventional routing schemes do not work well in these networks, so researchers have suggested using flooding-based routing schemes. However, these schemes waste a lot of energy and can have performance issues. To address this, researchers have explored "single-copy" routing schemes that use only one copy of a message, reducing the resource usage and improving performance. A theoretical framework has been proposed to analyze the performance of these schemes and derive upper and lower bounds on the delay. These single-copy schemes can be used in low resource situations and also help improve the design of general routing schemes that use multiple copies.
41303	4130327	Contextual grammars as generative models of natural languages	The paper discusses the use of contextual grammars, specifically those with maximal use of selectors, as a potential model for natural language syntax. These grammars involve a selection procedure where contexts are added to words based on a set of associated selectors. The paper argues that these grammars are able to accurately describe the syntax of natural languages, including complex structures such as reduplication, crossed dependencies, and multiple agreements. However, there are some center-embedded constructions that cannot be covered by these grammars. The paper also explores the possibility of associating a tree or dependence structure to the generated words, similar to that in descriptive linguistics and link grammars. 
41303	4130330	On some algorithmic problems regarding the hairpin completion	The article discusses algorithmic problems related to hairpin completion, including the membership problem and recognition algorithms for hairpin and iterated hairpin completion of a language. The authors propose efficient time algorithms for languages that can be recognized in O(f(n)) time. They also show that certain factors can be reduced for regular and context-free languages. The concept of hairpin completion distance between two words is introduced, and a cubic time algorithm for computing it is presented. The article also discusses a linear time algorithm for determining if the hairpin completion distance is connected to a given word. The article concludes with a list of open problems for further research. 
41304	4130486	Changes as First-Class Citizens: A Research Perspective on Modern Software Tooling.	Software is constantly evolving to adapt to the ever-changing real world. A new trend in software evolution research focuses on the concept of change as the driving force behind evolution. By treating change as a first-class entity and analyzing it in depth, researchers are able to gain a better understanding of the complex process of software evolution. A study of 86 articles provides an overview of the current state of this research and presents a roadmap for future directions and areas of study. This approach offers a more accurate and thorough understanding of software evolution and its impact on the constantly evolving context of the real world.
41304	4130441	Object-oriented reengineering patterns	Object-oriented development has become increasingly popular in the last twenty years, resulting in large, complex systems that are difficult to maintain. These systems often struggle to meet the changing needs of their customers. In this paper, we focus on the issue of understanding and reengineering these legacy systems. We introduce "reengineering patterns" as a way to address common problems that arise during this process. These patterns are based on successful techniques used by experts in planning, reverse-engineering, problem detection, migration, and software redesign. They have been validated in multiple industrial projects and represent the best practices for object-oriented reengineering.
41305	4130559	PAD-Net: Multi-tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing	The paper discusses the important tasks of depth estimation and scene parsing in visual scene understanding. It introduces a new approach, called the multi-task guided prediction-and-distillation network (PAD-Net), which utilizes a joint CNN to simultaneously predict intermediate auxiliary tasks and use them as input for the final tasks. This method aims to improve the robustness of deep representations and provide multi-modal information for better performance. The approach is tested on two datasets and results show its effectiveness in both depth estimation and scene parsing tasks.
41305	41305129	Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection.	The authors of this paper introduce a zoom-out-and-in network for generating object proposals. They observe that using the same features to classify anchors of different sizes is challenging. To overcome this, they propose placing anchors of different sizes at different depths within the network. The network utilizes two feature map streams, one for low-level details and the other for high-level semantics, to identify objects in an image. A map attention decision (MAD) unit is also introduced to adaptively activate certain feature channels based on the input context, resulting in improved performance compared to fixed convolutional kernels. Experimental results on three datasets show that their proposed algorithm outperforms other state-of-the-art methods in terms of region proposal recall and object detection precision.
41306	4130626	Binary-Code-Allocation Scheme in DS-CDMA Systems	The paper proposes a binary-code-allocation scheme for reducing multiple-access interference in direct-sequence code-division multiple-access systems. The scheme involves giving users an orthogonal binary sequence set and allowing them to choose a better sequence to maximize their signal to interference ratio. The scheme is analyzed through both exact analysis and approximation, and it is found that it can significantly reduce MAI with only a few bits of feedback information. The scheme can also be modified for multiuser adaptation in asynchronous channels, leading to improved performance and increased system throughput. Overall, the proposed scheme is simple, widely applicable, and robust for binary-sequence-adaptation in direct-sequence code-division multiple-access systems. 
41306	4130617	Distributed wireless information flow allocation in multiple access networks	The authors propose a framework for solving the distributed wireless information flow allocation problem in multiple access networks. The goal is to minimize power consumption while meeting each end user's minimum data rate requirement and complying with peak power and interference constraints. The problem is modeled as a potential game, and the existence and uniqueness of the Nash equilibrium are proven. The authors also show that the Nash equilibrium is the optimal solution to the problem. Two distributed algorithms are proposed to find the Nash equilibrium, and numerical results demonstrate the benefits of flow allocation and the effectiveness of the algorithms. The problem is also shown to have a layered structure.
41307	4130759	Real-Time Learning Analytics System For Improvement Of On-Site Lectures	The purpose of this study is to introduce a real-time lecture support system for on-site classrooms. The system uses e-learning and e-book systems to collect data from teachers and students during lectures, which is then analyzed and used to provide immediate feedback to the teacher. This allows the teacher to adjust their lecture based on student engagement and understanding. Through a case study and large-scale experiment, the authors confirmed the effectiveness of the system in improving teaching and learning in on-site classrooms. The use of real-time learning analytics is a new and valuable strategy for teachers to improve their lectures and should be considered for immediate implementation.
41307	4130763	Real-Time Learning Analytics of e-Book Operation Logs for On-site Lecture Support	A real-time learning analytics system has been proposed for use in classrooms. It utilizes an e-learning and e-book system to collect data on students' learning activities during lectures. This data is then analyzed and presented visually to the teacher on a web-based platform. The teacher can monitor the students' engagement levels, such as if they are following the lecture, on which page they are on, and if they are adding bookmarks or notes. A case study showed that the system was effective in promoting high synchronization between the teacher and students, with more students actively engaging in the lecture and using the e-book tools compared to a control group.
41308	413087	Dataflow-Based Lenient Implementation of a Functional Language, Valid, on Conventional Multi-processors	This paper presents a lenient implementation of a functional language, Valid, on traditional multi-processors using a data-flow execution scheme. This approach allows for highly concurrent execution of fine-grain function instances, which are created during the execution of a functional program. The lenient execution and split-phase operation help to minimize idle time caused by remote memory access and calls. However, to handle fine-grain parallelism on conventional multi-processors without specialized hardware, it is necessary to reduce overhead. The paper discusses compilation issues and runtime systems for supporting fine-grain parallel execution on two types of multi-processors: shared-memory and distributed-memory. Preliminary evaluations of the implementation on a Sequent Symmetry S2000 and Fujitsu AP1000 are also presented.
41308	4130838	A dataflow language with object-based extension and its implementation on a commercially available parallel machine	The proposed programming language, called "V," aims to simplify the process of writing massively parallel programs. It is based on a dataflow-based functional programming language to eliminate the issue of timing in parallel programs. The language also includes an object-based abstraction, called "agent," which allows for the creation of parallel entities with their own states and communication capabilities. Along with connecting agents, the language also offers features such as error handling and debugging tools. Overall, V offers a more efficient and user-friendly approach to writing massively parallel programs.
41309	413090	Coupling Transparency and Visibility: a Translucent Middleware Approach for Positioning System Integration and Management (PoSIM)	The advancement of wireless technology and the availability of various positioning techniques have created new opportunities for Location Based Services (LBSs). However, this also adds complexity to the development of LBSs. To address this issue, middleware such as PoSIM can be used to access and control different positioning systems on wireless devices. PoSIM offers two levels of visibility for positioning management: a simple, context-based approach and a more advanced access to configurations. This allows LBSs to easily and efficiently utilize multiple positioning sources. The paper focuses on describing the architecture and API of PoSIM, highlighting its design and implementation decisions.
41309	4130980	COSMOS: A Context-Centric Access Control Middleware for Mobile Environments	The increasing use of wireless portable devices and user mobility present new challenges for providing services in ubiquitous pervasive environments. One solution is to use mobile proxies, which act on behalf of limited wireless clients and have full visibility of their context. This includes factors such as access control rules, client location, user preferences, privacy requirements, terminal characteristics, and hosting environment. The paper introduces COSMOS, a context-centric access control middleware for the wireless Internet, which dynamically determines the context of mobile proxies and controls their access based on metadata and high-level authorization policies. The paper also demonstrates how COSMOS can be used to develop effective access control strategies for a context-dependent movie-info service deployed over an IEEE 802.11 network.
41310	4131028	Learning image similarity from Flickr groups using fast kernel machines.	This paper proposes a method for measuring image similarity by learning from online Flickr image groups. The process involves choosing 103 Flickr groups and building a one-versus-all multiclass classifier to classify test images into a group. The responses of the classifiers are used as features and the distance between feature vectors is calculated to measure image similarity. Experimental results on two datasets show that this approach outperforms using conventional visual features for image matching, retrieval, and classification. To train the classifiers quickly and accurately, the paper introduces a novel algorithm called Stochastic Intersection Kernel MAchine (SIKMA). This algorithm can produce a more accurate kernel classifier than a linear one in just a few minutes on large datasets. 
41310	4131027	Comparative object similarity for improved recognition with few or no examples	This paper discusses the importance of learning models that can recognize objects with few or no training examples, as most objects in the real world have a long-tailed distribution. The proposed approach uses comparative object similarity, where a good object model should respond more strongly to examples from similar categories than dissimilar ones. A regularized kernel machine algorithm is developed to incorporate this category dependent similarity regularization. Experiments on hundreds of categories demonstrate that this method can significantly improve performance, particularly for categories with no training examples. This approach has the potential to improve object recognition in real-world scenarios with limited training data.
41311	4131143	View Synthesis By Appearance Flow	The problem of novel view synthesis involves creating new images of an object or scene from different viewpoints. Instead of starting from scratch, this task can be approached by copying pixels from the original image. This is possible due to the high correlation between the visual appearance of different views of the same object. This can be learned by training a convolutional neural network to predict appearance flows, which are 2-D coordinate vectors that determine which pixels from the original image can be used to construct the new view. This approach can be extended to multiple input views, resulting in higher quality synthesized views compared to previous techniques. 
41311	4131163	Connecting missing links: object discovery from sparse observations using 5 million product images	Object discovery algorithms are used to group together image regions that come from the same object. This works well when there are many images with closely-spaced views of the object, creating strong connections. However, it is less effective when there are only a few images with sparse coverage of the object views. To address this issue, a new approach has been proposed. It uses a database of 5 million product images and represents each region in the input image as a "bag" of database object regions. Regions are grouped together based on their similar "bags of regions," allowing for the discovery of links between regions of the same object, even if they are captured from different viewpoints. This approach can effectively discover object instances even with limited coverage of the object's views.
41312	4131226	ReliableWeb Services: Methodology, Experiment and Modeling	This article discusses the impact of various parameters on the dependability of Web services and the methods of improving their availability through redundancy in space and time. The authors conducted experiments to compare the availability of replicated Web services with a single service. They used a replication manager to coordinate the services and employed a round robin algorithm for workload scheduling. A replication algorithm and system configuration are also described. The experiments showed that the proposed model, constructed with Petri-net and verified through different applications, accurately represents the characteristics of Web services. The obtained parameters can be used to demonstrate the dependability of Web services.
41312	4131253	User-Perceived Instantaneous Service Availability Evaluation	Today's businesses heavily rely on information and communications technology (ICT) infrastructures to provide dependable services. The dependability of these services is greatly influenced by the properties of individual infrastructure components. However, combining these properties for consistent dependability analysis can be challenging due to the varying usage of components by different service requesters. This paper introduces a methodology for evaluating user-perceived instantaneous service availability, using three input models: the ICT infrastructure, an abstract description of services, and a mapping of components for service requesters and providers. This allows for the automatic generation of availability models based on the specific ICT components used during service provision. The methodology takes into account the age, order, and time of usage of components to calculate instantaneous availability and can generate different availability models, such as reliability block diagrams and fault-trees. The approach is demonstrated by applying it to parts of the network infrastructure at the University of Lugano in Switzerland.
41313	41313106	CLUE: Achieving Fast Update over Compressed Table for Parallel Lookup with Reduced Dynamic Redundancy	The routing table size in backbone routers is growing rapidly, with some routers now reaching 400K entries. To address this issue, routing table compression is an effective solution. However, there is also a growing need for fast routing updates due to changes in network topology and new Internet features, as well as the increasing internet speed. To achieve high performance, backbone routers must handle three issues simultaneously: routing table compression, fast routing lookup, and fast incremental update. This is addressed by a complete solution called CLUE, which includes a compression algorithm, improved parallel lookup mechanism, and a new incremental update mechanism. CLUE has been proven to have a speedup factor proportional to the hit rate of redundant prefixes and has shown superior performance in large-scale experiments compared to previous solutions.
41313	4131311	Idle Detection Based Optimal Throughput Rate Adaptation in Multi-Rate WLANs	The current 802.11 WLANs support multiple transmission rates, allowing stations to adapt to varying wireless channels and achieve higher system throughput. However, designing effective rate adaptation algorithms requires consideration of four key factors. The authors propose a new algorithm, ITRA, which addresses these issues and aims to maximize overall system throughput. ITRA has two notable features: it uses a constant contention window to distinguish between collisions and channel errors, and it directly estimates network throughput to determine the appropriate transmission rate. Simulation results demonstrate the satisfactory performance of ITRA.
41314	413147	A High-Level Signal Integrity Fault Model and Test Methodology for Long On-Chip Interconnections	The paper presents a new method for generating test patterns to detect signal integrity problems on interconnects, taking into account the interconnection topology information. Previous methods using SPICE were found to be complex and time-consuming. To improve accuracy and save time, the paper proposes a new high-level signal integrity fault model that considers process variation and interconnect signal transition. The experimental results show that this model is more accurate for long interconnects compared to previous methods. Additionally, the proposed method is much faster than SPICE-based methods. Overall, this paper presents a more efficient and accurate approach to identifying and addressing signal integrity issues on on-chip interconnects.
41314	4131474	Increasing embedding probabilities of RPRPs in RIN based BIST	The paper introduces a new method called clustered reconfigurable interconnect network (CRIN) BIST, which aims to improve the success rate of detecting random-pattern-resistant-patterns in testing. This is achieved by using a scan-cell reordering technique based on the signal probabilities of test cubes and specific hardware blocks, resulting in higher embedding probabilities of care bit clustered scan chain test cubes. The proposed method also includes a simulated annealing based algorithm for maximizing the embedding probabilities of scan chain test cubes and an iterative algorithm for synthesizing the CRIN hardware. Experimental results show that the CRIN BIST technique achieves complete fault coverage with lower storage requirements and faster testing time compared to a previous method. 
41315	41315133	Joint tracking of clean speech and noise using HMMs and particle filters for robust speech recognition.	The proposed method aims to improve the performance of speech recognition in noisy environments by simultaneously tracking both the clean speech signal and the noise. This is achieved by using a combination of particle filters and hidden Markov models. The information gathered from speech tracking is then used to estimate the parameters of the noise. This approach provides more robustness against sudden changes in noise levels. Experimental results using the Aurora-2 connected digit recognition task showed a 12.15% improvement in performance compared to existing methods when the noise mean was updated every 300 milliseconds. This highlights the effectiveness of the proposed dynamic joint tracking framework. 
41315	4131584	A study on model-based error rate estimation for automatic speech recognition	A framework has been proposed for estimating the classification error rate in speech and speaker recognition systems using models. This allows for predicting the system's performance without the need for separate testing samples, making it desirable in theory and practice. However, the complexity of the multi-class comparison and dynamic time warping in these systems makes it difficult to calculate the error rate. To solve this, a one-dimensional misclassification measure is used to evaluate the distance between the model of interest and its competitors. This measure is then used to calculate the error rate for each class and overall task vocabulary. The measure can be approximated by comparing mixture Gaussian densities and HMMs. This approach accurately predicts error rates for various recognition problems, including those in noisy environments.
41316	41316116	Acoustic echo cancellation based on independent component analysis and integrated residual echo enhancement	This paper explores the use of a memoryless noise-suppressing nonlinearity in an acoustic echo canceler (AEC) with normalized least-mean square (NLMS) when there is ambient noise present. The addition of this nonlinearity is justified from an information-theoretic perspective and has a connection to independent component analysis (ICA). This approach allows for new algorithmic possibilities beyond traditional LMS techniques. By combining error enhancement and proper regularization, the AEC can be performed continuously and recursively in the frequency domain, even without the need for double-talk or voice activity detection. This technique has potential for better performance in the presence of both ambient noise and double-talk. 
41316	4131678	Enhancement of residual echo for improved acoustic echo cancellation	The paper discusses a method for improving the stability and performance of acoustic echo cancellation (AEC) in the presence of continuous distortion in the echo signal. This is achieved by using a noise suppressing nonlinearity in the adaptation loop, rather than as a post-processing technique. This nonlinearity is formulated as a solution to the signal enhancement problem. The results show that combining this method with other adaptive step-size algorithms improves the average echo return loss enhancement (ERLE) by 5-15 dB for additive white noise and 2 dB for speech coding distortion. The method also reduces misalignment by 5 dB or more. It is effective in both simulated and real acoustic echo scenarios and is a promising technique for enhancing AEC in the presence of linear and nonlinear distortion.
41317	4131742	Efficient evaluation of XQuery over streaming data	This paper discusses the importance of processing queries over streaming XML data, and presents a new framework and techniques for doing so. The authors propose optimizations to transform XQuery queries for single-pass execution, a methodology for determining when this is possible, and a code generation approach that can handle recursive functions and user-defined aggregates. Their implementation is evaluated using various benchmarks and real-world queries, and is shown to be significantly faster and more efficient than existing systems. The results demonstrate the effectiveness of their approach in handling large datasets and avoiding memory overflows.
41317	41317161	Effective and efficient sampling methods for deep web aggregation queries	The deep web contains a large amount of data on the World Wide Web, but executing high-level queries on this data is difficult due to limited access. This paper focuses on aggregation queries involving data enumeration, which requires sampling. Existing methods for sampling, such as HDSampler, are not effective for skewed data. The paper introduces two new sampling algorithms, ANS and TPS, which can accurately answer aggregation queries on skewed data without prior knowledge of the data. These algorithms also have lower sampling costs compared to HDSampler. Experiments show that ANS and TPS outperform HDSampler by a factor of 4 on average, and TPS has only one-third of the sampling costs. 
41318	4131821	External hashing with limited internal storage	This paper explores the use of external hashing and the potential for improving retrieval speed by storing extra information in internal storage. The first part of the paper focuses on a restricted class of algorithms and derives a lower bound on the required extra storage. An algorithm that achieves this bound is also presented. In the second part, more practical algorithms are developed and analyzed, including one that allows for retrieval in one access using only a small amount of extra storage per bucket. The cost of inserting a record is also considered and found to be low. Overall, this algorithm is highly competitive for applications with a need for fast retrieval.
41318	4131853	Evaluation measures of multiple sequence alignments.	MSAs are essential for understanding the structure, function, and evolution of proteins. A new algorithm called Circular Sum (CS) has been developed to evaluate the quality of MSAs. It uses a solution for the Traveling Salesman Problem to identify a circular tour connecting sequences in a protein family. This eliminates the need for calculating an evolutionary tree and reduces potential errors. The CS method provides an upper bound for the best possible MSA and can also formally score a specific MSA. It connects MSAs to evolutionary trees and can be used to evaluate different methods for creating MSAs. Unlike other measures, it does not require the reconstruction of a tree and weighs all evolutionary events equally. 
41319	4131987	Face verification of age separated images under the influence of internal and external factors	This paper focuses on the task of verifying the age of faces in images, taking into account various internal and external factors. The authors propose a hierarchical local binary pattern (HLBP) feature descriptor, combined with an AdaBoost classification framework, to effectively represent and classify faces. The results of experiments on aging datasets show that the proposed framework is robust for both adults and children. The study also looks at the effects of factors such as age gap, gender, ethnicity, pose, expressions, facial hair, and glasses on verification performance. It is found that age gap has a significant impact on accuracy and that cues can aid both humans and machines, but can also mislead. Pose and expression variations have a negative impact, while facial hair and glasses can be useful cues. The ethnicity of the faces also has some effect on performance, but overall, non-linear algorithms do not significantly improve results compared to linear ones.
41319	41319101	Can We Minimize the Influence Due to Gender and Race in Age Estimation?	Automatic human age estimation has become a popular research topic in recent years. Despite advancements, there are still challenges such as estimating age across different image acquisition methods, expressions, gender and races. The influence of race and gender is a common issue as collecting a diverse range of face images is difficult. This can lead to degraded performance when estimating ages of different races. To address this, a new scheme has been proposed to mitigate the effects of race and gender on age estimation. It has shown a significant improvement of over 20% on a widely used database, MORPH-II. This study is crucial for developing a practical age estimation system that can handle a mixture of races and genders.
41320	413204	Bridging the Gap between Enterprise Content Management and Creativity: A Research Framework	ECM is a growing area of research in Information Systems, with a focus on improving consistency, availability, and traceability of content. However, one important aspect that has been overlooked is the role of creativity. While measures such as time and budget are important for organizations, it is also crucial to consider the impact of ECM on employee creativity and innovation. This paper advocates for a research approach that considers both traditional control factors and the influence of ECM on creativity. A research framework is proposed, drawing from various literatures on ECM and creativity. This framework aims to further explore the relationship between these two concepts.
41320	4132041	The Shadow of ECM: The Hidden Side of Decision Processes	This article discusses the connection between competitive advantage and knowledge management through the use of enterprise content management (ECM) platforms. The study examines the relationship between knowledge management systems, ECM systems, and decision-making processes, and aims to determine if ECM systems can provide value to organizations. Through a case study, it is found that ECM systems can improve decision-making by increasing the quantity and quality of input and formalizing knowledge throughout the process. This research expands upon existing literature on knowledge management and highlights the importance of ECM in enhancing decision-making processes for organizations. 
41321	4132183	Inferring gene regulatory networks by integrating static and dynamic data.	The paper proposes a methodology for learning gene regulatory networks from DNA microarray data by integrating different data and knowledge sources. The method is applied to experiments on the yeast cell cycle, using data from deletion mutant experiments, gene expression time series, and the Gene Ontology. The method involves four phases: deriving an initial gene network from static data, selecting cell cycle-related genes from the Gene Ontology, using the network structure to initialize a dynamic model, and updating the network using a genetic algorithm. This approach is compared to a fully data-driven approach and is shown to be more effective in producing robust models. The learned networks can then be validated through wet-lab experiments.
41321	4132128	TA-clustering: cluster analysis of gene expression profiles through Temporal Abstractions.	This paper presents a new method for clustering short time series gene expression data. The technique uses qualitative representations of profiles, labelled with trend Temporal Abstractions (TAs), to dynamically identify clusters. Clustering is performed at three levels of aggregation, each representing a different degree of qualitative representation. The TA-clustering algorithm is shown to be robust, efficient, and outperforms the standard hierarchical agglomerative clustering approach in dealing with temporal dislocations. The results can be visualized as a three-level hierarchical tree of qualitative representations, making them easy to interpret. The algorithm is demonstrated on two simulated data sets and a study of gene expression data from S. cerevisiae.
41322	4132249	A function-decomposition method for development of hierarchical multi-attribute decision models	Function decomposition is a machine learning technique that creates a hierarchical structure from labeled data by identifying new aggregate attributes and their descriptions. These new attributes are simpler than the original set and are used to develop a multi-attribute decision model. This method, implemented in a system called HINT, was evaluated on a housing loans problem and three existing decision models. The results showed that function decomposition can produce accurate and transparent decision models. Additionally, human interaction, such as providing assistance or background knowledge, can improve the comprehensibility and accuracy of the models. 
41322	4132229	Machine Learning by Function Decomposition	We introduce a new machine learning technique that breaks down a problem into smaller, simpler parts by creating a hierarchy of intermediate concepts and definitions based on a set of training examples. This approach is inspired by the design of digital circuits using Boolean function decomposition. To address the high time complexity of finding the optimal decomposition, we propose a suboptimal heuristic algorithm. Our method, implemented in the HINT program, is evaluated on artificial and real-world learning problems and demonstrates good performance in both classification accuracy and discovery of meaningful concept hierarchies. 
41323	4132329	Research on modeling of complicate traffic simulation system	Traffic simulation has become a crucial tool in dealing with the growing complexity of traffic problems. System modeling has also become increasingly important in these simulations. Cellular automata, which are simple mathematical models, have been proven to be effective in describing complex traffic environments. A simulation model that combines agent technology, HLA/RTI technology, and expanded cellular automata is proposed in this paper. This model allows for flexibility and scalability, while also providing powerful computing resources to tackle complex traffic issues. The model utilizes expanded cellular automata and agent technology to represent the behaviors of passengers, vehicles, and traffic signals. It can be used to determine optimal solutions for traffic disasters, supervise large scale activities, and design traffic environments.
41323	4132333	Research on Grid-Based Traffic Simulation Platform	The complexity of traffic problems in reality has made it difficult to find effective solutions. Traffic simulation has emerged as an efficient method for analyzing traffic and its issues. However, as simulation areas become larger and more complex, traditional computing technology is unable to handle the demands for resources. This paper introduces a new simulation architecture, GHA, which utilizes the advantages of grid technology and combines it with agent technology. This allows for high-capacity computing resources and scalability, as well as increasing the authenticity of traffic simulation by incorporating agent models. A traffic simulation platform based on GHA is also implemented and performance tests are conducted. 
41324	4132415	Lazy home-based protocol: combining homeless and home-based distributed shared memory protocols	This paper introduces a new protocol for a page-based distributed shared memory system that combines the benefits of both homeless and home-based protocols. The protocol uses a homeless diff-based memory update during lock synchronization to reduce critical section time and a home-based page-based update using the invalidation coherence protocol, which is delayed until the next barrier time. This "lazy home-based" update has advantages such as less interruption in home nodes and reduced data traffic and message count. The paper also includes a detailed analysis of the protocol's impact on distributed shared memory applications.
41324	4132442	Performance improvement techniques for software distributed shared memory	The paper introduces a new lazy home-based protocol and two implementation techniques that utilize a relaxed memory consistency model. The first technique is a diff integration method that addresses common issues in migratory DSM applications. The second technique is a dynamic home migration protocol that solves the problems associated with static home assignment in the original home-based protocol. Performance tests using established DSM benchmark applications show that these techniques successfully resolve diff accumulation and wrong home assignment problems.
41325	4132544	An agent-based simulation for restricting exploitation in electronic societies through social mechanisms	The problem of non-cooperation in artificial agent societies is a major issue that can lead to free-riding, where some agents consume resources without contributing to the society. To address this problem, new mechanisms for self-organization and management in multi-agent societies have been developed. These mechanisms involve modeling nodes of a P2P system as interacting agents in different groups and using social mechanisms such as tags, gossip, and ostracism to separate cooperative and uncooperative agents. This system has shown promising results in reducing exploitation and encouraging cooperative behavior without centralized control. These mechanisms have potential for use in regulating distributed societies and offer new insights into policy mechanisms for such societies.
41325	413254	Mechanisms for the self-organization of peer groups in agent societies	The article discusses the use of new mechanisms for group self-organization in digital societies where individuals share goods. The focus is on preventing free riders from exploiting cooperative sharers in a decentralized manner. The authors propose a simulated open P2P system that utilizes social mechanisms like tags, gossip, and ostracism to encourage sharers to join better groups and restrict free riders without the need for central control. This approach is inspired by human society and is suitable for current open P2P systems. The goal is to create a fair and balanced system for sharing digital goods.
41326	4132626	Interfacing a cognitive agent platform with a virtual world: a case study using Second Life	Online virtual worlds are becoming popular for human interaction and simulating multi-agent systems. It would be useful to integrate high-level agent development tools, specifically BDI programming frameworks, with virtual worlds. However, this is a complex task as it requires mapping sensor readings from virtual environments to a logical model. In this paper, the authors explore this problem in the context of agent interactions in Second Life. They propose a framework that allows any multi-agent platform to connect with Second Life and demonstrate it using the Jason BDI interpreter. This framework can bridge the gap between virtual worlds and BDI programming, making it easier for agents to interact with humans in these environments. 
41326	4132665	Interfacing a cognitive agent platform with second life	Second Life is a popular virtual world that is being increasingly used for simulating human interactions and multi-agent systems. This presents a challenge in connecting high-level agent development tools, such as BDI programming frameworks, to Second Life. This paper addresses this issue by proposing a framework that allows any multi-agent platform to be connected to Second Life. The framework also includes an extension of the Jason BDI interpreter. This enables the mapping of sensor readings from Second Life simulations to a logical model of observed properties and events. This framework has the potential to enhance the capabilities of Second Life as a simulation platform and facilitate more advanced and accurate virtual human interactions.
41327	4132745	Refined Coding Bounds and Code Constructions for Coherent Network Error Correction	Coherent network error correction refers to the problem of controlling errors in network coding when the source and sink nodes have knowledge of the network codes being used. This problem is addressed by obtaining improved versions of the Hamming bound, Singleton bound, and Gilbert-Varshamov bound for linear network codes. These bounds are shown to be tight, meaning they are achievable by certain construction algorithms. These algorithms use different approaches, one utilizing existing network coding algorithms and the other using classical error-correcting codes. The significance of these tight bounds is that sink nodes with higher maximum flow values can have better error correction capabilities. 
41327	4132754	Refined Coding Bounds for Network Error Correction	The article discusses refined versions of the Hamming, Singleton, and Gilbert-Varshamov bounds for network error correction. These bounds are based on the weight properties of network codes, which are defined by a set of local encoding kernels in a linear network code. The refined Singleton bound is shown to be tight for linear message sets. This research contributes to the understanding and improvement of network error correction techniques.
41328	4132820	Multicast switching fabric based on network coding and algebraic switching theory	Scheduling algorithms play a critical role in improving the throughput of switches. However, they do not guarantee a consistent delay in the switching fabric. This paper presents a new load-balanced wire-speed multicast switching fabric that incorporates network coding. The fabric uses a two-phase self-routing structure based on Boolean-multiplexer to achieve efficient multicast communication. The proposed fabric promises to balance the load and improve the performance of multicast switches. Its effectiveness is demonstrated through simulation results.
41328	4132835	Lattice Classification by Cut-through Coding.	This paper proposes a new method for classifying finite lattices based on the concept of -ary cut-through codes, inspired by engineering techniques for high-speed switching with quality of service. These codes encode lattice elements using distinct -ary strings of equal length, with the property that the initial encoding symbols of any two elements can determine the meet and join of those elements. The paper also presents criteria for determining -ary cut-through codability in terms of lattice congruences. This new approach to classification also creates a chain of ideals within the lattice of lattice varieties.
41329	4132997	Multi-modal human-machine communication for instructing robot grasping tasks	The challenge of creating intelligent robots is to give them cognitive abilities that allow users to easily and intuitively program them. One way to achieve this is through interactive demonstration, where the machine can learn work tasks from the user. To make this process efficient and user-friendly, the robot must be able to focus its attention, understand spoken instructions, and interpret non-verbal cues such as gestures. Researchers have made progress in developing a hybrid architecture that combines statistical methods, neural networks, and finite state machines to create a system for teaching grasping tasks through man-machine interaction. This system combines visual attention and gestural instruction with speech recognition and linguistic interpretation, and allows for multi-modal communication for dextrous manipulation of objects.
41329	41329157	Neural architectures for robot intelligence.	The article argues that studying artificial control architectures for robot systems can offer insights for understanding the architecture of higher brains. The focus is on hand actions, which are connected to many higher cognitive abilities. Examples of this approach include a modular system for recognizing hand postures, using vision and tactile sensing for hand movements, and using hand gestures for robot teaching. The authors propose a data-mining perspective for real-world learning and emphasize imitation of observed actions rather than reinforcement-based exploration. They discuss a project in which a robot is taught actions through hand gestures and speech commands, and highlight the potential contributions of such systems to the study of natural and artificial cognitive systems.
41330	4133012	Scalable target detection for large robot teams	The paper introduces a new asynchronous display method called "image queue" for operators to search through large amounts of data collected by autonomous robot teams. The focus of the method is on urban search and rescue tasks, where it helps to identify targets of interest, such as injured victims, by presenting a comprehensive view of the environment. Compared to a traditional synchronous display with live video feeds, the image queue reduces errors and the operator's workload. It also allows for a call center approach to target detection and enables scalability for large multi-robot systems. The paper concludes that the image queue is a more efficient and effective approach for foraging tasks.
41330	4133041	Towards an understanding of the impact of autonomous path planning on victim search in USAR	Multirobot systems are becoming increasingly advanced and capable of being used in important areas such as urban search and rescue. The key to their practical usefulness is the ability to effectively control a large number of robots with only a small number of operators. This paper compares two methods for controlling a team of 24 robots in a foraging task in an urban search and rescue scenario. Both methods require operators to monitor video streams and teleoperate robots in difficult situations, but one also involves providing waypoints for exploration. Results show that autonomous path planning leads to better overall performance, although there is not a significant reduction in operator workload. 
41331	413316	Coalition formation with spatial and temporal constraints	The coordination of emergency responders and robots in disaster scenarios is a big challenge for multi-agent systems. The main issue is how to create the best teams (coalitions) of responders to handle the tasks in the affected area. These teams may have to form, disband, and reform in different parts of the disaster area, as there are typically more tasks than agents available. The tasks themselves are complex and have different deadlines and durations, making the problem even more difficult. This is known as the Coalition Formation with Spatial and Temporal constraints problem (CFSTP), which has been proven to be NP-hard. To solve this problem, a Mixed Integer Program has been designed for small-scale instances, and new anytime heuristics have been developed that can complete 97% of the tasks in large problems. These solutions are the first of their kind for the CFSTP.
41331	4133118	Decentralized Coordination in RoboCup Rescue	Emergency responders face several challenges when managing major disasters. These include a large number of rescue tasks, limited resources and personnel, varying levels of effort required, and the need to quickly adapt to new tasks. To effectively address these challenges, it is important for responders to form teams or coalitions with agents from different agencies. This allows for a more efficient use of resources and increases the chances of saving lives and infrastructure. To facilitate this process, a decentralized solution is necessary to avoid a single point of failure. The authors propose a novel algorithm, F-Max-Sum, which utilizes the Max-Sum algorithm to solve the coalition formation problem in disaster management. Empirical evaluations show that this algorithm outperforms other decentralized solutions. 
41332	4133214	Optimal secure partial-repair in distributed storage systems	In a distributed storage system, certain storage nodes may lose fragments of the source file. These nodes are labeled as "faulty," while those that have not lost any fragments are called "complete." In partial repair, a combination of faulty and complete nodes work together to transmit fragments and recover the lost ones. The goal is to minimize the required bandwidth for this process. However, there is a risk of an eavesdropper intercepting the repairing fragments and obtaining information about the source file. To address this, optimal secure codes are proposed to ensure minimal partial-repair bandwidth while preventing the eavesdropper from gaining any information. A special scenario is also considered for exact partial-repair.
41332	4133243	Partial Repair for Wireless Caching Networks With Broadcast Channels	This study focuses on the repair problem in wireless caching networks where some stored packets are lost. The researchers establish theoretical lower bounds for the number of transmission packets needed for repair over error-free broadcast channels. They also examine the effects of the distribution of lost packets among caching nodes. The study also looks into the development of repair codes and suggests an optimal exact repair solution for certain scenarios. The findings of this research can help improve the efficiency of repairing lost packets in wireless caching networks.
41333	4133331	Sebar: Social Energy Based Routing Scheme For Mobile Social Delay Tolerant Networks	Delay Tolerant Networks (DTNs) are networks that are not always continuously connected, such as mobile social networks. Routing in such networks is challenging due to network partitioning, delays, and changing topology. Social-based approaches have gained interest for DTN routing, using social behaviors of nodes to improve routing decisions. This paper proposes a new social-based routing approach for mobile social DTNs, using a metric called social energy. This metric quantifies a node's ability to forward packets and is generated through node encounters and shared among communities. The proposed SEBAR protocol considers social energy of encountering nodes and favors nodes with higher social energy. Simulations show the effectiveness of SEBAR compared to other existing DTN routing schemes.
41333	4133311	Complexity of Data Collection, Aggregation, and Selection for Wireless Sensor Networks	In this paper, the focus is on the efficient processing of information in wireless sensor networks. The time, message, and energy complexities of tasks such as data collection, data aggregation, and queries in a multi-hop network of n nodes are studied. A lower bound on the complexity for optimal methods is presented, followed by the presentation of efficient distributed algorithms for solving these problems. The goal is to provide an upper bound on the complexity that is asymptotically matching the lower bound, making the algorithms highly efficient. Overall, this paper aims to contribute to the understanding and improvement of the functionality of wireless sensor networks.
41334	4133477	Towards Truthful Mechanisms for Binary Demand Games: A General Framework	The Vickrey-Clarke-Groves (VCG) mechanisms are highly regarded in the world of truthful mechanism design, but they are limited in their applicability. They only work for optimization problems with a utilitarian objective function and require the output to optimize the objective function, which can be computationally challenging. If we use VCG mechanisms for polynomial-time algorithms that approximate the optimal solution, they may no longer be truthful. To address these limitations, researchers are exploring the possibility of designing a truthful non-VCG payment scheme that is computationally tractable for a given output method. This paper focuses on binary demand games and shows that a truthful mechanism exists if the output method satisfies a certain monotone property. It also presents efficient algorithms for computing truthful payments through various combinations of subgames.
41334	41334227	Low-cost routing in selfish and rational wireless ad hoc networks	This paper discusses the challenges of designing routing protocols for wireless networks where nodes may act in their own interests rather than strictly following the protocol. The proposed solution is a unicast routing protocol that takes into account the rational behavior of nodes and uses a pricing mechanism to ensure that nodes are incentivized to declare their true cost for relaying data. The paper also discusses a centralized method for computing the payment and a distributed implementation of the protocol. Extensive simulations show that the proposed protocol is effective in preventing collusion between nodes and achieving a low ratio of total payment to total cost. 
41335	4133596	Flow-level traffic matrix generation for various data center networks	With the rise of cloud computing technologies, the number of data centers deployed by governments, enterprises, and universities has increased. To reduce costs, various architectures and topologies have been proposed for data center networks. However, constructing a data center can be expensive and there are many technologies that can affect its structure. Before building a data center, its characteristics must be confirmed to meet requirements. This can be done through simulation and testing using a traffic generation method. The proposed method in this study uses flow-level traffic matrix and Python and iPerf programming languages to generate network traffic. The results showed that the generated traffic closely resembles real network traffic collected from a data center network. 
41335	4133579	FLAME: Flow level traffic matrix estimation using poisson shot-noise process for SDN	Network operation and management tasks such as traffic accounting, traffic engineering, and network design rely on an accurate and timely network Traffic Matrix (TM), which shows the traffic volume between origin and destination nodes. However, many existing methods assume that an accurate TM is readily available, which is not always the case. One challenge in implementing these methods is obtaining an accurate and timely TM. Additionally, it is important to estimate the future changes in the TM, rather than just relying on historical data. Software Defined Networking (SDN) has emerged as a promising solution, offering advantages over traditional IP-based networks. To address these issues, the FLAME method, based on the Poisson Shot Noise process, is proposed for estimating an accurate and timely TM in an SDN environment. Evaluation results show that FLAME is 66.36% similar to real traffic measurements in a data center network.
41336	4133618	Sentence Correction Incorporating Relative Position and Parse Template Language Models	Sentence correction is a growing concern in computer-assisted language learning. Despite efforts using grammar rules and statistical machine translation, current techniques are not strong enough to handle the common errors made by non-native speakers. To address this issue, a relative position language model and a parse template language model have been proposed to supplement traditional language modeling methods. A corpus of incorrect English-Chinese sentences and their corrections has been manually assessed by human evaluators. The results show that the proposed approach outperforms a top phrase-based statistical machine translation system in terms of error correction, as determined by human evaluation.
41336	41336102	Candidate Generation For Asr Output Error Correction Using A Context-Dependent Syllable Cluster-Based Confusion Matrix	The study focuses on error correction techniques for spoken language understanding in language learning and spoken dialogue systems. The techniques involve two stages: generating correction candidates and selecting them. The study introduces a Context-Dependent Syllable Cluster-based Confusion Matrix for generating correction candidates and a Contextual Fitness Score for selecting corrected syllable sequences based on their relationship to neighboring syllables. The final word sequence output is determined using an n-gram language model. Experiments show that this method outperforms conventional speech recognition with a 0.771 BLEU score compared to 0.742. These findings suggest that the proposed method can effectively improve spoken language understanding in language learning and spoken dialogue systems.
41337	413371	A benchmark library and a comparison of heuristic methods for the linear ordering problem	The linear ordering problem is a challenging combinatorial optimization problem that involves finding the best possible ordering of a set of items represented by a weighted digraph. This paper explores various heuristics and metaheuristic algorithms for solving this problem and presents the results of extensive computational experiments. Additionally, a new benchmark library called LOLIB is introduced, which contains both previously used instances and new ones for this problem. This research provides valuable insights and tools for tackling the NP-hard linear ordering problem.
41337	4133740	Direct methods with maximal lower bound for mixed-integer optimal control problems	Optimal control problems often involve making discrete decisions, such as choosing between different gears or valves that can only take certain values at a given time. While there has been progress in solving optimization problems with integer variables or continuous optimal control, combining the two is still an area of research. This article focuses on finding lower bounds for nonlinear mixed-integer programs through relaxation of the integer requirements. The authors present a new algorithm that uses a combination of the direct multiple shooting method, control grid refinement, and heuristic integer methods to solve mixed-integer optimal control problems with discrete-valued control functions. They demonstrate the effectiveness of this algorithm in a practical application involving energy optimization of a subway train with discrete gears and velocity limits.
41338	4133816	Adaptive tracking control for discrete-time switched nonlinear systems with dead-zone inputs.	 This paper examines the problem of adaptive tracking control for a discrete-time switched nonlinear system with dead-zone input, using a class of switching signals with average dwell time. The complex structure of these systems and the presence of dead-zones pose a significant challenge for control. To address this, fuzzy logic systems are used to approximate the unknown terms of each subsystem. Both stable and unstable subsystems are considered, and the controllers, update laws, and switching signals are designed to ensure stability and bounded signals. A simulation example is presented to demonstrate the effectiveness of the proposed approach.
41338	4133880	Passive control for networked switched systems with network-induced delays and packet dropout	Networked control systems, specifically networked switched control systems, have become increasingly popular for connecting controllers and sensors/actuators in controlled plants and processes. These systems are affected by network-induced delays and packet dropout, making them a special case of switched time-varying delay systems. To ensure stability and strict passivity in such systems, sufficient conditions have been derived for a class of switching signals with an average dwell time. Additionally, hybrid passive controller design has been proposed using linear matrix inequalities, taking into account the network-induced delays and packet dropout. This approach allows for efficient and effective control of networked switched control systems.
41339	413392	Exploring geospatial cognition based on location-based social network sites	Geospatial cognition is an important skill for navigating sophisticated urban spaces and making informed decisions in our daily lives. However, developing urban cognition can be a time-consuming and challenging task, especially in unfamiliar or rapidly changing cities. Gathering information from experienced locals can also be a tedious process. To address this, researchers are exploring the use of location-based social networks to collect and utilize the urban cognition of the crowd. They have developed a method to extract crowd movement data from these networks, which can provide valuable insights into common urban cognition and measure the socio-cognitive distances between different urban clusters. This data is then used to create a simplified two-dimensional map of cognitive relationships, allowing for a more intuitive understanding of urban space. In an experiment using Twitter data, this socio-cognitive map was found to accurately represent the cognitive proximity between different urban clusters. This map can be used for practical purposes, such as finding the nearest neighbor areas in terms of cognitive similarity.
41339	4133947	Twitter-based Urban Area Characterization by Non-negative Matrix Factorization	The widespread use of mobile devices and social media has led to the emergence of crowds as social sensors, able to share real-time information about urban spaces. To better understand urban areas, researchers are using location-based social networks to monitor crowd behavior and extract experiential features from Twitter data. This data is then analyzed using Non-negative Matrix Factorization to identify 5-dimensional social conditions that can characterize different urban areas. Through this approach, representative groups of urban areas can be identified based on their unique patterns, providing a comprehensive understanding of the daily lives of the crowds in these areas.
41340	4134032	Towards random field modeling of wavelet statistics	This paper explores the statistical properties of signals and images in the wavelet domain. The authors discover that the correlation between wavelet scales can be significant, despite commonly used decorrelated-coefficient models. They investigate different statistical-interaction models and propose an efficient and quick method for describing wavelet-based correlations. The paper also demonstrates how this method can be used for hierarchical Markov random field modeling of within-scale and across-scale dependencies. Overall, this paper highlights the importance of considering correlations between wavelet scales in statistical analysis and provides a useful tool for efficient modeling.
41340	413406	Wavelet shrinkage with correlated wavelet coefficients	This paper delves into the statistical analysis of wavelet coefficients for random signals and images. Most methods for wavelet shrinkage assume independence of the coefficients, but this paper challenges that notion and reveals that there can be significant correlations between different scales. Even pixels separated by multiple scales can exhibit strong correlations. The aim is to create a random field model that accurately describes these statistical correlations, and to showcase its effectiveness in Bayesian wavelet shrinkage for denoising signals and images. 
41341	41341149	Enabling advanced environmental conditioning with a building application stack	Building-focused applications have the potential to greatly improve the operation and sustainability of buildings. By addressing issues with current architectures and providing a framework for reliable and fault-tolerant operation, a building application stack allows for the development of advanced applications. These applications integrate sensors and actuators from both the building infrastructure and external networks, making use of the architecture's design pattern. Two examples of such applications, a demand-controlled ventilation system and a demand-controlled filtration system, were successfully implemented in a large commercial building. These applications were able to greatly reduce energy consumption while maintaining air quality, showcasing the potential of the architecture for widespread and rapid development in various buildings.
41341	4134196	HBCI: human-building-computer interaction.	Buildings consume a significant amount of energy globally. As smartphones and sensor networks continue to advance, buildings have the potential to offer personalized and energy-efficient services to occupants. This paper proposes a Human-Building-Computer Interaction (HBCI) system that connects buildings with their occupants by bridging the gap between the physical and digital worlds. The HBCI system consists of an Android mobile app, cloud-based RESTful services, and physical objects with QR tags. This system allows users to increase their comfort within a building while reducing energy consumption. The HBCI system demonstrates the potential for buildings to provide personalized and context-aware services to occupants while minimizing energy usage.
41342	4134265	Hdr Image Multi-Bit Watermarking Using Bilateral-Filtering-Based Masking	The paper introduces a new method for embedding multi-bit watermarks in High Dynamic Range (HDR) images, ensuring the watermark is imperceptible in both the HDR and Low Dynamic Range (LDR) versions. The method uses the wavelet transform of the Just Noticeable Difference (JND)-scaled space of the original image and a visual mask based on the Human Visual System (HVS) to improve image quality. Bilateral filtering is used to locate the detail portion of the image, where the watermark is embedded, and a contrast sensitivity function is applied to modulate the watermark intensity in each wavelet decomposition subband. Experimental results demonstrate the effectiveness of the proposed method in preserving image quality and being robust against tone-mapping operators (TMOs).
41342	4134277	Object-oriented stereo-image digital watermarking	The article introduces an object-oriented approach for watermarking stereo images. This method uses the depth information in stereo images to embed the watermark, using a wavelet-based technique called quantization index modulation. The results of experiments show that this method is both robust against JPEG and JPEG2000 compression and fragile against other signal manipulations. The study was conducted by SPIE and IS&T in 2008.
41343	4134357	Secure Annotation For Medical Images Based On Reversible Watermarking In The Integer Fibonacci-Haar Transform Domain	The increased use of digital image-based applications has led to large databases that are difficult to use and can pose privacy concerns, especially in medical applications. The common solution is to encrypt both the image and patient data separately, but this can be inefficient as it requires decrypting both files to access information. A new approach for secure medical image annotation is proposed, using a key-dependent wavelet transform, a secure cryptographic scheme, and a reversible watermarking scheme. This allows for the insertion of patient data into encrypted images without knowledge of the original image, encryption without losing information, and reversible recovery of the original image. Experiments show the effectiveness of this system.
41343	4134343	Reversible Data Hiding In The Fibonacci-Haar Transform Domain	This article describes a new method for reversible data hiding in digital images. The technique ensures that the original image can be fully recovered without any loss of quality after extracting the embedded information. This is achieved by using the Integer Fibonacci-Haar Transform, which is a novel wavelet domain based on a parameterized subband decomposition of the image. The parameterization is determined by a selected Fibonacci sequence, which adds an extra layer of security to the method. Experimental results demonstrate the effectiveness of this approach.
41344	4134440	Information retrieval system for human-robot communication: asking for directions	The ACE project aims to develop a robot that can navigate through unfamiliar city environments without relying on GPS or maps. Instead, the robot will interact with humans to gather direction information. A human-robot communication system has been created to allow the robot to ask for directions and store the information as internal knowledge. This system integrates linguistic theories and uses a topological route graph to give feedback to the human and navigate through unknown areas. This innovative approach to navigation could have significant implications for robots and their ability to operate in real-world settings.
41344	41344144	Increasing Helpfulness towards a Robot by Emotional Adaption to the User.	In this article, an emotional adaption approach is proposed to increase helpfulness towards robots in human-robot interactions. The approach is based on social-psychological theories and aims to induce empathy and a feeling of similarity between humans and robots. This is achieved through two methods: explicitly stating similarity before interaction and implicitly adapting the robot's emotional state to match the human user's mood. This leads to the generation of task-driven emotional expressions from the robot to maintain high empathy throughout the interaction. The approach was evaluated in a user study with positive results, showing significant effects on helpfulness, anthropomorphism, and animacy. Overall, the emotional adaption approach is effective in promoting a positive human-robot interaction.
41345	4134561	An Automatic Segmentation Combining Mixture Analysis and Adaptive Region Information: A Level Set Approach	This paper introduces a new automatic framework for color image segmentation that combines adaptive region information and mixture modeling. The region information is represented by a mixture of General Gaussian (GG) probability density functions (pdfs) and the segmentation is achieved by minimizing an energy functional based on region contours and mixture parameters. The proposed method allows for easy extension to any number of regions and is fully automatic. Additionally, it produces an accurate and representative mixture of pdfs. The approach utilizes both boundary and region information to guide the segmentation and is validated on real world color images.
41345	41345119	Automatic colour-texture image segmentation using active contours	The paper introduces a fully automatic method for segmenting colour/texture images, meaning that the initial region selection and number of regions are determined automatically. The method uses a mixture of probability density functions to represent region information based on colour and texture features. The segmentation is achieved by minimizing an energy functional that combines boundary and region information, allowing the initial region contours to evolve towards the actual boundaries and adjusting the mixture parameters to the region data. The method utilizes level sets to handle topology changes and ensure numerical stability. The effectiveness of the approach is demonstrated through synthetic and natural colour-texture image segmentation examples. 
41346	4134647	Evaluating Domain Design Approaches Using Systematic Review	Software Product Lines (SPL) are becoming increasingly popular among software companies as a way to achieve reuse. It involves three processes: domain engineering, application engineering, and management. Domain engineering focuses on developing assets that can be reused by different products, forming the core assets. The Domain Specific Software Architecture (DSSA) is a crucial part of this process and plays a key role in the success of the products developed from it. To design a good DSSA, a process must be followed to manage the variability and commonality within the domain. Companies transitioning from single system development to SPLs need systematic activities to utilize existing assets for developing a DSSA. This paper provides a systematic review of domain design approaches, helping companies understand the current landscape and choose or adapt an approach that fits their needs.
41346	4134650	Towards an effective integrated reuse environment	The success of reusing an object depends on the knowledge about its existence. This knowledge can come from past experience or can be obtained through information dissemination. Information retrieval plays a crucial role in disseminating knowledge about reusable objects. This concept is applied to software reuse in this work. By combining the fields of software reuse and information retrieval, an integrated environment is created to increase the level of reuse and improve software quality while reducing resource consumption. This promotes a more efficient and effective approach to software development.
41347	4134773	MAHRU-M: A mobile humanoid robot platform based on a dual-network control system and coordinated task execution	This paper presents a mobile humanoid robot platform that can perform various services for humans in their daily environments. To ensure efficient operation in complex environments, a dual-network control system was developed using high-speed IEEE 1394 and Controller Area Network (CAN). A service framework was also introduced to coordinate tasks for the robot. Performance evaluations of the framework and control system were conducted, showing efficient execution of tasks in human workspaces. The platform, MAHRU-M, is able to recognize designated objects and calculate their pose using a particle filter with back projection-based sampling. A unique approach for human-like arm inverse kinematics and a mean-shift algorithm for real-time and robust object tracking were also implemented. Overall, the results show successful execution of services in office or home environments.
41347	4134712	Real-Time Visual Tracking Insensitive to Three-Dimensional Rotation of Objects	Visual tracking is crucial for various applications, including controlling robots, surveillance, agriculture automation, and medical image processing. Fast and reliable visual tracking is important as it affects the overall system's reliability and real-time capabilities. However, estimating the configuration of a moving object's three-dimensional pose in real-time is challenging. Unlike humans, who can track an object without knowing its three-dimensional pose, existing visual tracking methods struggle with changes in pose due to rotation or occlusion. This paper proposes a novel visual tracker that is insensitive to three-dimensional rotation, translation, two-dimensional rotation, scaling, and shear of an object. It achieves this by updating the reference image used for tracking whenever the distortion rate exceeds a predetermined threshold. The algorithm has been successfully tested in real-time using a personal computer with a frame grabber. 
41348	4134840	A nonlinear iterative learning method for robot path control.	The proposed method is designed for non-linear dynamic systems with uncertain parameters. It uses a non-linear system model and the model algorithmic control concept in an iterative sequence. A condition for convergency is given, and the method is demonstrated to be effective for continuous-path control of a robot manipulator.
41348	4134818	Non-linear Task-Space Disturbance Observer for Position Regulation of Redundant Robot Arms against Perturbations in 3D Environments.	In this report, the difficulty of position regulation for redundant humanoid arm systems in complex 3D environments is addressed. Non-linear uncertainties and perturbations from obstacles can make this task even more challenging. The proposed solution is a non-linear task-space disturbance observer, which is demonstrated through simulations using a 7-DOF redundant robot arm system. Results show the effectiveness of this method compared to a conventional mass-damper based observer. The concept is then applied to a controller for human-like motion, and the system successfully continues to reach a target even when encountering obstacles. These results are compared to unperturbed motion, further demonstrating the benefits of the proposed method.
41349	4134932	Performance evaluation of network mobility handover over future aeronautical data link	The aviation industry is working towards standardizing data communication systems for air traffic management in the future. The International Civil Aviation Organization (ICAO) is focusing on standardizing an IPv6-based aeronautical telecommunications network and future radio access technologies. This paper integrates L-DACS 1, a possible future radio access technology, with a realistic IPv6-based network and analyzes the impact of handover delay on TCP performance. The simulation experiments use realistic Frame Error Rate (FER) values and show that by reducing the handover latency and introducing a Home Agent (HA)-buffering method, the transmission completion time can be reduced by at least 10%. These improvements are especially significant for larger data transmissions over a wireless link with a data rate of 31.5kbits^-^1.
41349	4134986	Self-Organizing Smart Grid Services	Decentralized and self-organizing mechanisms are effective solutions for handling the large amounts of data generated by various components in the smart grid. This approach involves a middleware process that operates on the infrastructure level, providing services like routing and data filtering. The decision level, represented by a dynamic meta model, controls the infrastructure level through processes that design, supervise, and control agents. The two levels are connected through a feedback loop, ensuring that changes on one level are reflected in the other. This system is governed by self-organizing mechanisms based on reflection principles. Two categories of metrics are used to evaluate self-organizing systems: performance metrics related to the target application, and metrics that focus on the self-organizing properties of the algorithms. 
41350	4135075	Socially augmenting employee profiles with people-tagging	Employee directories are important for connecting people within an organization for collaboration, problem-solving, and accessing expertise. However, maintaining accurate and updated user profiles is often neglected. This paper presents a system that allows users to tag each other with keywords, which are displayed on their profiles. This "people-tagging" acts as a form of social bookmarking, allowing users to organize their contacts and search for individuals based on specific topics. It also has the added benefit of collectively maintaining each other's profiles. User studies show that people-tagging is effective for contact management and the tags accurately reflect interests and expertise. There were no reports of offensive or inappropriate tags, leading to the belief that people-tagging will become an important relationship management tool in organizations.
41350	4135058	Community insights: helping community leaders enhance the value of enterprise online communities	Online communities are becoming increasingly popular in the business world as a means to improve productivity and share expertise. However, the success of these communities relies heavily on the leaders who oversee them. Despite this, current technology often fails to provide adequate support for community leaders due to a lack of understanding of their specific needs and a focus on member rather than leader-centric tools. To address this issue, a new tool called Community Insights (CI) has been developed to provide leaders with actionable analytics to foster healthy communities and benefit both members and the organization. Through a long-term deployment to leaders of 470 communities, CI has been shown to be effective in providing leaders with useful, actionable and contextualized analytics. This research also offers valuable insights into which metrics are most useful for leaders, the importance of actionable and contextualized data, and how leaders can effectively make sense of community data.
41351	413512	Explorations in an activity-centric collaboration environment	The demonstration showcases a new hybrid collaboration technology that combines features of informal and formal collaboration tools. It supports lightweight and flexible activities with dynamic membership and a mix of real-time and delayed collaboration. The system will be introduced and audience members will have the opportunity to use it in various exercises. 
41351	4135130	A team collaboration space supporting capture and access of virtual meetings	This paper introduces TeamSpace, a collaborative workspace system designed for geographically distributed teams. It addresses the challenges of managing shared work processes and artifacts in a project, and aims to integrate synchronous and asynchronous team interactions in a task-oriented environment. The focus is on supporting virtual meetings as a key aspect of teamwork, and the prototype of TeamSpace includes features for seamless management of both asynchronous and synchronous meetings. The system also allows for easy access to captured synchronous data, which is integrated with other relevant information, providing users with a comprehensive view of current and past team activities. 
41352	4135245	Return On Contribution (ROC): A Metric for Enterprise Social Software.	The value of enterprise social media applications and their impact on components and users is not easily measurable in traditional economic terms. Instead, a new approach called Return On Contribution (ROC) is proposed, which focuses on the human aspect of service to others. This metric is designed to help manage social software systems and track collaboration among employees through the creation and consumption of information and knowledge. By using ROC, the performance of various social media applications can be monitored, along with understanding usage patterns and the performance of employees. This has implications for organizational knowledge exchange, the role of "lurkers," and the types of measurements that are valuable to employees, managers, and system administrators.
41352	413524	Dogear: Social bookmarking in the enterprise	The article discusses a social bookmarking service designed specifically for large enterprises. It covers design principles related to online identity, privacy, and information discovery. The service also focuses on extensibility and follows a web-friendly architectural style. The key design features of the service are described, along with the results of an eight-week field trial, which included user activities and a survey of its benefits. The feedback from the trial was positive and suggested potential enhancements to the service. The article concludes with a discussion on potential extensions and integration with other corporate collaboration applications.
41353	41353165	Overview of VideoCLEF 2008: Automatic Generation of Topic-based Feeds for Dual Language Audio-Visual Content.	VideoCLEF was created in 2008 to develop and evaluate tasks related to analyzing and accessing multilingual multimedia content. The first task, Vid2RSS, focused on classifying dual language videos from Dutch television. Participants were given Dutch archival metadata, speech transcripts in both Dutch and English, and ten thematic category labels to assign to the videos. Five groups participated, using various classifiers and sources of training data. The Dutch speech transcripts and archival metadata were found to be effective as indexing features, but no group was able to significantly improve performance by combining feature sources. The translation task was evaluated and deemed valuable for non-Dutch speaking English speakers, while the keyframe extraction task showed promising results in automatically selecting representative shots. Future years of VideoCLEF will expand the corpus and tasks.
41353	4135310	DCU at VideoCLEF 2009	DCU participated in the VideoCLEF 2009 Linking Task and used the Lemur information retrieval toolkit to identify relevant related content. They implemented two variations of their approach, one searching the Dutch Wikipedia with the exact words from the ASR transcription and the other using automatic machine translation to search the English Wikipedia. The first approach, using stemmed and stopped Dutch Wikipedia, yielded the best results. However, due to lack of equivalent articles between Dutch and English Wikipedia, some hits were lost and in extreme cases, no output was returned. Despite including a preprocessing phase, some unuseful types of pages still negatively affected their second approach.
41354	4135453	Pic-A-Topic: efficient viewing of informative TV contents on travel, cooking, food and more	Pic-A-Topic is a prototype system that allows users to selectively view topical segments of recorded TV shows. It uses closed captions and electronic program guide texts to perform topic segmentation and topic sentence selection, and presents a clickable table of contents to the user. Previous work has shown that Pic-A-Topic's accuracy in segmenting travel shows is comparable to manual segmentation. This paper demonstrates that the latest version of Pic-A-Topic is also effective in segmenting other genres such as travel, cooking, food, and talk/variety shows, using genre-specific strategies. An experiment with 26.5 hours of Japanese TV shows shows that Pic-A-Topic's segmentations for non-travel genres are just as accurate as those for travel. The system's accuracy is around 82% of manual performance on average, and the combination of cue phrase and vocabulary shift detection is successful for all targeted genres.
41354	4135433	Topic Set Size Design With The Evaluation Measures For Short Text Conversation	Short Text Conversation (STC) is a new NTCIR task that focuses on the question of whether systems can use old comments from a microblog repository to satisfy the author of a new post. The official evaluation measures for STC are nG@1, nERR@10, and P+, which evaluate navigational intents. This study uses the topic set size design technique of Sakai to determine the number of test topics, and concludes that 100 topics should be created. What sets this task apart is that the topic set size is statistically meaningful for each evaluation measure. The study also shows that nG@1 requires more topics than nERR@10 and P+. This task is the first to use a principled approach to create a new test collection at TREC-like evaluation conferences.
41355	413555	Energy optimal coding for wireless nanosensor networks	Wireless nanosensor networks (WNSNs) are made up of tiny nanosensors that can detect and sense events at the nanoscale. This technology has many potential applications, such as drug delivery and air pollution monitoring. However, one challenge is the limited energy of the nanosensors, making energy-efficient protocols crucial for these networks. This paper focuses on a WNSN with on-off keying (OOK) modulation and proposes a minimum transmission energy (MTE) coding scheme. This scheme minimizes the energy consumption per symbol by mapping m-bit symbols into n-bit codewords with the least number of high-bits. The optimal setting for m and n is determined to achieve the lowest energy consumption per data bit, serving as the lower bound for energy consumption in WNSNs. Numerical results show the effectiveness of the MTE coding scheme.
41355	4135572	Energy-Efficient Prefix-Free Codes for Wireless Nano-Sensor Networks Using OOK Modulation	Wireless nano-sensor networks (WNSNs) are composed of tiny nano-sensors with the ability to detect and sense events at the nano-scale, making them ideal for applications such as air pollution surveillance. Due to their highly energy-constrained nature, efficient communication techniques are crucial for WNSNs. This paper focuses on WNSNs using on-off keying (OOK) modulation and presents an integer nonlinear programming problem to minimize transmission energy consumption while maintaining a desired throughput. Two algorithms, BT-WD and BT-LD, are also proposed for constructing low-weight prefix-free codes, which are shown to be more effective than fixed-length codes. These codes allow for more flexible throughput control and lower energy consumption in scenarios with low or medium bit error rates.
41356	41356101	Improving discriminative sequential learning by discovering important association of statistics	Discriminative sequential learning models like CRFs have been successful in natural language processing and information extraction due to their ability to capture nonindependent and overlapping features of inputs. However, imbalanced classes, irregular phenomena, and ambiguity in training data can negatively affect their performance. To address this, a data-driven approach has been proposed that discovers and emphasizes important associations in the training data and incorporates them into these models. Experiments on phrase-chunking and named entity recognition tasks using CRFs have shown improved accuracy. This approach also highlights a potential connection between association mining and statistical learning, offering a strategy to enhance learning performance with patterns discovered from large datasets. 
41356	4135678	High-Performance Training of Conditional Random Fields for Large-Scale Applications of Labeling Sequence Data	Conditional random fields (CRFs) have proven to be effective in predicting and labeling structured data in various applications, including natural language tagging and parsing, image segmentation and object recognition, and protein secondary structure prediction. This is due to their ability to incorporate multiple overlapping features and their ability to globally normalize and optimize. However, the training process for CRFs can be time-consuming due to the need for intensive computations. In this paper, the authors propose a high-performance training method for CRFs using massively parallel processing systems, enabling the handling of large datasets with thousands of sequences and millions of features. This approach was tested on a natural language processing task and showed significant improvements in both computational time and prediction accuracy.
41357	4135778	I Can Already Guess Your Answer: Predicting Respondent Reactions during Dyadic Negotiation	Negotiation is an important part of our daily lives, but it can be difficult to predict how someone will respond to an offer. This study looked at acoustic and visual cues to see if they could predict a person's immediate reaction in a negotiation. The researchers used a dataset of 42 simulated negotiations and found that other sources of information, such as the nonverbal behavior of the proposer, could also predict reactions. Mutual behavioral cues, like symmetry and asymmetry, were also effective predictors, as they captured the nature of the interaction. Finally, the study identified specific audio-visual cues that were most predictive of a respondent's immediate reactions. 
41357	4135744	Mutual Behaviors during Dyadic Negotiation: Automatic Prediction of Respondent Reactions	This paper discusses a study on predicting a person's immediate reaction to a negotiation offer in face-to-face interactions. The study is based on the theory of social rapport and focuses on mutual behaviors, which are nonverbal characteristics influenced by the interaction. These include behavioral symmetry and asymmetry between the two negotiators. The researchers also highlight the importance of finding audio-visual mutual behaviors that can be automatically extracted for use in a real-time decision support tool. They introduce a dataset of 42 face-to-face interactions and conduct experiments to demonstrate the significance of multimodal and mutual behaviors in negotiation. 
41358	4135828	Convergent Lagrangian and Contour Cut Method for Nonlinear Integer Programming with a Quadratic Objective Function	This paper describes a new and efficient method for solving nonlinear separable integer programming problems with a quadratic objective function. The method combines the Lagrangian dual method with a duality reduction scheme using contour cut. It determines lower and upper bounds for the problem at each iteration using the Lagrangian dual search. To eliminate the duality gap, a new cut-and-partition scheme is used, taking advantage of the quadratic contour structure. The algorithm is proven to find an exact solution in a finite number of iterations and has been tested on problems with up to 2000 integer variables. The results are compared to other methods, showing the effectiveness of the proposed approach. 
41358	4135848	Towards Strong Duality in Integer Programming	This paper discusses the use of the Lagrangian dual method for solving integer programming problems. The authors utilize perturbation analysis to derive new properties of Lagrangian duality, including a necessary and sufficient condition for a primal optimal solution to be obtained through Lagrangian relaxation. They also systematically examine the solution properties of Lagrangian relaxation problems. To address the duality gap between the primal and dual problems, the authors propose a reformulation of the primal problem using a pth power transformation of the constraints. This reformulation has an asymptotic strong duality property and can achieve primal feasibility and optimality when p is larger than a certain threshold. Additionally, the authors demonstrate that the duality gap for this reformulation decreases as p increases, assuming a single constraint.
41359	4135982	Pinball loss minimization for one-bit compressive sensing: Convex models and algorithms.	The 1bit-CS method, which uses a single comparator to quantize signals, is gaining popularity due to its low power and high speed. However, when noise is present during signal acquisition and transmission, the decoding performance of 1bit-CS is affected. To address this issue, the pinball loss function is introduced, which combines the one-sided ℓ1 loss and the linear loss. Two convex models, the elastic-net pinball model and its modified version with an ℓ1-norm constraint, are proposed to minimize the pinball loss. Dual coordinate ascent algorithms are designed to efficiently solve these models, with their convergence being proven. Numerical experiments demonstrate the effectiveness of the proposed algorithms and the superior performance of the pinball loss function for 1bit-CS.
41359	4135986	Ramp loss linear programming support vector machine	This paper discusses the ramp loss, a non-convex loss function for classification that allows for effective local search. The piecewise linearity of the ramp loss makes it suitable for use with the l1-penalty, resulting in the ramp loss linear programming support vector machine (ramp-LPSVM). This approach is shown to be effective in minimizing the ramp loss and promoting sparsity. The paper also discusses the similarities between ramp-LPSVM and hinge loss SVMs, and proposes both a local minimization algorithm and a global search strategy. Numerical experiments demonstrate that ramp-LPSVM outperforms hinge SVMs in terms of robustness and sparsity.
41360	4136070	A hierarchy-based fault-local stabilizing algorithm for tracking in sensor networks	This paper presents a new approach, called hierarchy-based fault-local stabilization, for handling faults in the Stalk algorithm for tracking in sensor networks. This technique allows for efficient self-healing and fault containment, ensuring that Stalk can still maintain its data structure and perform its functions even when parts of the network are corrupted. The local stabilization is achieved by slowing down the propagation of information as the hierarchy levels increase, preventing misinformation from spreading too far. This approach maintains efficiency and availability even in the absence of faults, with operations taking O(d) time and updates taking O(d*log network diameter) amortized time. Additionally, the tracked object can move freely while Stalk is working, making it more adaptable to changes in the network.
41360	4136044	Trail: A distance-sensitive sensor network service for distributed object tracking	The article discusses the use of wireless sensor networks to track and control mobile objects. The protocol, called Trail, supports distance-sensitive tracking by providing timely information to subscribers based on their proximity to the object. Unlike traditional methods, Trail does not rely on a hierarchy of clusters and instead uses a distributed data structure that is updated locally when the object moves. This approach reduces costs, improves fault tolerance, and optimizes network usage. The protocol is reliable and energy efficient, and can be customized for different application settings. The performance of Trail is evaluated through analysis, simulations, and experiments on a sensor network.
41361	4136129	Automatic Localization and Boundary Detection of Retina in Images Using Basic Image Processing Filters.	This paper proposes a system for automatically localizing and detecting boundaries in retina images to assist ophthalmologists in accurately diagnosing eye diseases such as glaucoma and diabetic retinopathy. The system has three main phases: preprocessing, segmentation, and detection. The preprocessing phase enhances the image and removes noise. The segmentation phase extracts features from main parts of the image, including the optic disc, blood vessels, and fovea. The detection phase identifies and classifies the input image as either left or right eye, helping ophthalmologists to periodically check for diseases. Basic image processing filters are used in all phases, and a simple approach is used to detect left and right images. The system was tested on a subset of images from the DRIVE database and showed promising results.
41361	41361259	A Hybrid Approach for Predicting River Runoff.	Time series prediction has gained interest from researchers and practitioners across various fields, resulting in numerous proposed approaches. The traditional sliding window technique was commonly used to preprocess data before using learning models like fuzzy neural networks for prediction. To enhance prediction accuracy, the authors suggest a new approach that combines chaotic theory, recurrent fuzzy neural network (RFNN), and K-means. Fuzzy neural networks have been successful in modeling and predicting nonlinear hydrology time series such as rainfall, water quality, and river runoff. Chaotic theory, a branch of physics and mathematics, has also been applied to solve practical problems in industries. The proposed approach uses chaotic theory to transform data into phase space before using a hybrid model, RFNN-KM, which includes multiple RFNNs combined by K-means algorithm. Experiments with Srepok River runoff data in Vietnam show that the proposed approach outperforms the combination of RFNN and sliding window technique on the same data.
41362	4136229	Nonpositive Curvature and Pareto Optimal Coordination of Robots	This article discusses the coordination of multiple robots in a shared environment with the goal of optimizing travel time while avoiding collisions. Each robot has a graph representing its possible configurations, and the optimal coordination is defined as a set of points that satisfy both the goal and avoiding obstacles. The article proves a finite bound on the number of optimal coordinations, based on the fact that the coordination space has no positive curvature. This bound also applies to systems with moving obstacles. The proofs use concepts from geometric group theory and CAT(0) geometry.
41362	41362100	Exact Pareto-Optimal Coordination of Two Translating Polygonal Robots on a Cyclic Roadmap	The article discusses a method for planning optimal collision-free motions for two polygonal robots that must stay on a given graph. The robots have initial and goal positions, and the goal is to find Pareto-optimal coordination strategies where neither robot can improve without making the other worse off. This problem can be translated into finding the shortest paths in the coordination space, which is a Cartesian product of the roadmap. The algorithm computes an upper bound on the cost of each motion and can find all Pareto-optimal coordinations in a finite amount of time. The time complexity of the algorithm depends on the number of edges in the roadmap, the number of coordination space obstacle vertices, and the length of the shortest edge in the roadmap.
41363	4136359	Hierarchical control for self-assembling mobile trusses with passive and active links	This paper discusses the concept of active modular trusses, which are self-reconfiguring robots comprised of both active and passive modules. These trusses have many potential applications, such as space exploration and construction tasks. The paper presents a hardware design for truss climbing and hierarchical algorithms for controlling these hyper-redundant modular trusses. The authors also introduce the concept of "Shady," a module that can grip and move along a window frame to provide personalized shading. They discuss the challenges of control and planning for these systems, as well as potential benefits of using active and passive modules in self-reconfiguring robots. 
41363	41363167	Miche: Modular Shape Formation by Self-Disassembly	The article discusses the creation of a system of robots that can be assembled into various shapes and then self-disassembled in an organized manner to achieve a specific goal shape. The robots are autonomous cubes that can communicate and connect with each other. They are controlled by microprocessors and use magnetic connections and infrared communication. The system allows for virtual sculpting through a computer interface and a distributed process. Algorithms are used to determine which modules are part of the final shape and which are not, minimizing information transmission and storage. The robots are able to disengage their connections and fall away due to gravity once the desired shape is achieved. The system has been successfully tested with hundreds of trials, demonstrating its ability to form complex 3D shapes.
41364	4136421	Robust MIMO cognitive radio systems under temperature interference constraints	Cognitive radio (CR) systems protect primary users (PU) by imposing temperature interference constraints on secondary users (SU). However, SUs may violate these limitations if perfect SU-to-PU channel state information (CSI) is not available. This paper introduces a novel and distributed design for MIMO CR networks that is resilient to imperfect SU-to-PU CSI. The system design is formulated as a noncooperative game and robust global interference constraints are enforced through pricing. The prices are additional variables to optimize. Using finite-dimensional variational inequalities (VI) in the complex domain, the proposed NE problem is analyzed and alternative distributed algorithms are developed with their convergence properties.
41364	4136462	Robust MIMO Cognitive Radio Via Game Theory	Cognitive radio systems aim to improve spectral efficiency by allowing primary and secondary users to coexist without causing interference. However, in real-world situations with imperfect channel state information, perfect system design can lead to violations of interference limits. To address this issue, the paper proposes a robust design for CR systems with multiple PUs and noncooperative SUs in either SISO or MIMO channels. This design is formulated as a noncooperative game, where SUs compete for resources while maximizing their own information rates and adhering to power and interference constraints. By considering worst-case robustness and using variational inequality theory, the paper studies the existence and uniqueness of Nash equilibria and develops distributed algorithms for finding them. Numerical methods for calculating robust transmit strategies for each SU are also proposed.
41365	4136556	FlashLight: A Lightweight Flash File System for Embedded Systems	NAND flash memory has become a popular choice for storage due to its high performance. However, to design a flash file system that can fully utilize its capabilities, two key aspects need to be considered. Firstly, an efficient index structure is needed to manage file and data locations. For large storage capacities, the index structure must be stored in the flash memory to reduce memory consumption, but this can impact system performance. Secondly, a garbage collection (GC) scheme is necessary to reclaim obsolete pages. This can lead to additional read and write operations, so a novel approach is required. The article introduces FlashLight, a new flash file system that addresses these issues through a lightweight index structure and an efficient GC scheme. Experiments show that FlashLight outperforms UBIFS by up to 27.4&percnt; by reducing index management and GC overheads by up to 33.8&percnt;.
41365	4136566	ActiveSort: Efficient external sorting using active SSDs in the MapReduce framework.	The increasing demand for data-intensive computing has led to a challenge in efficiently processing I/O operations. SSDs offer a solution by improving I/O throughput and reducing data transfer through the use of active SSDs. This work proposes ActiveSort, a mechanism that utilizes active SSDs to improve the performance of external sorting algorithms in data-intensive frameworks like Hadoop. By performing merge operations within the SSD, ActiveSort reduces I/O transfer and improves sorting performance. Evaluation results show that Hadoop applications using ActiveSort outperform the original Hadoop by up to 36.1%, while reducing write operations by up to 40.4% and improving SSD lifetime.
41366	413664	Transparently bridging semantic gap in CPU management for virtualized environments	Consolidated environments are becoming more common as they combine different workloads using virtual desktop infrastructure and cloud computing. However, this leads to a mismatch between the virtual machine monitor and guest operating systems, resulting in inefficient resource management. In particular, CPU management for virtual machines can greatly impact I/O performance. To address this issue, this paper proposes virtual machine scheduling techniques that bridge the semantic gap by making the virtual machine monitor aware of task-level I/O demands. This improves I/O performance without affecting CPU fairness. Additionally, performance anomalies from indirect use of I/O devices are addressed through scheduling techniques. These techniques were implemented in the Xen virtual machine monitor and evaluated with various benchmarks and real workloads on Linux and Windows guest operating systems. 
41366	4136612	Task-aware virtual machine scheduling for I/O performance.	Virtualization is becoming increasingly popular in virtual desktop and cloud computing environments due to its ability to handle various and unpredictable workloads. However, the lack of knowledge about each virtual machine makes resource allocation and virtual machine scheduling challenging. This paper introduces a task-aware virtual machine scheduling mechanism that uses inference techniques to determine the I/O-boundness of guest-level tasks and prioritize them for prompt handling. This mechanism, called partial boosting, focuses on improving the performance of I/O-bound tasks within heterogeneous workloads while maintaining CPU fairness among virtual machines. The prototype was evaluated using synthetic and realistic workloads and showed improved I/O performance and CPU fairness. The implementation was done on the Xen virtual machine monitor and the credit scheduler.
41367	413672	Dense Subset Sum May Be the Hardest.	The SUBSET SUM problem involves finding whether a given set of positive integers can be combined to reach a specific target sum. It is still unknown whether there is a faster algorithm than the O*(2(n)(/2)) one proposed by Horowitz and Sahni in 1974. Recent research has focused on the problem's complexity when considering the maximum bin size, which is the largest number of subsets that can yield the same sum. This research has led to the discovery of a truly faster algorithm for instances with small bin sizes, as well as a connection to additive combinatorics and information theory. These findings challenge the current belief that instances with larger bin sizes are more difficult, and provide progress towards solving the open question. 
41367	4136718	Circumspect Descent Prevails In Solving Random Constraint Satisfaction Problems	The study focuses on stochastic local search algorithms for solving the K-satisfiability (K-SAT) problem on random instances. A new algorithm, ChainSAT, is introduced which moves in the energy landscape without increasing energy and focuses on variables in unsatisfied clauses. It is found that ChainSAT, along with other focused algorithms, can solve large K-SAT instances in linear time, even beyond the previously identified solution space transitions. This is surprising as the algorithm is designed to get trapped in the first local energy minimum it encounters, but it never encounters such minima. The geometry of the solution space is also explored through stochastic local search algorithms. 
41368	4136851	Elliptic Curve Scalar Multiplier Design Using FPGAs	A new elliptic curve scalar multiplier with variable key size has been implemented as a coprocessor using a Xilinx FPGA. This design uses the internal memory of the FPGA and is contained within a single chip, resulting in a compact and efficient design. By minimizing data transfer between the FPGA and host, the design achieves high performance. Experimental results demonstrate that the carefully designed hardware architecture is highly regular and makes efficient use of the FPGA's resources. 
41368	413683	Mimo Transceiver Design Based, On A Modified Geometric Mean Decomposition	The article discusses a MIMO joint transceiver design that can operate at 350 MHz on a Xilinx Virtex-4 xc4vlx200ff1513-12 FPGA. This design is capable of achieving a data throughput of 11.2 Gbps and utilizes a modified Geometric Mean Decomposition (GMD) for a flat fading MIMO channel with VBLAST MIMO detection. The design process involves using Matlab Simulink to create a model, which is then transformed into a VHDL description using Xilinx System Generator. The results of speed and area for the synthesized designs are also presented.
41369	413695	Decentralized dynamic task planning for heterogeneous robotic networks.	This paper introduces a decentralized model and control framework for task planning in a network of different types of robots. The framework allows for the design of missions, or sets of tasks, to achieve overall objectives without being limited by the capabilities of the robots. The concept of skills, defined by the mission designer, is used to distribute tasks among the robots. A decentralized control algorithm is also developed, based on the concept of skills, to coordinate task assignment and execution. The paper discusses the conditions under which the decentralized model is equivalent to a centralized one and provides experimental results to demonstrate its effectiveness in a real-world scenario.
41369	4136934	A Networked Evidence Theory Framework For Critical Infrastructure Modeling	This paper discusses a decentralized approach for data fusion and information sharing using evidence theory and the transferable belief model. These methods are used to combine data from various sources to improve decision making in critical infrastructure protection. Due to the complexity and interconnectedness of infrastructure systems, a distributed approach is necessary with minimal data exchange between stakeholders. The proposed decentralized extension of the transferable belief model allows for the application of evidence theory in a distributed manner. The results from a case study demonstrate the effectiveness of this approach in fusing data for interdependent critical infrastructure systems.
41370	413703	Impact of blog design features on blogging satisfaction: an impression management perspective	According to global trends, many bloggers are motivated by self-presentation and use third party tools and hosting services to enhance their blogs. By providing user-friendly tools for self-presentation, blogging satisfaction can be increased. This is supported by the theory of impression management, where bloggers use content, functional, and aesthetic design features to manage their online identity and seek confirmation from readers. This perceived identity verification can lead to a heightened sense of satisfaction in blogging. Results from a survey and focus group discussion support this idea, and have both theoretical and practical implications.
41370	4137030	Students' participation intention in an online discussion forum: Why is computer-mediated interaction attractive?	The effectiveness and efficiency of teaching through online discussion forums (ODF) may be hindered by lower student participation rates. This paper examines the motivational factors that influence students' intention to participate in ODFs. Drawing on social psychology and the theory of reasoned action, a conceptual model is developed and tested through a survey. The results show that students' expectations of enjoyment and usefulness, as well as peer pressure, positively influence their intention to participate. The perceived importance of learning also plays a moderating role. The implications of these findings for theory and practice are discussed. 
41371	413710	Mapping the Cerebral Sulci: Application to Morphological Analysis of the Cortex and to Non-rigid Registration	The authors propose a method for extracting parametric representations of the cerebral sulci from magnetic resonance images and discuss its application in quantitative morphological analysis and spatial normalization and registration of brain images. The method uses deformable models based on characteristics of the cortical shape, with an active contour being initialized along the outer boundary of the brain and deforming along the medial surface of a sulcus under the influence of an external force field. Results of this methodology and its applications are presented in the paper. 
41371	4137188	Spatial transformation and registration of brain images using elastically deformable models.	This paper discusses a technique for spatially transforming and registering brain images using elastic deformable models. This method has various applications in medical fields such as neurosurgery and image analysis. The algorithm uses a deformable surface to represent the cortical surface and create a map between corresponding regions in two brain images. This map is then used to determine a three-dimensional elastic warping transformation, allowing for variations in the elastic properties of different brain regions. The framework also incorporates prestrained elasticity to account for structural irregularities, such as ventricular expansion and tumor growth. The performance of the algorithm is evaluated using magnetic resonance images.
41372	4137235	A specialisation calculus to improve expert systems communications	This work aims to improve the behavior of traditional input/output expert systems, which only provide certainty values for propositions in uncertain reasoning contexts. Instead, this work proposes a more informative approach where the expert system provides a set of formulas, including both propositions and specialized rules with unknown propositions in their left part. This approach is based on a family of propositional rule-based languages founded on multiple-valued logics, and a deductive system using a Specialisation Inference Rule (SIR) is presented. The soundness and literal completeness of this system are proven, and its implementation utilizes techniques of partial evaluation. This approach also offers a way of validating knowledge bases. 
41372	4137236	Fuzzy logic and probability	This paper discusses a new method of using logical frameworks for probabilistic reasoning. Most existing approaches to probability in logic are based on classical two-valued logic. The authors propose a fuzzy logic of probability that is complete in a probabilistic sense. This is achieved by considering probability values as truth-values of fuzzy propositions associated with crisp ones. The paper also explores ways to extend this approach to handle conditional probabilities and other forms of uncertainty. The goal is to provide a more flexible and comprehensive method for incorporating probability into logical reasoning.
41373	4137320	An OFDM based MAC protocol for underwater acoustic networks	This paper explores the use of OFDM based MAC protocols in underwater acoustic networks. Due to the challenges posed by underwater channels, existing protocols are not suitable. To address this, the authors propose a new protocol called TDM with FDM over OFDM MAC (TFO-MAC) which combines TDM and FDM/OFDM to optimize the use of available bandwidth. This is achieved through dynamic channel assignment, power optimization, and modulation method selection by powerful base stations. The problem is formulated as a mixed integer programming problem and a greedy algorithm is proposed to solve it. Simulation results demonstrate that TFO-MAC can achieve high throughput and fairness in a cellular-like underwater network.
41373	4137331	Pressure Routing for Underwater Sensor Networks	The study introduces the concept of a SEA Swarm, a group of sensors that drift with water currents to monitor underwater events in real-time. The swarm is accompanied by drifting sonobuoys that collect data from the sensors and transmit it to a monitoring center. The goal of the study is to design an efficient routing algorithm to reliably report sensor data to the sonobuoys. The main challenges are ocean currents and limited resources. The paper presents HydroCast, a hydraulic pressure based routing protocol that utilizes pressure levels to route data to surface buoys. The paper also proposes a novel routing mechanism and a dead end recovery method, and validates them through simulations. 
41374	41374217	P2P Content Distribution to Mobile Bluetooth Users	The widespread use of handheld devices, particularly smart phones, for personal entertainment has led to the integration of Bluetooth technology in these devices. This allows for the distribution of entertainment content, such as music and videos, through various means such as opportunistic downloads and peer-to-peer collaboration. However, the design of peer-to-peer content distribution protocols is heavily influenced by the unique characteristics of Bluetooth, which differs from traditional Internet-based content distribution methods. Despite this, there is a lack of understanding about the performance of Bluetooth in dynamic environments with factors like mobility, interference, and different versions and chipsets. Through extensive measurement studies, it has been found that Bluetooth-based content distribution suffers from issues like time-consuming resource discovery and limited bandwidth, even with the latest Bluetooth version. To address these challenges, effective strategies are discussed to improve the performance of resource discovery and downloading phases.
41374	41374161	USHA: a practical vertical handoff solution.	 behavior in various vertical handoff scenarios.In order to address the need for a universal seamless handoff solution for mobile users, the authors present the Universal Seamless Handoff Architecture (USHA). This solution aims to provide continuous and uninterrupted internet service while switching between wireless networks. USHA is simple and requires minimal modification to the current internet infrastructure, making it easy to implement in real-world situations. Through testbed experiments, the authors demonstrate that USHA successfully maintains application connectivity in various handoff scenarios. They also show that when switching from a low capacity to a high capacity link, there is no service latency, but when switching from a high capacity to a lower capacity link, there may be non-negligible latency unless early handoff notification is available. Overall, USHA satisfies the critical design requirements of low latency and low packet loss for seamless handoff.
41375	4137513	β-PSR: A Partial Spectrum Reuse scheme for two-tier heterogeneous cellular networks	This paper presents a Partial Spectrum Reuse (PSR) scheme, called "β-PSR", to increase spectrum efficiency in two-tier heterogeneous cellular networks. The scheme assigns a uniform portion β of the spectrum to each micro Base Station (BS) at random. The optimal value of β for minimizing service outage probability is analyzed, but it is not given in an explicit form. However, a closed-form limit of β is derived when the user data rate requirement is close to zero compared to the system bandwidth. This limit is a function of traffic intensity, macro/micro BS density, and transmit power. Numerical results show that the optimal β is well approximated by the derived limit, leading to an adaptive PSR scheme with near-optimal performance based on statistical network information.
41375	41375163	Optimal Combination of Base Station Densities for Energy-Efficient Two-Tier Heterogeneous Cellular Networks	This paper uses stochastic geometry theory to analyze the optimal density of base stations (BS) in both homogeneous and heterogeneous cellular networks in order to minimize energy cost. For homogeneous networks, upper and lower bounds for the optimal BS density are derived. For heterogeneous networks, the analysis reveals the best type of BS to be deployed for capacity extension or switched off for energy saving. The optimal strategy depends on the ratio between the cost of micro BSs and macro BSs. Numerical results show that deploying micro BSs in dense urban areas can reduce energy cost by 40% and up to 35% with the use of BS sleeping capability. The optimal combination of macro and micro BS densities can be calculated numerically or approximated with a closed-form solution. 
41376	4137648	A pair of forbidden subgraphs and perfect matchings	This paper examines the connection between forbidden subgraphs and the existence of a matching in a graph. A graph G is considered H-free if no graph in the set H is an induced subgraph of G. The study focuses on the set H of connected graphs with three or more vertices. The paper provides a complete characterization of H in two scenarios: when all graphs in H are triangle-free, and when H consists of two graphs. It also looks at the case of odd order graphs and defines a near-perfect matching as one where all but one vertex are connected by edges. The paper further characterizes H in cases where every H-free graph of sufficiently large odd order has a near-perfect matching.
41376	413768	A note on 2-factors with two components	This note discusses a minimum degree condition for a graph to have a 2-factor with two components. Dirac's theorem states that if the minimum degree of a graph of order n is at least 12n, then it has a hamiltonian cycle. Brandt et al. proved that if n=8, then the graph has a 2-factor with two components. However, there are infinitely many graphs of odd order and minimum degree 12(|G|-1) that do not have a 2-factor. But if a graph is hamiltonian, the minimum degree condition can be relaxed. The note proves that a hamiltonian graph of order n=6 and minimum degree at least 512n+2 has a 2-factor with two components.
41377	41377102	Facility location in sublinear time	This paper presents a randomized approximation algorithm for the metric Minimum Facility Location problem, with uniform costs and demands and the option for every point to open a facility. The algorithm has a running time of O(n log2n), where n is the number of metric space points, making it sublinear with respect to the input size. The authors also show that there is no o(n2)-time algorithm, even a randomized one, that can approximate the optimal solution for this problem. This result extends to other related problems such as minimum-cost matching, bi-chromatic matching, and n/2-median.
41377	41377101	Approximate clustering via core-sets	This paper discusses the use of core-sets in clustering problems, where a small set of points can be extracted to perform efficient approximate clustering. The size of these core-sets is independent of the dimension, which is a surprising property. The paper presents (1+ &egr;)-approximation algorithms for k-center and k-median clustering in Euclidean space, with a running time that depends linearly on the number of points and dimension, and exponentially on 1/&egr; and k. This is a significant improvement over previous methods. Additionally, the paper presents results for (1+ &egr;)-approximate 1-cylinder clustering and k-center clustering with outliers. 
41378	4137873	Lifelong learning of human actions with deep neural network self-organization.	Lifelong learning is crucial in autonomous robotics, but current deep neural models for action recognition from videos do not support it. Thus, a self-organizing neural architecture is proposed, which uses growing self-organizing networks with recurrent neurons to process time-varying patterns. The architecture also includes a set of hierarchically arranged recurrent networks for unsupervised learning of action representations. This results in a system that can incrementally process perceptual cues and adapt its responses over time. Experimental results show that the proposed model is competitive with state-of-the-art methods for batch learning, even when sample labels are missing or corrupted. The model also demonstrates the ability to adapt to non-stationary input without catastrophic interference.
41378	41378107	Self-organizing neural integration of pose-motion features for human action recognition	The recognition of human movements is important for various artificial systems such as human-robot communication and action classification. This process involves a large amount of visual information and learning-based mechanisms to generalize training actions and classify new samples. To work in natural environments, it is crucial to have efficient and robust recognition of actions, even under noisy conditions. Studies of the mammalian visual system suggest separate neural pathways for processing pose and motion features, which are then integrated for action perception. A neurobiologically-inspired approach using self-organizing networks is proposed to achieve noise-tolerant real-time action recognition. This model learns spatio-temporal dependencies and adapts its structure to match the input space. Experiments show that this approach outperforms previous methods on a dataset of full-body actions and ranks among the best for a benchmark of daily actions.
41379	4137913	Food Image Recognition Using Pervasive Cloud Computing.	Food image recognition is becoming increasingly important in e-health applications. However, this is a challenging task due to the diverse nature of food and the impact of elements such as color, light, and view angles on food images. In response to this challenge, the use of SIFT and Gabor descriptors as food image features and the KMeans algorithm for feature clustering has been proposed based on empirical and experimental research. To improve the performance of food image recognition, the use of pervasive cloud computing is suggested due to the high computing requirements for a large number of concurrent recognition requests. Evaluations have shown that this approach can achieve acceptable recognition rates, and the use of MapReduce programming has provided significant performance advantages compared to traditional client-server approaches.
41379	4137935	A Hybrid Emotion Recognition on Android Smart Phones.	Keeping track of people's emotions is important for the elderly, those with sub-health conditions, and various patients in order to maintain a positive mood. However, recognizing emotions in real-time is difficult due to its complex nature. It is important to achieve this in a non-intrusive way, and with the rise of popular android smartphones equipped with sensors, this can be achieved. This paper proposes an approach using heart rate and speech content obtained from the phone's camera and microphone. The emotions of anger, joy, normal, and sadness can be classified based on heart rate, and further improved by analyzing emotional keywords in speech. The approach was evaluated and found to have a recognition accuracy of 84.7%.
41380	413800	Clustering of time-course gene expression profiles using normal mixture models with autoregressive random effects.	Gene expression data, such as yeast cell cycle data, is often expressed periodically over time. However, traditional Fourier series approximations used to cluster this data have been found to be insufficient due to their lack of accounting for the correlation between expression measurements over time and the correlation among gene expression profiles. To address this issue, a new mixture model with autoregressive random effects of the first order has been proposed. This model has been shown to outperform existing models in clustering time-course data, particularly when the gene profiles are highly correlated. The model has been successfully applied to both synthetic and real datasets, producing relevant clusters of co-regulated genes. The developed R package allows for flexibility in specifying the random effects, leading to improved modelling and clustering of time-course data.
41380	4138026	Private Distributed Three-Party Learning of Gaussian Mixture Models.	This paper proposes a method for preserving privacy in a three-party scenario where data is distributed horizontally among the parties. The focus is on cooperative training of multivariate mixture models, with the goal of learning global parameters while keeping individual data private. The proposed scheme does not require a trusted party and ensures that no information is exposed during inter-party communications. The method is illustrated using a Gaussian mixture model for cluster analysis, highlighting the important role of clustering in statistical learning and data mining. With the increasing use of big data across multiple independent parties, privacy preservation in cross-party communications is a crucial issue.
41381	4138145	Age-related differences in the initial usability of mobile device icons	Mobile devices have the potential to greatly benefit older adults (age 65+) but they have been slow to adopt them. While research has focused on improving usability for this population, little attention has been given to the difficulty they may have with existing graphical icons. In response to this, a qualitative exploratory study and a follow-up experimental study were conducted to determine which icon characteristics are most helpful for initial icon usability for older adults. The study found that older participants had more difficulty with existing icons, but that icons with semantically close meaning, familiarity, labeling, and concreteness were easier for them to use. These findings can inform designers in creating icons that are better suited for older adults' abilities and technology experience.
41381	4138113	Multi-Layered Interfaces to Improve Older Adults’ Initial Learnability of Mobile Applications	Mobile computing devices have the potential to support older adults in their daily lives, but they often struggle to learn and use them. One way to improve the learnability of these devices is through a Multi-Layered (ML) interface, where novice users start with a simplified interface before progressing to a more complex one. A study was conducted with 16 older adults (ages 65-81) and 16 younger adults (ages 21-36) to compare the effects of a 2-layer interface with a non-layered interface on learning tasks on a mobile device. Results showed that the ML interface was more beneficial for older adults, as it helped them to master basic tasks and retain that ability 30 minutes later. However, transitioning to the more complex layer negatively affected their performance on previously learned tasks, but not on new tasks. Overall, the ML interface approach is suitable for improving the learnability of mobile applications, especially for older adults.
41382	4138225	Participatory design with individuals who have amnesia	The article discusses a participatory design process with individuals who have anterograde amnesia, making it difficult for them to form new memories. The design process is described, along with techniques used to support memory during and between design sessions. The authors identify cognitive assumptions of participatory design that do not work when working with individuals with amnesia. They introduce an analytical framework for researchers and practitioners to use when working with people with cognitive impairments. A cognitive deficit unrelated to memory encountered during the process is analyzed using this framework, and an unexpected benefit is highlighted.
41382	4138245	Participatory design of an orientation aid for amnesics	Our team of six individuals with anterograde amnesia worked together to design and evaluate an orientation aid called the OrientingTool. We detail the methods we used to involve participants with severe cognitive impairments in the design process. The OrientingTool is a software application for Personal Digital Assistants that helps amnesics orient themselves when lost or disoriented. Two studies were conducted to test the effectiveness of the tool in real-life situations. Results show that the OrientingTool can improve an amnesic's independence and confidence when disoriented. This study demonstrates the potential for participatory design to be used successfully with individuals who have significant cognitive disabilities.
41383	4138338	Linear algorithms to recognize interval graphs and test for the consecutive ones property	The paper discusses the consecutive ones property in a matrix, where ones in each column appear consecutively. It presents a data structure to efficiently test for this property and find the desired permutation of rows in linear time. One use of this property is in identifying interval graphs, which are graphs where vertices correspond to intervals on the real line and are adjacent if their intervals intersect. The paper also mentions a previous characterization of interval graphs using the consecutive ones property. To simplify the algorithm, the columns of the matrix are processed in a specific order. This leads to an algorithm for recognizing interval graphs, which can be extended to the general consecutive ones problem. 
41383	4138312	Testing for the consecutive ones property, interval graphs, and graph planarity using PQ-tree algorithms	PQ-trees are a type of data structure that can represent permutations of a set where certain subsets occur consecutively. This structure is useful for efficient manipulation of data and can be applied to problems such as testing for consecutive ones property in matrices and graph planarity. The algorithms for these tests are also linear in complexity, making them efficient for large inputs. Additionally, the consecutive ones test can be extended to interval graphs using a fast recognition algorithm for chordal graphs. Overall, PQ-trees provide a useful tool for representing and manipulating data in a variety of applications. 
41384	4138486	Adaptive maximum margin criterion for image classification	This paper introduces a new method, called the adaptive maximum margin criterion (AMMC), for image classification. While many algorithms have been developed for discriminant analysis, they often treat all training samples equally and overlook the varying impacts of each sample on learning a discriminative feature subspace. The AMMC addresses this issue by assigning different weights to each training sample, taking into account their individual contributions to feature space learning. Additionally, the paper also proposes a semi-supervised version of the AMMC, called the semi-supervised adaptive maximum margin criterion (SAMMC), which incorporates both labeled and unlabeled samples for improved classification performance. The effectiveness of these methods is demonstrated through experimental results. 
41384	4138456	Ordinary preserving manifold analysis for human age estimation	In this paper, the authors introduce a new technique for estimating human age using facial and gait features. They were inspired by the idea that high-dimensional facial images and gait sequences can be represented in lower-dimensional aging manifolds. By projecting these samples onto a low-dimensional submanifold, the authors aim to minimize the distance between samples with similar age values while maximizing the distance between samples with different age values. To predict the age values from the projected features, a multiple linear regression function with a quadratic model is learned. The authors show the effectiveness of their approach through experiments on two databases.
41385	4138585	RT-ROS: A real-time ROS architecture on multi-core processors	ROS, an open-source operating system for robots, is popular and rapidly advancing in the robotics community. However, because it runs on Linux, ROS cannot guarantee real-time capabilities, which are necessary for tasks such as robot motion control. This paper introduces a new real-time architecture for ROS called RT-RTOS, designed for multi-core processors. It allows real-time and non-real-time tasks to be executed separately on different cores, providing real-time support for real-time tasks and other functions on Linux. Through experiments with real robot applications on a dual-core processor, it is shown that RT-RTOS effectively enables real-time support for ROS with high performance by utilizing the multi-core architecture.
41385	4138513	Formal Modeling and Automatic Code Synthesis for Robot System	Robot control systems are complex and challenging to develop. This paper presents a formal model-based approach for automatically generating executable C++ code for the Robot Operating System (ROS). The internal interactions of the robots are modeled using a network of timed automata, and safety requirements are verified using Uppaal. The code synthesis method is designed to support more complex structures and features, such as timers and committed locations, and to seamlessly connect with ROS instructions. A case study of a robot with a seven-degree-of-freedom manipulator successfully implemented the generated code in a ROS development environment. This method offers an efficient and reliable way to develop robot control systems.
41386	4138662	Online reranking via ordinal informative concepts for context fusion in concept detection and video search	This paper proposes a new approach, called ordinal reranking, to improve the ranking of search results or detection lists by utilizing co-occurrence patterns of semantic concepts. The approach is based on ranking functions, which are more effective than classification methods in identifying ordinal relationships. It also does not require ad hoc thresholding or offline learning/training data. A concept selection measurement, wc-tf-idf, is also introduced to select relevant concepts for reranking. This approach can be applied to various tasks such as video search and concept detection and has shown significant improvements in performance over existing methods in terms of mean average precision. 
41386	4138683	ContextSeer: context search and recommendation at query time for shared consumer photos	ContextSeer is a search system designed to improve the quality of searches and provide supplementary information on media-sharing sites like Flickr. It uses context cues such as visual content, high-level concepts, time, and location metadata to enhance search results and recommend relevant tags and canonical images. The system includes an ordinal reranking algorithm and a feature selection method called wc-tf-idf to improve the semantic coherence of text-based search results. It also uses an efficient algorithm called cannoG to select multiple canonical images without clustering. ContextSeer has been evaluated using a benchmark called Flickr550, and has shown significant performance gains in both Flickr550 and TRECVID search benchmarks. A subjective test also confirms the effectiveness of cannoG in recommending multiple canonical images. 
41387	4138753	Quantifying spatio-temporal dependencies in epileptic ECOG	Researchers have found evidence that the mechanisms responsible for epileptic seizures can be better understood by continuously tracking the brain's spatio-temporal mappings. They propose using a SOM-based similarity index measure to quantify these dynamics, which has been shown to be both statistically accurate and computationally faster, making it suitable for real-time analysis. In order to track changes in this measure over time and space, a spectral clustering approach is used. Preliminary analyses on multivariate epileptic ECOG data show that patterns of spatio-temporal dynamics vary from seizure to seizure, suggesting that there may be distinct patterns associated with the transition from inter-ictal to pre-post ictal stages.
41387	41387112	On Visually Evoked Potentials in EEG Induced by Multiple Pseudorandom Binary Sequences for Brain Computer Interface Design.	Visually evoked potentials (VEPs) have become increasingly important in the development of brain-computer interfaces (BCIs) over the last two decades. The P300 response, a major signal of interest, has been extensively studied. However, researchers have also looked into steady state VEPs, which occur in response to flickering visual stimuli, and the use of m-sequences (a type of pseudorandom sequence) to induce responses in the visual cortex. In this paper, the focus is on the use of multiple m-sequences for intent discrimination in BCIs. The study found that the bit presentation rate of the m-sequences may affect classification accuracy and speed, but the results were mixed and further research is needed. This includes exploring different classifier schemes and signal processing methods for better information fusion.
41388	4138837	Query Complexity in Expectation.	The article discusses the query complexity of computing a function in expectation, where the algorithm must output a random variable with an expected value equal to the function value using as few input queries as possible. Both the randomized and quantum query complexity are characterized by two polynomial degrees, with the quantum complexity potentially being much smaller for some functions. These complexities are related to the extension complexity of polytopes, which can be upper bounded using quantum query algorithms. An example is given for approximating the slack matrix of the perfect matching polytope. The article concludes by showing that these complexities correspond to the Sherali-Adams and Lasserre hierarchies.
41388	4138835	New Results on Quantum Property Testing	Quantum algorithms have shown promise in achieving faster solutions for property testing problems. One example is testing the similarity of probability distributions using an oracle, where quantum algorithms can determine if two distributions are identical or significantly different in a shorter time compared to classical algorithms. Another example is in graph isomorphism testing, where quantum algorithms with oracle access can achieve a significant speed-up compared to classical algorithms. Lastly, quantum algorithms based on Shor's algorithm and a modified classical lower bound have shown significant speed-ups in testing periodicity, providing an alternative to a recent result by Aaronson. These examples demonstrate the potential of quantum algorithms in achieving polynomial or even constant speed-ups compared to classical algorithms.
41389	4138963	Algorithmic complexity of protein identification: combinatorics of weighted strings	In this article, we address a problem in computational biology called the ONE-STRING MASS FINDING PROBLEM. This involves finding a substring in a given string with a specific weight, where the weight of a string is the sum of the weights of its individual letters. We aim to find a data structure and query algorithm that can efficiently solve this problem, with a focus on using minimal storage space and query time. We present two algorithms, LOOKUP and INTERVAL, which have subquadratic storage space and sublinear query time. We also discuss other variations of the problem and how our algorithms can be adapted to solve them. Additionally, we explore the combinatorial properties of weighted strings.
41389	4138956	Good splitters for counting points in triangles	A set of n points in the plane needs to be stored in a way that allows for efficient calculation of the number of points inside any given triangle. A solution to this problem has been developed with query time of &Ogr;(√n log n), storage space of &Ogr;(n log n), and preprocessing time of &Ogr;(n3/2 log n). The method is easy to implement and has small constants in its asymptotic bounds.
41390	4139034	LP-based Approximation Algorithms for Capacitated Facility Location	The capacitated facility location problem with hard capacities involves finding the optimal way to open facilities and assign clients to those facilities in order to minimize costs. This problem is NP-hard and current approximation algorithms are based on local search techniques. However, there is still room for improvement and one approach is to use a linear programming lower bound. The researchers have made progress in this area by presenting a 5-approximation algorithm for a special case where all facility costs are equal. This algorithm involves breaking the problem into smaller single-demand capacitated facility location problems, solving them, and then combining the solutions.
41390	4139018	Improved Approximation Algorithms for the Uncapacitated Facility Location Problem	The uncapacitated facility location problem involves choosing which facilities to open and assigning demand locations to them in order to minimize costs. Facilities have fixed costs and service costs are based on the distance between facilities and demand locations. The problem assumes a symmetric distance function and a triangle inequality. A new algorithm has been developed that provides a (1+2/e)-approximation, with $1+2/e \approx 1.736$, which is a significant improvement from previous guarantees. The algorithm uses rounding and techniques from linear programming, randomized rounding, and decomposition. This algorithm builds on the work of Shmoys, Tardos, and Aardal.
41391	4139124	A Flexible, Extensible Simulation Environment for Testing Real-Time Specifications	MTSim is a versatile simulation platform designed for the Modechart toolset (MT). It allows users to customize and extend their simulations by plugging in their own viewers and injecting events into the execution trace. MTSim also offers features such as monitoring and assertion checking, as well as user-specified handlers for handling assertion violations. The paper also introduces WebSim, a suite of simulation tools for MT, and an application-specific component of MTSim that simulates the cockpit of an F-18 aircraft and responds to user inputs to model a bomb release function. With its flexible and interactive features, MTSim provides users with a comprehensive and customizable simulation experience. 
41391	4139116	A Methodology And Support Tools For Analysis Of Real-Time Specifications	The paper discusses the importance of precise specification and formal analysis in time-critical functions in embedded systems, as software control becomes more prevalent. It introduces Modechart, a graphical specification language designed for this purpose. The main focus is on methods and tools for representing and analyzing properties of time-critical systems specified in Modechart. The paper presents a verification methodology that utilizes the structure of Modechart to determine if a system satisfies required properties. It also describes the implementation of a mechanical verifier, which has been integrated into the Modechart Toolset prototype development environment.
41392	4139232	A search-based bump-and-refit approach to incremental routing for ECO applications in FPGAs	Incremental physical CAD, also known as engineering change order (ECO) process, is a common approach to making design changes in circuitry. This is typically done late in the design process to correct logical or technological issues. Incremental routing is an important aspect of this methodology, as it allows for changes to be made only to the affected portion of the circuit while minimizing changes to the rest. In this article, a new approach called bump and refit (B&R) is introduced for efficient incremental routing in FPGAs. This method rearranges existing nets within their channels to find valid routes for the new or modified nets without using extra resources or affecting the electrical properties of existing nets. The B&R approach is compared to other incremental routing techniques and found to be significantly faster and more effective, especially for FPGAs with complex switchboxes. However, it may be slower for FPGAs with a different type of switchbox. Overall, B&R offers a promising solution for efficient incremental routing in modern FPGAs.
41392	4139234	A depth-first-search controlled gridless incremental routing algorithm for VLSI circuits	The engineering change order (ECO) process involves making changes to VLSI circuits after their layouts are completed in order to fix electrical issues or design mistakes. To save time and resources, engineers aim to only re-route the affected part of the circuit while minimizing changes to the rest in order to preserve its electrical properties. A new algorithm has been developed to find incremental routing solutions for VLSI circuits that require variable width and spacing on interconnects. This algorithm uses a gridless framework and a novel DFS controlled process to minimize changes to existing nets while still routing the new or modified nets. Experimental results show that this algorithm successfully routes over 98% of ECO-generated nets with a lower failure rate compared to previous techniques. It is also able to route wide nets with a reasonable number of vias and near-minimal net lengths.
41393	4139311	CDMA transmission with complex OFDM/OQAM	The article suggests a new method, called OFDM/OQAM, as an alternative to the commonly used MC-CDMA technique for downlink transmission. This new method combines the benefits of both OFDM and CDMA, while also providing a theoretically optimal spectral efficiency. The proposed method uses an orthogonally multiplex quadrature amplitude modulation (OQAM) to achieve perfect reconstruction of complex symbols over a distortion-free channel. The effectiveness of the proposed method is demonstrated through a comparison with conventional MC-CDMA and an OQAM-CDMA combination using realistic channel models. 
41393	4139317	Channel estimation methods for preamble-based OFDM/OQAM modulations	This paper proposes the use of OFDM/OQAM as an alternative to conventional OFDM with cyclic prefix for transmission over multi-path fading channels. OFDM/OQAM has two distinct features - no guard interval and orthogonality only in the real field and a distortion-free channel. This makes it challenging to use conventional channel estimation methods for OFDM. Therefore, the paper presents two new channel estimation methods for OFDM/OQAM. The first method uses a pair of real pilots, while the second method utilizes imaginary interference to improve the channel estimation quality. The performance of OFDM/OQAM is compared to CP-OFDM in an IEEE 802.22 channel using both methods. 
41394	4139434	An approach for an AVC to SVC transcoder with temporal scalability	The scalable extension (SVC) of H.264/AVC allows for temporal, spatial, and quality scalability in the encoded bitstream, providing flexibility for various devices and networks. However, this scalability must be implemented at the encoder side, making it difficult for existing H.264/AVC content to benefit from SVC's tools. As the migration from MPEG-2 to H.264/AVC is currently underway, it is unlikely that a switch to SVC will occur soon. Therefore, efficient techniques for converting single-layer content to a scalable format are needed. This paper discusses an approach for temporal scalability transcoding from H.264/AVC to SVC, which reduces coding complexity by 55.75% without compromising coding efficiency.
41394	413944	Motion-based temporal transcoding from H.264/AVC-to-SVC in baseline profile	Video content often takes up a lot of storage space and requires significant resources for network transmission. To address this issue, the use of compression is necessary. Scalable Video Coding (SVC) offers temporal, spatial, and quality scalability, allowing for the organization of encoded bitstreams into layers. This makes it adaptable to various consumer electronics devices and networks. As most video content is currently encoded using H.264/AVC, techniques for converting from single-layer H.264/AVC to scalable bitstreams need to be implemented. This paper discusses a method for transcoding from H.264/AVC-to-SVC with temporal scalability in Baseline Profile, resulting in a 64% reduction in coding complexity without sacrificing efficiency.
41395	4139526	RD-Optimization for MPEG-2 to H.264 Transcoding	This paper presents a low-complexity algorithm for inter-frame prediction in an MPEG-2 to H.264 transcoder. The algorithm uses MB information from MPEG-2, including coding modes, coded block patterns, and mean and variance data, to compute an optimal MB coding mode decision with reduced computational complexity. Decision trees are built using data mining algorithms and RD optimized mode decisions, resulting in highly efficient mode decisions. The proposed transcoder is 35% faster than the RD optimized H.264 reference transcoder with minimal PSNR degradation. It also outperforms the SAE cost-based H.264 transcoding by over 3 dB.
41395	4139514	Very low complexity MPEG-2 to H.264 transcoding using machine learning	This paper introduces a new algorithm for macroblock mode decision in inter-frame prediction for video transcoding. The algorithm uses machine learning techniques to classify MPEG-2 macroblocks into one of the 11 coding modes in H.264. This approach is based on the idea that there is a correlation between the motion compensated residual in MPEG-2 and the MB coding mode in H.264. By utilizing this correlation, the algorithm significantly reduces the complexity of the MB mode computation process. The proposed transcoder is compared to a reference transcoder and results show a reduction of over 95% in encoding time with minimal impact on quality and bitrate.
41396	4139624	Conceptual Feedback For Semantic Multimedia Indexing	This paper discusses the problem of automatically detecting visual concepts in images or video shots. The current state-of-the-art systems involve feature extraction, classification, and fusion. However, these approaches often compute detection scores independently for each concept. The proposed method, "conceptual feedback," takes into account the relationships between concepts to improve detection performance. The method involves adding a vector of normalized detection scores to the pool of available descriptors, which is then processed and fused with the original detection scores. This feedback can be iterated multiple times and is compatible with using temporal context to improve performance. In an evaluation, the method showed a 15.3% relative improvement in overall performance compared to only using temporal re-scoring.
41396	4139645	MRIM-LIG at ImageCLEF 2009: robotvision, image annotation and retrieval tasks	The MRIM group at the LIG in Grenoble conducted experiments for the ImageCLEF 2009 campaign, specifically focusing on the Robotvision task. The proposal for this task involved using a generative approach inspired by a language model for information retrieval. To address the unique challenges of the Robotvision task, post-processing techniques were added to account for images belonging to multiple classes and being interconnected. While the results are in need of improvement, the use of a language model for Robotvision showed promise. Additionally, results for the Image Retrieval and Image Annotation tasks are also discussed.
41397	4139726	Dynamic load balancing without packet reordering	Dynamic load balancing is a technique used by ISPs to prevent network congestion from load spikes or link failures. However, current methods for splitting traffic across multiple paths can cause packet reordering and impact TCP congestion control. The proposed FLARE algorithm splits traffic at the granularity of bursts of packets, avoiding packet reordering and achieving accuracy and responsiveness similar to packet switching. FLARE is easy to implement and requires minimal router state. 
41397	413970	What's going on?: learning communication rules in edge networks	Many traffic analysis tools only focus on identifying high volume flows of data, but fail to recognize the underlying structure in network traffic. This structure is important because it reveals patterns in the timing of flow exchanges. Existing tools also struggle to identify malicious traffic, such as network-wide scans for vulnerable hosts. The solution to this problem is eXpose, a technique that uses packet timing information to learn the underlying rules that govern communication on a network. This technique uses statistical rule mining to extract significant communication patterns, even without prior knowledge of what to look for. eXpose also introduces templates to capture rules that would otherwise go unnoticed. Deployments of eXpose have shown success in network monitoring, diagnosis, and intrusion detection with minimal false positives.
41398	4139860	Energized geocasting model for underwater wireless sensor networks.	In sensor networks, energy is crucial for communication between nodes. The lifespan of the network is affected by the energy available at each node. Researchers have been working on ways to conserve energy, and this paper proposes a new energy efficient technique for underwater sensor networks. It builds upon the RMTG protocol and takes into account the current energy level of nodes to select the best relay node. By preferring shorter paths, the algorithm reduces energy consumption and extends the lifespan of nodes. Simulation results show that this algorithm outperforms the RMTG protocol in terms of network energy, path energy, and the number of dead nodes.
41398	41398145	A fault-tolerant routing protocol for dynamic autonomous unmanned vehicular networks	This paper discusses the need for a fault-tolerant routing scheme for unmanned autonomous vehicle (AUxV) networks operating in adversarial environments. The risk involved in their applications makes it imperative to consider the heterogeneity of AUxV nodes and their varying capabilities. The proposed solution, ULARC, is a cross-layer and learning automata (LA) based routing algorithm that uses the theory of LA to select an optimal path for routing. It also incorporates a cross-layer architecture for sleep scheduling of nodes, making it energy-efficient. Additionally, an α-based scheduling scheme is used to further reduce network overhead and improve energy efficiency.
41399	4139949	Combination and Integration in the Perception of Visual-Haptic Compliance Information	Researchers studied how individuals perceive the compliance of materials in a virtual environment through both visual and haptic cues. Two experiments were conducted where subjects had to discriminate between different levels of compliance. The results showed that each modality (vision or haptic) independently produced a compliance estimate, which were then integrated to create an overall value. The integration process was found to be a weighted summation of two random variables, defined by the single modality estimates. This model provided accurate predictions, even if the weights were not optimal. Results also showed that the weights were optimal when vision and haptic inputs were congruent, but not when they were incongruent.
41399	4139940	Haptic Classification of Facial Identity in 2D Displays: Configural versus Feature-Based Processing	This study explored how haptic feedback can be used to classify the identity of upright, inverted, and scrambled faces in 2D raised-line displays. Results showed that participants were able to accurately identify upright and scrambled faces, but had difficulty with inverted faces. This suggests that haptic representations of facial identity prioritize the upright orientation, similar to vision and 3D haptic displays. However, the use of scrambled faces indicates that this process may rely on local feature information rather than a configural representation. This finding aligns with previous research on recognition of non-face objects. Overall, there appears to be a difference in how 2D and 3D haptic displays are processed for facial recognition. 
41400	4140064	Familiarity or conceptual priming: event-related potentials in name recognition.	Recent research has focused on the different components of recognition memory, as measured by event-related potentials (ERPs). Recollection is typically accompanied by a late, positive deflection in the parietal region, while an earlier, frontal component has been suggested to reflect familiarity. However, this frontal component, known as the FN400, has also been proposed to be linked to implicit memory, specifically conceptual priming. A study using an episodic memory task with famous and non-famous names found that the FN400 was affected by frequency (how common the names were), while the parietal component was influenced by fame (how well-known the names were). This suggests that the FN400 is associated with familiarity, but not conceptual priming.
41400	4140026	Sub-homogeneous positive monotone systems are insensitive to heterogeneous time-varying delays.	We prove that a positive monotone system with bounded time-varying delays is globally stable if and only if the same system without delays is globally stable. This is based on an expansion of a previous stability result for monotone systems with constant delays. We also establish a new test for global stability under the assumptions of positivity and sub-homogeneity. This test is both necessary and sufficient if the system has only one equilibrium point in the positive region. Our results are a generalization of existing ones for positive linear systems. 
41401	4140142	Face Alignment Using Segmentation and a Combined AAM in a PTZ Camera	This paper presents a new approach to face alignment using the Active Appearance Model (AAM) in surveillance systems with PTZ cameras. The AAM typically struggles with face images affected by lighting, background clutter, and camera orientation, so the authors propose a robust fitting method that combines Person-specific and Generic models. Face segmentation is achieved using histogram back-projection and skin color histograms, which are updated with a skin mask from the AAM. This method also improves face recognition by combining Generic and Person-specific models, with minimal impact on processing time. Experimental results show that the proposed method produces accurate face parameters and is not sensitive to initial shapes.
41401	4140152	Combining SVM classifiers for multiclass problem: its application to face recognition	In face recognition, a simple classifier such as k-NN is often used. To improve robustness, multiple binary classifiers are combined to create a multi-class classifier. Two common methods for this are one-per-class (OPC) and pairwise coupling (PWC). The accuracy of these methods relies on the accuracy of the base classifiers, for which support vector machine (SVM) is a suitable choice. This paper explores the strengths and weaknesses of OPC and PWC, and proposes a new method that combines the two with a rejection approach using SVM. Experiments on a real face database show that this new method reduces error rates.
41402	414027	A QoS degradation policy for revenue maximization in fault-tolerant multi-resolution video servers	Video servers, which support multiple clients and multimedia content, are at high risk for disk failures due to their use of large-scale disk arrays. To address this weakness, they often reserve a significant amount of their resources for potential disk failures, resulting in under-utilization during normal operation. This paper proposes a solution to improve resource utilization in these fault-tolerant multi-resolution video servers. By using a method that degrades the quality of service and an admission control algorithm, the system can maximize revenue while guaranteeing a minimum quality level for each client in case of a disk failure. This approach utilizes the multi-resolution property of video streams and achieves graceful degradation at near-optimal levels. Simulation results demonstrate that this technique can significantly increase the number of clients admitted for video service and greatly enhance resource utilization rates.
41402	4140218	Replica Striping for Multi-resolution Video Servers	Multi-resolution video compression techniques are used to support different clients with varying quality of service parameters. However, this can lead to unbalanced use of server resources, limiting the number of admitted clients. To address this issue, a new replication scheme called Splitting Striping units by Replication (SSR) is proposed. This scheme involves storing data in different ways on primary and backup copies, with two striping unit sizes. An adaptive admission control algorithm is also introduced to make the best use of server resources. Simulations show that this approach effectively increases the number of admitted clients.
41403	414030	Measuring technology effects on software change cost	The article presents a methodology for accurately measuring the impact of technology on software change effort. It uses metrics to analyze small changes in software and determine the influence of technology. The methodology is demonstrated through a case study that examines the effect of two specific technologies: a version-sensitive source code editor and a domain-engineered application environment. This approach allows for a more precise understanding of how technology affects software change effort. 
41403	4140329	Inferring Change Effort from Configuration Management Databases	This paper presents a methodology and algorithm for analyzing the historical effort required for developers to make changes to software. The algorithm identifies factors that have contributed to increased difficulty in making changes over time. This has implications for understanding cost drivers in software development. One research finding is that a system under study showed a 20\% increase in difficulty of changes per year, indicating a "decaying" trend. Additionally, the paper quantifies the difference in effort between fixing faults and adding new functionality, with fixes requiring 80\% more effort after accounting for size. The methodology can also be used as a project management tool, allowing developers to identify code modules that have become more difficult to change and matching changes to appropriate experts. The methodology uses data from a change management system and can also incorporate monthly time sheet data. The results were validated through a survey of developers, showing a match between the algorithm's predicted effort and the developers' opinions. The methodology also includes a technique using the jackknife method to determine significant factors contributing to change effort. Overall, the method is effective even with limited time sheet data. 
41404	41404126	Convergence and attractivity of memristor-based cellular neural networks with time delays.	This paper discusses the convergence and attractivity of memristor-based cellular neural networks (MCNNs) with time delays. The MCNN is modeled using a differential inclusion with a realistic memristor model. The global solutions of the MCNN are proven to be essentially bounded. The state of the MCNN is shown to converge to a critical-point set in the saturated region of the activation function, if the initial state is also in the saturated region. The convergence time period is finite and can be estimated using given parameters. The paper also proves the positive invariance and attractivity of the state in non-saturated regions. Simulation results of numerical examples are provided to support the theoretical findings.
41404	41404144	Analysis and design of associative memories based on recurrent neural networks with linear saturation activation functions and time-varying delays.	The letter discusses the use of recurrent neural networks with linear saturation activation functions and time-varying delays to analyze and design autoassociative memories. The authors establish sufficient conditions for the networks to have multiple equilibria in the saturation region and its boundaries. They also derive a formula for the number of spurious equilibria. Four design procedures are then developed based on stability results, with two allowing for learning and forgetting capabilities. Simulation results validate the effectiveness of the approach.
41405	4140543	A Complete Algorithm For Designing Planar Fixtures Using Modular Components	Modular fixturing systems are commonly used in manufacturing for holding parts in place during machining operations. However, designing the optimal arrangement of the modules can be time-consuming and requires expertise. In this paper, a new algorithm is presented that takes a polygonal description of the part and efficiently generates all possible fixture designs that kinematically constrain the part in the plane. The algorithm is based on part geometry and a graphical force analysis, and is capable of finding the optimal fixture design based on a chosen quality metric. While the algorithm only considers planar forces and motions, it is a crucial step towards solving the larger 3-D fixture design problem. This approach has potential applications for a wide range of manufactured parts.
41405	414052	On the existence of modular fixtures	Modular fixtures are becoming increasingly popular in flexible manufacturing and job shop machining. They consist of fixels that secure and position a specific part. While human intuition and trial-and-error are typically used to design fixtures, there are cases where custom tooling is necessary. The authors of this paper seek to answer the question of whether a fixture can exist for a given part and fixture model, known as fixturable. They examine two types of fixtures using either 3 locators and a clamp or 4 clamps. The authors provide two positive results, showing that certain cross-sections are fixturable, but also one negative result where a class of cross-sections is not fixturable. This research sheds light on the potential applications of different modular fixture models.
41406	4140623	Offline recognition of unconstrained handwritten texts using HMMs and statistical language models.	This paper discusses a system for recognizing handwritten text in English without any constraints on the data. The system utilizes Statistical Language Models to enhance its performance. Experiments were conducted using data from single and multiple writers and lexicons of varying sizes (10,000 to 50,000 words). Results showed that using language models can significantly improve the accuracy of the system, with error rates reduced by 50% for single writer data and 25% for multiple writer data. The paper also compares this approach to other methods in the literature and proposes an experimental setup for recognizing unconstrained text. 
41406	4140646	Offline Recognition of Large Vocabulary Cursive Handwritten Text	This paper introduces a system that can recognize cursive handwritten lines of text offline. It uses continuous density HMMs and Statistical Language Models, and is designed for data from a single writer without any prior knowledge of the text's content. The system has a recognition rate of around 85% when tested with a lexicon of 50,000 words. The researchers also made changes to the experimental setup compared to their previous work on recognizing single words. The experiments were conducted on a publicly available database.
41407	4140789	Video indexing and similarity retrieval by largest common subgraph detection using decision trees	The largest common subgraph (LCSG) is a useful measure of similarity between a query and a database of models, but it is time-consuming to calculate. To improve the speed of matching, new algorithms have been developed that use prior knowledge of the database. This paper introduces a new algorithm based on similar principles that greatly reduces the computational complexity of finding the LCSG between a known database and an online query. This makes it more efficient to detect the LCSG in real-time.
41407	4140714	A new algorithm for error-tolerant subgraph isomorphism detection	The paper presents a new algorithm for detecting error-correcting subgraph isomorphism from a set of model graphs to an unknown input graph. The algorithm utilizes a compact representation of the model graphs, which is generated in an offline preprocessing step. This representation allows for common subgraphs to be represented only once, reducing the computational effort required for matching with the input graph. As a result, the algorithm has a sublinear dependency on the number of model graphs. Additionally, the algorithm can be combined with a cost estimation method to further improve its run-time performance. 
41408	4140850	Asymptotically optimal geometric mobile ad-hoc routing	The paper introduces AFR, a new distributed mobile ad-hoc routing algorithm that only requires communication between direct neighbors. The algorithm has a worst-case route cost of &Ogr;(c2), which is a significant improvement compared to previous algorithms. It is the first algorithm with a cost bounded by a function of the optimal route. A lower bound of $Ogr;(c2) is also established, making AFR asymptotically optimal. The paper also presents a non-geometric algorithm with the same lower bound, but it requires some memory at each node. This highlights the trade-off between using geometry and memory for routing.
41408	4140832	Minimizing interference in ad hoc and sensor networks	The main challenge in wireless communication, especially in ad hoc networks, is reducing interference. The level of interference experienced by a node is determined by the number of other nodes within its transmission range. To decrease interference, a node can decrease its transmission power, but this may result in dropped communication links. This paper focuses on minimizing average interference while maintaining network properties like connectivity, point-to-point connections, or multicast trees. A greedy algorithm is presented that can achieve an O(log n) approximation to the interference problem with a connectivity requirement. This algorithm is proven to be asymptotically optimal, with a lower bound of Ω(log n) even in a more restricted interference model. The algorithm can also be used for network properties that can be represented as a 0-1 proper function.
41409	4140921	Articulated object registration using simulated physical force/moment for 3D human motion tracking	This paper presents a 3D registration algorithm that uses simulated physical force/moment to track human motion. The algorithm works by aligning a 3D model with sparsely reconstructed points from multiple cameras. The displacement between the model and points generates a simulated force/moment that is used in an Iterative Closest Points approach. The algorithm also incorporates human kinematic constraints through a hierarchical scheme for model state updating. Experiments on synthetic and real data show that the method is efficient and robust for tracking unconstrained human motion.
41409	4140933	A Hybrid Framework for 3-D Human Motion Tracking	This paper introduces a hybrid framework for tracking 3-D human motion using multiple synchronized cameras, with potential applications in surveillance systems. The proposed framework combines stochastic sampling and deterministic optimization to balance efficiency and robustness. A low-dimensional representation of motion statistics is learned and implemented during tracking, reducing the number of particles needed. Additionally, a local optimization method based on simulated physical force/moment is incorporated to further improve tracking accuracy. Results from experiments on real human motion sequences demonstrate the effectiveness and efficiency of this framework compared to other particle filtering methods. 
41410	4141035	CI-Graph simultaneous localization and mapping for three-dimensional reconstruction of large and complex environments using a multicamera system	The paper focuses on a new algorithm called CI-Graph SLAM, which uses submapping and graphical methods to efficiently map large environments. This method is faster and more consistent than the classical extended Kalman filter (EKF) solution. The algorithm creates a graph of submaps and a spanning tree with certain properties, allowing for efficient updates and corrections. It was tested in both synthetic and real-world environments, where it outperformed other techniques for loop closure detection and submap estimation. The paper concludes that CI-Graph SLAM is a valuable approach for SLAM, with potential applications in various fields such as robotics and autonomous vehicles. 
41410	4141021	EKF SLAM updates in O(n) with Divide and Conquer SLAM	 in many realThe paper introduces Divide and Conquer SLAM (D&C SLAM), an algorithm for Simultaneous Localization and Mapping using the Extended Kalman Filter. This algorithm overcomes the limitations of standard EKF SLAM, including reducing computational cost and improving consistency of vehicle and map estimates. Unlike other techniques, D&C SLAM provides an exact solution without approximations or simplifications. The algorithm is able to map a wider range of environments in real time and its effectiveness is demonstrated through empirical results. The paper provides a description and analysis of the algorithm, as well as its computational cost and consistency properties. Overall, D&C SLAM offers an improved solution for SLAM problems.
41411	4141158	User Guidance Of Resource-Adaptive Systems	This paper introduces a framework for designing resource-adaptive software systems for small mobile devices. The framework allows users to control tradeoffs between different aspects of quality of service. The paper presents a model for capturing user preferences and prototype interfaces for eliciting these preferences. It also describes the integration of the framework into an existing software infrastructure for ubiquitous computing. The main research question addressed is whether it is feasible to coordinate resource allocation and adaptation policies in a way that users can understand and control in real time. The paper includes an evaluation of both systems and usability, including a user study. The contributions of this work include design guidelines, APIs for integrating new applications, and a way to model quality of service tradeoffs using utility theory that can be easily used by users with diverse backgrounds. 
41411	41411108	Edge Analytics in the Internet of Things	The article discusses GigaSight, a large repository of user-generated video content with a focus on privacy and access control. The system uses VM-based cloudlets to perform video analytics at the edge of the Internet, reducing the need for high bandwidth connections to the cloud. One type of analytics used is denaturing, which reduces the quality of video content to protect privacy. Another type is content-based indexing for search. This article is part of a special issue on smart spaces, highlighting the increasing use of high-data-rate sensors, such as video cameras, in the Internet of Things. 
41412	4141253	Evaluating The Effects Of Land-Use Development Policies On Ex-Urban Forest Cover: An Integrated Agent-Based Gis Approach	The DEED model, an agent-based model using spatial data, was used to evaluate the impact of lot-size zoning and municipal land-acquisition strategies on forest-cover outcomes in Scio Township, Michigan. The model incorporated agent characteristics, behavior, and landscape aesthetics data from surveys and spatial analyses. Results showed that large lot-size zoning policies lead to sprawl and increased forest cover, with municipal land acquisition having a stronger effect on forest conservation. The location strategy for land acquisition was found to be more effective at increasing forest levels than zoning policies alone. Using this integrated GIS and ABM approach provided valuable insights into the long-term effects of land-use development policies on forest cover.
41412	414128	Multi-dimensional vegetation structure in modeling avian habitat	This study aimed to assess the impact of forest and landscape structure on habitat mapping using remote sensing technology. Data was collected from a test site in northern Michigan, including radar and Landsat imagery, bird presence data, and field observations. A modeling methodology called GARP was used to analyze the contribution of multi-dimensional forest and landscape structure variables. Results showed that including biomass and spatial neighborhood data improved the accuracy of bird habitat prediction compared to using vegetation type alone. The study suggests that incorporating multi-dimensional information may be beneficial for habitat modeling at the landscape level and recommends further research on other variables and species using newer remote sensing capabilities.
41413	4141352	Ambient Assisted Living [Guest editors' introduction]	Ambient assisted living (AAL) is the use of technology in a person's daily living and working environment to help them maintain an active and independent lifestyle as they age. The AAL community focuses on human activity recognition and behavior understanding, which involves detecting and recognizing actions and situations within an environment. AAL systems not only observe but also interact with users through prompts and haptic responses. Context awareness and predictive capability are important elements in providing appropriate responses to user needs. This involves using sensors to gather information about the current situation and anticipating future needs. AAL research plays a crucial role in creating technology that can improve the lives of older adults and keep them connected to society.
41413	4141314	A Multi-agent Architecture for Multi-robot Surveillance	This paper presents a multi-agent architecture that facilitates the coordination of a group of robots for surveillance tasks, specifically environment exploration. The architecture includes two protocols for role allocation and gathering information from the environment. A communication model using communication channels and a publish-subscribe scheme ensures scalability as the number of robots increases. A case study is presented where agents controlling the robots are trained to recognize patterns and determine their distance and orientation from the robots. Overall, this architecture offers efficient and effective support for coordinating multiple robots in surveillance tasks.
41414	4141455	An over-segmentation method for single-touching Chinese handwriting with learning-based filtering	The segmentation of touching characters in Chinese handwriting is a difficult task for offline recognition. To address this issue, a new over-segmentation method is proposed in this paper. This method uses geometric features and a learning-based filter to identify potential cuts in the handwriting strokes. The filter is trained to remove unlikely cuts, improving the accuracy of the segmentation. Additionally, this method can handle strings with more than two characters. Tests on two large databases show that this method is effective for both character segmentation and text line recognition in single-touching Chinese handwriting.
41414	4141452	A New Radical-Based Approach To Online Handwritten Chinese Character Recognition	The paper suggests a new method for recognizing handwritten Chinese characters online. It involves using statistical classification and over-segmentation to identify radicals, as well as using a lexicon to recognize the characters. The approach has been applied to left-right structured characters and is being expanded to other structures. Initial results on a set of 4,284 characters with 1,118 radicals show that the proposed approach is more effective than previous methods.
41415	4141512	A system for supporting group learning that enhances interactions	This paper discusses a system that supports group learning and encourages interactions among learners. Previous systems for collaborative learning have shown limitations in promoting interactions and discussions. To address this, the proposed system incorporates a board game and computer simulation to externalize each learner's ideas and facilitate collaboration. The system was tested in a public elementary school and results showed that it effectively enhanced interactions, discussions, and learner engagement. The aim of the system is to promote mutual learning through active participation and sharing of ideas among learners.
41415	4141522	Towards the Integration of Physical and Virtual Worlds for Supporting Group Learning	This paper discusses the integration of physical and virtual worlds for supporting group learning. The goal of the system is to promote mutual learning through interactions and discussions among learners. Previous experiments have shown that existing systems for collaborative learning may not be effective in supporting interactions and discussions. To address this, the proposed system allows learners to externalize their ideas in a board game, which is then visualized in a computer simulation. This system was tested with fifth-grade students studying environmental problems and was found to be effective in enhancing interactions, promoting discussions, and increasing student engagement. 
41416	4141692	Program Graph Structuring for Execution in Dynamic SMP Clusters Using Moldable Tasks	The paper discusses task scheduling in dynamic SMP clusters using moldable computational tasks. These tasks are used as the atomic elements in scheduling algorithms, with a guarantee of schedule length. The shared memory system architecture utilizes dynamic processor clusters and fast data transfers within these clusters. The clusters are implemented using system on chip (SoC) modules connected by a central global network. The proposed task scheduling algorithm for program macro dataflow graphs involves identifying moldable tasks and scheduling them using a 2-phase algorithm. The algorithm has been implemented and tested with simulated program graphs.
41416	4141626	Task Scheduling for SoC-Based Dynamic SMP Clusters with Communication on the Fly	The paper discusses program scheduling techniques for modular parallel architectures based on System on Chip (SoC) technology. The architecture includes a global communication network that connects multiple SoC modules, allowing for dynamic creation of SMP clusters to facilitate efficient data transfers between processors. Programs are represented as macro data flow graphs, and the proposed scheduling algorithm has two phases. The first phase uses a genetic algorithm with ETF heuristics to distribute program nodes among SoC modules, while the second phase schedules computation and communication within the modules to optimize the use of dynamic processor switching and data read on the fly mechanisms. Simulation results show the effectiveness of the algorithm.
41417	4141748	High-Data-Rate DPC-OF/TDMA Based on Multi-Layer STBC Coded MIMO-OFDM	The authors discuss their work on developing a high-speed DPC-OF/TDMA system that can reach data rates beyond 100 Mbps. They introduce a new multi-layer STBC OFDM scheme that combines spatial diversity and spatial multiplexing to improve performance. Simulations are conducted under different MIMO antenna configurations, with a focus on achieving a packet error rate of 1.0e-2 with an Eb/No of 20.0dB. The results show that the evolved DPC-OF/TDMA can achieve a data rate of 921.6Mbps with an average spectral efficiency of 7.2bit/s/Hz using a bandwidth of 128MHz. This demonstrates the potential of the system for high-speed data transmission.
41417	414177	Subcarrier-Grouping Based Detection Schemes For Multi-Layer Stbc Coded Mimo-Ofdm	The Multi-Layer STBC OFDM scheme combines spatial diversity and spatial multiplexing to take advantage of both techniques. It uses STBC encoding on multiple layers of transmit antennas and employs OFDM signals to handle frequency-selective fading channels. The scheme includes two subcarrier-grouping based detection schemes, which utilize the correlation between subcarriers to reduce complexity while maintaining performance. Simulation results show that the proposed detection scheme can reduce complexity by 51.79% with a 0.5dB performance loss for a 4x4 system and by 69.96% with a 1.0dB performance loss for a 6x6 system, compared to the Exhaustive Detection Scheme. 
41418	41418168	On the Context-aware, Personalized Delivery of Process Information: Viewpoints, Problems, and Requirements	Enterprises are facing a growing amount of data, which makes it challenging to provide employees and decision-makers with the necessary information. This is especially difficult when it comes to delivering both structured and unstructured information based on the current context and user. Examples of unstructured information include office documents and emails, while structured information includes business process models and data from enterprise information systems. To address this issue, a paper presents findings from three studies in the automotive and healthcare industries, as well as an online survey. The first step is identifying problems with process-oriented information management, and the second step is determining requirements for effectively handling process information.
41418	41418158	Process-oriented Information Logistics: Aligning Enterprise Information with Business Processes	Enterprises are facing a growing amount of data, including office files, emails, and process descriptions. This data overload makes it challenging for knowledge-workers to find the information they need for their tasks. Aligning process-related information with business processes is particularly difficult as they are typically managed separately. This paper proposes process-oriented information logistics (POIL) as a solution to bridge this gap. POIL allows for the delivery of process-related information to knowledge-workers in a process-oriented and context-aware manner. A clinical use case and prototype are presented to demonstrate the practical application and benefits of POIL.
41419	414194	Multi-perspective Anomaly Detection in Business Process Execution Events.	Anomaly detection is crucial for preventing fraud and security breaches in process model executions. Current approaches focus on control flow and point anomalies but struggle with false positives for unexpected events. This paper proposes a new approach that considers time and resources to detect contextual anomalies. It also accounts for unexpected events by using likelihood of occurrence to reduce false positives. The approach analyzes multiple events together to detect collective anomalies. A prototype implementation and real-life process execution logs from various domains were used to evaluate the effectiveness and practicality of the approach. This approach shows promise in effectively detecting anomalies in process model executions.
41419	4141968	SeaFlows Toolset - Compliance Verification Made Easy for Process-Aware Information Systems.	The demand for business process compliance has increased, making the verification of process models against compliance rules crucial in enterprise computing. The Sea Flows Toolset allows for compliance checking in process-aware information systems. It has a user-friendly interface for modeling compliance rules and integrating them into process models. To accommodate various verification scenarios, the toolset offers two compliance checking approaches: structural and data-aware behavioral. The structural approach uses compliance rules to detect non-compliance, while the data-aware approach addresses the issue of state explosion by creating a more compact abstract process model. Overall, the Sea Flows Toolset provides a comprehensive and flexible framework for compliance checking of process models.
41420	4142041	Policy improvement by a model-free Dyna architecture.	This paper proposes a new approach to accelerate policy improvement in reinforcement learning. The approach combines two learning schemes, one using temporal difference method and the other using relative values, to improve the learning process. Instead of creating a complex world model, a simple predictor of average rewards is introduced to the actor-critic architecture in simulation mode. The proposed method is tested on a pendulum system and compared to other methods in a labyrinth exploration experiment, showing faster convergence and better performance. This demonstrates the adaptability and robustness of the proposed method in handling unknown dynamics and achieving desired outputs.
41420	414201	A Self-Improving Fuzzy Cerebellar Model Articulation Controller with Stochastic Action Generation	This article introduces a modified version of the fuzzy cerebellar model articulation controller (FCMAC) that incorporates reinforcement learning. The model uses a likelihood scheme to predict the evaluation of possible actions and selects the best action based on an approximating evaluation model. The FCMAC has three main components: a fuzzy quantizer for mapping input to memory, an action evaluation module for producing expected evaluation signals, and an action selection unit for choosing the best action using a probability distribution function. The proposed model is tested on a pendulum system and shows improved performance and applicability compared to traditional models. 
41421	4142154	Automata Evaluation and Text Search Protocols with Simulation Based Security.	This paper discusses efficient protocols for solving two problems: pattern matching and secure oblivious automata evaluation. The first problem involves two parties, where one holds a pattern and the other holds a text, and the goal is to learn where the pattern appears in the text without revealing it or learning anything else about the text. The paper presents several protocols for different levels of security and also generalizes one solution to solve related problems. The second problem involves one party holding an automaton and the other holding an input string, and they need to determine if the automaton accepts the input without learning anything else. The paper presents a novel protocol for this problem that achieves full security against malicious adversaries.
41421	4142177	Text search protocols with simulation based security	The paper proposes a secure protocol for solving the pattern matching problem in a two-party setting, where one party holds a pattern and the other holds a text. The protocol allows the first party to learn where the pattern appears in the text without revealing it to the second party or gaining any other information about the text. This is the first protocol to achieve full security against malicious adversaries for this problem. It is based on a new protocol for secure oblivious automata evaluation, where one party holds an automaton and the other holds an input string, and they must determine if the automaton accepts the input without gaining any additional information.
41422	4142291	Agent interaction in distributed POMDPs and its implications on complexity	Effective coordination is crucial for agents to achieve their goals in a multi-agent system. Researchers have used decision theory to model the coordination problem, but the most general models are very complex to solve optimally. Some more limited models are easier to solve, but it is not clear why this is the case. This study aims to fill this gap by identifying a condition that distinguishes between problems that are in the class NP and those that are harder. This condition relates to the amount of information each agent has about the others and whether it can be represented succinctly. Problems that meet this condition are shown to be NP-complete. This concept is illustrated with two interaction protocols, and for problems that do not meet the condition, an NP approximation can be generated using the theoretical results.
41422	41422152	Solving transition independent decentralized Markov decision processes	The formal treatment of collaborative multi-agent systems has not kept pace with the advancements in individual agent decision making. Recent research on decentralized Markov Decision Processes (MDPs) has helped bridge this gap, but the computational complexity of these models remains an issue. To address this, a specific type of decentralized MDPs with independent agent transitions has been identified. This class of problems involves independent agents who collaborate through a global reward function that is based on their state and action histories. A new algorithm has been developed to solve this class of problems optimally and in an anytime manner. This is the first algorithm to do so for a non-trivial subclass of decentralized MDPs, paving the way for further research on both exact and approximate algorithms in this field.
41423	4142316	A novel local surface feature for 3D object recognition under clutter and occlusion.	The paper introduces a novel surface feature called the TriSI feature, designed for recognizing 3D objects in cluttered and occluded environments. The feature is constructed by creating a unique Local Reference Frame (LRF) for each feature point using neighboring triangular faces, and generating three signatures from the LRF's orthogonal axes. These signatures are then combined into a compressed TriSI feature. The paper also proposes a hierarchical feature matching algorithm for 3D object recognition using the TriSI feature. Experimental results on popular datasets demonstrate the effectiveness of the TriSI feature in various challenging conditions, and the proposed algorithm achieves the best overall recognition results on four standard datasets. 
41423	41423114	A Novel Local Surface Description for Automatic 3D Object Recognition in Low Resolution Cluttered Scenes	This paper introduces a new local surface descriptor, called 3D-Div, which is rotation invariant and based on the concept of 3D vector field's divergence. The descriptor is generated by parameterizing a local surface patch around a randomly selected 3D point and constructing a unique Local Reference Frame (LRF). A normalized 3D vector field is then computed and referenced with LRF vectors to generate the final 3D-Div descriptor. This descriptor was tested on a challenging dataset and achieved 93% accuracy for automatic 3D object recognition, outperforming existing state-of-the-art depth kernel descriptors.
41424	4142428	Automatic ink mismatch detection for forensic document analysis	Handwritten document examination often involves determining if any part of the text has been altered or forged with a different pen. This study introduces the use of hyperspectral imaging to detect ink mismatches in handwritten notes. A new technique for selecting informative bands from hyperspectral images is proposed and implemented in an end-to-end camera-based document imaging system. A database of handwritten notes was collected and made publicly available for this purpose. The proposed method improves accuracy by up to 15% compared to using all bands. This study provides solutions for challenges in camera-based hyperspectral document imaging and demonstrates the effectiveness of the proposed method through extensive experiments.
41424	4142477	Hyperspectral recovery from RGB images using Gaussian Processes.	The proposed method aims to recover spectral information from RGB images with known spectral quantization. This is achieved by combining natural spectra modeled using Gaussian Processes with the RGB images. The use of Process Kernels helps to model the smoothness of reflectance spectra, while also promoting non-negativity for better estimation of reflectance values. The Gaussian Processes are inferred in sets using clusters of spatio-spectrally correlated hyperspectral training patches. These sets are then transformed to match the spectral quantization of the test RGB image. By extracting overlapping patches from the RGB image and matching them to the hyperspectral training patches, the desired Gaussian Processes are inferred and combined with the original processes to construct the final image. The approach uses a fully Bayesian model inspired by the Beta-Bernoulli Process and has been evaluated on three hyperspectral datasets, demonstrating its effectiveness in extracting spectral details from RGB images.
41425	4142545	Mapping Trigger Conditions onto Trigger Units during Post-silicon Validation and Debugging	On-chip trigger units are used for identifying important events during post-silicon validation and debugging. However, their implementation limits the types of events that can be programmed for detection during runtime. This means that some events that were not considered during design cannot be detected due to hardware constraints. To solve this problem, we propose adding architectural features to the trigger units and using an algorithmic approach to automatically map trigger conditions onto the units. This will allow for more flexibility in detecting events of interest during post-silicon validation and debugging. 
41425	4142518	Automated silicon debug data analysis techniques for a hardware data acquisition environment	Silicon debug is a challenging task due to limited access to internal signals of the chip. Trace buffers are used to acquire data in real time, but they only provide access to a limited set of signals. Configuring the trace buffer to trace relevant signals can be a time-consuming process. This paper proposes a method to automate this process by using UNSAT cores to identify important signals and finding alternatives for untraceable signals. When integrated with a debugging methodology, this approach has been shown to reduce potential suspects by 30% with only 8% of registers traced, proving its effectiveness.
41426	414261	Injective optimal realizations of finite metric spaces	A finite metric space (X,d) can be represented by a weighted graph (G,w) with minimal total edge weight, called an optimal realization. Such realizations have applications in fields like phylogenetics, psychology, and compression software. It was previously believed that any map from the vertex set of G to the "tight span" of d must be injective, but this has been disproven. However, this paper shows that certain optimal realizations always have injective maps from their geometric realization to the tight span. These optimal realizations can be constructed from non-injective ones and may lead to new methods for computing optimal realizations from tight spans.
41426	4142632	Optimal realizations of two-dimensional, totally-decomposable metrics	A realization of a metric d on a finite set X is a weighted graph ( G , w ) with X as its vertex set, where the shortest-path distance between elements of X is equal to d. An optimal realization has the smallest sum of edge weights among all realizations. Optimal realizations are important in fields like phylogenetics and internet tomography, and A. Dress (1984) showed that they are related to a complex called the tight-span of d. He also conjectured that the tight-span must contain an optimal realization. This paper proves the conjecture for a class of metrics called totally-decomposable metrics with tight-span dimension two. This result has implications for solving the minimum Manhattan network problem.
41427	41427137	Experiment for Using Web Information to do Query and Document Expansion.	The ImageCLEF photo task for this year has made some changes compared to previous years. The caption field in image annotations and the narrative field in text queries have been removed, along with the visual queries in the image collection. This means that there is less information available for matching query words and annotations, making it more challenging. To address this, the researchers have explored the web to expand the queries and documents. This has resulted in a 16.11% improvement in performance, but there is also an increase in noise. The media mapping method used in previous years has been applied to expand queries, and it has shown to be effective in the new task. However, document expansion has caused a 28.24% decrease in performance due to the excess noise. 
41427	414276	Approaches of using a word-image ontology and an annotated image corpus as intermedia for cross-language image retrieval	ImageCLEFphoto2006 explores two types of intermedia. One approach involves using a word-image ontology to map images to fundamental concepts, and then measuring similarity between images based on the kind-of relationship in the ontology. The other approach uses an annotated image corpus to map images to text descriptions of the concepts, and measures similarity between images using text counterparts and the BM25 method. The official runs showed that both visual query and intermedia were effective. When comparing runs using only textual query to those incorporating visual query, the latter showed a significant improvement of 71%-119% in performance. Even when example images were removed from the collection, there was still a performance improvement of 21%-43%.
41428	4142812	Multi-dimensional regression analysis of time-series data streams	This paper discusses the challenge of performing on-line, multi-dimensional analysis and data mining of stream data in real-time production systems and dynamic environments. Due to the potentially infinite amount of data, traditional methods of storing and scanning are not feasible. The paper proposes a compressed regression approach and a partially materialized data cube model to minimize the memory and storage requirements. An exception-guided drilling approach is also developed for efficient exception-based analysis. Algorithms are proposed and compared for analyzing time-series data streams. The study concludes with the identification of the most efficient algorithm for multi-dimensional stream data analysis.
41428	41428134	Designing a scalable processor array for recurrent computations	The paper discusses the design of a coprocessor (CoP) that efficiently executes recursive algorithms with uniform dependencies. The design is based on two objectives: a fixed bandwidth to main memory and scalability to higher performance without increasing memory bandwidth. The CoP consists of an access unit (AU) with multiple queues, a processor array (PA) with connected processing elements (PEs), and input/output networks for data routing. The design addresses input/output bottleneck and scalability issues, and the use of multiple queues eliminates the need for explicit data addresses. A mapping algorithm is presented that partitions a data dependence graph (DG) and schedules the execution of blocks on the PA. Results show that for a fixed chip area, there is little impact on throughput when using a linear PA compared to a square mesh, and the design is not sensitive to the division of chip area between PA and AU. The study shows that a low-cost, memory bandwidth-limited, and scalable coprocessor system is feasible for evaluating recurrent algorithms with uniform dependencies.
41429	4142929	Scalable Incremental Test-case Generation from Large Behavior Models	Model-based testing is a widely used method for automating the generation and execution of tests while achieving a desired level of coverage. However, applying this technology to large and complex systems can be challenging due to the state-space explosion caused by the size of specification models. This paper introduces a new approach for test-case generation that addresses this complexity by using a synchronous specification language and supporting incremental test-case generation through compositional modeling. The effectiveness of this approach is demonstrated through two industrial case studies involving an electronic control unit and a railway interlocking system. The scalability of the approach is also evaluated through a series of increasingly complex test models.
41429	4142941	Integrating Model-Based Testing and Analysis Tools via Test Case Exchange	Currently, there is a push in Europe's embedded system design industry to better integrate tools that support development, validation, and verification processes. This involves combining model-driven development with model-based testing and analysis. The use of meta-models is proposed to facilitate interoperability between different modeling notations. However, integrating tools with different language subsets and semantic differences is complex and costly. As an alternative, the exchange of test cases generated from models is proposed. This simplifies the mapping between tools and allows for formal analysis of the test cases. A test case generator called Ulysses is integrated with the CADP toolbox, allowing for test case exchange and verification of properties. This approach has been successfully demonstrated in the paper.
41430	4143034	A Formal Semantics of Data Flow Diagrams	This paper presents a formal semantics of data flow diagrams used in Structured Analysis, using an abstract model for data flow transformations. The semantics are comprised of VDM functions that transform a data flow diagram into a VDM specification. This allows for both a graphical view through the diagram and a textual view through the specification. The paper focuses on the reasoning behind the transformation choices and provides annotated VDM functions and examples to describe the transformation process. The ultimate goal is to provide software analysts and designers with a comprehensive understanding of the system being modeled.
41430	414302	An Executable Subset of Meta-IV with Loose Specification	The ESPRIT project EP5570, called IPTES1, is developing a methodology and environment for incremental prototyping of embedded computer systems. As part of this project, an interpreter for an executable subset of a VDM dialect has been created. After a comparative study of various notations inspired by VDM, the BSI/VDM-SL2 notation was selected for its loose specification capabilities. This subset is unique in that it contains a larger portion of looseness compared to other executable VDM dialects. The article focuses on the constructs of this subset and how they have addressed the issue of looseness. It also discusses the connection between the semantics of the subset and the full BSI/VDM-SL notation.
41431	4143122	A parallel algorithm for zero skew clock tree routing	This paper discusses clock skew, a major factor in determining system performance in deep sub-micron fabrication technology. Previous methods for zero skew clock tree routing assumed uniform wire sizes and did not produce exact zero skew. The paper proposes an iterative algorithm for exact zero skew wire-sizing, which was shown to significantly reduce source sink delay compared to uniform wire sizes. To improve efficiency, a parallel algorithm using a cluster-based clock tree construction algorithm and the zero skew wire-sizing algorithm was also proposed, resulting in an average speedup of 7.8 on an 8-processor server without sacrificing solution quality.
41431	4143172	Parallel global routing algorithms for standard cells	This paper presents three parallel algorithms that utilize the TimberWolfSC global router for efficient circuit routing. The algorithms were implemented using Message Passing Interface (MPI) and evaluated on various parallel platforms. Results showed significant speedups and maintained high quality solutions, with some circuits experiencing a reduction in runtime from 30 minutes to just 5 minutes. The algorithms achieved speedups of 4.0 to 5.0 on 8 processors with minimal degradation of solution quality. These findings demonstrate the effectiveness of parallelization for improving the efficiency and accuracy of circuit routing using TimberWolfSC.
41432	414329	Design, construction, and application of a generic visual language generation environment	The development of visual programming languages (VPLs) and their supporting environments can be time-consuming and tedious. To simplify this process, researchers have created high-level tools, but none can fully replicate the ease of creating textual languages with lex/yacc tools. This paper introduces VisPro, a generic visual language generation environment, which improves upon the conventional model-view-controller framework by decoupling its functional modules for independent development and integration. VisPro consists of visual programming tools that divide the VPL construction process into lexicon definition and grammar specification. The compiler for the VPL is automatically generated based on the grammar specification, creating a programming environment with a visual editor. The paper showcases VisPro's capabilities by creating a simple visual language and a more complex visual modeling language for distributed programming.
41432	41432136	VisPro: A Visual Language Generation Toolset	The paper discusses the challenges of implementing visual programming languages (VPLs) and introduces VisPro, a toolset designed to simplify the process. VisPro works similarly to lex/yacc tools for textual languages and consists of a set of visual programming tools. The toolset divides the construction of a VPL into two steps: defining visual objects and a visual editor, and specifying the language grammar with graph rewriting rules. This allows for the automatic creation of a compiler for the VPL. The end result is a seamless visual programming environment with a built-in compiler and visual editor. 
41433	4143379	BinProlog: a Continuation Passing Style Prolog Engine	BinProlog is a Prolog system that is highly efficient, compact, and can be easily used on different platforms. It uses a unique method called source-level transformation to convert clauses into binary continuation passing, making it completely free of side effects. The compiler and engine are written in Prolog and use a simplified version of the WAM, which makes it faster and more optimized for executing binary logic programs. BinProlog outperforms other systems that use the full WAM due to its optimized performance. In this article, the author provides a brief explanation of the system, its features, and presents performance data to support its efficiency.
41433	4143375	A Novel Term Compression Scheme and Data Representation in the BinWAM	This paper introduces a new term representation for the BinWAM, a simplified WAM engine used in the BinProlog system. This representation, called last argument overlapping, utilizes a unique untagged pointer representation known as tag-on-data. The paper also discusses a Cheney-style copy-term algorithm that uses this representation for BinProlog's fast copy once implementation of findall. By using this term representation and a limited amount of instruction folding, the BinWAM is shown to be a viable alternative to its more complex predecessor, while still maintaining competitive performance with commercial Prolog systems. This simpler implementation of BinWAM also makes BinProlog easier to understand and use.
41434	414344	RHENE: A Case Retrieval System for Hemodialysis Cases with Dynamically Monitored Parameters	The paper introduces RHENE, a case-based retrieval system for patients with nephropatologies undergoing hemodialysis. Retrieval of similar cases involves considering both static and dynamic features, and RHENE uses a multi-step procedure for this. The first step involves grouping and classification based on static features to reduce the search space. The second step involves retrieving locally similar cases based on time-dependent features, and computing a weighted average of their distances for a global similarity measure. The paper focuses on implementing this step efficiently using a dimensionality reduction technique called Discrete Fourier Transform and index structures like k-d trees. This allows for efficient retrieval of similar cases, allowing physicians to assess the quality of hemodialysis services.
41434	4143462	A CBR-Based, Closed-Loop Architecture for Temporal Abstractions Configuration	In the field of hemodialysis, a case-based, closed-loop architecture is being implemented to configure temporal abstractions (TA) for time series data. This approach allows for quick selection of suitable TA parameters by referencing similar previously configured cases. The retrieved configuration and time series data are then used to generate qualitative states, trends, and significant combinations through a TA processing module. These results can be evaluated and used to improve future TA configuration sessions through human supervision, thus closing the loop. This work is being integrated with RHENE, a system for case-based retrieval in hemodialysis that can work with both raw and preprocessed time series data.
41435	4143514	Integrating Rule-Based and Case-Based Decision Making in Diabetic Patient Management	The integration of rule-based and case-based reasoning is valuable in medical applications, as it allows for the utilization of both general rules and specific patient cases. This paper presents a decision support tool for managing Insulin Dependent Diabetes Mellitus that combines these two methods. The system aims to provide physicians with a flexible solution for therapy planning by utilizing the strengths of both approaches. The integration is done without favoring one method over the other, but instead leveraging their complementary nature. Rules are used to make suggestions based on structured knowledge, while case-based reasoning is used to adapt the rules to the specific patient's characteristics and experiences. The system has been tested on simulated diabetic patient cases and will be integrated into the T-IDDM architecture.
41435	414357	A multi-modal reasoning methodology for managing IDDM patients	The authors propose a methodology for managing treatment decisions for patients with insulin dependent diabetes mellitus (IDDM). This approach combines case based reasoning (CBR) and rule based reasoning (RBR) to overcome the limitations of using either method alone. RBR uses formalized knowledge to suggest treatment options, while CBR adapts these rules based on the patient's unique characteristics and previous experiences. In situations where the case library is not representative, only RBR is used and the resulting treatment plan is added to the case library for future use. The methodology was evaluated on both simulated and real patient data, and was developed as part of the T-IDDM project. It is integrated into a web-based platform for managing IDDM.
41436	4143699	Three-way decisions based on decision-theoretic rough sets with dual hesitant fuzzy information.	Decision-theoretic rough sets (DTRSs) are a popular model for making three-way decisions in the field of risk decision-making. However, estimating the loss function of DTRSs in complex and uncertain environments is a challenge. Dual hesitant fuzzy sets (DHFSs) offer a new way to handle uncertain information and can be used to determine the loss function of DTRSs. In this study, the hesitant format of DHFSs is integrated into DTRSs to create a new three-way decision model. This model takes into account the loss functions of DTRSs with dual hesitant fuzzy elements (DHFEs) and includes two methods for deriving three-way decisions: one based on scores and accuracies of DHFEs, and the other using a ranking method with a stochastic strategy. A case study on emergency blood transshipment is used to demonstrate and compare the effectiveness of these methods.
41436	41436277	An interactive approach to probabilistic hesitant fuzzy multi-attribute group decision making with incomplete weight information.	The paper introduces an interactive approach to probabilistic hesitant fuzzy multi-attribute group decision making (P-HFMAGDM) with incomplete weight information. This approach uses probabilistic hesitant fuzzy elements (P-HFEs) to express the assessments of decision makers for alternatives over attributes and accounts for partially known weight information on attributes. The paper presents the axiomatic definition of distance measures for P-HFEs and develops various distance measures. It also proposes the probabilistic hesitant fuzzy positive and negative ideal solutions and establishes a multi-objective optimization model. An interactive approach is then developed to allow decision makers to add or modify preference information. A case study on evaluating a VR project declaration is used to demonstrate the effectiveness of the proposed approach.
41437	4143720	A Framework for Adaptive Wavelet Prediction in Self-Sizing Networks	This paper proposes a traffic predictor using multiresolution decomposition for adaptive bandwidth control in locally controlled self-sizing networks. These networks can automatically allocate link and switch capacity based on real-time traffic data, providing packet-level quality of service. The study shows that using a wavelet-based method is more effective than other popular methods, such as the Gaussian predictor, for this type of network. The Haar wavelet is found to be the most suitable for traffic prediction, and the impact of other wavelet parameters is also examined. A novel adaptive wavelet predictor is also introduced, which can adjust to sudden changes in incoming traffic.
41437	414379	Social distance aware resource allocation in wireless networks	In this paper, the authors discuss the importance of social connectivity networks in determining a user's resource requirements. They argue that despite the focus on technical aspects in wireless network design, there has been limited research on incorporating social and business connectivity into the design process. The authors propose the concept of "effective distance" as a measure of the sociotechnical context and use it to improve resource allocation and user utility. They demonstrate this through a case study of a 802.11e compliant WLAN, where users choose to communicate through VoIP. The authors provide a utility function for VoIP calls and use this to optimize resource allocation based on the effective distance and expected quality requirements. Their results show that incorporating the social context in network design can greatly improve user utility, particularly for VoIP traffic.
41438	4143880	Deep Convolutional Neural Networks for Large-scale Speech Tasks.	This paper discusses the use of Convolutional Neural Networks (CNNs) for large vocabulary continuous speech recognition (LVCSR) tasks. CNNs are a type of neural network that can effectively handle spectral variations and correlations in signals, making them a suitable model for speech. The authors investigate the optimal architecture for CNNs in LVCSR tasks, including the number of convolutional layers, hidden units, and pooling strategy. They also explore ways to incorporate speaker-adapted features into the CNN framework, and propose a strategy for using ReLU+dropout during Hessian-free sequence training. Experiments on 3 LVCSR tasks show that the proposed CNN model with speaker-adapted and ReLU+dropout techniques achieves state-of-the-art results with a 12%–14% relative improvement in word error rate (WER) compared to a strong DNN system.
41438	414385	An Iterative Relative Entropy Minimization-Based Data Selection Approach for n-Gram Model Adaptation	The success of statistical n-gram language models heavily relies on the amount and relevance of the training text. The language modeling community is now exploring the use of large amounts of text from the internet to supplement limited in-domain resources. However, the style and content of these texts often differ significantly from the specific domain of interest. In this paper, a method based on relative entropy is proposed to select subsets of sentences that match the domain of interest. The results of using this method for language model adaptation in two speech recognition tasks show improvements in both word error rate and language model perplexity. This highlights the effectiveness of the proposed subset selection scheme in improving speech recognition performance.
41439	414394	Handling continuous attributes in Ant Colony Classification algorithms	Real-world classification problems often involve both continuous and nominal attributes. However, many Ant Colony Optimisation (ACO) classification algorithms can only handle nominal attributes directly. This paper proposes two new methods to address this limitation. The first method allows for more flexible representation of continuous attribute intervals, while the second method considers attribute interaction, which is not accounted for in previous approaches. Empirical evaluation on eight datasets shows that these methods lead to more accurate classification models. The paper also provides a brief overview of Ant-Miner and cAnt-Miner, as well as discussing the proposed methods and presenting results. The paper concludes with future research directions.
41439	4143938	Utilizing multiple pheromones in an ant-based algorithm for continuous-attribute classification rule discovery.	The cAnt-Miner algorithm is a method used for finding classification rules in problem domains with continuous attributes. An updated version of this algorithm, called @mcAnt-Miner, has been proposed with several improvements. This new version uses multiple types of pheromones, one for each class value, for better prediction. The use of pre-selected class values also allows for more accurate heuristic functions, dynamic discretization of continuous attributes, and a rule quality measure that considers rule confidence. Experimental results on 20 datasets show that @mcAnt-Miner significantly improves classification accuracy compared to cAnt-Miner and is comparable to other popular rule induction algorithms. 
41440	4144063	Task allocation to actors in wireless sensor actor networks: an energy and time aware technique	Task allocation is an important aspect of cooperative applications in embedded systems, such as wireless sensor and actor networks (WSANs), which have constraints on latency and energy usage. Existing algorithms for task allocation focus mainly on energy savings and do not consider time constraints, leading to longer completion times for tasks and a higher chance of network malfunction. This paper proposes a two-phase task allocation technique that takes into account both energy efficiency and reduction of completion times for tasks in WSANs. The first phase assigns tasks equally to actors to measure their capabilities, and in the second phase, tasks are allocated based on these capabilities to minimize completion times. Simulation results show a 45% improvement in task completion times compared to the commonly used opportunistic load balancing algorithm. This technique provides a better balance between load balancing and completion times in WSANs compared to opportunistic load balancing.
41440	4144041	An Efficient Algorithm to Detect Faulty Reading in Wireless Sensor Network Using the Concept of Reputation	This paper presents a new method for detecting and identifying faulty sensors in a sensor network. The approach uses reputation concepts and special beacon nodes to minimize false alarms and scale well to large networks. The algorithm is localized and efficient, with low computational overhead for sensors and most tasks performed by beacon nodes. Despite a low complexity, the algorithm has a high probability of correctly diagnosing faults, even in the presence of large fault sets. Simulation results demonstrate the algorithm's ability to accurately identify faulty sensors with high confidence and minimal communication requirements.
41441	4144119	Autonomic specification of self-protection for distributed MARF with ASSL	The paper discusses the successful implementation of formal specifications and code generation for the Autonomic Distributed Modular Audio Recognition Framework (AD-MARF) system. Using the Autonomic System Specification Language (ASSL), a self-protecting mechanism was designed and specified for DMARF. The ultimate aim is to have an autonomic computing layer for DMARF by incorporating autonomic properties at every stage of pattern recognition. Results are presented that complement previous work on self-healing and self-optimization properties. This achievement marks a significant step towards creating a fully autonomous and self-managing audio recognition system. 
41441	4144132	Towards Autonomic Specification of Distributed MARF with ASSL: Self-healing	This paper outlines the implementation of a self-healing property in the Distributed Modular Audio Recognition Framework (DMARF) using the Autonomic System Specification Language (ASSL). ASSL is used to enhance DMARF's capabilities in autonomous systems, reducing the need for human intervention. The authors add an autonomic middleware layer to DMARF and specify the core stages of its pattern-recognition pipeline as autonomic elements, managed by a separate autonomic manager. The corresponding algorithms are also developed. This work aims to improve the functionality and autonomy of DMARF, making it suitable for use in various autonomous systems.
41442	4144251	Privacy streamliner: a two-stage approach to improving algorithm efficiency	The paper discusses the challenges faced by data owners in balancing privacy preservation, utility optimization, and algorithm efficiency when releasing data with sensitive information. The authors propose a new approach, called privacy streamliner, to improve algorithm efficiency by separating the processes of privacy preservation and utility optimization. This is achieved by identifying a set of privacy-preserving solutions that do not reveal information to adversaries and optimizing utility within this set. The proposed approach is applied to micro-data release using publicly known generalization algorithms, and the results show improved efficiency compared to existing solutions. 
41442	4144218	k-jump strategy for preserving privacy in micro-data disclosure	The goal of disclosing micro-data with sensitive attributes is to balance data utility for analysis with protecting private information. However, recent studies have shown that knowledge of the disclosure algorithm can make it unsafe for protecting privacy. This paper proposes k-jump algorithms as a solution, which can transform an unsafe algorithm into a large family of safe options. These different k-jump algorithms have varying levels of data utility, making it possible to choose one that both eliminates adversarial inferences and improves the data utility of disclosed micro-data. This approach allows for a secret choice to be made, ensuring both privacy protection and data utility.
41443	4144334	Minimum factorization agreement of spliced ESTs	The paper discusses the importance of producing spliced EST sequences in understanding alternative splicing. Given an EST sequence, there may be multiple spliced EST sequences associated with it due to different alignments against the genome. The paper introduces the Minimum Factorization Agreement (MFA) problem, which aims to extract a subset of spliced EST sequences that agree on a common alignment region with the genome or gene structure. The paper presents algorithms for solving the MFA problem, which have been shown to efficiently find the correct spliced EST associated with an EST even with rough alignment. The MFA method also has potential applications in producing and analyzing spliced EST libraries based on biological criteria. 
41443	4144328	PIntron: A fast method for gene structure prediction via maximal pairings of a pattern and a text	The challenge of accurately and efficiently predicting gene structure from large clusters of transcript sequences has traditionally been tackled by combining various tools not specifically designed for this task. However, a new method has been proposed that combines a fast algorithm for computing splice alignments with an efficient algorithm that utilizes the redundancy of information in transcript clusters to select the most reliable splice site junctions. This method, called PIntron, has been shown to be both fast and accurate, with the ability to process complex genes in just a few seconds and a high level of accuracy when compared to ENCODE data. PIntron is available for use at http://www.algolab.eu/PIntron.
41444	4144411	Comparison of tree-child phylogenetic networks.	Phylogenetic networks are a type of representation used to study the evolutionary relationships between species. They are a more generalized version of phylogenetic trees and can account for non-tree-like events such as recombination, hybridization, and lateral gene transfer. While there have been attempts to develop a well-founded distance measure for these networks, it has been largely unsuccessful except for a specific type called regular networks. In this paper, a new class of phylogenetic networks called tree-child phylogenetic networks is introduced and a distance measure is defined using a representation called path multiplicity vectors. Algorithms for reconstructing, computing distance, and aligning these networks have also been developed and can be accessed through online tools. 
41444	414443	A distance metric for a class of tree-sibling phylogenetic networks.	Phylogenetic trees can sometimes contain reticulate evolutionary events, which means that there may be multiple paths of evolution from one species to another. This makes comparing phylogenetic trees more difficult than comparing traditional trees. So far, attempts to create a reliable distance measure for all phylogenetic networks have failed. The only practical solutions are to use rough estimates of similarity or to limit the class of phylogenetic networks being compared. This article presents a new distance measure for a specific class of phylogenetic networks, which can be computed efficiently and can also be used to reconstruct networks from DNA sequence data. The Bio::PhyloNetwork package in BioPerl includes algorithms for working with phylogenetic networks. Additional information and experiments can be found on Bioinformatics online.
41445	4144569	A Named Entity Recognition Method Based on Decomposition and Concatenation of Word Chunks	Our method for named entity (NE) recognition involves decomposing and combining word chunks, rather than using word sequences. This allows us to access features such as the first and last words of a chunk, which are not available in sequence-based methods. However, chunks may contain multiple NEs, so we use operators like SHIFT, POP, JOIN, and REDUCE to properly assign NE labels. We tested our method on a dataset of 200,000 annotations from 8,500 news articles in Japanese, and found that it is faster than other methods while maintaining high accuracy.
41445	4144544	Japanese dependency parsing using co-occurrence information and a combination of case elements	The paper introduces a method for enhancing Japanese dependency parsing by incorporating large-scale statistical information. This method considers two types of information that were not previously utilized in statistical parsing methods: dependency relations among verb case elements and co-occurrence relations between a verb and its case element. This information can be obtained from automatic parsing of large corpora. An experiment was conducted to compare the results of this method with an existing machine learning based parsing method, and it was found that the proposed method improves the accuracy of the results. 
41446	4144612	Detecting Curved Symmetric Parts Using a Deformable Disc Model	Symmetry is a valuable tool used by researchers in human and computer vision to identify part structure in images without prior knowledge of the scene. This is achieved by using the concept of a medial axis, which is the center of maximal inscribed discs that form a symmetric part. A framework proposed by LEV09 involves using a multiscale super pixel segmentation to generate a sequence of deformable maximal inscribed disc hypotheses. However, this framework has limitations in capturing a wider range of symmetric parts. To overcome this, the authors introduce a global cost that combines pairwise and higher-level smoothing terms, which is then minimized using dynamic programming. This new framework outperforms the previous one on two datasets. 
41446	414468	A coarse-to-fine taxonomy of constellations for fast multi-class object detection	This paper proposes a novel approach for improving the speed of recognition systems for multi-class part-based object representations. The key idea is to construct a taxonomy of constellation models that is automatically built and optimized to minimize the expected computations during recognition. This approach is applied to the hierarchy-of-parts model and results in improved efficiency in both the representation of object structure and the number of modeled classes. Even with a small number of object categories, this approach achieves a significant speed-up on the ETHZ and TUD datasets, and on a larger scale, it achieves detection times that are logarithmic in the number of classes.
41447	4144716	Registration of Range Images Based on Segmented Data	A new method for registering range images has been developed, which utilizes the results of the segmentation process. The first set of points for registration is obtained from the first range image, while the second set is obtained by projecting the first set onto geometric parametric models from the second range image. The transformation between the two sets is then computed, resulting in a more precise registration compared to traditional methods. 
41447	4144710	A Generic Framework for Assessing the Performance Bounds of Image Feature Detectors.	Over the last decade, local feature detection has been a popular research area in computer vision with various applications such as image matching and registration. This has led to the development of numerous feature detection methods. As the interest in feature-based applications continues to grow, it is important to evaluate and compare the performance of these methods. To address this, a generic framework based on the repeatability measure is proposed in this paper. This framework allows for the assessment of the performance of feature detectors and identifies any significant differences between them based on image transformations. Results obtained from a large image database show the effectiveness of this framework and its potential for remote sensing applications.
41448	41448120	Slepian-Wolf Coding for Nonuniform Sources Using Turbo Codes	This paper discusses the efficiency and optimality of the recently proposed turbo-binning scheme for the uniform source Slepian-Wolf coding problem. It examines the case of nonuniformly distributed sources and finds that any algebraic binning scheme based on linear codes is only optimal for nonuniform sources in an asymptotic sense. The paper proposes two modifications to enhance the performance of the turbo-binning scheme for nonuniform sources: carefully designing the constituent encoder structures to match the turbo code to the source distribution, and using variable-length syndrome sequences to index the bins. Simulations demonstrate that these strategies can result in a significant improvement in compression rate for highly nonuniform sources.
41448	4144826	Properties Of Space-Time Codes For Frequency Selective Channels And Trellis Code Designs	This paper discusses the diversity gain and coding gain of space-time codes (STCs) in frequency selective channels. It shows that STCs can provide more diversity order in these channels compared to flat fading channels. However, certain factors like spatial dependence, correlated taps, and non-uniform power delay profile can result in a penalty in coding gain. The best codes for spatial independence with uncorrelated taps and uniform power distribution are also found to be optimal for other channel scenarios with the same number of taps. The study also presents a method for designing the best STTC for frequency selective channels. An example STTC is shown to outperform a delay diversity code by 3.4dB in a channel with 3 uncorrelated uniform taps. Finally, it is demonstrated that an STC designed for a channel with the maximum expected memory can still perform well in channels with less memory.
41449	41449102	Password-Authenticated Key Exchange between Clients with Different Passwords	The paper introduces a new framework called Client-to-Client Password-Authenticated Key Exchange (C2CPAKE) which allows for a secure end-to-end channel between clients. This is different from existing schemes which rely on a pre-shared password between a client and server. The proposed framework uses two different passwords from each client and does not require any pre-shared secret. The paper defines security notions and types of attacks for this new framework and proves that their scheme is secure against all types of attacks. Two secure C2C-PAKE schemes are presented, one for a cross-realm setting and another for a single-server setting. 
41449	41449198	Practical Password-Authenticated Three-Party Key Exchange	Password-based authentication key exchange (PAKE) protocols are commonly used in "client-server" applications, such as e-banking, where a shared password is used between a client and a server. However, with the rise of modern communication environments like ad-hoc networks and ubiquitous computing, a secure peer-to-peer channel is often preferred. In this scenario, users may not share a password with each other. This paper presents an efficient PAKE protocol for a three-party setting, where two users share a password with a server, but not with each other. The protocol achieves forward secrecy, is light-weighted, and does not require the use of expensive signature schemes or zero-knowledge proofs. It only requires users to remember human-memorable passwords, and all other necessary information is made public. The protocol is comparable in efficiency to the most efficient PAKE protocol in the random-oracle model, without the use of random oracles.
41450	4145035	An Improved Method of Multiplication on Certain Elliptic Curves	The Frobenius endomorphism is a tool that can greatly improve the efficiency of multiplication on certain elliptic curves. This note introduces a new method to reduce the length of the Frobenius expansion in order to further optimize its use. This is an improvement on previous work by Solinas and M眉ller. Experimental results are also provided and compared to standard curves, showing the improved time-performance of multiplication on these optimized elliptic curves over small finite fields. 
41450	414502	Scene conditional background update for moving object detection in a moving camera.	
41451	4145176	Learning a model of facial shape and expression from 4D scans.	The field of 3D face modeling has a gap between high-end and low-end methods. High-end methods offer realistic facial animation but require manual work, while low-end methods use less expressive models. A new model, FLAME, aims to bridge this gap by learning from accurately aligned 3D scans and incorporating articulated jaw, neck, and eyeball movements, pose-dependent blendshapes, and global expression blendshapes. The model is trained from over 33,000 scans and is more accurate and expressive than other existing models. FLAME can be easily fit to data and is available for research purposes. 
41451	41451103	Embodied hands: modeling and capturing hands and bodies together.	Humans use coordinated hand and body movements to communicate and solve tasks. To create realistic virtual characters, it is important to capture and replicate this coordinated activity. However, most methods treat the 3D modeling and tracking of bodies and hands separately. To address this, a new model called MANO (hand Model with Articulated and Non-rigid defOrmations) was developed. This model is learned from high-resolution 3D scans of hands and captures non-rigid shape changes with pose. It is also compatible with standard graphics packages and can fit any human hand. By attaching MANO to a standard 3D body shape model (SMPL), a fully articulated body and hand model (SMPL+H) is created. This model has been successfully used to fit complex, natural activities captured with a 4D scanner, resulting in realistic full body performance capture. The models and data are available for research purposes at http://mano.is.tue.mpg.de.
41452	4145215	Modeling human locomotion with topologically constrained latent variable models	This paper discusses the use of activity-specific motion models for human pose and motion estimation. While these models simplify monocular tracking, they do not address learning models for multiple activities or stylistic variations, or how to combine them with natural transitions between activities. The paper presents a new approach, called the locally-linear Gaussian process latent variable model (LL-GPLVM), which uses a prior to preserve local structure in the training data. The computational complexity of the Gaussian process latent variable model (GP-LVM) is reduced by incorporating sparsification and dynamics within the LL-GPLVM. This allows for the learning of smooth latent models for different activities within a shared latent space, including specific topologies and transitions between activities.
41452	4145230	Backing Off: Hierarchical Decomposition of Activity for 3D Novel Pose Recovery	Model-based 3D human pose estimation involves complex state spaces, especially with even simple models of the human body. To efficiently search for the best pose, low-dimensional activity models can be used if the activity class is known beforehand. However, this approach limits the range of motion that can be recovered. Alternatively, searching the full state space allows for any type of motion to be recovered, but is difficult and expensive. This paper proposes a middle ground approach using a hierarchical Gaussian process latent variable model to learn activity at different scales within the human skeleton. By training on full-body activity data and exploring subtrees independently, novel poses can be recovered. Experimental results on motion capture data and monocular video sequences demonstrate the effectiveness of this approach, and comparisons are made with existing low-dimensional activity models.
41453	41453171	Groups, fixed sets, symmetries, and invariants	The paper discusses a method for analyzing symmetries in two-dimensional shapes that can be seen from any viewpoint, including those with skewed symmetries caused by perspective distortions. This approach differs from previous methods that only consider affine skewing. The key is identifying structures that remain unchanged under symmetries, creating subgroups of projective transformations with simpler invariants. These invariants make it easier to detect and verify symmetries in a more specific manner.
41453	4145314	Moment invariants for recognition under changing viewpoint and illumination	Generalised color moments are a feature extraction method that combines shape and color information in a way that is invariant to changes in viewpoint and illumination. This allows for effective recognition of objects in both indoor and outdoor images. The paper presents a systematic overview of these moment invariants and their potential for dealing with various combinations of deformations and photometric changes. Through a series of experiments, their effectiveness is demonstrated. Additionally, the paper argues that these invariants can also be applied to 3D objects and scenes by using invariant neighbourhoods. 
41454	4145411	Integrated Feature Selection and Parameter Optimization for Evolving Spiking Neural Networks Using Quantum Inspired Particle Swarm Optimization	This paper introduces a new technique, Quantum-inspired Particle Swarm Optimization (QiPSO), for optimizing features and parameters in the Evolving Spiking Neural Network (ESNN). QiPSO uses binary structures to represent information and simultaneously optimizes both the ESNN parameters and relevant features using a wrapper approach. The effectiveness of this method is demonstrated on a synthetic dataset, showing that it can efficiently find the best combination of ESNN parameters and identify the most relevant features. This study presents a unique approach to improving the performance of ESNN and highlights the potential of incorporating quantum-inspired techniques in neural network optimization.
41454	4145427	An extended Evolving Spiking Neural Network model for spatio-temporal pattern classification.	This paper presents a new model, called an Evolving Spiking Neural Network (ESNN), for classifying spatio-temporal data (STD). The ESNN model includes a layer that captures both spatial and temporal components of the STD and transforms them into high dimensional spiking patterns. These patterns are then learned and classified in the evolving classification layer of the ESNN using a fast time-to-first-spike learning algorithm. The proposed method is evaluated on a benchmark sign language video and shows improved classification accuracy compared to traditional time-delay MLP neural network models. The paper concludes by discussing potential future directions for the development of ESNN models for STD.
41455	41455319	Statistical Methods for Estimation of Direct and Differential Kinematics of the Vocal Tract.	This article discusses two statistical methods, Artificial Neural Networks and Locally-Weighted Regression, for estimating the kinematic relationships of the speech production system. These methods are necessary for understanding the motor system involved in speech production and can help in areas such as determining speech production goals and acoustic-to-articulatory inversion. The models are optimized and evaluated using synthetic speech data, showing high accuracy in estimating both direct and differential kinematics. Locally-Weighted Regression performs the best overall and requires only a modest amount of training data. The methods are also applied to real-time MRI data with consistent results. Overall, these methods show promise for accurately estimating kinematics in speech production.
41455	41455128	Computing vocal entrainment: A signal-derived PCA-based quantification scheme with application to affect analysis in married couple interactions	Entrainment is a natural occurrence in human interactions where individuals adapt their behaviors to each other. This has been a focus of psychological studies for years and can provide insight into communication and mental health. However, accurately measuring entrainment is difficult due to its subtle nature. In this paper, an unsupervised signal-derived approach is proposed to quantify vocal entrainment, using specific vocal characteristics to measure similarity between individuals in a conversation. These measures were tested in psychology-inspired experiments and were found to carry meaningful information about behavioral dependency and affective states in marital conflicts. Additionally, using these measures in affect recognition experiments resulted in a 62.56% accuracy in differentiating between positive and negative affect, further supporting the role of entrainment in affective processes during interactions.
41456	4145645	Computing vocal entrainment: A signal-derived PCA-based quantification scheme with application to affect analysis in married couple interactions	Entrainment is a natural occurrence in human-human interactions where both parties adapt their behaviors to each other. This has been studied in psychology for many years and can be described quantitatively to aid in understanding communication, especially in mental health. However, measuring entrainment is challenging due to its subtle nature. In this paper, a new unsupervised method using vocal characteristics is proposed to quantify vocal entrainment. This method was tested on interactions between distressed married couples and showed promising results in accurately differentiating between positive and negative affect. These findings suggest that entrainment plays a crucial role in affective processes during interpersonal interactions.
41456	4145612	An Analysis Of Pca-Based Vocal Entrainment Measures In Married Couples' Affective Spoken Interactions	Entrainment is a crucial factor in understanding the interactions between married couples. A new technique has been introduced to measure vocal entrainment using Principal Component Analysis (PCA). This measure quantifies the preservation of variability in one spouse's speaking characteristics when projected onto the other spouse's speaking characteristics. The analysis of real couples' interactions shows that when one spouse is perceived to have positive emotions, there is a higher level of vocal entrainment compared to when they are perceived to have negative emotions. Statistical analyses were also conducted to determine the strength and directionality of vocal entrainment in different emotional contexts. These findings, along with a baseline prediction model, demonstrate the validity and usefulness of the proposed PCA-based vocal entrainment measure. 
41457	4145736	Security constructs for regulatory-compliant storage	The storage community has responded to the increasing number of electronic records legislation by enhancing data storage to include privacy, auditability, and a "chain-of-custody." This has been aided by the development of compliance platforms by storage vendors, such as EMC Centera Compliance Edition,™ NetApp SnapLock,™ and IBM Tivoli Security Compliance Manage.™ These platforms add storage management policies to existing systems, which aid organizations in meeting compliance guidelines. However, these features do not provide strong evidence of compliance and can be easily subverted by the file system owner. To address this issue, the use of cryptographic techniques such as digital audit trails, fine-grained secure deletion, and per-block authenticated encryption can provide irrefutable evidence of compliance with regulations. These techniques have been implemented in the ext3cow file system and have been found to have a minimal performance impact.
41457	4145730	Version management and recoverability for large object data	Large data objects are typically accessed through file systems, but these systems lack sufficient metadata and do not have a general purpose query engine. Storing objects in databases solves these issues, but databases are inefficient for updating data and may relax integrity and consistency constraints for large objects. This is especially problematic when multiple users or applications need to access large objects at the same time. To address these shortcomings, an architecture based on the Datalink data type has been developed. This allows large objects in a database to be continuously available for read access and can be read and written through a file system interface, without compromising on version management, consistency, and recoverability guarantees. 
41458	4145825	Enumeration and classification of benzenoid systems. 32. Normal perifusenes with two internal vertices	A normal perifusene with two internal vertices belongs to the class PF2, which includes systems made of pyrene and annealated catafusenes. The mathematical solution for the numbers of these systems, based on the number of hexagons, is found using combinatorial summation. A similar class, PF2', includes peribenzenoids with n(i) = 2 such as pyrene and annealated catabenzenoids. Recent computerized enumerations and extensions to higher h values are discussed. Another class, PF2*, is defined as normal perihelicenes with n(i) = 2 and their numbers for given h values are reported. These were obtained by subtracting PF2' from PF2. An analytical method, called combinatorial enumeration, is used to reproduce some of the lowest numbers without computers.
41458	4145819	Plane elementary bipartite graphs	This paper discusses the properties of plane elementary bipartite graphs, which are connected graphs where the union of all perfect matchings forms a connected subgraph. The authors show that for a plane bipartite graph to be elementary, the boundaries of all faces must be alternating cycles with respect to a perfect matching. They also extend the concept of a Z-transformation graph to plane bipartite graphs and obtain similar results to those for hexagonal systems. The paper also introduces the concept of reducible faces and obtains a decomposition for plane elementary bipartite graphs. Additionally, they provide methods for constructing these graphs and derive upper and lower bounds for the number of reducible faces.
41459	4145939	A novel framework for segmentation of deep brain structures based on Markov dependence tree.	The goal of this study is to create a new method for accurately segmenting multiple deep brain structures in medical brain images. This is a challenging task due to the small size and variable shapes of these structures, as well as blurry boundaries and irrelevant background edges. The proposed method uses a template-based approach that combines edge features, region statistics, and inter-structure constraints to detect and locate the target structures without the need for manual initialization. The multi-object template is organized in a hierarchical Markov dependence tree and optimized using a top-to-down strategy. The final segmentation is refined through non-rigid registration between an exemplar image and the target image. Only one training example is needed for this approach, and it has been successfully tested on a publicly available database with expert-segmented brain structures. The results show promising accuracy, with an average Dice score of 0.80 for the caudate nuclei, 0.81 for the putamina, and 0.84 for the thalami.
41459	4145961	Multi-Scale Structured Cnn With Label Consistency For Brain Mr Image Segmentation	The paper introduces a new method for segmenting brain MR images using deep learning techniques. The method utilizes a multi-scale structured convolutional neural network (MS-CNN) to capture features for each sub-cortical structure and generate a label probability map. However, the initial result is not smooth due to the complex background and lack of spatial constraints. To improve the accuracy, a dynamic random walker with a decayed region of interest is proposed to enforce label consistency. The method is evaluated on two data-sets and shows better segmentation results compared to existing methods. 
41460	4146089	Piecewise planar super-resolution for 3D scene	This paper discusses a method for improving the resolution of 3D scenes using multiple low-resolution images. The main challenge is accurately aligning the images due to differences in depth and uncalibrated cameras. The proposed solution involves estimating the depth map for the images and treating registration as a planar segmentation problem. This allows for accurate transformation of each plane in the images. Additionally, the method addresses occlusion by using depth information. The authors also suggest combining multi-view and single view reconstruction for better results. This is achieved by using dictionary learning to incorporate high frequency information from multi-view reconstruction. Experimental results demonstrate the effectiveness of this approach.
41460	4146079	3D Human Face Recognition Using Summation Invariants	.The article discusses a new family of geometrically invariant features called summation invariants, which are used to recognize 3D human facial surfaces. These features are extracted from a rectangular region surrounding the nose in a 3D facial depth map. Through experimentation, the most efficient 2D summation invariant features are identified, and the proper pre-processing method for these features is investigated. The proposed features are tested on data from the Face Recognition Grand Challenge v1.0 dataset and show a significant improvement in performance compared to the baseline algorithm included in the dataset.
41461	41461114	Space efficient signature schemes from the RSA assumption	The RSA assumption is a crucial aspect of signature schemes, as it provides highly reliable security. However, there are only a few known digital signature schemes based on this assumption. This paper proposes several new signature schemes that aim to improve efficiency. The first scheme offers the shortest signatures and public key length compared to existing schemes, but has heavy signing and verification algorithms. The second scheme has a longer public key but more efficient signing and verification costs. The third scheme has even more efficient signing and verification algorithms, but the signature size is longer. These schemes are all based on a new observation about the relationship between m-time signature schemes and short signature schemes.
41461	41461115	Dynamic Threshold Public-Key Encryption With Decryption Consistency From Static Assumptions	Dynamic threshold public-key encryption (dynamic TPKE) is an extension of regular TPKE that allows for decryption servers to join the system after it is set up and for the sender to choose the authorized set and decryption threshold at the time of encryption. The only currently known dynamic TPKE scheme requires a random oracle extension for decryption consistency. However, this paper presents two new methods for constructing dynamic TPKE schemes without relying on random oracles. The first method is a generic construction from public-key encryption with non-interactive opening (PKENO), while the second method uses a specific PKENO scheme and efficient zero-knowledge proofs to achieve a stronger notion of decryption consistency. These constructions rely on static assumptions, such as the decisional linear assumption in bilinear groups.
41462	4146240	An adaptive file-system-oriented FTL mechanism for flash-memory storage systems	The popularity of flash memory has raised concerns about its performance degradation due to its unique characteristics. To address this issue, a new file-system-oriented flash translation layer is proposed. This layer includes a filter mechanism that separates access requests for file-system metadata and file contents, improving performance. A recovery scheme is also proposed to maintain file system integrity. The layer is implemented as a Linux device driver and evaluated with ext2 and ext3 file systems. Realistic traces are also used to test the layer with NTFS. Results show significant performance improvement with minimal system overhead when compared to traditional file systems. 
41462	4146273	A reliability enhancement design under the flash translation layer for MLC-based flash-memory storage systems	Flash memory has become a popular choice for storage, but its reliability has decreased over time. This article discusses a solution to this issue by enhancing the flash management layer, specifically the flash translation layer. The proposed design, which includes a log-based write strategy and hash-based caching policy, aims to improve both performance and ECC redundancy. Additionally, the article presents strategies for managing bad blocks and analyzes the failure rate of flash-memory storage systems. The proposed design is evaluated through experiments and has shown to greatly improve reliability with minimal system overhead. 
41463	4146349	Efficient identification of hot data for flash memory storage systems	This research focuses on the impact of identifying hot data in flash memory storage systems. It proposes a method that utilizes multiple independent hash functions to accurately identify hot data with limited space requirements. This reduces the chance of false identification and improves the performance of flash memory access. The research also includes an efficient implementation and an analysis of the chances of false identification. Experiments were conducted to validate the effectiveness of the proposed method and positive results were obtained.
41463	4146346	MLC-flash-friendly logging and recovery for databases	This research addresses the need for efficient data manipulation on flash-memory storage systems, which have become popular for mobile devices. The focus is on recovery designs using steal and no-force policies over the widely-used ARIES-based recovery in database systems. The paper proposes a design that reduces logging overheads and improves recovery performance for DBMSs on MLC and SLC flash memory. A logging mechanism with start, update, and end log blocks is suggested to support transaction recovery while considering wear-leveling and garbage collection. A recovery mechanism is also proposed to efficiently identify transactions that need to be redone or undone, in line with the steal and no-force policies. Extensive experiments demonstrate the effectiveness of the proposed methodology in significantly improving recovery performance compared to previous work.
41464	414640	Inferring persistent interdomain congestion.	There is a lot of interest in the technical and policy communities about persistent congestion between different networks and how it affects consumers. To provide evidence for these discussions, a system and method was developed to measure congestion on thousands of interdomain links without direct access. This system, called Time Series Latency Probes (TSLP), identifies links with recurring congestion and was deployed at 86 locations globally. The results showed a correlation with other metrics of interconnection performance impairment. A study was conducted on interdomain links of eight major U.S. broadband providers and no widespread congestion was found, although some links did show recurring congestion patterns. The limitations and potential for third-party monitoring of internet interconnection are also discussed.
41464	4146426	Understanding Internet traffic streams: dragonflies and tortoises	This article discusses the concept of network traffic streams and how they form into flows through Internet links. The authors introduce a method for measuring the size and duration of these streams and use it to analyze traffic at two different sites. They find that while most streams are short-lived (less than 2 seconds) and carry a small percentage of total bytes, a significant number of streams can last for hours or days and account for a high proportion of bytes on a link. The authors also identify two types of streams based on their duration (dragonflies and tortoises) and stress the importance of ISPs understanding the distribution of stream sizes and their impact on network performance.
41465	4146563	Small-delay-fault ATPG with waveform accuracy	Small-delay faults in circuits are traditionally detected by sensitizing transitions on a path from input to output that goes through the fault site. This can result in incorrect classifications of undetected or falsely detected faults. To address this issue, an automatic test pattern generation algorithm called WaveSAT has been developed. It considers waveforms and gate delays, filtering out small glitches. The algorithm is based on an optimized encoding of the test generation problem using Boolean satisfiability and has been successfully applied to various circuits. WaveSAT is able to detect testable faults and also generate a formal proof for undetectable faults, making it the first scalable and complete algorithm for this purpose.
41465	4146539	Selective Hardening in Early Design Steps	Hardening a circuit against soft errors is an important step in the design process and should be done before the circuit layout is finalized. To reduce the soft error rate (SER) at a reasonable cost, it is recommended to only harden certain parts of the circuit. Critical spots in the circuit that are prone to causing system malfunctions should be given priority when selecting which locations to harden. This criticality is determined by parameters that may not be available in early design stages. A selection strategy that only considers gate-level information is suggested and is validated using an accurate SER estimator based on the UGC particle strike model. Despite only using partial information, this strategy significantly reduces the circuit's susceptibility to soft errors and outperforms other topological strategies in terms of hardware overhead and protection.
41466	4146618	Minimal counterexamples for linear-time probabilistic verification.	Counterexamples for property violations play a crucial role in debugging faulty systems and verifying complex systems through counterexample-guided abstraction refinement. This study suggests the use of minimal critical subsystems of discrete-time Markov chains and Markov decision processes as counterexamples for violated ω-regular properties. The minimality of these subsystems is measured by the number of states or transitions, a task that is known to be NP-complete for Markov decision processes. The authors propose a method for computing such subsystems using mixed integer linear programming and demonstrate its effectiveness through experiments. The results indicate that this approach produces significantly smaller counterexamples compared to existing techniques. 
41466	4146678	A Flexible Framework for the Automatic Generation of SBST Programs.	SBST techniques are used to test processors and processor cores for permanent faults or in safety-critical applications. However, the manual effort and high costs associated with generating an SBST program can be a challenge. This paper proposes an automatic approach for generating SBST programs by using an automatic test pattern generation framework and a validity checker module. These tools allow for the specification of constraints and can generate more efficient test sequences than manual approaches. The proposed method was evaluated on a MIPS-like microprocessor and showed superior fault efficiency for both end-of-manufacturing and in-field testing. Overall, this approach offers a more efficient and cost-effective solution for generating SBST programs.
41467	414677	A new feature extractor invariant to intensity, rotation, and scaling of color images	This paper introduces a new approach for extracting invariant features from images using principal component analysis and a competitive learning algorithm. It can be applied to binary, gray-level, or colored-texture images larger than 256x256 pixels. The method can extract features that are invariant to translation, scaling, rotation, and color intensity. In experiments, this technique successfully differentiated images with the same shape but different colored textures. The results showed high recognition accuracy and efficient computational time compared to traditional methods. 
41467	4146711	Stereoscopic Face Reconstruction From A Single 2-Dimensional Face Image Using Orthogonality Of Normal Surface And Y-Ratio	This paper presents a modified method for reconstructing 3-dimensional (3D) faces from a single 2-dimensional (2D) image using the Lambertian model. The method involves several steps, including estimating facial height, calculating normal surface and albedo, and correcting errors. The proposed method uses a pattern morphing method (PMM) to estimate normal surface and a Y-ratio calculation for improved computational time. The accuracy of the method was tested on two face databases and compared to other existing methods. The results showed that the proposed method can accurately reconstruct 3D faces with an error of less than 6%. 
41468	4146838	Statistical tools flavor side-channel collision attacks	Collision attacks exploit the similarity of side-channel leakages to bypass common side-channel distinguishers like correlation power analysis and mutual information analysis. These attacks compare two selective observations, making them similar to simple power analysis attacks. A multi-query collision attack presented at CHES 2010 can detect multiple collisions by comparing leakage averages, but it only works if the leakages and processed intermediate values are related. To improve upon this, the authors propose using higher-order statistical moments and probability density functions to detect collisions, instead of just evaluating means. This approach is supported by practical evidence from four case studies involving FPGA-based masked hardware and a software implementation using boolean masking. 
41468	4146830	Compact and secure design of masked AES S-box	Composite field arithmetic is a method used in the implementation of the S-box block of the AES algorithm. It breaks down computations into lower order fields and computes the inverse there. This approach has been used to reduce the area and increase the security of AES implementations. Canright's design using this technique only requires 92 gates for an S-box block. Another approach by the IAIK laboratory uses masking to increase security, and our paper combines the ideas of both approaches to create a compact masked S-box. We also present an implementation using Canright's polynomial functions for a fair comparison. Our design, which uses two special normal basis, is proven to be the smallest and its security is supported by lemmas.
41469	414696	IT Security in Lübeck – The design of a modern and future-proof security curriculum	The University of Lubeck has launched a new degree program in IT Security in response to the growing demand for secure and reliable systems. This program was established in 2016 due to the high demand for trained security professionals both nationally and internationally. The university's strong focus on security and dependability research also played a role in the creation of this program. The main goal of the program is to train computer scientists who possess both practical skills and theoretical knowledge to build and protect secure IT systems, following the security by design principle on a large scale. 
41469	414699	Power Attacks Resistance of Cryptographic S-boxes with added Error Detection Circuits	In recent years, there has been an increase in side-channel attacks on cryptographic algorithms, highlighting the ease of extracting secret keys. To counter this, various schemes have been developed and implemented to protect cryptographic devices. However, most of these protection schemes only target one type of side-channel attack, raising concerns about potential vulnerabilities to other types of attacks. This paper focuses on the impact of adding fault detection circuitry to a cryptographic device (to protect against fault injection attacks) on its resistance to power attacks. The study specifically looks at the S-box component in the AES and Kasumi ciphers and found that the presence of the added circuitry decreased the device's resistance to power analysis attacks.
41470	4147096	Privacy preserving payments on computational RFID devices with application in intelligent transportation systems	Electronic cash is a method of payment that protects the user's identity, making it suitable for systems like public transportation. One specific scheme, developed by Brands, is efficient during the spending phase and uses public-key cryptography. However, the payment devices used in these systems must be affordable and energy-efficient, which can lead to slower transaction times. In this study, researchers demonstrate that with advanced implementation techniques, it is possible to use full-size e-cash schemes on inexpensive payment devices. They successfully implemented Brands' scheme on a computational RFID-token, with the spending protocol taking only 13 milliseconds to execute. However, reloading the card, which is done offline, can be time-consuming and solutions to this issue are discussed. 
41470	414708	The yin and yang sides of embedded security	Pervasive computing, enabled by interconnected embedded systems, has become a reality in recent years. As a result, embedded security has become increasingly important in a variety of applications, including preventing destructive forces such as the Stuxnet virus and protecting intellectual property in consumer products like iTunes and Kindle. At the same time, embedded security researchers face challenges in balancing the opposing forces of security and functionality, as illustrated by the concept of yin and yang. In this presentation, the speaker discusses research projects that address both aspects of embedded security, such as developing a high-performance digital signature engine for secure car-to-car communication, and designing a lightweight encryption algorithm for low-power applications like RFID tags. The speaker also highlights the vulnerability of widely deployed devices to physical attacks, demonstrating the need for strong security measures.
41471	4147141	A scalable and programmable simplicial CNN digital pixel processor architecture	The authors suggest a new architecture for an image processor that is based on the mathematical concept of simplicial cellular neural networks. They introduce instruction primitives for common image processing tasks and demonstrate their effectiveness with binary and gray scale images. The proposed design can be manufactured using advanced CMOS technology and is suitable for pixel-level processing due to its efficient digital circuits and wiring.
41471	4147169	An Analog Vlsi Front-End For Auditory Signal Analysis	Researchers have found that using principles from the human auditory system can improve speech recognition systems in noisy environments. However, implementing these algorithms on a general-purpose computer is not feasible due to their high computational complexity. To address this issue, a low-power analog and mixed-mode circuit system has been developed for auditory signal processing. This system aims to minimize device-mismatch limitations that could affect the performance of the speech recognition system. The system is based on biological auditory models and has been tested on a CMOS prototype. Results from software simulations on a speech database show promising results, with linear discriminant analysis used to reduce feature dimension and interface with hidden Markov models.
41472	41472165	Enhancing web services description and discovery to facilitate composition	Web services are becoming increasingly popular in the business world, but their adoption has been hindered by the manual effort required to use them. In order to increase automation and agility, the authors propose the use of more expressive descriptions for Web services. They introduce METEOR-S front-end tools for annotating source code and generating semantic descriptions, as well as WSDL-S, a language designed to incorporate semantic descriptions into the widely used WSDL 2.0 format. These tools aim to streamline the use of Web services and make them more adaptable to the constantly changing needs of businesses.
41472	4147269	Meteor-s web service annotation framework	The World Wide Web is not only used for data, but also for a wide range of resources called Web services. These services are made possible by standards such as UDDI, WSDL, and SOAP. However, to fully realize the potential of Web services, advancements in areas like service interoperation, discovery, composition, and orchestration are needed. The use of semantics and ontologies from the Semantic Web can greatly improve the effectiveness and scalability of these services. The MWSAF framework combines existing Web service technologies with Semantic Web ideas to enhance service discovery and composition. This paper presents an approach to automatically annotate Web service descriptions with ontologies and categorize them into domains. An empirical study is also included to evaluate the performance of this approach.
41473	414738	A Generalization of the Convex Kakeya Problem	This article discusses the problem of finding the smallest convex region that contains a translate of each given line segment in the plane. This problem is a generalization of Kakeya's problem, which involves finding a region that allows a needle to be rotated 360 degrees within it. The article proves that the optimal solution to this problem is always a triangle, and presents an algorithm that can compute this triangle in ¿(nlogn) time. It also shows that if the goal is to minimize the perimeter instead of the area of the region, then placing the segments with their midpoint at the origin and taking their convex hull results in an optimal solution. Additionally, it is proven that the smallest enclosing disk of any compact convex figure is the smallest-perimeter region that contains a translate of any rotated copy of the figure.
41473	4147353	Farthest-polygon Voronoi diagrams	The article explains the concept of farthest-site Voronoi diagrams for a family of k disjoint connected polygonal sites. These diagrams are used to determine the closest site to a given point, based on the distance to the closest point on the site. The complexity of this diagram is shown to be O(n), and an efficient algorithm with a time complexity of O(nlog^3n) is proposed for its computation. The article also discusses some structural properties of the diagram, including the possibility of having k-1 connected components in a Voronoi region, but if one component is bounded, it covers the entire region.
41474	414744	An Experimental Assessment of the 2D Visibility Complex	In this study, the authors investigate the size of the 2D visibility complex for randomly distributed unit discs in the plane. They find that the number of free bitangents is linearly related to the number of discs, and they examine how this relationship is affected by the density of the discs. They present an approximation for the number of free bitangents based on the density and number of discs, and note that this approximation can be used to predict the onset of the linear behavior for low densities. Overall, the study provides insights into the behavior of the 2D visibility complex in different density scenarios.
41474	414749	The Expected Number of 3D Visibility Events Is Linear	The paper discusses the expected number of maximal nonoccluded line segments tangent to four uniformly distributed unit balls in 3-dimensional space. The results show that this number is linear, which is an improvement from the previous bound of O(n^{8/3}). This also suggests that the storage requirement for the visibility complex, a data structure for encoding visibility information, may not be as prohibitive as previously thought. The linear bound also applies to other distributions, such as the Poisson distribution, and extends to balls of various radii, polyhedra, and even nonfat 3-dimensional objects. The paper also provides insight into the expected size of other global visibility data structures, such as the aspect graph.
41475	41475100	Approximate Query Processing For Efficient Content-Based Image Retrieval Based On A Hierarchical Som	This paper introduces a new method for similarity matching in image retrieval using a hierarchical self-organizing map (SOM). The approach involves mapping high dimensional input vectors to a low dimensional grid using a local membership function, maintaining the relationships between the input vectors and their neighboring weight vectors. A hierarchical tree is then used to reduce the computation cost of finding the best match unit. The k nearest neighbors of a query vector are retrieved using an approximate query processing approach. Experimental results demonstrate the effectiveness of this approach on both synthetic datasets and image databases. 
41475	4147563	A modified support vector machine and its application to image segmentation	Researchers are increasingly interested in studying the support vector machine (SVM) due to its numerous practical applications in fields such as pattern recognition, multimedia, image processing, and bioinformatics. One of the main challenges in SVM research is finding ways to improve the efficiency of the original model without compromising its classification performance. In this paper, the authors propose a modified SVM that utilizes support vector properties and a pruning strategy to eliminate redundant training vectors while preserving the support vectors. The experiments conducted on real images demonstrate that this approach significantly reduces the computational cost while maintaining similar levels of accuracy. This approach also proves effective for image segmentation.
41476	4147646	Hybrid orbiting-to-photos in 3D reconstructed visual reality	Virtual navigation through 3D images has become increasingly popular in various applications. This paper focuses on a specific type of virtual travel maneuver, orbiting to photos that show a point-of-interest. The challenge with this maneuver is providing appropriate feedback to the user about each photo while allowing them to manipulate three degrees-of-freedom for orbiting. The authors propose a hybrid approach that combines features from two existing methods, resulting in a more favorable user experience and preference for outdoor scenes. Experimental results show that this approach is successful in providing useful information and improving the virtual navigation experience.
41476	4147666	Computing similarity transformations from only image correspondences	Our proposed solution involves calculating the relative pose between two generalized cameras while also accounting for the internal scale of each camera. This method can be applied to finding a similarity transformation between two coordinate systems, making it useful for visual odometry and structure from motion. Unlike other methods that use 3D information, our approach uses 2D image correspondences, eliminating depth uncertainty. The method takes advantage of a known vertical direction, obtained from IMU data or vertical vanishing point detection, and solves the relative pose and scale problem as a Quadratic Eigenvalue Problem. Our experiments show that our approach outperforms other methods, especially in scenes with increased depth.
41477	4147741	Position control of X-Y table at velocity reversal using presliding friction characteristics	This paper discusses a method for precision position control of a CNC machining center's X-Y table during velocity reversal. The authors analyze the effects of presliding friction and propose a compensation method based on these characteristics. They also investigate the transition time between presliding and sliding regimes, and establish a relationship between this time and the acceleration at zero velocity. The paper suggests a way to estimate the transition time without measuring velocity and confirms its validity through experiments. Additionally, the authors address the issue of torsional displacement in the X-Y table system and propose a combined compensation method for both friction and displacement. Experimental results demonstrate the effectiveness of the proposed approach.
41477	414773	A New Biased Discriminant Analysis Using Composite Vectors for Eye Detection.	A new approach for eye detection using composite vectors is proposed. The covariance of these vectors is used as a generalization of pixel covariance in a composite biased discriminant analysis (C-BDA) method. The proposed detector combines Haar-like features with composite features obtained from C-BDA to achieve real-time detection with an average execution time of 5.5 ms on a typical PC. Experimental results on two databases show robust performance against variations such as facial pose, illumination, eyeglasses, and partial occlusion, with a detection rate of 98.0% for the CMU PIE database and 95.1% for a real-world data set. The proposed detector also shows comparable performance to manually located eye coordinates for face recognition on the real-world data set.
41478	4147831	Extension based Limited Lookahead Supervision of Discrete Event Systems	Chung-Lafortune-Lin studied the supervisory control of discrete event systems using limited lookahead, where control is determined by truncating the plant behavior within the lookahead window. This approach has been modified to extend the plant behavior beyond the window, eliminating the need for pending traces. This avoids the issue of choosing a conservative or optimistic attitude towards pending traces, which can lead to violations or restrictive control policies. The proposed method also uses relative closure to ensure non-blocking behavior, even when the desired behavior is not relatively closed. It possesses all the desirable properties of the conservative approach and has been applied to concurrency control in database management systems.
41478	4147889	Optimal Nonblocking Directed Control of Discrete Event Systems	Directed control is a refined approach to supervisory control in discrete event systems. It involves selecting one controllable event to enable at each state, rather than computing a maximal allowable set of controllable events. Previous research has focused on developing a framework and algorithm for optimal directed controllers in acyclic plants. This paper presents a new synthesis approach for general plants, including those with cycles, providing a complete solution to the optimal directed control problem. Despite the inclusion of cyclic plants, the complexity of this approach remains polynomial in size.
41479	4147978	Multi-scale similarities in stochastic neighbour embedding: Reducing dimensionality while preserving both local and global structure.	Stochastic neighbour embedding (SNE) and its variants are methods used for reducing the dimensionality of data in a nonlinear way. These methods rely on soft Gaussian neighbourhoods to measure similarities between pairs of data points and aim to recreate these neighbourhoods in a lower-dimensional space. Previous research has explored the robustness of these methods to norm concentration and proposed enhanced cost functions, such as sums of Jensen-Shannon divergences. This paper introduces a new refinement called multi-scale similarities, which use exponentially increasing bandwidths to replace traditional single-scale neighbourhoods. The objective of multi-scale similarities is to optimize the embedding quality on all scales, preserving both local and global neighbourhoods without the need for the user to specify a scale. Experiments show that this approach improves the quality of dimensionality reduction and captures the structure of data more accurately. 
41479	4147921	Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in dimensionality reduction based on similarity preservation	Stochastic neighbor embedding (SNE) and its variants are methods of dimensionality reduction (DR) that use normalized softmax similarities to reproduce similarities observed in high-dimensional data. These methods have shown impressive results in mitigating the curse of dimensionality. This paper investigates the cost function used in SNE and its variants, and proposes a new cost function based on the generalized Jensen-Shannon divergence. Experimental results show that this new cost function produces better embeddings that preserve small neighborhoods, compared to the previous single KL divergence used in SNE and t-SNE, and the weighted mixture used in NeRV. This suggests that future improvements in similarity-based DR may come from advancements in defining the cost function.
41480	4148014	A hybrid method of fuzzy simulation and genetic algorithm to optimize constrained inventory control systems with stochastic replenishments and fuzzy demand	This paper discusses multi-periodic inventory control problems and the assumptions typically used to study them. The two main assumptions are continuous review, where orders can happen at any time, and periodic review, where orders can only be placed at the beginning of each period. The paper proposes a new approach that relaxes these assumptions and considers the times between replenishments as independent random variables. The decision variables in this problem are of integer type and there is a single space constraint. Demands are treated as fuzzy numbers and a combination of back-order and lost-sales is considered for shortages. The paper presents a hybrid method of fuzzy simulation and genetic algorithm to solve this problem, and compares its performance with an existing hybrid simulation and simulated annealing algorithm through numerical examples. The results show that the proposed method performs better. The paper also demonstrates the applicability of the proposed methodology through sensitivity analysis on its parameters.
41480	4148041	Replenish-up-to multi-chance-constraint inventory control system under fuzzy random lost-sale and backordered quantities	This paper discusses a mathematical model for inventory control in a multi-product setting, where replenishment time periods are random. The decision variables are integers, with constraints on service-level and space limitations. Shortages are allowed in the form of fuzzy random quantities. The model aims to maximize total profit while meeting budget and service-level constraints. A hybrid approach, using fuzzy and stochastic simulation, along with particle swarm optimization, is used to solve the model. Numerical examples are provided to demonstrate the effectiveness of this approach compared to another hybrid algorithm. Results show that the proposed approach outperforms the other algorithm in terms of both objective functions and computational time.
41481	4148128	From Fixed-Length to Arbitrary-Length RSA Padding Schemes	Signing with RSA involves applying a hash or redundancy function to a message, adding padding, and then exponentiating it with the decryption exponent. This is commonly used in various standards. However, a new paper suggests using a secure padding scheme for signing long messages and a separate one for fixed-size messages. This shifts the focus to finding a secure encoding for RSA signatures, highlighting the challenge of finding a secure redundancy function for short messages. This is still an open problem.
41481	4148141	Universal Padding Schemes for RSA	This paper discusses the common practice of using RSA for encryption and signing. Typically, a padding scheme is applied to the message before it is exponentiated with the public or private exponent. The paper introduces a simplified approach where PSS, a padding scheme typically used for signing, can also be used for encryption. This results in a secure encryption scheme in the random oracle model and allows for the use of the same RSA key-pairs for both encryption and signing. This simplifies the process for PKIs and public-key implementations. The paper also shows that PSS can be used for any trapdoor partial-domain one-way permutation, further simplifying the use of RSA for both encryption and signing.
41482	414822	Routing and peering in a competitive Internet	 for locations outside of B.The Internet today is made up of independent network providers who are mainly concerned with maximizing their own profits. This has implications for how these providers choose to interconnect with each other. The placement of these interconnection links is a complex problem, but there are some simple solutions for certain cases. Another phenomenon, known as "hot-potato" routing, where outgoing traffic exits a provider's network as quickly as possible, is also influenced by the economic incentives of the providers. In cases where providers can charge each other for flow on their links, there are bounds on the efficiency of this scheme. This is a significant change from traditional analyses of routing, which assumed a single operator was in charge of the network with the goal of improving overall performance. Understanding the economic incentives driving network providers is necessary to understand how they form interconnections, which can be classified as transit or peer relationships. 
41482	4148245	Partially Optimal Routing	Communication networks, like the Internet, are made up of different administrative domains. While it is common for each domain to use the most efficient routing within their own network, this may not always result in the overall best performance for the entire network. To address this issue, a model of partially optimal routing is proposed, where optimal routing within subnetworks is combined with selfish routing across domains. It is shown that in some cases, this approach may actually lead to worse network performance, particularly when Braess' paradox is present. The worst-case loss of efficiency is estimated to be no more than 25% in certain scenarios, but can be much higher in others. Conditions for traffic engineering to be individually optimal for service providers are also discussed.
41483	414830	Handling Occlusion and Large Displacement through Improved RGB-D Scene Flow Estimation	The accuracy of scene flow is limited by challenges like occlusion and large displacement motion. These problems are often related, as large displacement motion can lead to occluded regions. To address this, a new method using RGB-D data is proposed. This method models occlusion and estimates scene flow simultaneously, while also using an over-parameterized representation to handle large displacement motion. A two-stage optimization process is used, with a new PatchMatch method applied in the RGB-D image space to reduce complexity. This method outperforms others on the Middlebury dataset and is also successful on real data from a Kinect sensor. 
41483	41483106	Hierarchical Filtered Motion for Action Recognition in Crowded Videos	The topic discussed is the challenge of recognizing actions in videos with cluttered and moving backgrounds. The main difficulty is the contamination of motion fields by background motions. To address this issue, the authors propose a hierarchical filtered motion (HFM) method that uses motion history images (MHI) as representations of motion. This method involves detecting interest points, applying global and local motion filters, and using a Gaussian-mixture-model-based classifier for action recognition. The proposed approach achieves state-of-the-art results on the KTH dataset and outperforms existing techniques in cross-dataset experiments. 
41484	4148447	Spectral clustering based on iterative optimization for large-scale and high-dimensional data.	Spectral graph theory has been an important aspect of manifold learning and is widely used in data clustering. However, its computational demands make it difficult to handle large and high-dimensional datasets. The development of data on the Web has also posed challenges for traditional single-task clustering, leading to the rise of multi-task clustering for applications like video segmentation. In this paper, the authors introduce a Spectral Clustering based on Iterative Optimization (SCIO) method that efficiently solves the spectral decomposition problem for large and high-dimensional datasets and is effective in multi-task clustering. Experiments on synthetic and real-world datasets prove the effectiveness of this approach. 
41484	4148470	Action recognition using spatial-optical data organization and sequential learning framework.	Human action recognition in videos is a difficult task due to complex movements, diverse backgrounds, and a gap between low-level features and high-level semantics. Current methods have made progress by focusing on robust feature description and elaborate learning models, often utilizing two-stream networks with RGB and optical flow frames. However, these features struggle with limited representation as RGB videos have redundant static appearance and optical flow videos lack detailed appearance. To address this, we propose a new algorithm with a hierarchical weighting segmentation and optical flow-based data organization, along with a lightweight deep learning model using Convolutional 3D and Recurrent Neural Network. Our method achieves good results on a popular dataset, showcasing its effectiveness for complex human action recognition.
41485	4148532	A latent shared-component generative model for real-time disease surveillance using Twitter data	The use of data mining to address social problems, also known as "data science for social good," has gained attention from researchers and institutions. This paper focuses on using data to monitor dengue epidemics in small geographical areas. A simple yet effective model is developed to connect fluctuations in disease cases and disease-related Twitter posts. A hidden Markov process is used to drive both the fluctuations in dengue cases and tweets, with a random source of tweets added to represent posts when no cases are recorded. The model is learned through a Markov chain Monte Carlo algorithm and has shown success in predicting disease counts using data from Brazilian towns.
41485	4148525	Infection Hot Spot Mining from Social Media Trajectories.	Traditionally, high risk zones for disease are identified based on residence or work address. However, this does not provide information about where people are actually getting infected. The availability of spatial data from geotagged social media posts allows for a more accurate and detailed understanding of disease spread. By analyzing the sequential locations of diseased individuals, two stochastic models have been developed to identify the most likely areas where a person has been infected (Visit Model) and where they have become infected (Infection Model). These models have been successfully applied to over 100 million geotagged tweets from Brazil in 2015, specifically targeting the identification of infection hot spots for dengue. This demonstrates the effectiveness of using social media data for disease surveillance.
41486	4148641	Infinite Author Topic Model based on Mixed Gamma-Negative Binomial Process.	The incorporation of side information such as authors, time stamps, and emotional tags into traditional text mining models has become increasingly popular in fields like information retrieval, natural language processing, and machine learning. One approach is the Author Topic Model (ATM), which uses author interests as side information in a topic model. However, the ATM requires a predetermined number of topics, which can be challenging in real-world scenarios. To address this issue, the Infinite Author Topic (IAT) model is proposed, which uses a stochastic process to determine the number of topics from the data itself. This model also includes a multi-author contribution component and an efficient Gibbs sampling inference algorithm. Experiments on real-world datasets show the IAT's ability to learn topics, author interests, and the number of topics simultaneously.
41486	4148668	Topic Model for Graph Mining.	Graph mining is a popular research area due to its numerous applications, including representing unstructured and structured data as graphs. However, current research on graphs struggles to discover latent topics within graph-structured data, which is crucial for both unsupervised and supervised learning. Standard topic models are successful in discovering latent topics, but they cannot be directly applied to graph-structured data due to the "bag-of-word" assumption. To address this issue, a new graph topic model (GTM) is proposed, which uses Bernoulli distributions to model the edges between nodes in a graph. This allows the edges to contribute to latent topic discovery, improving the accuracy of supervised and unsupervised learning. Experimental results show that GTM outperforms latent Dirichlet allocation in classification, using the discovered topics to represent graphs.
41487	4148776	On Finding Templates on Web Collections	Templates are blocks of code used by website creators to maintain a consistent design and navigation across multiple web pages. They are often generated using tools or programs that publish content from a database. While templates are helpful, they can negatively impact the quality of results from systems that automatically process website information, such as search engines. This is because the information in templates is redundant and storing it multiple times can waste computational resources. This paper discusses methods for detecting templates in a scenario where there are multiple templates present. Previous research has focused on detecting a single template, but this scenario is more realistic and requires adjustments to existing algorithms. The proposed methods involve partitioning the collection of web pages into clusters and using a single-template detection procedure on each cluster. A new algorithm is also proposed, which has a linear complexity and is effective in identifying a template with only a small set of pages. Experimental results show that this approach is efficient and accurate.
41487	4148751	Using latent-structure to detect objects on the web	The paper discusses the importance of identifying relevant web pages for integrating and locating content. It focuses on identifying pages that contain objects with a latent structure, and proposes an algorithm to automatically extract statistically significant patterns from these objects. This approach has advantages over traditional text classifiers, as it only requires positive examples and can pinpoint the location of the object within a page. The algorithm also includes an online learning component for continuous improvement. Experimental results show that this approach is effective and outperforms traditional classifiers, especially with the use of online learning. 
41488	41488125	Single image super-resolution via phase congruency analysis	The process of creating high-resolution images, known as single image super-resolution (SR), is a challenging task. While self-example-based methods can produce sharp edges, they struggle with textures. Some methods use higher-level image segmentation and external texture databases, but these require a lot of human involvement. This paper discusses the limitations of example-based techniques and proposes a new method using scale space analysis and a robust pixel classification method. This approach can effectively differentiate between edges, textures, and flat regions in images, and adaptively prioritize high-frequency details and scale invariant fractal properties. Experimental results show that this approach produces high-quality images with minimal artifacts. 
41488	4148865	Lgps: Phase Based Image Quality Assessment Metric	The Log Gabor Phase Similarity (LGPS) is a new technique for assessing the quality of digital images. It uses phase maps, which capture the most important cognitive features of an image, to measure the similarity between different images. This metric is based on the log Gabor transform, which can capture changes in image details regardless of contrast fluctuations. The process involves decomposing the image using log Gabor filters and then computing phase maps from the filter responses. A window-based similarity metric is used to evaluate the resemblance between phase maps and predict the perceived quality of images with various distortions. Experimental results and comparisons show the effectiveness of LGPS in image quality assessment.
41489	4148926	Model transformations for migrating legacy models: an industrial case study	Many automotive companies, including General Motors, have adopted Model-Driven Development (MDD) in their vehicle control software development. GM has been using a custom-built, domain-specific modeling language for this purpose. However, with the development of AUTOSAR, a standard for integrating components from different suppliers, there is a need to migrate these GM-specific models to AUTOSAR models. This paper discusses the use of model transformations, specifically using MDWorkbench and ATL, to address the challenges in this migration process. The authors share their experience and provide recommendations for further research in this field.
41489	4148934	Model Transformation Intents and Their Properties	The concept of model transformation intent is introduced as a way to define the purpose of a transformation. A framework is outlined for describing the specific properties a model transformation needs to meet in order to achieve its intended purpose. The framework is applied to six common model transformation intents, with a case study from the automotive industry used to showcase its practical usefulness. The paper highlights the diverse range of intents involved in industrial model-driven software development, emphasizing the importance of clearly defining and understanding the intent of a model transformation.
41490	4149076	Debug Aware AXI-based Network Interface	As System on Chips (SoCs) become more complex with multiple cores and interconnections, traditional debugging methods need to be improved. This includes not only validating the computational part of a design, but also monitoring and validating communication and synchronization among cores. This paper proposes a debug aware network interface (NI) compatible with AXI standard, which allows for cross-trigger debugging. This means that transactions from one processing element can be monitored and traced by a cross-trigger unit and routed to another processing element or Shared Debugging Unit (SDU). The benefits of this approach include real-time detection and bypassing of severe faults, no need for large internal trace memory, and easier debugging of applications running on multiple processors.
41490	4149014	Hierarchical Embedded Logic Analyzer for Accurate Root-Cause Analysis	Post-silicon debugging is a crucial process in locating errors that were not found during pre-silicon verification. However, it can be challenging due to limited real-time observability and controllability of signals in hardware prototypes. Design for Debug (DFD) techniques, such as Embedded Logic Analysis (ELA), aim to improve signal observability and streamline error analysis. This paper proposes a hierarchical trigger generator that utilizes ELA to efficiently acquire debug data and provide comprehensive trace information for root cause analysis. This approach offers advantages such as facilitating failure localization, avoiding costly trace signal interfacing, and being customizable for multiple debug rounds. Overall, the proposed method improves the post-silicon debugging process for faster and more accurate error identification.
41491	414911	A family of iterative methods with sixth and seventh order convergence for nonlinear equations	This paper introduces a new family of iterative methods for solving nonlinear equations with sixth and seventh order convergence. These methods combine known methods of third and fourth order with Newton's method and use a precise approximation for the last derivative, resulting in high convergence and reducing the number of functional evaluations required. The methods achieve efficiency indices of 1.5651 and 1.6266, making them competitive with other methods. The paper also introduces a new efficiency index that takes into account both computational effort and functional evaluations per iteration, and uses this index to compare the new methods with others in several numerical tests. 
41491	414916	Efficient three-step iterative methods with sixth order convergence for nonlinear equations	This paper introduces two new three-step iterative methods for solving nonlinear equations, with a sixth convergence order. These methods are derived from existing third order methods, combined with Newton's method and a suitable derivative approximation. This results in a higher convergence order and reduces the number of functional evaluations required. The first method is based on Potra-Pták's method and the second on Homeier's method, achieving an efficiency index of 1.5651. These methods are comparable to Parhi and Gupta's method, and also outperform methods proposed by Kou and Li, Wang et al., and Chun in terms of efficiency and computational cost. The convergence results are proven and confirmed through numerical tests.
41492	4149261	King-Type Derivative-Free Iterative Families: Real and Memory Dynamics.	This article introduces a biparametric family of order four derivative-free iterative methods for solving nonlinear equations. By manipulating the error equation, these methods can be modified to achieve higher convergence orders up to six. The stability analysis of the family without memory is conducted on quadratic polynomials, revealing areas in the parameter plane with good performance. To further investigate the stability of the family with memory, a discrete multidimensional dynamical system is associated with it. By studying the fixed and critical points of this system, the most stable methods can be identified.
41492	4149233	An efficient two-parametric family with memory for nonlinear equations	A new two-parametric family of derivative-free iterative methods is introduced for solving nonlinear equations. This family has an optimal order of four and uses two self-accelerating parameters to improve convergence rate. These parameters are calculated in each step using information from the current and previous iterations. The corresponding R-order is 7 and the efficiency index is 1.913. Numerical examples and comparisons with existing eighth-order schemes validate the theoretical results. The method is also shown to be stable through analysis of its dynamical behavior. This new family of methods offers a promising solution for solving nonlinear equations efficiently and effectively.
41493	4149324	Efficient and correct execution of parallel programs that share memory	This paper discusses an optimization problem that arises when parallel programs are executed on shared-memory MIMD computers. These computers consist of multiple processors executing sequential program segments and accessing shared variables asynchronously. The goal is to ensure that the execution is sequentially consistent, meaning it appears as if all the instructions were executed sequentially. To achieve this, the paper proposes using a conflict graph similar to that used in distributed databases, which takes into account the program text to avoid the need for locks. This has implications for the design of multiprocessors and offers new optimization techniques for parallel languages with shared variables.
41493	4149315	On-line scheduling in the presence of overload	This article discusses the scheduling of sporadic tasks on a single processor. These tasks have varying arrival times, importance values, execution times, and deadlines. The objective is to maximize the total value of completed tasks. An online scheduling algorithm is proposed that achieves optimal performance when the system is not overloaded and provides a reasonable guarantee when the system is overloaded. The algorithm is efficient, with a time complexity of O(log n) per task, where n is the number of tasks in the system. The article also provides upper bounds on the best possible performance guarantee for online algorithms in different scenarios.
41494	41494140	Group sparse representation for image categorization and semantic video retrieval.	In this paper, a new approach to image representation called group sparse representation (GSR) is proposed for image classification and video retrieval. The idea is to represent a test image as a combination of all training images, using weight coefficients for each image and class. The method is based on a group nonnegative garrote model, resulting in sparse representations that are suitable for discriminant analysis. Experiments on various image and video datasets demonstrate the efficiency and effectiveness of the proposed approach. This approach shows promise in the field of multimedia content analysis and management.
41494	4149421	Sparse Unsupervised Dimensionality Reduction for Multiple View Data	This paper proposes a framework for extracting multiple high-dimensional visual features from a single image, treating the images as multi-view data. The goal is to find a low-dimensional consensus representation that leverages the complementary nature of the multiple views. The framework involves learning low-dimensional patterns from each view and constructing a consensus representation using a base matrix and loading matrix. To select the most discriminative features, the loading matrix is regularized with an $\\ell_{1}$-norm and the base matrix is constrained to be orthogonal. An alternating algorithm, called spectral sparse multiview embedding, is used to efficiently solve the problem. Additionally, a novel structured sparsity-inducing norm is imposed on the loading matrix to share information across subsets of the views. This method is called structured sparse multiview dimensionality reduction. Experiments on benchmark and real-world image data sets demonstrate the effectiveness of the proposed algorithms.
41495	41495195	Cartoon synthesis using constrained spreading activation network	This paper proposes a method for creating cartoons by controlling the character's path in a background image. Pre-experiments were conducted to compare different features such as edge, motion, and color, and a Cartoon Frame Relationship Network was created. A Constrained Spreading Activation Algorithm was used to select visually similar frames to generate the next frame. The synthesized cartoons were then coordinated with the background image's perspective. The experiment results showed that the proposed features were effective in evaluating similarity and the algorithm selected more similar frames compared to others. This approach can create visually smooth cartoons from an existing library.
41495	4149546	Retrieval-based cartoon gesture recognition and applications via semi-supervised heterogeneous classifiers learning	2D cartoon has become an essential aspect in many fields, but the process of creating it requires a lot of manual work. To address this issue, a heterogeneous cartoon gesture recognition method with various applications is proposed in this paper. The method involves assigning different features to express cartoon and human-subject images, followed by integrating shared structure learning and graph-based transductive learning into a joint framework for reliable classification. This framework allows for quantitative evaluation of similarities between cartoon and human-subject gestures. Extensive experiments on self-defined datasets have proven the effectiveness of this method. The paper also showcases the potential applications of this method in different areas of the 2D cartoon industry.
41496	4149676	Power-controlled feedback and training for two-way MIMO channels	In this paper, the authors discuss the use of feedback in communication systems and how it relates to channel state information. They point out that while most models assume perfect channel state information and noiseless feedback links, this is not the case in practical systems. The paper focuses on the achievable diversity multiplexing tradeoff using i.i.d. Gaussian codebooks, taking into account errors in channel estimation and feedback links in frequency division duplex systems. The authors present a key result that shows that having only one bit of feedback information is just as effective as having more bits, and that this one-bit performance is equivalent to having perfect channel state information and noiseless feedback. They also introduce the concepts of power controlled feedback and training, which are important in dealing with imperfect channel estimation and noisy feedback links. Additionally, they provide an asymptotic expression for the joint probability of the SNR exponents of eigenvalues of the actual channel and the estimated channel.
41496	41496131	Achievable diversity and multiplexing in multiple antenna systems with quantized power control.	The article discusses a multiple antenna system with limited feedback, where the transmitter uses quantized channel state information for power control. It is found that, like systems without feedback, there is a tradeoff between diversity order and multiplexing gain. However, in systems with only power control, it is not possible to achieve a non-zero diversity order at the maximum multiplexing gain. This is due to the analysis of the distribution of order statistics of the eigenvalues of the channel matrix, which is crucial in determining the diversity order. 
41497	4149776	Efficient implementation of 3G-324M protocol stack for multimedia communication	This paper discusses the implementation of 3G-324M, the multimedia transmission protocol stack for 3G communication, in order to support real-time video, audio, and data communication among different 3G handsets. The authors present efficient approaches and experiences in the implementation process, including event-driven communication, single-step message transformation, and serialization of nested multiplex table entries. The implementation has been successfully tested in a heterogeneous 3G communication environment and has shown satisfactory performance in transmitting real-time video, audio, and data. 
41497	4149720	Object-oriented design and implementations of 3g-324m protocol stack	This paper presents an object-oriented design and effective implementation of the 3G-324M protocol stack for real-time multimedia transmission. The focus is on the H.245 (control) and H.223 (multiplexing) protocols, which are part of the 324M class hierarchical structure. The implementation has been tested in 3G infrastructures in Hong Kong and China industries, showing efficient processing and transmission of real-time video, audio, and data. This implementation has the potential to improve the quality and efficiency of multimedia transmission in 3G networks.
41498	414987	Improving charging capacity for wireless sensor networks by deploying one mobile vehicle with multiple removable chargers.	Wireless energy transfer is a promising technology for extending the lifespan of wireless sensor networks (WSNs). However, existing studies on sensor charging assume the deployment of one or multiple charging vehicles, which may not be feasible for a real sensor network. This paper proposes a charging model where a single vehicle carries multiple low-cost removable chargers, each powered by a portable high-volume battery. By placing a charger near each energy-critical sensor, the vehicle can charge multiple sensors simultaneously. The paper also presents scheduling algorithms to minimize the dead duration of sensors and the travel distance of the vehicle. Experimental simulations show promising results for the proposed algorithms.
41498	4149825	Adaptive Transmission Power Control for Reliable Data Forwarding in Sensor Based Networks.	Wireless sensor networks (WSNs) often require high reliability for data forwarding to the sink, but this can be challenging due to the lossy nature of wireless channels. Broadcast technology can improve reliability by allowing data to be received by multiple nodes. The proposed APRF scheme uses cross-layer optimization to increase transmission power for nodes with remaining energy, reducing communication delay and data packet retransmissions. Theoretical and experimental results showed that APRF under an initial transmission power of 0 dBm reduced delay by 40.37% and increased data volume by 10.08%, with a 13.77% drop in network lifetime and a 10.08% reduction in reliability compared to a retransmission-only mechanism. 
41499	4149966	Attacks vs. Countermeasures of SSL Protected Trust Model	This paper discusses the weaknesses of current anti-spoofing methods and suggests a new SSL protected trust model. It also outlines attacks on this model and presents a solution in the form of the Automatic Detecting Security Indicator (ADSI) scheme. This scheme randomly generates and embeds images into the browser to detect spoofing attempts. When a mismatch is detected, an alarm is triggered to alert the user. This approach is less burdensome for users and requires minimal changes to the browser, making it easy to implement. It also does not require a Logo Certification Authority or personalization, making it suitable for use in internet cafes.
41499	4149971	Efficient group key management for multi-privileged groups	Multi-privileged group communications, which involve multiple data streams and varying levels of access privileges for group members, have been extensively studied in traditional wired networks and the Internet. However, with the rise of mobile and wireless networks, the integration of these networks with the traditional Internet has resulted in the formation of the mobile Internet. This has led to the need for multi-privileged group communications in the mobile Internet as well. In this paper, the authors review existing rekeying schemes for secure multi-privileged group communications and propose a new efficient scheme called ID-based Hierarchical Key Graph Scheme (IDHKGS). This scheme utilizes a key graph and unique IDs assigned to each node to handle user join/leave and access privilege changes, significantly reducing rekeying overhead.
41500	41500105	Spatio-temporal data evolutionary clustering based on MOEA/D	Evolutionary clustering, a method that evolves with time, is gaining importance in data mining research. It is effective in clustering dynamic data, but faces the challenge of considering two conflicting criteria: snapshot quality and history cost. Existing methods combine these into a single objective and use optimization techniques. This paper introduces a new approach using a multi-objective evolutionary algorithm based on decomposition (MOEA/D) to optimize both criteria in the evolutionary k-means algorithm (EKM). Results show that this algorithm outperforms EKM.
41500	4150050	A multi-objective evolutionary algorithm for the deployment and power assignment problem in wireless sensor networks	The paper discusses the importance of optimal locations and transmit power levels in designing a Wireless Sensor Network (WSN). It introduces the multiobjective Deployment and Power Assignment Problem (DPAP) and proposes a solution using the Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D). The MOEA/D approach decomposes the problem into smaller subproblems and solves them simultaneously, taking into account the objective preferences of each subproblem. This results in improved performance compared to traditional methods, as shown in simulation results. The proposed approach provides a diverse set of high quality network designs, making it easier for decision makers to choose the best option. 
41501	4150178	Assessing the Performance of a Novel Tag-Based Reader-to-Reader Communication Paradigm Under Noisy Channel Conditions.	RFID technology has enabled a wide range of applications beyond simple identification of people and goods. In this paper, a new approach is explored where the remaining data storage capacity of passive RFID tags is utilized as a virtual communication channel. This allows for the transmission of information between the tag and reader, expanding the potential uses of RFID technology. The paper discusses the potential benefits and challenges of this approach and highlights its potential for innovative applications.
41501	4150144	Tag-based cooperative data gathering and energy recharging in wide area RFID sensor networks	The Wireless Identification and Sensing Platform (WISP) combines RFID technology and wireless sensors to create a powerful identification and sensing platform. However, challenges arise with the need for regular recharging of WISPs, hindering the deployment of large-scale RFID sensor networks (RSNs). In order to address this issue, the paper presents cooperative solutions that use RFID technology for both data transmission and energy transfer. These solutions involve RFID mobile readers gathering data from WISPs and wirelessly recharging them, as well as cooperating with each other to reduce data delivery delay. This is achieved through clustering WISPs, determining the number of needed mobile readers, and planning their tour based on energy and time constraints. Simulations show that these solutions are more effective in handling scalability issues compared to traditional approaches.
41502	4150233	Developing Knowledge-Based Intelligent Multimedia Tutoring Systems Using Semantic Content-Based Modelling	The paper explores the challenges of incorporating multimedia into traditional intelligent tutoring systems (ITS). It proposes a new model, using multimedia frames (m-frames), to integrate multimedia syntax and semantics into the ITS architecture. The model is applied in the development of ARISTOTLE, an intelligent multimedia tutoring system (IMTS) designed to teach young children about zoology. The paper highlights the potential of the model for developing effective IMTSs and discusses its implementation in ARISTOTLE. This research contributes to the development of knowledge-based IMTSs, which can effectively utilize semantic video and audio content.
41502	4150221	Integrating logical video and audio segments with content-related information in instructional multimedia systems	The use of multimedia in instructional systems has become a common way to deliver large amounts of information. However, this often leads to students feeling overwhelmed and disoriented due to the lack of guidance and organization in the content. To address this issue, a new architecture for instructional multimedia systems has been proposed. This architecture incorporates pedagogy and individualization to reduce information overload and disorientation. It also utilizes multimedia frames (m-frames) to index and integrate content, particularly in video and audio segments. A system called MAT, which teaches young children about animals, is used as an example to demonstrate the effectiveness of this architecture.
41503	4150331	A framework for adaptive parameter estimation with finite memory	The article discusses the problem of estimating an unknown parameter using sequential measurements from different statistical experiments. The approach involves adaptively selecting the next experiment based on past observations, using a finite memory framework. This is achieved through finite-state parametric Markov chains, with the asymptotic performance of the estimation scheme being linked to the steady-state distributions of these chains. The optimal selection of experiments is reformulated as designing a family of Markov chains with desired steady-state distributions. The article also suggests a quantitative criterion for optimal selection based on minimax ratio regret. 
41503	4150314	Bits From Photons: Oversampled Image Acquisition Using Binary Poisson Statistics	This study introduces a new image sensor that works similarly to traditional photographic film, with each pixel giving a 1-bit measurement of light intensity. The performance of this sensor is analyzed using a quantized Poisson statistics method, and it is shown that with a single-photon quantization threshold and high oversampling factors, the estimation variance is close to that of an unquantized sensor. The maximum-likelihood estimator is proven to be globally optimal and numerical results support the effectiveness of the proposed image reconstruction algorithm. The study also suggests potential use of this sensor in high dynamic range photography.
41504	4150427	Distributed Memory Partitioning of High-Throughput Sequencing Datasets for Enabling Parallel Genomics Analyses	High-throughput sequencing instruments can now process billions of short genomic fragments in a single run, referred to as 'reads'. These datasets have many applications in genomics, metagenomics, and transcriptomics, but analyzing them can be computationally and memory intensive. In this paper, a parallel algorithm is introduced for partitioning large read datasets for distributed-memory parallel analyses. The algorithm also constructs and partitions the associated de Bruijn graph, allowing for applications like de novo assembly to utilize the partitions. A quality evaluation mechanism is proposed and the algorithm is shown to produce high quality partitions. The implementation is available on github.com/ParBLiSS/read_partitioning.
41504	4150416	A Parallel Algorithm for Spectrum-based Short Read Error Correction	High-throughput DNA sequencing involves correcting sequence errors by utilizing redundant sampling and low error rates. This is crucial in many applications of this technology and has led to the development of several error correction methods. With the increasing throughput of these technologies, some are now generating billions of reads per run. In this paper, a parallel algorithm is proposed for error correction methods that use the frequency spectrum of kmers in input reads. This method is specifically designed for distributed memory parallel computers and clusters and has been shown to achieve near linear speedup and the ability to handle larger data sets. The approach is based on the Reptile method, which utilizes two different lengths of frequency spectrum for identifying correction possibilities and providing contextual information. 
41505	4150536	Antiweb-wheel inequalities and their separation problems over the stable set polytopes	A stable set in a graph is a group of vertices that are not connected to each other. Finding the maximum weight stable set is a difficult problem. One approach is to optimize a linear function over the convex hull STAB(G) of incidence vectors of stable sets. However, it is difficult to find a concise characterization of STAB(G) using linear inequalities. Instead, the goal is to find large classes of valid inequalities that can be efficiently separated. Some examples of these classes include trivial, edge, cycle, and wheel inequalities. This paper introduces a new class called (t)-antiweb-s-wheel inequalities, which is a combination of (t)-antiweb and wheel inequalities. Efficient separation algorithms are also provided for these new inequalities.
41505	4150520	Influence Digraphs Induced by Time-Stamped Graphs (Extended Abstract)	A time-stamped graph is an undirected graph with real numbers assigned to each edge. The influence between vertices is determined by the presence of an increasing path. The induced influence digraph is a directed graph that records these influences. This paper focuses on the realizability problem, which asks if a given parameter value can be achieved in a time-stamped graph. The problem is solved for the parameter of number of arcs, with the restriction that the graphs are trees. It is found that all realizable values can be achieved by a tree homeomorphic to K2 or K1,3. The paper also explores other related questions and has been submitted to a journal for review.
41506	41506118	Auswahl von Kameraaktionen zur wissensbasierten Szenenexploration	The process of scene exploration is based on selecting camera parameters for capturing an image that must be optimal for further processing, as demanded by the strategy of active vision. This article discusses the selection of camera parameters through a selection of camera actions represented as concepts in a knowledge base of a semantic network. The knowledge base also contains information about the scene and objects involved in the task. To select the optimal camera action from multiple executable actions at a given time, a scoring calculation is required. The article introduces a new scoring calculation based on ideas from decision theory. The camera actions are scored based on their utility, which is determined by evaluating the instances of the objects in the scene. The effectiveness of this approach is demonstrated through the exploration of an office scene.
41506	41506107	Erweiterte Realität und 3-D Visualisierung für minimal-invasive Operationen durch Einsatz eines optischen Trackingsystems	Optical tracking systems offer the highest level of accuracy for determining the position of a moving object. Therefore, it is logical to use such a system for determining the position of an endoscope in minimally invasive surgery, in order to support the surgeon with a three-dimensional model of the surgical area (and thus the ability for three-dimensional viewing) as well as augmented reality. The use of optical tracking systems for this purpose has not yet been investigated. For the first time, this article presents a method for markerless registration of an endoscope with CT/MR data. The developed methods require only a few minutes of processing time and are therefore suitable for use in the operating room. 
41507	4150714	Relevant values: New metadata to provide insight on attribute values at schema level	Research on data integration has led to the development of languages and systems that can effectively combine different data sources to create a unified representation. However, most existing approaches only consider intensional knowledge, ignoring the valuable information provided by extensional knowledge. To address this limitation, the authors propose a technique that adds "relevant values" to the intension of an attribute, extracted from its values. These relevant values not only enrich the schema with domain knowledge, but also aid users in creating and refining queries. The technique is automatic, independent of the attribute domain, and utilizes data mining clustering techniques and emerging semantics from data values. It can be customized with various similarity metrics and is particularly useful for managing frequently changing sources, such as in the Semantic Web.
41507	4150740	A New Type of Metadata for Querying Data Integration Systems	Research on data integration has led to the development of languages and systems that can effectively combine multiple data sources into a single integrated representation. However, most approaches only consider intensional knowledge and do not take into account extensional knowledge. In this paper, the authors propose a method to enhance the intension of attributes by incorporating "relevant values," which are extracted from the actual attribute values. This not only enriches the data schema with domain knowledge, but also aids users in creating and refining queries. The proposed technique is automatic, works for any attribute domain, and utilizes data mining and semantics to handle constantly evolving data sources.
41508	415087	Offering A Product Recommendation System in E-commerce	The paper presents a product recommendation system for Business-to-customer e-commerce that uses both explicit and implicit ratings. The system recommends products based on the purchase patterns of previous users who have similar patterns to the new user. It uses a weighted cosine similarity measure to find the closest user profile in the database. The system also incorporates Association rule mining to improve the recommendations. Time of transaction is also taken into account to avoid sequence recognition problems. Experimental results show that the proposed method performs well with implicit ratings and the use of association rule further enhances the system's performance.
41508	4150817	Design of SMACA: synthesis and its analysis through rule vector graph for web based application	A web search engine uses indexing to organize web-pages in a database. Both forward and inverted indexing are used to efficiently retrieve data based on user queries. This paper proposes a solution to improve indexing using non-linear single cycle multiple attractor cellular automata (SMACA) and analyzes it using rule vector graph (RVG). SMACA allows for efficient searching with O(n) complexity and provides implicit memory for storing patterns. The search operation involves running a cellular automata (CA) for one time step and requires storage of the CA rule vector (RV) and seed values. SMACA is based on the theoretical foundation of CA technology. 
41509	4150989	A NOC closed-loop performance monitor and adapter	In a Network-on-Chip (NoC), the number of buffers allocated to each communication channel affects performance and power consumption. A runtime mechanism is needed to automatically adjust buffer size based on communication patterns. This paper proposes a control mechanism to resize buffers in an adaptive router, monitored and controlled for each channel. It also presents a technique to isolate faulty buffers, which are crucial for communication performance. Experimental results show that the proposed architecture can decrease latency by 80% and increase throughput by 45% in the absence of faults. In the presence of faults, the proposed architecture maintains performance while saving up to 25% power compared to a homogeneous router.
41509	41509104	Reconfigurable Routers for Low Power and High Performance	Network-on-chip (NoC) designs aim to balance factors such as latency, power dissipation, and energy. However, setting all parameters at design time can lead to either excessive power usage or higher latency when application communication patterns change. Large buffer sizes can ensure performance for different applications, but also contribute significantly to overall router power consumption. This is worsened by the fact that buffers are often sized for worst case scenarios, leading to extra power usage for more common cases. To address this issue, the proposed solution is a reconfigurable router that dynamically allocates buffer slots to improve efficiency under varying communication loads. This approach can result in up to 52% power savings and a 64% smaller buffer size while maintaining performance similar to that of a homogeneous router.
41510	415106	Model Checking for Modal Dependence Logic: An Approach Through Post's Lattice	This paper explores the computational complexity of model checking in an extended version of modal dependence logic. This extension allows for the use of arbitrary Boolean connectives, building upon the original version introduced by Jouko V\"a\"an\"anen which included the dependence atom dep(.). The study utilizes a Lattice approach developed by Emil Post to classify all possible Boolean functions, considering various fragments of the logical language that incorporate modalities $\Diamond$ and $\Box$, the dependence atom, and logical symbols for arbitrary Boolean functions. 
41510	4151057	The Complexity of Reasoning for Fragments of Autoepistemic Logic.	Autoepistemic logic, introduced by Moore in 1985, is an extension of propositional logic that includes the modal operator L, indicating belief. It is used to model the behavior and reasoning of a rational agent about their own beliefs. This article examines the computational complexity of the three main decision problems in autoepistemic logic: expansion existence, brave reasoning, and cautious reasoning. It also looks at the complexity of checking if a given set of formulae represents a stable expansion and counting the number of stable expansions in a knowledge base. The authors improve on existing upper bounds and provide the first analysis of the counting problem in autoepistemic logic.
41511	4151151	Empowering Evolving Social Network Users with Privacy Rights.	The issue of privacy on social networks is a major concern and there is ongoing debate about how to effectively protect users' privacy rights. Many ideas have been proposed, but no single approach has emerged as the leading solution. This paper introduces a new conceptual model for privacy in social networks, highlighting its uniqueness compared to current research. The authors also discuss a potential query language, called PiQL, that was developed based on this model to support user-driven privacy policy creation and enforcement. The strength of this model is its ability to be extended through the use of linguistic constructs in query languages like SQL, as demonstrated in PiQL.
41511	4151117	An Algebraic Language for Semantic Data Integration on the Hidden Web	Semantic integration in the hidden Web is an evolving research field that requires unique techniques due to frequent changes, conflicts, and the large amount of data. The paper introduces Integra, an algebraic language, and BioFlow, a query language, for integrating Life Sciences data on the hidden Web. The algebra treats web forms as user-defined functions and their responses as traditional relations. This allows for the extension of traditional relational algebra to include integration primitives such as schema matching, wrappers, form submission, and object identification. Two new operators, link and combine, are also proposed for horizontal and vertical integration. The algebra can be applied to traditional relations without integration and is built on existing literature.
41512	4151244	Static Evaluation of Software Architectures	The software architecture is a crucial aspect of the software system lifecycle as it directly affects business goals, functional and quality requirements. Architecture evaluations are essential in determining the effectiveness of the architecture for its intended usage. This paper shares practical experience and outlines when and how static architecture evaluations contribute to architecture development. Ten purposes and needs for static architecture evaluations are identified, supported by case studies from industry and academia. The results of architecture evaluations influence subsequent steps in architecture development.
41512	4151217	Making testing product lines more efficient by improving the testability of product line architectures	Product line engineering has been proven to be a beneficial approach to software development, resulting in cost and time savings for organizations. However, the testing process has not kept up with these advancements and the relative cost for testing product lines is now higher than traditional single system development. This is due to technical and organizational challenges that hinder the pace of testing in product line engineering. To address this issue, this paper proposes considering testability during architectural design to improve the efficiency and effectiveness of testing. The relationship between testability and product line architecture is explored, emphasizing the importance of high testability for reducing testing effort and achieving coverage criteria. A systematic approach is also outlined to help organizations improve and evaluate the testability of their product lines at the architectural level. 
41513	415134	Matching parse thickets for open domain question answering.	Parse thickets are a unified representation of a paragraph of text that combines traditional parse trees with anaphora and rhetoric information. They are useful for answering complex questions and improving search accuracy. The operation of generalization measures the distance between paragraphs of text by finding the maximal common sub-graph of two parse thickets. This technique is evaluated in various search domains and has shown to improve search accuracy compared to other methods. An open source plug-in for SOLR has been developed for easy integration with industrial search engines. The impact of different sources of discourse information on the search accuracy is also analyzed. 
41513	4151321	Exhaustive simulation of consecutive mental states of human agents	The article discusses the development of a software component that can simulate the mental states of human agents. This approach is based on exhaustive search and uses a logic program in a natural language multiagent mental simulator. This software is capable of generating a wide range of possible mental states, including beliefs, desires, and intentions, and can be applied in various domains without the need for specific training data. The component has been evaluated for correctness, coverage, and complexity and can be integrated into systems such as eBay's human behavior simulation system, personalized assistants, and decision support systems. It has the potential to improve predictions of human behavior in various situations.
41514	4151433	The mean-field computation in a supermarket model with server multiple vacations	The study of queueing networks with server vacations is limited but interesting and challenging. This paper presents a unified method for analyzing a supermarket model with multiple server vacations. By using functional analysis and an operator semigroup, the paper derives an infinite-dimensional system of differential equations and provides a mean-field limit for the Markov processes. An algorithm is also provided to compute the fixed point of the system, which is used to analyze the performance of the supermarket model, including stationary queue length and expected sojourn time. The paper also includes numerical examples to demonstrate the impact of crucial factors on performance. This approach can be applied to other complex supermarket models, making it useful for practical applications in areas such as computer networks and transportation systems.
41514	4151445	Using Index in the MapReduce Framework	MapReduce is a programming framework designed by Google for processing large amounts of data. It works by splitting data into blocks and using Maps to process each block, then combining the outputs with Reduces. However, this approach can be inefficient when only a subset of the data is needed, as it still processes irrelevant data and requires a large number of Maps. To address this issue, the use of indexes in MapReduce has been proposed, where only a few Maps are generated and data is accessed through an index. A cost model has been developed to compare this approach with the traditional full scan execution and can be used to choose the most efficient execution mode for a given query. Experiments have shown that the index access execution outperforms full scan execution when the predicate selectivity is low, and the cost model accurately predicts the execution cost.
41515	4151537	EFFORT: energy-efficient opportunistic routing technology in wireless sensor networks.	 The effective use of energy in wireless sensor networks (WSNs) is crucial and routing mechanisms play a significant role in energy consumption. Existing routing schemes have two common issues: certain sensors may quickly drain their energy and packet retransmissions over unreliable links can consume significant energy. To address these issues, a new energy-efficient routing scheme called EFFORT is proposed. It maximizes data gathering in WSNs by utilizing opportunistic routing and a new metric to determine suitable forwarders and relay priorities for each sensor. EFFORT outperforms other routing protocols in terms of energy efficiency and network lifetime, as shown in simulation results. This paper is copyrighted by John Wiley & Sons, Ltd in 2011.
41515	415158	Joint Design of Asynchronous Sleep-Wake Scheduling and Opportunistic Routing in Wireless Sensor Networks	Wireless sensor networks face the challenge of designing a routing system that maximizes their lifetime, due to unreliable links and limited power supply. A recent approach is using opportunistic routing, which takes advantage of path diversity and improves transmission reliability. This paper proposes a joint design of asynchronous sleep-wake schedules and opportunistic routing, called ASSORT, to further extend the network's lifetime. By appropriately arranging sensor nodes to sleep, energy consumption is reduced. Simulation results demonstrate that ASSORT outperforms other routing schemes in terms of network lifetime extension. This highlights the effectiveness of combining opportunistic routing and sleep-wake scheduling for maximizing the lifetime of wireless sensor networks.
41516	4151657	The McEliece Cryptosystem Resists Quantum Fourier Sampling Attacks	Quantum computers have the potential to break traditional public-key cryptosystems like RSA and El Gamal by being able to factor large integers and extract discrete logarithms. To prepare for the possibility of quantum computers becoming a reality, researchers have been working on developing \emph{post-quantum} cryptosystems that can be implemented using classical computers now, but will remain secure against quantum attacks. This article focuses on the McEliece cryptosystem, which uses \emph{well-permuted, well-scrambled} linear codes and has been shown to be resistant against attacks based on generating and measuring coset states. The authors show that the natural case of the Hidden Subgroup Problem, to which the McEliece cryptosystem reduces, cannot be solved using strong Fourier sampling or measurements of coset states. By extending this result to subgroups of arbitrary structure, including the automorphism groups of linear codes, the authors provide the first rigorous results on the security of the McEliece cryptosystem against quantum adversaries. This strengthens its potential as a post-quantum cryptosystem.
41516	415164	The Symmetric Group Defies Strong Fourier Sampling	The technique of Fourier sampling, developed by Bernstein, Vazirani, Simon, and Shor, has led to significant speedups in quantum algorithms compared to classical counterparts. This approach has been successful in solving problems for abelian groups, such as factoring and discrete logarithms. However, it has been found that this method is not effective in resolving the hidden subgroup problem for symmetric groups. This means that the Graph Isomorphism problem cannot be solved using this approach. To overcome this limitation, future quantum approaches may need to use entangled measurements on multiple coset states instead of measuring individual coset states. 
41517	4151752	Robust network supercomputing without centralized control	Traditional approaches to network supercomputing rely on a central master process and numerous worker processes, but this can lead to performance issues and a single point of failure. To address this, a decentralized algorithm is proposed where workers can determine if tasks have been completed and collect results locally. The algorithm has a failure model where the probability of a worker returning a wrong result is less than 1/2. It is proven that the algorithm, which operates synchronously and randomly, can successfully complete n tasks in Θ(log n) rounds with a high probability of correct results. The message complexity is Θ(n log n) and the bit complexity is O(n2 log3 n).
41517	4151743	Brief announcement: decentralized network supercomputing in the presence of malicious and crash-prone workers	Internet supercomputing is a method of solving complex problems by utilizing a large number of interconnected computers. Previous research has introduced a decentralized approach and provided a synchronous algorithm for performing independent tasks with a high probability of success, even when dealing with malicious behavior. However, this approach made a strong assumption about the average probability of correct results from non-crashed processors. This work presents a more efficient algorithm that can handle a stronger adversary in a failure model with crashes. The algorithm has a work complexity of Θ(tlogn), message complexity of Θ(nlogn), and bit complexity of O(tnlog3n). It can successfully complete t tasks with n processors in Θ(t⁄n logn) rounds with a high probability of correct results.
41518	4151812	Towards automation & augmentation of the design of schedulers for cellular communications networks.	Evolutionary Computation is a technique used to automatically improve small cell schedulers in a realistic 4G-LTE cellular network simulation. These evolved schedulers are then further enhanced by human design to increase their robustness. Extensive analysis shows that evolution has discovered a new scheduling technique that performs well across cells of varying sizes and conforms to accepted scheduling frameworks. The evolved solutions outperform a human-engineered benchmark by up to 50%. This approach is also shown to be flexible, allowing for tailored algorithms to be evolved for specific scenarios and corner cases. This work, published in Evolutionary Computation in 2018, suggests that this method can help network operators create unique algorithms for different deployments and delay costly hardware upgrades.
41518	4151873	Multi-level Grammar Genetic Programming for Scheduling in Heterogeneous Networks.	Heterogeneous Networks (HetNets) allow telecommunication companies to better utilize their network resources through Co-ordination of Inter-Cell Interference (CICI) using scheduling algorithms. The current state-of-the-art for scheduling in HetNets is the Grammar-Guided Genetic Programming (GGGP) algorithm, which evolves an expression from a given grammar to schedule transmissions. However, this may not always result in optimal performance. In this work, we explore the use of a layered grammar approach to improve the results obtained by GGGP. Our experiments show that starting with a restricted grammar and gradually expanding it after 10 generations outperforms GGGP, regardless of the initial population generation algorithm or maximum initial tree depth. This demonstrates the potential for improving CICI in HetNets through the use of layered grammars.
41519	4151981	Stabilization of nonlinear delay systems using approximate predictors and high-gain observers	The article presents a solution to the problem of stabilizing nonlinear systems with long delays at the input and output, using only output feedback. The solution is global and uses the predictor approach over a period that combines the delays, and can handle nonlinear systems with sampled measurements and control applied using a zero-order hold. The sampling/holding periods should be short but do not need to be constant. The proposed approach is based on a class of Lipschitz strict-feedback systems with disturbances and involves a successive approximation of the predictor map, a high-gain sampled-data observer, and a linear stabilizing feedback. The method is robust to perturbations in the sampling schedule and can handle different sampling and holding periods. The approach is also applicable to linear systems, where the predictor can be explicitly calculated. 
41519	41519212	Predictor-Based Output Feedback for Nonlinear Delay Systems	This article presents two solutions to the problem of stabilizing nonlinear systems with long delays at the input and output using only output feedback. Both solutions are global and use the predictor approach, combining input and output delays. They also account for systems with sampled measurements and control applied through a zero-order hold, with the requirement of short (but not necessarily constant) sampling periods. The first approach is applicable to general nonlinear systems with an explicit solution map, while the second approach is specific to globally Lipschitz strict-feedback systems with disturbances. The article provides two examples, demonstrating the effectiveness of these solutions.
41520	4152010	Evaluation of semi-automatically generated accessible interfaces for educational games.	The use of videogames in educational settings, known as serious games, is becoming increasingly popular. However, this also raises ethical concerns about accessibility for all students, as videogames can be difficult for some to access. To address this issue, researchers have explored the potential of automatic adaptations to game interfaces, specifically for point-and-click adventure games, a popular genre in serious games. The adapted games were tested with users who have disabilities, and while they were found to be usable, there were still some usability issues that impacted the overall experience. The researchers suggest further improvements and a refined interface model to address these limitations. 
41520	4152032	A narrative metaphor to facilitate educational game authoring	The paper introduces WEEV, a methodology for educators to collaborate in creating educational point-and-click adventure games using a narrative-based approach. WEEV uses a visual language to represent the story flow and has been implemented into a tool based on an established platform for educational games. The tool was improved through feedback from evaluations and testing by educators, but there are still some user-interaction issues and the educational effectiveness of the resulting games needs further analysis. However, the paper suggests that WEEV can simplify the game creation process and provide educational value by using representations that educators can understand.
41521	4152140	An Analysis of Reliable Delivery Specifications for Web Services	The delivery of messages is crucial for the success of Web Services and there are two main specifications, WS-Reliability and WS-ReliableMessaging, competing in this area. This paper provides an analysis of these specifications, focusing on their similarities and differences. The goal is to identify any gaps and provide recommendations for improvement. The analysis reveals differences in philosophies and proposes ways to address the gaps. This information is important for developers and users of Web Services to understand and utilize these specifications effectively. 
41521	4152125	Experiences in Deploying Services within the Axis Container	The Web Services framework is based on the Service Oriented Architecture model, which allows for the development of applications that are loosely coupled and easier to manage. These applications are typically hosted within a Web Service Container, with several options available depending on the platform and programming language. This paper focuses on Java-based applications and specifically looks at the Apache Axis Web Service container, which is the most popular choice for Java. The paper discusses the experiences of deploying Web Services, specifically WS-ReliableMessaging, within this container, highlighting the problems and limitations encountered and providing solutions. The paper also includes recommendations to improve the flexibility of the container for more complex Web Service applications.
41522	415228	A Method Suitable for Vicarious Calibration of a UAV Hyperspectral Remote Sensor	A sensor on a UAV is prone to vibration and natural elements like wind, making calibration challenging. Vicarious calibration, which is closer to real-world conditions, is used alongside laboratory calibration for remote sensors. However, the existing vicarious calibration for UAVs only uses a reflectance-based method, not taking into account the largest source of uncertainty - aerosol-type assumptions. To address this, an improved irradiance-based method is proposed, which considers the difference in radiative transfer between satellites and UAVs. Simulation and field experiments show that this method has higher accuracy and lower uncertainty, making it more suitable for vicarious calibration of UAV sensors. 
41522	4152230	Vicarious calibration of Beijing-1 multispectral imagers	In 2008, a field calibration was conducted at the Dunhuang site to calibrate the multispectral imagers on the Beijing-1 satellite. Three methods were used to calculate the top-of-atmosphere radiance: reflectance-based, irradiance-based, and cross-calibration. The reflectance-based method had an error of over 10% when the aerosol optical depth was above 0.2, while the cross-calibration method had an error of less than 7% when carefully selected images were used. The final calibration coefficients were obtained from the irradiance-based data on September 6, 2008, with an estimated error of less than 5%. The results showed that the field calibration was successful in identifying and minimizing errors, ensuring accurate data from the Beijing-1 satellite.
41523	4152330	Adaptive Markov Random Field Approach for Classification of Hyperspectral Imagery	This letter introduces an adaptive Markov random field (MRF) technique for classifying hyperspectral imagery. The method incorporates a relative homogeneity index for each pixel to determine the spatial contribution in the MRF classification, preventing overcorrection in areas with high spatial variation. Support vector machines are also used for improved class modeling and estimation of spectral contribution. Experiments on synthetic and real hyperspectral data show that this approach outperforms traditional methods in accurately classifying both homogeneous regions and class boundaries.
41523	4152331	Aggregation Decoding for Multi-failure Recovery in Erasure-Coded Storage	Data reliability is crucial for large-scale storage systems, and erasure codes have been used to ensure high reliability through data recovery. However, this also leads to a significant amount of data being transmitted in the network, which can greatly affect the performance of the cluster. Previous research has mainly focused on reducing data transmission cost for single-failure recovery, but this does not efficiently support multi-failure recovery. To address this issue, this work introduces the Mean Time To Multi-Failure (MTTMF) metric and proposes using Aggregation Decoding, a network topology-aware erasure coding scheme, for multi-failure recovery. Additionally, two methods are proposed to reduce redundant transmission in multi-failure recovery. Experimental results show that this solution can save up to 40% of bandwidth costs.
41524	4152417	A Novel MKL Model of Integrating LiDAR Data and MSI for Urban Area Classification	The HF-MKL model is a novel approach for urban classification that combines features from two data sources, spectral images and LiDAR data. This model utilizes Gaussian kernels with different bandwidths to measure the similarity of samples at different scales, and then integrates these multiscale kernels using a linear combination. The weights of the different kernels are determined by finding a projection based on maximum variance, allowing for the discriminative ability of the heterogeneous features to be exploited and integrated. The model is optimized using a support vector machine and experiments on real data sets have shown that it outperforms other state-of-the-art algorithms in terms of classification accuracy.
41524	4152485	Hyperspectral Image Classification Using Convolutional Neural Networks and Multiple Feature Learning.	This paper introduces a new approach for hyperspectral imagery (HSI) classification using a combination of convolutional neural networks (CNNs) and multiple feature learning. The proposed method utilizes a novel CNN architecture with various features extracted from raw imagery as input. These features are then combined in a concatenating layer to form a joint feature map, which is used to predict the final labels for each HSI pixel. This approach not only improves feature extraction through CNNs, but also utilizes both spectral and spatial information for more accurate classification. The effectiveness of this method is demonstrated through experiments on three benchmark data sets, showing a significant improvement in classification accuracy.
41525	4152520	Distributed multi-robot coordination in area exploration	The paper suggests a reliable and efficient coordination algorithm for multiple robots with limited communication capabilities. It uses a distributed bidding model and incorporates measures to address the restricted range. The bidding algorithm takes into account the distances between robots, promoting close proximity. The paper also introduces a map synchronization mechanism to reduce data exchange when subnetworks merge. Simulation results validate the effectiveness of these measures in improving coordination and handling the limited communication range. This algorithm makes coordination more realistic in multi-robot applications.
41525	4152562	Modeling multiple robot systems for area coverage and cooperation	This paper proposes a distributed model for cooperative multiple mobile robot systems. The mobile robots in this system have sensing, computation, and communication capabilities and share sensory information through an ad hoc wireless network. The use of Voronoi diagram and Delaunay triangulation allows for modeling of area coverage and cooperation in the mobile sensor network. The paper also presents a fault tolerant algorithm for autonomous deployment of the robots, enabling the system to reconfigure itself and increase the area covered. Additionally, a formation control algorithm is discussed, allowing the mobile sensor network to track moving targets and cover a larger area along specific paths.
41526	4152684	Searching and Browsing Collections of Structural Information	This paper proposes a new approach to querying structured textual information, such as SGML/XML documents. Utilizing knowledge about the structure of documents can improve retrieval by allowing for more precise information needs. However, traditional probabilistic retrieval models have difficulty handling structural information. To address this, the paper presents a new retrieval function based on the probabilistic model that can handle structural roles assigned to individual terms. Efficient evaluation of queries in this framework requires appropriate index structures, including text and structure indexes. The implementation also includes additional features, such as a table of contents for browsing. Initial evaluations show promising results on collections of unstructured documents.
41526	4152679	Structured name-spaces in secure databases	In order to ensure confidentiality in secure logic-based databases, it is necessary to modify the intended model of the databases. This can be achieved through changes in the databases' name-space, or signature, which helps maintain a high level of confidentiality. The study focuses on extensions to an open databases universal name-space that preserve its static and dynamic semantics. These structured name-spaces can reflect the sharing of protection units among user groups and the need to keep them confidential. Additionally, for multi-level databases, these extensions can be incorporated into a hierarchical name-space based on the security levels. This allows for a more effective management of confidentiality demands. 
41527	415270	Dynamic User Demand Driven Online Network Selection.	Network selection is crucial in maximizing the benefits of different wireless networks. In order to improve user experience, researchers have studied how to select the best network in a dynamic environment where user demand and network handoff costs constantly change. Dynamic network selection is a promising solution, but it struggles to balance meeting user demand and minimizing handoff costs. To address this issue, an online network selection algorithm is proposed which learns the optimal policy while considering handoff costs. Two additional algorithms are also developed, taking advantage of inherent dependencies in the problem for faster convergence. Simulations demonstrate that these algorithms outperform existing methods by 10%.
41527	4152750	Exploiting User Demand Diversity in Heterogeneous Wireless Networks	Radio resource management (RRM) is important for maximizing efficiency in different types of wireless networks. While previous efforts have focused on using network diversity to improve throughput, this approach ignores the impact of user demand. To address this, a new concept called user demand diversity gain is introduced, which takes into account the personalized demands of users when assigning radio resources. This leads to a shift in optimization from maximizing throughput to maximizing quality of experience (QoE) for users. A game formulation called QoE game is proposed, and a distributed learning algorithm is developed to achieve a QoE equilibrium. A cloud-assisted learning framework is also proposed to reduce the cost of the learning algorithm. Simulation results show that the user demand diversity gain and the proposed algorithm improve system efficiency and fairness in QoE. 
41528	41528109	Data Quality Guided Incentive Mechanism Design for Crowdsensing.	Crowdsensing relies on participants using their physical resources and manual efforts to collect data. However, low quality data can harm the accuracy and availability of crowdsensing services. Existing incentive mechanisms have not addressed the issue of data quality. This paper proposes a quality-based incentive mechanism that pays participants according to their data's effectiveness. The mechanism estimates data quality and rewards participants accordingly, aiming to motivate them to efficiently perform tasks. The mechanism was implemented and evaluated, showing improved service quality and profit for the service provider compared to traditional data collection models and uniform pricing schemes. This highlights the importance of considering data quality in the design of incentive mechanisms for crowdsensing.
41528	41528119	Online Auctions with Dynamic Costs for Ridesharing	Ridesharing companies like Uber and Lyft have been successful in the business world, but they still face challenges such as a lack of a well-designed rush hour pricing strategy and waste of seat resources. The online ridesharing model is more practical, but it presents difficulties in design. To address these issues, the authors propose a complementary online auction design called the No Preemption auction. This auction mechanism is truthful, budget balanced, computationally efficient, and makes efficient use of seat resources. The authors also provide theoretical proof of its effectiveness and evaluate its performance using real taxi-trace data in Shanghai, which showed positive results in terms of revenue and social welfare.
41529	415293	Radar waveform design in a spectrally crowded environment via nonconvex quadratic optimization	This paper addresses the challenge of designing radar signals in a crowded spectrum, where there is a high demand for both military and civilian wireless services. The goal is to create optimized radar waveforms that are compatible with other licensed electromagnetic radiators. The authors propose using a radio environmental map to guide the waveform optimization process, which focuses on improving radar performance while adhering to spectral constraints. The feasibility of this approach is thoroughly examined, and a solution technique is proposed that involves transforming the problem into a simpler convex optimization task. The resulting waveforms are evaluated based on their signal-to-interference plus noise ratio, spectral shape, and autocorrelation function. The analysis shows that the proposed method can effectively balance these factors to achieve optimal radar performance in a crowded spectrum.
41529	4152919	Code design for radar STAP via optimization theory	This paper discusses the issue of optimizing radar space-time adaptive processing (STAP) codes in the presence of colored Gaussian disturbance. A code design algorithm is proposed that aims to maximize detection performance while controlling for temporal and spatial Doppler estimation accuracy, as well as similarity with a pre-fixed radar code. The problem is solved using a convex relaxation method known as semidefinite programming (SDP). The resulting optimal solution is obtained through a rank-one decomposition of the relaxed solution. The algorithm's performance is evaluated using simulated data and the KASSPER datacube. 
41530	41530200	Fuzzy OLAP association rules mining based novel approach for multiagent cooperative learning	The paper introduces a new approach for multiagent learning in cooperative learning systems. This approach incorporates fuzziness and online analytical processing (OLAP) based data mining to effectively process information from the agents. Using this method, the actions of other agents can be estimated even if they are not in the visual environment of the agent being considered. This is done by extracting online association rules from a data cube. Additionally, a new action selection model is proposed based on association rule mining. The paper also discusses generalizing states by mining multiple-level association rules from a fuzzy data cube. Results from a pursuit domain demonstrate the effectiveness and robustness of this approach.
41530	41530258	Employing OLAP mining for multiagent reinforcement learning	This paper introduces a new learning method that combines online analytical processing (OLAP) and data mining techniques. The authors first outline a data cube OLAP architecture that stores and processes data reported by agents. This allows for the estimation of another agent's actions, even if they are not in the visual environment. They then present an action selection model based on association rules mining. Lastly, they use multiple-level association rules to generalize states that have not been experienced enough. Experiments on a pursuit domain demonstrate the success of this approach.
41531	4153123	From knowledge based software engineering to knowware based software engineering	The paper discusses the development and evolution of PROMIS, a knowledge-based software engineering system introduced in the 1990s. PROMIS aims to automatically generate applications using both domain knowledge and software knowledge. However, the system lacked a suitable representation for domain knowledge. Recently, the concept of knowware, a commercialized form of domain knowledge, has been introduced and is used in conjunction with PROMIS and J2EE to generate applications. The paper also introduces the definitions of knowware, knowledge middleware, and knowware engineering, and discusses the life cycle models and implementation designs for knowware. The paper concludes by discussing the integration of PROMIS, knowware, and J2EE in the PROMIS/KW** framework for application system generation and domain knowledge modeling.
41531	4153185	A merging-based approach to handling inconsistency in locally prioritized software requirements	The priority of requirements is crucial in resolving inconsistencies and making trade-off decisions in software development. However, in distributed development, such as Viewpoints-based approaches, different stakeholders may assign varying levels of priority to the same shared requirement. This can create a dilemma for developers when handling inconsistencies. To address this issue, a merging-based approach is proposed in this paper for the Viewpoints framework. This approach transforms the local priorities of each viewpoint into a stratified knowledge base and considers the relationships between viewpoints as integrity constraints. By merging these knowledge bases, a global prioritization is obtained, which can help developers make informed decisions when handling inconsistencies.
41532	4153249	Communication-Free Widened Learning of Bayesian Network Classifiers Using Hashed Fiedler Vectors.	Widening is a technique that utilizes parallel resources to improve upon greedy algorithms and find better solutions. Previous uses of Widening have involved communication between parallel workers to maintain their distances in the model space. However, a new approach has been introduced that does not require communication, using Locality Sensitive Hashing on Bayesian networks' Fiedler vectors. This method has been shown to produce superior classifiers compared to standard implementations and those generated with only a greedy heuristic. This communication-free, widened extension to a standard machine learning algorithm has the potential to greatly enhance the efficiency and effectiveness of learning algorithms.
41532	4153212	Bucket Selection: A Model-Independent Diverse Selection Strategy for Widening.	Greedy algorithms are commonly used in data mining to find models, but they can lead to suboptimal solutions. Widening is a technique that uses parallel resources to broaden the search and improve the results. The main component of widening is the selector, which chooses the next models to refine. This selector needs to promote diversity in the selected models so that the parallel workers can explore different parts of the model space. However, this approach can be challenging if there is no suitable distance measure for the models. In this paper, the bucket selector is proposed as a faster and more effective model-independent selection strategy, performing better than existing strategies in cases without a diversity measure. 
41533	4153337	Bounded diffusion for multiscale edge detection using regularizedcubic B-spline fitting	The paper suggests that the regularization factor α is a more effective scale parameter for edge detection compared to the standard deviation (σ) of the Gaussian pre-filter. This leads to the development of a multiscale edge detector (MRCBS) that uses an adaptive scale based on local noise levels, adjusted thresholds for edge details, and anisotropic diffusion for further noise suppression. The α scale space is found to better capture the evolutionary behavior of edges at different scales. This approach improves the accuracy and robustness of edge detection in various applications. 
41533	4153320	Roof edge detection using regularized cubic b-spline fitting	The paper discusses a modified scheme for detecting step edges in an image using 1-D Regularized Cubic B-Spline (RCBS) fitting. This approach simplifies the computation by transforming the regularized fitting into a quadratic energy equation. However, the original scheme has limitations such as non-linearity, limited accuracy, and high computational cost. To overcome these limitations, the modified scheme uses 1-D RCBS fitting on both horizontal and vertical orientations of an image window, generating two 1-D signals that provide accurate information for detecting roof edges. The experimental results demonstrate the sensitivity of this approach to small signals, making it effective for roof edge detection. 
41534	4153477	AudioDAQ: turning the mobile phone's ubiquitous headset port into a universal data acquisition interface	AudioDAQ is a new platform for continuous data acquisition using the headphone port of a mobile phone. Unlike other phone peripherals, AudioDAQ uses the microphone bias voltage to draw power and encodes data as analog audio, making it compatible with a wider range of phones. It also utilizes the phone's built-in voice memo application for data collection, allowing for simple analog peripherals without requiring a microcontroller. This design is more efficient, universal, and requires no modifications to the phone. The system has been tested and can continuously capture EKG signals for an extended period of time, sending the data to the cloud for storage, processing, and visualization. 
41534	415349	A case for custom silicon in enabling low-cost information technology for developing regions	The paper discusses the potential of information and communications technology (ICT) to make a deep social impact in developing regions. However, the current ICT devices such as laptops and mobile phones are too expensive for many scenarios. The authors argue that custom integrated circuits can enable the development of low-cost information access devices, which will be more widely accessible. They demonstrate this through the example of a simple solution for providing information to illiterate populations through audio recordings. By using custom silicon, the device can be made at a much lower cost, with lower power consumption, and optimized for usability. The resulting device can be built for $7.77, which is a third of the cost of using off-the-shelf components.
41535	4153521	An Automated Tensorial Classification Procedure for Left Ventricular Hypertrophic Cardiomyopathy.	Cardiovascular diseases are a major cause of death worldwide and effective classification tools are crucial in prevention and treatment. The use of statistical learning theory in magnetic resonance imaging has allowed for the diagnosis of various cardiomyopathies. A two-stage classification scheme has been proposed to differentiate between different types of hypertrophic cardiomyopathies and healthy patients. This involves using a multimodal processing pipeline to calculate robust tensorial descriptors of myocardial mechanical properties from magnetic resonance tagged images. A homomorphic filtering procedure is then used to align the images and provide 3D tensor information. Results have shown that this approach can effectively classify hypertrophic cardiomyopathies even with limited samples. 
41535	415359	Unsupervised 4D myocardium segmentation with a Markov Random Field based deformable model.	A new method for segmenting the myocardium in Magnetic Resonance Imaging (MRI) is proposed using a stochastic deformable model. The segmentation is approached as a probabilistic optimization problem, where the optimal surface is found for the myocardium in a discrete space. This is achieved by first detecting the left ventricle using existing image analysis tools. The segmentation is then obtained by maximizing the posterior marginals for the myocardium location in a Markov Random Field framework, which combines temporal-spatial smoothness with intensity and gradient features. This unsupervised approach uses Maximum Likelihood estimation to determine the field parameters. The results of this method were found to be similar to manually segmented images in terms of cardiac function parameters in a group of 43 patients with Acute Myocardial Infarction. Overall, this new method offers a flexible and reliable approach to segmenting the myocardium in MRI images.
41536	4153668	Sufficient conditions for triangle-free graphs to be optimally restricted edge-connected	In graph theory, a k-restricted edge-cut is a set of edges that, when removed from a connected graph G, disconnects it into components with at least k vertices. A graph that allows for k-restricted edge-cuts is called k-connected. The k-edge-degree of a graph is the minimum number of edges between a connected subgraph of size k and its complement. A graph is considered k-optimal if its k-restricted edge-connectivity is equal to its minimum k-edge-degree, and super-k if every minimum k-restricted edge-cut isolates a connected subgraph of size k. This paper examines the cases of k=2 and k=3 and establishes lower bounds for the size of components left by a minimum k-restricted edge-cut in triangle-free graphs that are not k-optimal. Sufficient conditions for a triangle-free graph to be k-optimal and super-k are also discussed.
41536	4153622	Note on the connectivity of line graphs	The article discusses the concept of restricted edge-cuts in a connected graph G, defined by a subset of edges that, when removed, disconnect the graph into components with at least two vertices each. The minimum number of edges in a restricted edge-cut is known as the restricted edge-connectivity, denoted λ2(G). This value is always greater than or equal to the edge-connectivity (λ(G)) and the vertex-connectivity (κ(G)). In 1969, Chartrand and Stewart proved that if λ(G) is at least 2, then the vertex-connectivity of the line graph L(G) is at least equal to λ(G). The article presents a new result that shows when |V(G)| is at least 4 and G is not a star, the vertex-connectivity of L(G) is equal to λ2(G). This result also implies the previously known inequality λ2(G) ≤ ξ(G), where ξ(G) is the minimum edge degree.
41537	4153718	Supporting distributed groups with a Montage of lightweight interactions	The Montage prototype is a tool that allows remote collaborators to communicate through audio and video "glances." In a study of a distributed group, it was found that Montage was used for quick and casual interactions that were similar to face-to-face conversations. However, it did not replace other forms of communication such as email, voice-mail, and scheduled meetings. The prototype also integrated other applications to help coordinate future contact. Data was collected on usage, and both video-tape and user perception were analyzed. It was concluded that Montage was a convenient tool for staying connected with colleagues without leaving the office, but other tools were still necessary for organizing future interactions.
41537	4153721	Supporting Collaboration through Teleproximity	The article discusses the importance of creating a sense of proximity among distributed collaborators. This involves facilitating easy contact initiation, allowing for contact at shared events, and establishing shared awareness. Three prototypes, including Montage, Forum, and Piazza, are described as examples of how these attributes can be achieved. Montage enables video conferencing on desktops, Forum allows for participation in virtual presentations, and Piazza fosters shared awareness among group members. Reflecting on the use of these prototypes reveals potential for further development in creating a sense of nearness among distributed collaborators. 
41538	4153828	Coupled Gaussian process regression for pose-invariant facial expression recognition	The authors propose a new approach for recognizing facial expressions at various poses using 2D geometric features. The method involves mapping the facial landmark points from non-frontal poses to the corresponding locations in the frontal pose, and then using a multi-class SVM for expression recognition. They also introduce a novel Coupled Gaussian Process Regression (CGPR) model to learn the mappings for pose normalization. This approach is able to handle expressive faces at a wide range of pan and tilt rotations and accurately predict continuous head poses. It is also the first method to be both face-shape-model-free and perform well with discrete training poses.
41538	4153819	DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval	This paper presents a solution to the challenge of efficiently retrieving video content from large datasets while maintaining high performance. Existing methods either sacrifice performance for efficiency by representing videos as global vectors, or incur high computational costs with fine-grained approaches. The proposed Distill-and-Select (DnS) framework uses Knowledge Distillation to train multiple Student Networks with varying trade-offs between performance and efficiency, as well as a Selector Network to quickly direct samples to the appropriate student. This approach achieves state-of-the-art performance on various video retrieval tasks and offers a good balance between speed, storage space, and performance. The dataset and implementation are publicly available.
41539	4153953	Bus interconnection networks	Bus interconnection networks are used to connect processors and are represented by hypergraphs. A survey of different methods for constructing these networks with specific parameters, such as maximum processor degree and network diameter, is provided. The problem for point-to-point networks has been extensively researched, resulting in proposed families of networks. Two approaches for constructing bus networks using these point-to-point networks are discussed: considering the dual of the network and generalizing known constructions. The tools developed in the theory of hypergraphs and directed hypergraphs are summarized for handling this approach.
41539	4153955	Line directed hypergraphs	This article introduces the concept of line dihypergraphs, which are an extension of line digraphs. It discusses their properties, specifically in regards to connectivity parameters, and shows that the line dihypergraph has a greater arc connectivity than the original dihypergraph. The De Bruijn and Kautz dihypergraphs, which are well-known bus networks, are then shown to be iterated line digraphs. The article concludes by providing brief proofs of the high connectivity of these dihypergraphs.
41540	415402	On perfect neighborhood sets in graphs	A dominating set of a graph G is a subset of vertices S, where every vertex of G not in S is adjacent to a vertex in S. A vertex upsilon of G is called S-perfect if it is adjacent to exactly one vertex in S. A perfect neighborhood set of G is a set where every vertex is either S-perfect or adjacent to an S-perfect vertex. It is shown that for all graphs G, the maximum cardinality of a minimal dominating set (Gamma(G)) is equal to the maximum cardinality of a perfect neighborhood set (Theta(G)). This was proven in a 1999 study by Elsevier Science B.V. 
41540	4154010	A theorem of Ore and self-stabilizing algorithms for disjoint minimal dominating sets	Ore's theorem states that in a graph with no isolated nodes, a minimal dominating set D will have V - D as a dominating set. This means that such graphs must have two disjoint minimal dominating sets, R and B. A self-stabilizing algorithm has been developed to find these sets efficiently. Additionally, the theorem also guarantees the existence of two disjoint sets, R and B, where R is maximal independent and B is minimal dominating. Another self-stabilizing algorithm has been developed to find these sets. Both algorithms work in the Distance-2 model, but can be converted to the Distance-1 model for faster running times.
41541	41541104	Scheduling Nonlinear Sensors For Stochastic Process Estimation	This paper focuses on the problem of using a limited number of sensors to estimate the state of a stochastic process, which is important in applications such as target tracking and simultaneous localization and mapping (SLAM). The challenge lies in dealing with unknown stochastic systems, nonlinear measurements, and limited resources. The authors propose an algorithm that can be applied to general stochastic processes and nonlinear measurements with a time complexity linear in the planning horizon. It offers significant computational advantages over the existing polynomial-time algorithm, achieving a performance only 1/2 away from the optimal. The algorithm also has the same time complexity as the state-of-the-art algorithms for linear systems and measurements. The authors prove two key properties of the entropy of the batch state vector conditioned on the measurements, which enable the efficient evaluation of the algorithm. 
41541	41541220	Localization from semantic observations via the matrix permanent	This paper proposes a new approach to robot localization using object recognition and semantic information from sensors. Correct data association between the object detections and landmarks on a map is crucial for accurate localization. The authors propose a sensor model that encodes semantic observations and reduces the problem of computing likelihood to a matrix permanent. This enables a polynomial-time approximation to the set-based Bayes filter. The paper also addresses the problem of active semantic localization, where the robot's trajectory is planned to improve localization. The approach is demonstrated in simulations and real environments, showing robust global localization. Comparisons are made with traditional lidar-based methods.
41542	4154221	Razumikhin-type theorems on stability of stochastic retarded systems	The Razumikhin-type theorems have been developed and are effective for the stability of deterministic retarded dynamic systems. However, the stability is not in a global sense for stochastic retarded functional differential equations (SRFDEs). Recent research by Mao (1997a) has provided useful criteria for exponential stability of SRFDEs, but general asymptotic stability has not been considered. This article aims to establish Razumikhin-type theorems on asymptotic stability of stochastic retarded systems, a generalization of Mao's result. It also considers generalised exponential stability for better estimation of Lyapunov exponent of time-variant systems. By understanding the underlying principles of the Razumikhin-type theorems, it is possible to apply them to stochastic retarded systems.
41542	4154245	pth moment exponential stability of stochastic functional differential systems with Markovian switching and impulsive perturbations	The paper examines the stability of stochastic functional differential systems with Markovian switching and impulsive perturbations. Using Lyapunov functions and Razumikhin techniques, the authors establish criteria for pth moment exponential stability and apply them to delay systems. An example is provided to demonstrate the effectiveness of the proposed criteria.
41543	4154324	Gradient-based boosting for statistical relational learning: The relational dependency network case	Dependency networks are models that approximate the joint probability distribution of multiple random variables by using conditional distributions. Relational Dependency Networks (RDNs) are an extension of these networks to relational domains. However, RDNs have a more complex model-selection problem due to the need to explore an unlimited number of relational abstraction levels. The current learning methods for RDNs only learn a single probability tree per random variable, but a new approach using gradient-based boosting has been proposed. This method allows for the creation of highly complex features over multiple iterations, resulting in a more expressive model. Experimental results on various datasets have shown that this boosting method is more efficient in learning RDNs compared to other statistical relational learning approaches.
41543	415438	Gradient-based boosting for statistical relational learning: the Markov logic network and missing data cases	Statistical Relational Learning (SRL) models, which combine logic and probabilities, have become increasingly popular in recent years. One such model, Markov Logic Networks (MLNs), is highly expressive but has a complex learning process. Most methods for learning MLN structure involve a two-step process of searching through possible clauses and then using gradient descent to learn weights. A new functional-gradient boosting algorithm has been developed that can learn both weights and structure simultaneously. Additionally, most SRL learning methods assume a closed-world, where unobserved data is assumed to be false. This assumption is challenged by an extension of the algorithm that can handle missing data through an EM-based approach. Results in various domains have shown the effectiveness of this approach in learning MLNs, Relational Dependency Networks, and relational policies in the presence of missing data.
41544	4154446	MLPNN adaptive controller based on a reference model to drive an actuated lower limb orthosis	This paper proposes a method for controlling an actuated orthosis using an adaptive controller based on a reference model. The adaptive controller only requires knowledge of the global structure of the dynamic model and uses a Multi-Layer Perceptron Neural Network (MLPNN) to estimate dynamics related to inertia, gravitational and frictional forces, as well as other unmodeled dynamics. The stability of the system is analyzed using the Lyapunov formalism, and a first order Taylor series expansion is used to handle the nonlinearities related to the MLPNN. Experimental results using a real orthosis on a dummy show that the proposed controller is effective and robust, performing well in both assistive and resistive force scenarios. 
41544	4154448	Bounded Control Of A Full-Exoskeleton Device With Four (4) Degree Of Freedom	This study focuses on the control of a lower limb exoskeleton with four degrees of freedom, designed for rehabilitation of individuals with hip or knee impairments. The exoskeleton mimics human gait using Central Pattern Generators (CPGs), with the use of time polynomial functions to control initial conditions. A bounded controller is used to ensure safe and accurate movement within the range of motion required for walking. The wearer remains passive, and simulation results using Matlab/Simulink support the effectiveness of this approach. Overall, this study presents a promising solution for rehabilitative devices for lower limb impairments.
41545	41545126	Fourier-domain beamforming: the path to compressed ultrasound imaging	Sonography is a medical imaging technique that uses multiple transducer elements to produce images of tissues. These signals are sampled and processed through digital beamforming, which requires high sampling rates that result in large amounts of data. A new technique called compressed beamforming has been developed, which reduces the number of samples needed to reconstruct an image with strong reflectors. However, it cannot handle speckle, an important aspect of medical imaging. To address this, researchers have extended this technique to use beamforming in frequency, taking advantage of the low bandwidth of ultrasound signals. They have also developed a compressed sensing (CS) technique to further reduce the sampling rate. These methods have been successfully demonstrated on in vivo cardiac data and implemented on an ultrasound machine, showing the potential for smaller, more efficient, and cost-effective ultrasound machines in the future.
41545	41545110	Reduce and Boost: Recovering Arbitrary Sets of Jointly Sparse Vectors	Compressed sensing is a rapidly developing field that focuses on accurately and efficiently recovering a sparse vector in a high dimensional space using a small set of nonadaptive linear measurements. This concept has been extended to a finite set of sparse vectors with a common sparsity pattern. However, recovering an infinite set of jointly sparse vectors poses a challenge due to the infinite structure. To address this, a single, reduced-size finite-dimensional problem can be solved to recover the entire set. This can be further reduced to the basic model of a single sparse vector by randomly combining measurements. An empirical boosting strategy is proposed to improve recovery ability, and numerical experiments show promising results in terms of run time and recovery rate. 
41546	4154690	Integrating inductive neural network learning and explanation-based learning	The article discusses the importance of combining inductive and analytical learning in order to improve learning methods. The authors propose a method called explanation-based neural network learning (EBNN), which combines explanation-based learning with inductive learning. This method uses a neural network representation of domain knowledge and constructs explanations by chaining together inferences from multiple neural networks. Unlike traditional symbolic approaches, EBNN focuses on extracting derivatives of the target concept from the explanation, which are then used to bias the inductive learning process. The results of experiments on a simulated robot control task show that EBNN requires fewer training examples and is robust to errors in the domain theory.
41546	4154688	Explanation-Based Generalization: A Unifying View	The focus of machine learning research has been on how to create general concepts from specific training examples. Previous methods have relied on empirical approaches with no domain-specific knowledge. However, recent developments have allowed for the application of domain-specific knowledge to create valid generalizations from a single training example. These methods are able to explain why the training example belongs to the concept being learned. This paper introduces a new method called EBG, which unifies previous approaches to explanation-based generalization. EBG is demonstrated through several example problems and is used to compare existing systems. This approach also highlights potential research areas in explanation-based generalization.
41547	4154769	Collecting and Analyzing Data from Distributed Control Programs	This paper discusses a collection of tools designed for developers to track and analyze data in C/C++ programs while they are running. These logging tools offer various options for storing data and allow for synchronized logging in distributed programs. One option is to store data in an SQL database. The authors have also developed analysis tools that use interval temporal logic to retrieve data from the database and answer common developer inquiries. The logging tools have been fully implemented and their performance is presented in the paper. The analysis tools are currently undergoing testing with real data from NASA applications.
41547	4154713	Perception, planning, and control for autonomous walking with the Ambler planetary rover	Creating a robotic system that can function independently on other planets is a difficult task. The robot must be able to rely on itself, operate with limited power, and navigate through various types of terrain. This article discusses a software system that combines perception, planning, real-time control, and task-level control to operate the Ambler, a six-legged rover, autonomously. The system uses a highly deliberate approach, where actions are goal-oriented and planned in detail using terrain and vehicle models. This approach is believed to be suitable for meeting the challenges of reliability, efficiency, and terrain traversal for autonomous planetary rovers. The Ambler system has been extensively tested and has shown promising results in terms of capability and performance.
41548	41548167	Experiential Sampling on Multiple Data Streams	Multimedia systems often have to handle multiple data streams, each containing a significant amount of redundant and noisy data. To optimize computing resources in real-time applications, it is important to focus on a relevant subset of data streams and use it to build an accurate model of the environment. This issue is addressed by formulating it as an experiential sampling problem and proposing an approach to efficiently utilize computing resources on the most informative data streams. The paper also introduces a generalized version of this framework for multiple data streams and presents an evaluation measure for its effectiveness. The framework has been successfully applied to various tasks such as traffic monitoring, face detection, and monologue detection.
41548	4154884	Multimedia data mining: state of the art and challenges	The advancements in multimedia data technology have led to the creation of large multimedia databases, which has presented a challenge in analyzing and extracting useful knowledge from this vast amount of data. This has led to the emergence of Multimedia Data Mining (MDM), which involves finding patterns in media data such as audio, video, image, and text to improve decision making. MDM has been the focus of significant research efforts, with methods and tools being developed to organize, manage, and search data from various domains. This paper provides a comprehensive survey of MDM, covering feature extraction, transformation, and representation techniques, data mining techniques, and current systems in different application domains. The review also identifies remaining gaps for future research and discusses open research directions.
41549	4154934	A unified framework to exploit information in BCI data for continuous prediction	In this paper, the authors propose a unified framework for developing effective learning algorithms for continuous prediction using EEG signals in brain-computer interface (BCI). The framework is based on variational Bayesian method and aims to fully utilize the information contained in BCI data. The method involves dividing trials in the training data set into segments and addressing key issues such as unknown intention and making decisions based on individual time intervals. Two auxiliary distributions are introduced in the lower bound on the log posterior to address these issues. The proposed method was evaluated on three BCI competition data sets and showed high accuracy.
41549	4154940	Expectation-maximization method for EEG-based continuous cursor control	Developing effective learning algorithms for continuous prediction of cursor movement using EEG signals is a difficult task in BCI research. The paper proposes a new statistical approach using the expectation-maximization method to train a classifier for EEG-based cursor control. This involves dividing training data into segments, with unknown labels for each segment. To handle this uncertainty, the paper uses hidden variables in a lower bound on the log posterior and maximizes it using an EM-like algorithm. Experimental results demonstrate that this method has high accuracy compared to others.
41550	4155044	Counting in Trees for Free	The undecidability of MSO logic with Presburger constraints for ordered unranked trees can be resolved by using a decidable modal fixpoint logic. This can be characterized using deterministic Presburger tree automata, allowing for the expression of numerical document queries. Surprisingly, the complexity of satisfiability for this extended logic is the same as the original logic. Non-emptiness for Presburger tree automata is PSPACE-complete, but a tractable subclass exists. Polynomial time and linear space algorithms exist for deciding whether a tree satisfies a formula, and a linear time construction for a Presburger formula for the Parikh image of a regular language is presented.
41550	4155033	On distributed monitoring of asynchronous systems	Distributed systems are complex and difficult to analyze, especially when active entities behave asynchronously. This makes it challenging to verify their correctness. To address this, the paper proposes using a simple model called asynchronous automata to monitor distributed systems at runtime. The goal is to check a property against an unknown or complex system, for which classical static analysis is not possible. This requires distributed monitoring, where the monitor does not have a global view of the system. The paper introduces the concept of locally monitorable trace languages and shows that if certain conditions are met, a property can be locally monitored. Additionally, the paper discusses the relationship between recognizable countable unions of locally safety languages and deterministic languages. 
41551	4155144	RFTL: improving performance of selective caching-based page-level FTL through replication.	Flash memory technology is greatly affected by the type of workload it is performing, leading to poor performance on random writes. To address this issue, Demand-based Flash Translation Layer (DFTL) was introduced, which selectively caches page-level address mappings. However, DFTL can still experience high cache miss rates, causing significant overhead. A new method called Replication-based DFTL (RFTL) has been proposed to minimize this overhead by replicating the cached mapping table. An analytical model was developed to study RFTL's performance, and EagleTree simulator was extended to implement it. Experimental results with synthetic workloads show that RFTL outperforms DFTL, especially for read-dominant workloads, with a 20% improvement in performance and 10% improvement in I/O throughput.
41551	4155132	A semi-preemptive garbage collector for solid state drives	NAND flash memory is a popular storage media due to its low-latency access and low power consumption. However, its garbage collection (GC) process, which reclaims invalid pages to create free blocks, can cause performance degradation when run concurrently with other I/O operations. The GC process is triggered by a low watermark on free blocks and other device metrics, leading to workload-dependent I/O performance. In this paper, a semi-preemptive GC scheme is proposed to improve performance by interrupting the GC process to service pending I/O requests and merging internal GC operations with I/O requests. Experiments show that this scheme improves response time by up to 66.56% and reduces variability by 83.30% for write-dominant workloads. 
41552	4155229	Four-Chamber Heart Modeling and Automatic Segmentation for 3D Cardiac CT Volumes	This paper presents an automatic heart chamber segmentation system, which is crucial for accurately measuring cardiac function. Two main tasks involved in developing this system are heart modeling and fitting the model to new volumes. The model must be flexible yet accurate, taking into account the complex and non-rigid nature of the heart and its four chambers. Key landmarks, such as valves and cusp points, are explicitly represented in the model and can be reliably detected to guide the fitting process. The paper also introduces two methods for establishing mesh point correspondence, necessary for building a statistical shape model. The proposed approach uses recent advances in learning discriminative object models and a large database of annotated CT volumes to automatically segment the four heart chambers. Experiments show that this approach is efficient and robust, with results outperforming current methods. The study also reports stable results on a large dataset of 323 cardiac CT volumes, with a speed of less than eight seconds for automatic segmentation.
41552	4155267	Robust 3D Segmentation of Pulmonary Nodules in Multislice CT Images	The proposed algorithm efficiently and accurately segments 3D pulmonary nodules in multislice CT scans. It combines a parametric Gaussian model with a non-parametric 3D segmentation method to consider both spatial and intensity proximities in the 4D spatial-intensity joint space. This allows for reliable segmentation of various types of nodules, including difficult ones such as part- or non-solid nodules. The system can process a 32x32x32-voxel volume in just six seconds on average. Experimental results demonstrate the effectiveness of this approach.
41553	41553100	Nonparametric Regression between General Riemannian Manifolds	Nonparametric regression between Riemannian manifolds is studied using regularized empirical risk minimization. The regularization functionals used in this approach must take into account the geometry of the input and output manifolds and be independent of the chosen parametrization. Three simple regularization functionals are defined and analyzed, and a general scheme for solving the resulting optimization problem is presented. Examples of applications include interpolation on the sphere, fingerprint processing, and correspondence computations between 3D surfaces. Interesting and sometimes counterintuitive implications and new open problems specific to learning between Riemannian manifolds are discussed, highlighting differences from traditional multivariate regression in Euclidean space.
41553	41553169	A kernel two-sample test	In this article, the authors propose a framework for comparing and analyzing distributions. They introduce a test statistic called the maximum mean discrepancy (MMD), which measures the largest difference in expectations between two distributions. The MMD is based on a reproducing kernel Hilbert space and can be computed in quadratic time. The authors also present three distribution-free tests based on the MMD, including one that uses large deviation bounds and another that relies on the asymptotic distribution of the statistic. They demonstrate the effectiveness of their tests in various scenarios, such as attribute matching and comparing distributions over graphs. Overall, the MMD offers a versatile and efficient approach for comparing distributions and can be adapted to different function classes. 
41554	4155463	Transferring Human Impedance Behavior to Heterogeneous Variable Impedance Actuators	This paper discusses different methods for controlling robots with variable impedance actuators (VIAs) in a way that mimics human behavior. The focus is on transferring impedance modulation strategies from human demonstrators to robots with different levels of complexity. Three main approaches are identified: direct, feature-based, and inverse optimal transfer. The first approach is limited to highly biomorphic systems, while the latter two are more versatile and can be applied to various VIAs regardless of their mechanical design. The paper proposes a constraint-based method and an apprenticeship learning framework as examples of these transfer strategies and evaluates their effectiveness in terms of efficiency, ease of use, and task performance through simulations and real-world experiments. 
41554	4155450	Learning Utility Surfaces for Movement Selection	Humanoid robots are complex machines that have more degrees of freedom than necessary for the tasks they are designed to perform. This redundancy is traditionally managed through optimal control in the null-space, where a cost function is used to encode secondary movement goals. However, designing these cost functions has been a time-consuming and ad-hoc process. To address this issue, a new approach has been developed that utilizes advances in statistical machine learning. By observing the behavior of a mechanical system performing a task and separating it into task and null-space components, an open-form model of the cost function can be created using a statistical learning algorithm. This approach has shown promising results in replacing the original control scheme and improving the robot's behavior.
41555	415554	Learning coupling terms for obstacle avoidance	Autonomous manipulation in dynamic environments is crucial for robots to perform everyday tasks. There are two main approaches in the literature for this - using a complex planning system or modifying a simple plan with sensory feedback. Dynamic Movement Primitives (DMPs) provide a versatile and robust starting point for a controller that can be modified in real-time with a non-linear coupling term. This allows for fast and reactive obstacle avoidance, similar to human behavior. Researchers propose a method for learning this coupling term from human demonstrations and testing its ability to avoid obstacles in a reactive manner. This research aims to develop more advanced reactive control strategies, reducing the need for computationally expensive planning methods.
41555	41555125	Learning to grasp under uncertainty	The article discusses a new approach for robots to learn motion primitives that can handle uncertainties in state estimation. This is achieved by teaching the robot to use line manipulation strategies during reaching and preshaping, so that the object can be maneuvered into a pose that is more likely to result in a successful grasp. Unlike traditional methods, this approach considers the interdependence of grasp planning and motion planning, and takes into account that the robot may not have perfect knowledge of the object's pose. The technique uses Dynamic Movement Primitives and a reinforcement learning algorithm to optimize the chance of a successful grasp. The results of an empirical evaluation show that this approach leads to more robust motion primitives for grasping both simple and complex objects, and can also generalize to new object positions and state estimation uncertainties.
41556	4155655	Modularity and directionality in genetic interaction maps.	Genetic interactions between genes can reveal important functional relationships, but the vast amount of information from large-scale genetic interaction assays can be challenging to interpret due to experimental noise. A computational approach has been developed to organize these interactions into modules, providing insights into the function of cellular machineries and global properties of interaction maps. Further analysis showed that a significant portion of observed aggravating interactions are unidirectional, where one gene can buffer the effects of perturbing another gene but not vice versa. A freely accessible web tool has been created to browse these results, and additional data is available online.
41556	4155639	Discovering the hidden structure of complex dynamic systems	Dynamic Bayesian networks are a useful tool for representing complex dynamic systems, but often there is no expert available to help construct the model. In these cases, learning the structure of the system is an alternative approach. The paper discusses the computational challenges of learning in dynamic systems, especially when some variables are partially observed or unknown. The authors propose a novel approximation scheme to efficiently gather expected sufficient statistics and also address the problem of discovering hidden variables without exhaustive search. Their algorithm searches for violations of the Markov property in the data and introduces hidden variables to explain them. Empirical results demonstrate the effectiveness of the algorithm in learning complex systems in a computationally feasible manner. 
41557	4155742	A system for exact and approximate genetic linkage analysis of SNP data in large pedigrees.	The use of SNP data in genetic linkage analysis is challenging due to technical, methodological, and computational obstacles. Superlink-Online SNP is an online system that streamlines this process by providing a flexible workflow and powerful analysis tools. It utilizes thousands of CPUs to perform tasks much faster than a single computer. The system offers services such as parallelized MCMC analysis and a novel algorithm for maximum-likelihood haplotyping. This allows for genetic analyses that were previously not possible. Researchers can access Superlink-Online SNP for free and the source code is available for download. Supplementary data is also available online.
41557	415573	Scheduling Mixed Workloads in Multi-grids: The Grid Execution Hierarchy	The goal of this study is to optimize the scheduling of a workload that includes both large, resource-intensive tasks and short, low-resource tasks. To achieve this, the concept of a grid execution hierarchy is proposed, where grids are sorted by size and execution overheads increase with grid size. A scheduling algorithm is developed based on the multilevel feedback queue approach, which considers a task's resource requirements and expected turnaround time to find the best matching grid. This approach is similar to the shortest processing time first policy, where a task is constantly evaluated and migrated to a more suitable grid level if needed. The algorithm is tested on a production system for genetic linkage analysis and shows improvement in response time for short tasks while maintaining efficient throughput for larger tasks.
41558	4155843	Incorporating Transaction Semantics to Reduce Reprocessing Overhead in Replicated Mobile Data Applications	Transactional replication, specifically the anywhere-anytime-anyway approach, can have unstable behavior when the workload increases. To address this issue, a two-tier replication algorithm was proposed in a previous study. However, this can lead to heavy reprocessing overhead. This paper suggests a different approach - merging histories instead of reprocessing - to reduce this overhead. This involves merging tentative transactions into the base history when a mobile node connects to the base nodes. However, conflicts between the two histories can result in a set of undesirable transactions that need to be backed out. The paper presents novel rewriting algorithms for backing out these transactions, which take transaction semantics into account and are more effective than traditional approaches. 
41558	415583	A two snapshot algorithm for concurrency control in multi-level secure databases.	This article discusses a concurrency control algorithm for replicated, secure, multi-level databases. It addresses the issue of indirect channels in secure databases and the risk of high level transactions being starved by malicious low level processes. The proposed algorithm, which is an improvement on existing techniques, uses a combination of multi-version and replicated databases to avoid starvation without introducing indirect channels. It also reduces the number of old data copies that need to be maintained. The paper proves the correctness of the algorithm and shows that it is free of indirect channels and starvation.
41559	4155958	Using Coupling-Based Weights for the Class Integration and Test Order Problem	In software development, integrating and testing classes can be complicated due to relationships such as method calls, inheritance, and aggregation. Classes must be integrated and tested in a specific order, and cyclic dependencies can create difficulties that require expensive and error-prone "stubbing" operations. This is known as the class integration and test order (CITO) problem. This paper introduces new techniques and algorithms to solve the CITO problem, including improved edge weights that better model the cost of stubbing and the use of node weights to incorporate more information. A new algorithm for computing integration and test orders is also presented, which is found to be more efficient and yield better results compared to existing approaches. 
41559	4155931	Automatically testing interacting software components	Integration testing for object-oriented software aims to ensure effective communication and compatibility between different objects. This can be challenging when components are developed by different vendors and in different languages. A previous paper proposed a model based on finite state machines to facilitate this testing process. It identifies relevant actions, transforms them into a data flow graph, and creates test specifications based on specific paths through the graph. This paper delves into the details of the automated tool used for this method, including how it addresses various problems and adds new capabilities to generate executable test cases. This approach follows theoretical procedures and produces comprehensive test coverage. The tool supports specification-based testing and bridges the gap between theory and practice.
41560	4156033	Near Optimal Placement Of Virtual Network Functions	NFV is a new approach to networking where network functions are run on standard servers located in small cloud nodes spread across the network. This allows for greater cost efficiency and control of network flows. However, the placement of these virtual functions within the physical network is a key challenge as it affects the network's performance, reliability, and operation cost. This paper addresses this challenge by proposing near optimal approximation algorithms that consider the distance cost between clients and virtual functions, as well as the setup costs. These algorithms are evaluated through simulations and shown to perform well in realistic scenarios, achieving constant approximation factors while adhering to capacity constraints. 
41560	4156073	Non-Cooperative Cost Sharing Games via Subsidies	In this system, selfish users choose services from facilities based on cost. A central authority can offer subsidies to improve overall performance, funded by user taxes. The system is modeled as a non-cooperative game, with the goal of minimizing payment for each user. Two models are investigated: integral (one set per user) and fractional (multiple sets per user). A subsidy mechanism is designed for each model, with the objective of improving system performance while collecting minimal taxes. The performance is measured by the price of anarchy (ratio of Nash equilibrium cost to optimal solution cost) and the taxation ratio (fraction of taxes collected from users). The subsidy mechanism for the fractional model achieves a price of anarchy of $O(\frac{\log f}{\epsilon})$ and a taxation ratio of ε, while the integral model achieves a price of anarchy of $O(\frac{\log f\log (\frac{n}{\epsilon})}{\epsilon})$ and a taxation ratio of ε. 
41561	4156122	Optimal sleep-state control of energy-aware M/G/1 queues.	This paper discusses the optimal control of sleep states in an energy-aware M/G/1 queue. The model considers a range of policies where the server can enter random sleep states upon becoming idle to save energy. Jobs are served after a setup time and the system is analyzed under different cost metrics and distributions. The paper presents a more general model and shows that the optimal control of idle time and sleep states is deterministic, without any benefit from randomization. Two popular cost metrics, energy and response time, are considered and it is proven that the optimal strategy is to either use only the idle state or go to a fixed sleep state and wait for a set number of jobs before starting the setup.
41561	4156138	Energy-Aware Control Of Server Farms	This study explores the trade-off between energy consumption and performance in server farms, where idle servers can be turned off to save energy. However, turning the server back on incurs a setup cost in terms of delays and energy usage. A dispatcher assigns jobs to servers based on a task assignment policy, and jobs can either be queued or immediately served depending on the server's state and service discipline. The study considers a family of idle time control policies that use a timer to switch servers to a sleep state when idle. Results show that using the right combination of idle time control and task assignment policies can result in 10-30% cost savings in multi-server systems.
41562	415623	Csma Networks In A Many-Sources Regime: A Mean-Field Approach	The Internet of Everything (IoE) has led to a significant increase in the number of devices and applications that rely on wireless connectivity. This has resulted in wireless networks becoming larger and more complex, with a diverse range of traffic profiles and performance requirements. While there are established methods for evaluating the throughput of persistent sessions in these networks, they do not provide insight into the delay performance of flows with intermittent packet arrivals. To address this issue, a mean-field approach has been developed to analyze buffer contents and packet delays in wireless networks with many sources. This approach simplifies the analysis and provides accurate approximations for a large-scale network.
41562	4156240	Mean-Field Analysis of Ultra-Dense CSMA Networks	Distributed algorithms like CSMA are commonly used in large wireless networks to share the transmission medium among competing users. However, traditional models for CSMA assume that users always have packets to transmit, which is not always the case. This creates a complex interaction between the activity states and buffer contents. To understand this dynamic, a meanfield approach was developed for networks with many users. This approach identified a time-scale separation between the activity states and buffer contents and yielded a deterministic dynamical system to describe the network dynamics on a larger scale. The fixed point of this system accurately approximates the stationary distribution of buffer contents and packet delay, even with a moderate number of users.
41563	4156333	Fixed and adaptive model-based controllers for active queue management	This paper introduces new model-based controllers for active queue management (AQM) that support TCP flows. The controllers are based on linearizing a previously developed TCP model using fluid flow and stochastic differential equations. Comparing a simple PI controller to RED AQM, it is found that the PI controller outperforms RED. However, as the number of flows increases, both AQMs struggle to maintain performance. To address this issue, the paper proposes an externally excited adaptive loop (EEAL) controller, which shows faster response to flow changes and provides a dynamic estimate of the number of flows. Overall, the EEAL AQM is shown to be more effective than the PI AQM in managing TCP flows.
41563	4156362	On the autocorrelation structure of TCP traffic	The research community has been focusing on the statistical characteristics of network traffic, particularly its long range dependence, in recent years. There have been claims that the TCP protocol can generate long range dependent traffic, but this paper argues that the correlation structures of TCP traffic can only span a finite range of time-scales. The two mechanisms responsible for this behavior are timeouts and congestion avoidance, and analytical models accurately predict the range and strength of the correlation structure. A comprehensive model of TCP also shows that it exhibits a finite range of time-scales. These claims are supported by simulations and suggest that long range dependence is not possible in TCP traffic due to its inherent finite time-scales.
41564	415644	The Complexity Of Concept Languages	Terminological Knowledge Representation Systems are designed to organize and process knowledge using taxonomies, or hierarchical structures. These systems use a specialized reasoning engine to make logical inferences based on the information in the taxonomy. The taxonomy is created using a concept language, which has a specific set-theoretic meaning. These systems are often praised for their efficient reasoning abilities, which is a major reason for their use.
41564	4156415	An epistemic operator for description logics	Description logics are a type of formal logic that is used to model frame-based systems. However, they are limited in their ability to handle certain features of these systems, such as nonmonotonic reasoning and procedural rules. To address this limitation, an epistemic operator can be added to the description logics, creating a fragment of a first-order nonmonotonic modal logic. This allows for more sophisticated query formulation and enables the handling of procedural rules. An effective procedure for answering queries using the epistemic operator is provided, along with an analysis of the computational complexity of reasoning with it.
41565	4156549	Interventions and belief change in possibilistic graphical models	Causality and belief change are crucial in various applications, and this paper focuses on these concepts in possibilistic graphical models. The authors demonstrate that interventions, which represent causal relationships between events, can be viewed as a belief change process. This can be handled using a possibilistic version of Jeffrey's rule of conditioning. The paper also addresses new challenges in revising graphical models when interventions are involved. The order of introducing observations and interventions is important and the structure of possibilistic networks must be modified to properly handle sequences of both. An efficient method for revising possibilistic causal trees is also presented.
41565	41565121	Possibilistic causal networks for handling interventions: a new propagation algorithm	This paper presents two significant contributions to the advancement of possibilistic causal networks. The first contribution is the representation of interventions within these networks, using the "DO" operator from possibility theory. This allows for various ways of representing interventions in possibilistic networks. The second contribution is a new propagation algorithm that can handle both observations and interventions effectively. The algorithm has a minimal additional cost for handling interventions and is particularly suitable for handling sequences of observations and interventions. Overall, these contributions enhance the capabilities of possibilistic causal networks for modeling and analyzing causal relationships.
41566	4156636	Merging Qualitative Constraint Networks in a Piecewise Fashion	We have developed a method for merging qualitative constraint networks (QCNs) that efficiently combines conflicting QCNs into a consistent global representation. This algorithm is not limited to a specific qualitative formalism and works by merging constraints between the same pairs of variables. We have also defined constraint merging operators that ensure the resulting QCNs meet logical expectations. This approach addresses the problem of merging QCNs in a flexible and effective manner.
41566	4156620	Merging qualitative constraint networks defined on different qualitative formalisms	This paper discusses the challenge of combining qualitative constraint networks (QCNs) that use different formalisms. The model focuses on formalisms where entities and their relationships are defined in the same domain. This method is a precursor to a previous framework that dealt with a set of QCNs using the same formalism. It involves translating the input QCNs into a common formalism, using either an equivalent or an approximation approach. These approaches utilize the concepts of refinement and abstraction between qualitative formalisms, which are introduced in the paper. This research aims to improve the integration of QCNs in different domains.
41567	4156716	Probabilistic object recognition using multidimensional receptive field histograms	This paper discusses a probabilistic object recognition technique that does not require matching of images. It builds upon previous work using multi-dimensional receptive field histograms and extends it to compute the probability of an object's presence in an image. The method is shown to be robust and efficient, with low computational cost and linear complexity. Results from experiments with a database of 100 objects demonstrate the effectiveness of the approach. The technique can be used with a variety of filters and is not limited by changes in viewpoint. The paper also introduces a new technique for determining the probability of an object in a scene based on multidimensional receptive field histograms.
41567	41567178	A comparison of position estimation techniques using occupancy grids	This paper discusses the use of occupancy grids, which are a robust way of representing a mobile robot's local environment for sensor-based locomotion and position estimation. However, current methods for position estimation using occupancy grids are unreliable and computationally expensive. The paper presents four techniques for position estimation, including a method that combines global and local occupancy grids, a Hough transform for extracting line segments from grids, and the use of an extended Kalman filter. The paper also describes four matching techniques for obtaining the innovation vector needed for the Kalman filter equations. Experimental results show that combining segments from both local and global occupancy grids gives better results than direct matching of grids or a mix of segment and grid matching.
41568	4156826	On the size of computationally complete hybrid networks of evolutionary processors	An HNEP is a hybrid network of evolutionary processors, which is a graph where each node is associated with an evolutionary processor, a set of words, an input filter, and an output filter. The evolutionary processors have a finite set of mutations that can be applied to strings. The HNEP functions by rewriting words and redistributing them according to a communication protocol based on filtering. The filters are defined by random-context conditions. HNEPs can generate and accept languages, and this paper proves that any recursively enumerable language can be determined by a 7-node GHNEP and AHNEP. It also shows that GHNEPs and AHNEPs with only 2 nodes are not computationally complete.
41568	4156810	Hybrid networks of evolutionary processors are computationally complete	An HNEP is a hybrid network of evolutionary processors that consists of language processors located in nodes of a virtual graph. These processors can perform point mutations (insertion, deletion, substitution) on words according to predefined rules. Each node has an input and output filter based on random-context conditions. After mutations are applied, words that pass the output filter move through the network and enter nodes with matching input filters. This type of network is computationally complete, even in the simplest form where each node can only perform one type of mutation. It is also possible to create HNEPs that can produce any recursively enumerable language using a fixed graph structure based on a common alphabet.
41569	4156930	On competence in CD grammar systems	This paper explores the capabilities of cooperating distributed grammar systems (CDGSs) when the cooperation protocol is based on the level of competence on the underlying sentential form. A component is considered to be competent on a sentential form if it can rewrite a specific number of nonterminals in the string. By using this cooperation strategy, CDGSs can provide new characterizations of language families such as random context languages, context-free languages, and ET0L systems with random context. This research can contribute to solving long-standing problems in the theory of regulated rewriting.
41569	415692	Variants Of Competence-Based Derivations In Cd Grammar Systems	The paper discusses new cooperation protocols for cooperating distributed grammar systems. These protocols are based on the number of nonterminals present in the sentential form after a component finishes a derivation phase. This number indicates the grammar's competence and efficiency on a given string. The authors prove that if the derivation mode is t-mode, these systems can generate random context ET0L languages. If the derivation mode is limited to k steps, the systems can generate any recursively enumerable language. These findings demonstrate the potential of these CD grammar systems in language generation.
41570	415705	Interactive Out-of-Core Visualisation of Very Large Landscapes on Commodity Graphics Platform	The Batched Dynamic Adaptive Meshes (BDAM) technique is a new, efficient way to render and manage large textured landscapes. It utilizes a paired tree structure, with a tiled quadtree for texture data and a pair of bintrees for geometry. These small triangular patches are created and optimized offline using high quality simplification and tristripping algorithms. At each frame, hierarchical view frustum culling and view-dependent texture/geometry refinement is performed with a stateless traversal algorithm. This allows for continuous adaptive terrain rendering using out-of-core data. The proposed technique is not processor intensive and takes full advantage of current graphics hardware. The results of a virtual fly-through over a textured digital landscape derived from aerial imaging are discussed.
41570	4157010	Far voxels: a multiresolution framework for interactive rendering of huge complex 3D models on commodity graphics platforms	This article presents a method for efficiently creating and inspecting large surface models. The technique combines visibility culling and out-of-core data management with a level-of-detail framework. The process involves creating a coarse volume hierarchy using binary space partitioning, with leaf nodes containing a fixed number of triangles and inner nodes containing fixed-sized voxels. A visibility-aware algorithm then creates a directional approximation of the model's appearance for each voxel. During rendering, the volumetric structure is refined and rendered in order, with the use of vertex programs and hardware occlusion queries for efficient culling. The approach is shown to be effective for interactive rendering of complex surface models on standard graphics platforms.
41571	4157136	Easy Access to Huge 3D Models of Works of Art	Automatic shape acquisition technologies have advanced quickly, leading to the easy production of large amounts of 3D data. The high accuracy of range scanning technology makes it ideal for use in the Cultural Heritage field. However, two issues arise in this particular application: how to visualize the complex data on standard computers and how to make the visualization tools user-friendly for non-experts. To address these issues, a new visualization system has been developed for use in museums or expositions, with the ability to be used on the web. The system allows users to interact with a large 3D model and view linked multimedia data through a simple "point and click" approach. This is achieved through a continuous level-of-detail representation and automatic selection of the best-fit level of detail for efficient visualization.
41571	4157166	Multiresolution volume visualization with a texture-based octree	This paper proposes a new approach for 3D texture-based volume rendering in order to maintain an interactive rate when dealing with large datasets. The traditional one-texel per voxel approach is substituted with a hierarchical approach that prioritizes nearly homogeneous and lower interest regions. This is achieved through a simple traversal of the volume data's octree representation. The algorithm selects a set of octree nodes to be rendered based on a user-defined image quality, which takes into account data homogeneity and importance. Each node's representation in the texture memory is set independently, resulting in a variable resolution texture model that reduces memory size and improves rendering speed through reduced texture swapping.
41572	4157225	What Is the Set of Images of an Object Under All Possible Illumination Conditions?	The appearance of an object is influenced by both the perspective from which it is viewed and the lighting conditions. If two objects always have distinct appearances under different poses and lighting, then they can always be recognized. This paper examines the set of images of an object under variable illumination, including multiple light sources and shadows. The authors prove that for a convex object with a Lambertian reflectance function, the set of images under point light sources at infinity forms a convex polyhedral cone in n-dimensional space. This cone can be constructed from just three images and can be extended to objects with different shapes and reflectance functions. These findings have implications for object recognition and are supported by examples throughout the paper.
41572	4157254	From few to many: illumination cone models for face recognition under variable lighting and pose	The authors introduce a method for recognizing human faces that addresses challenges like lighting and viewpoint variations. They use a small number of training images to reconstruct the shape and albedo of a face, creating a generative model. This model is then used to render or synthesize images of the face in different poses and lighting conditions. The recognition algorithm compares the test image to the closest approximated illumination cone using Euclidean distance. The method is tested on a large database of images and performs well, outperforming other popular recognition methods. 
41573	4157325	Crowdsourcing translation: professional quality from non-professionals	Crowd-sourcing translations from non-professional translators can lead to poor and unpolished results without proper quality control. To improve the translation quality, we employ various methods such as obtaining multiple translations and choosing the best one, and using features to evaluate both the translations and translators. These features include factors like country of residence, language model perplexity, and editing rate. By scoring the translations using these features, we can distinguish between acceptable and unacceptable translations. We conduct a study using Mechanical Turk to recreate the NIST 2009 Urdu-to-English evaluation set and demonstrate that our models can produce translations of similar quality to professional translators at a significantly lower cost.
41573	4157339	Fast, cheap, and creative: evaluating translation quality using Amazon's Mechanical Turk	Manual evaluation of translation quality is often considered to be a time-consuming and expensive process. However, a cheaper and faster alternative is proposed by using Amazon's Mechanical Turk to pay a large number of non-expert annotators small sums of money. In a study where $10 was offered to recreate judgments from a previous translation task, it was found that the non-expert judgments had a high level of agreement with the existing gold-standard judgments and were more strongly correlated with expert judgments than the commonly used Bleu metric. Additionally, Mechanical Turk has been successfully used to calculate human-mediated translation edit rate (HTER), conduct reading comprehension experiments, and create high-quality reference translations.
41574	4157459	Bootstrapping statistical parsers from small datasets	The co-training method presented here allows for the improvement of statistical parsers by using a small amount of manually parsed data and a larger pool of raw sentences. This approach has been proven effective in improving parser performance. The method is also applicable when the manually parsed data is from a different domain than the raw sentences or testing material. This shows that boot-strapping can still be useful even without manually produced parses from the target domain. 
41574	4157438	Estimation of stochastic attribute-value grammars using an informative sample	The article discusses ways to reduce the computational complexity of estimating stochastic attribute value grammars by using a subset of the training data. Results from the Wall Street Journal corpus show that using an informative sample can sometimes lead to better estimation results than using the entire training set. The use of a Gaussian prior can also help reduce overfitting in unlexicalised models, but does not have a significant impact on performance for lexicalised models with overlapping features. This approach is useful when there are too many parses in the training set or when retrieving them from a packed representation is time-consuming.
41575	4157521	Cooperative gestures: multi-user gestural interactions for co-located groupware	Multi-user, touch-sensing input devices allow for the use of cooperative gestures, where multiple users' gestures are interpreted as one command. These gestures can enhance teamwork, increase awareness of system events, and add a unique aspect to activities. The paper explores potential uses for cooperative gestures, focusing on CollabDraw, a system for collaborative art and photo manipulation. The authors discuss design considerations and present a preliminary design framework. Future research in this area is also identified.
41575	4157539	Learning HCI design: mentoring project groups in a course on human-computer interaction	The Computer Science Department at Stanford University has implemented an experimental course on human-computer interaction (HCI) over the past two years. The course includes a 12-week project where students work in small groups to analyze a work environment, design and implement a prototype user interface, and evaluate it with project clients. In order to provide real-world practical design experience, mentors from local industry were invited to guide and advise the student groups. These mentors are unpaid volunteers and have been valuable in helping students develop their projects. The authors discuss the background of the projects, the role of mentors in the learning process, the requirements and benefits for mentors, and how to support the mentoring process. They also welcome sharing experiences with others who have implemented similar courses with mentor involvement. Mentors are considered a critical aspect of the design project in the course, and the authors have gained valuable insights from this experience. 
41576	4157654	Certificateless Ring Signatures.	Ring signature schemes allow a single person to sign on behalf of a group without revealing their identity. This is useful in various security applications, but implementing it with traditional public key infrastructure is complex and inefficient. Identity-based solutions also have drawbacks, making certificateless cryptography a better option. However, creating a certificateless ring signature scheme is challenging, as many require verifying the validity of multiple public keys. This paper presents the first certificateless ring signature scheme that does not require this step, making it more efficient for both the signer and verifier.
41576	4157640	Multi-key leakage-resilient threshold cryptography	This paper discusses the concept of leakage-resilient threshold cryptography, which aims to ensure the availability of security services like encryption and authentication even in the face of key-exposure attacks. This approach involves using a threshold number of secret keys in the main cryptographic function, such as decryption or signing. The paper introduces a new multi-key leakage-resilient security model specifically for threshold cryptography and presents two constructions that provide formal security guarantees within this model. These include a dynamic threshold public key encryption scheme and a threshold ring signature scheme. These constructions allow for multiple keys to be leaked and used simultaneously without compromising security.
41577	4157762	BLAC: Revoking Repeatedly Misbehaving Anonymous Users without Relying on TTPs	Some credential systems allow users to authenticate anonymously, but this can lead to misbehavior. To address this issue, some systems allow selective deanonymization of users upon complaint to a Trusted Third Party (TTP). However, this punishment is too severe and cannot be generalized to subjective forms of misbehavior. A new system, BLAC, allows service providers to revoke credentials of misbehaving users without relying on a TTP. This allows for subjective judgment of misbehavior without fear of arbitrary deanonymization. Additionally, a d-strikes-out revocation policy is implemented, revoking users who have repeatedly misbehaved a certain number of times. This allows for the blocking of anonymous users who have engaged in subjective misbehavior, such as defacing web pages.
41577	4157757	Blacklistable anonymous credentials: blocking misbehaving users without ttps	Many credential systems allow for anonymous authentication, but this can lead to users misbehaving. To address this issue, some systems allow for selective deanonymization of misbehaving users upon a complaint to a trusted third party (TTP). However, this punishment may be too severe. To limit the power of the TTP, systems like "e-cash" have been proposed, but they only work for specific types of misbehavior. In order to address more subjective forms of misbehavior, a new anonymous credential system has been developed where services can "blacklist" misbehaving users without involving a TTP. This allows for subjective judgments of misbehavior without the fear of arbitrary deanonymization.
41578	4157883	Robust independent component analysis by iterative maximization of the kurtosis contrast with algebraic optimal step size.	Independent component analysis (ICA) is a statistical method used to break down a set of random variables into independent components. This is achieved through deflation-based implementations, such as the popular FastICA algorithm, which extract the components one by one. In this paper, a new method called RobustICA is introduced, which uses exact line search optimization to find the best step size for extracting the independent components. This method is efficient and can handle real and complex-valued mixtures without the need for prewhitening. It also has a high convergence speed and performs well on real-world data, as demonstrated in a comparison with other techniques on synthetic data and in the extraction of atrial activity in electrocardiograms.
41578	415782	A contrast function for independent component analysis without permutation ambiguity.	The brief discusses the problem of blind source separation (BSS) using independent component analysis (ICA). It introduces a linear combination of fourth-order marginal cumulants as a valid contrast function for ICA, which eliminates the permutation ambiguity typically associated with the method. This contrast function can be optimized using a cost-efficient Jacobi-type pairwise iteration and leads to optimal finite-sample performance in the case of two real-valued signals. The proposed technique also offers a fully blind solution by using initial source estimates from a classical ICA stage. An experimental study demonstrates the effectiveness of this method compared to previous techniques. 
41579	4157966	Support vector machine learning from heterogeneous data: an empirical analysis using protein sequence and structure	The use of large and diverse biological datasets requires a theoretical framework that can integrate different types of data, such as DNA and protein sequences, protein structures, and gene expression data. Kernel methods, specifically the support vector machine (SVM) algorithm, have emerged as a powerful tool for combining these datasets. Additionally, recent extensions of the SVM allow for weighting of different datasets based on their utility for a specific classification task. In this study, the performance of the SVM in predicting gene function from protein sequence and structure data was investigated. Results showed that the SVM is robust to noise in the input datasets and that a naive, unweighted combination of datasets can perform as well or better than a more sophisticated approach. However, the weighted approach may be necessary for experiments with multiple noisy datasets.
41579	4157940	Bhattacharyya Expected Likelihood Kernels	A new class of kernels between distributions is introduced, which allows for discriminative estimation using support vector machines while taking advantage of the properties and assumptions of generative models. This is achieved by associating a generative model with each data point and computing the kernel by integrating the products of these models for two data points. The kernel satisfies Mercer's condition and can be computed in closed form for various models such as exponential family models, mixtures, hidden Markov models, and Bayesian networks. For other models, it can be approximated using sampling methods. Experiments demonstrate its effectiveness in tasks such as text and protein sequence classification.
41580	4158076	Combining Software and Hardware Verification Techniques	The industry requires a practical technology to formally verify software and hardware co-designs. This is achieved by combining successful verification techniques from both domains, such as BDD-based symbolic model checking for hardware and partial order reduction for concurrent software programs. A modified version of partial order reduction is proposed in this paper, which can be used with any BDD-based verification tool. A co-verification methodology is also described, utilizing these techniques together. Experimental results show the effectiveness and potential for industrial use of this combined verification approach for moderate-sized systems. 
41580	41580104	Static Partial Order Reduction	The state space explosion problem is a major issue in automatic verification algorithms. One effective solution is partial order reduction, which takes advantage of the fact that concurrent programs often do not rely on the order of events. A new version of this reduction technique is presented in this paper, which allows for the reduction to be set up during the compilation of the system description. This eliminates the need for specialized verification algorithms and allows for a simpler state space search. The technique can be easily implemented with existing verification tools and has been successfully applied to SDL programs in a hardware-software co-verification project. 
41581	4158121	The decision problem for the probabilities of higher-order properties	The probability of a property on the class of finite relational structures is the limit of the fraction of structures satisfying the property as the number of elements approaches infinity. This is known as the 0-1 law, which holds for properties expressible in first-order or fixpoint logic. However, the 0-1 law does not hold for second-order properties, and the decision problem for these probabilities is unsolvable. This article investigates logics that are more expressive than fixpoint logic and still adhere to the 0-1 law. Two logics, iterative logic and strict &Sgr;11, are studied and their associated decision problems are determined to be PSPACE-complete and NEXPTIME-complete, respectively. The proofs use combinatorial techniques such as generalizations of Ramsey's Theorem.
41581	4158133	On the Complexity of Bounded-Variable Queries	In relational databases, the complexity of evaluating a query is often much higher than the complexity of the data itself. This is because the query may involve intermediate results that are exponentially larger than the input. To address this issue, a study was conducted on the complexity of query evaluation with intermediate results of polynomial size. By restricting the number of individual variables used in the query, it was found that the gap between expression and combined complexity, and data complexity, is reduced or eliminated. This supports the common practice of minimizing intermediate relation sizes in database query evaluation and suggests using variable minimization as a strategy for optimizing queries.
41582	4158237	Behavioral-level synthesis of heterogeneous BISR reconfigurable ASIC's	This paper discusses behavioral-level synthesis techniques for designing reconfigurable hardware, specifically for three applications: fault tolerance, manufacturability, and application specific programmable processors (ASPPs). The focus is on efficient built-in self-repair (BISR) methods, which involve replacing a failed module with a backup. However, the paper introduces new heterogeneous BISR approaches that allow for replacing a failed module with a spare of a different type. These approaches use behavioral-level synthesis to explore the design space and include two methods: assignment and scheduling and transformations. Experimental results demonstrate the effectiveness of these techniques.
41582	415820	High level synthesis for reconfigurable datapath structures	This paper introduces high level synthesis techniques for creating restructurable datapaths, which can be applied in various scenarios such as fault tolerance, yield improvement, and designing programmable processors. The main focus of the paper is on built in self repair (BISR) techniques, which address fault tolerance and yield improvement. The BISR methodology includes two approaches that utilize the exploration capabilities of high level synthesis: one involving resource allocation, assignment, and scheduling, and the other involving transformations. The authors demonstrate the effectiveness of these approaches through experiments on benchmark examples.
41583	41583105	Exposure in wireless Ad-Hoc sensor networks	Wireless ad-hoc sensor networks bridge the gap between the Internet and the physical world. However, a major issue in these networks is coverage calculation, which is directly related to exposure. Exposure measures how well an object can be observed by the sensor network over time. In this study, the concept of exposure is formally defined and an algorithm for its calculation is developed. This algorithm is efficient and works for any distribution of sensors, sensor models, and network characteristics. It provides accurate results and its performance is evaluated through experimental results and scalability analysis. The algorithm can be used to determine the worst case coverage in sensor networks.
41583	41583117	Challenging benchmark for location discovery in ad hoc networks: foundations and applications	The authors have created a comprehensive and challenging benchmark data set for ad-hoc location discovery (LD). This data set consists of real-life distance measurement data and serves as a common ground for understanding, evaluating, and comparing LD algorithms. The data set was constructed using a novel analysis methodology that takes into account the difficulty of discovering locations, especially in dense networks where measurement noise can significantly impact the continuous optimization process. The authors also introduce new metrics and a fast simulation methodology to assess the difficulty of the continuous optimization problem. The benchmark data is publicly available and has been used to evaluate and compare six popular LD algorithms.
41584	4158427	On the computation of McMillan's prefix for contextual nets and graph grammars	Recent research has focused on using unfolding semantics for verification purposes. This began with a paper by McMillan, which presented an algorithm for creating a finite complete prefix of the unfolding of a safe Petri net. This allows for a condensed representation of the reachability graph. Extending this algorithm to contextual nets and graph transformation systems is challenging due to the potential for events to have multiple causal histories. A new abstract algorithm has been proposed that generalizes McMillan's construction for bounded contextual nets without encoding into plain P/T nets. To allow for an inductive definition of concurrency, the algorithm associates histories with both events and places. The proposed algorithm can also be applied to graph transformation systems, overcoming limitations of previous algorithms based on read arc encoding. 
41584	4158446	Contextual Petri nets, asymmetric event structures, and processes	The article presents an event structure semantics for contextual nets, an extension of P/T Petri nets. This new type of net allows transitions to check for tokens without consuming them, and is represented by asymmetric event structures, which replace symmetric conflict with a relation modeling asymmetric conflict or weak causality. The article introduces a categorical approach to the semantics of contextual nets, using a chain of coreflections to link the category of semi-weighted contextual nets to the category of finitary prime algebraic domains. An unfolding construction is used to generate an occurrence contextual net and an asymmetric event structure, and configurations of the latter are shown to form a finitary prime algebraic domain. The article also explores the relationship between the proposed unfolding semantics and other deterministic process semantics for contextual nets. 
41585	4158559	Hierarchical Design Rewriting with Maude	Architectural Design Rewriting (ADR) is a framework for designing dynamic software architectures using conditional rewrite rules and an algebraic presentation. The key features of ADR, including hierarchical design and inductively-defined reconfigurations, make it an expressive and versatile approach. This paper introduces Hierarchical Design Rewriting (HDR), a flavor of ADR that uses hierarchical graphs to handle system specifications with both symbolic and interpreted components. The authors also present a prototype tool based on Maude to support HDR. This demonstrates that HDR is both a formal approach and a practical framework for designing and analyzing software architectures. The paper also outlines a methodology for applying ADR concepts to other scenarios, using a specific algebra of designs and a particular scenario as an example.
41585	4158566	Orchestrating Transactions in Join Calculus	In this article, the principles of distributed transactions are discussed and an operational model is defined to meet basic requirements. A prototyping implementation is also presented using join-calculus. The model extends BizTalk with multiway transactions and utilizes a unique algorithm for distributed commit. It is also capable of handling dynamically changing communication topologies and can be applied to various languages. The model is based on a two-level classification of resources, making it easily adaptable to distributed calculi and languages, providing a consistent transactional mechanism. 
41586	4158632	Ubiquitous sketching for social media	The rise of digital social media has revolutionized how we communicate and manage our relationships, but the use of sketching as a social medium has been largely overlooked. This is a missed opportunity, as sketches have the ability to convey visual ideas quickly and effectively, require minimal detail, and capture the personal touch of handwriting. The integration of sketching into social media has the potential to enhance communication and make information sharing more timely. A study of a system called UbiSketch, which combines digital pens, paper, and mobile phones, found that it enabled users to take advantage of sketching's unique benefits and enhance social interaction. This highlights the potential for ubiquitous sketching to enrich our communication practices.
41586	4158621	A diary study of mobile information needs	The use of mobile devices has a significant impact on the types of information people seek and the methods they use to access it. This is due to the challenges of constantly changing location and social context, limited time for information retrieval, and the need to multitask. To improve the design of mobile technology, it is crucial to understand the information needs and interaction difficulties that arise in these mobile contexts. Through a two-week diary study, researchers found that people use a variety of creative methods to obtain necessary information depending on their available time, resources, and situational context. The study's key findings and their implications for mobile technology design are discussed.
41587	4158744	Task allocation learning in a multiagent environment: Application to the RoboCupRescue simulation	Coordinating agents in a complex environment is challenging, especially when certain task characteristics are unknown, such as the required number of agents. In these situations, agents not only need to coordinate on tasks, but also learn the necessary number of agents for each task. To address this issue, a selective perception reinforcement learning algorithm has been developed, allowing agents to learn the required number of agents for a given task. Despite the presence of continuous variables in the task description, the algorithm enables agents to learn their expected reward based on the task and number of agents. The algorithm has shown improved performance in the RoboCupRescue simulation environment.
41587	4158736	A modal semantics for an argumentation-based pragmatics for agent communication	This paper introduces a modal semantics for a new approach to conversational agents, based on social commitments and arguments. The formal framework, called Commitment and Argument Network (CAN), consists of three main elements: social commitments, agent actions, and arguments. These elements are formalized using a logical model called DCTL*CAN, which combines CTL* and dynamic logic. This model allows for a unified representation of the relationships between social commitments, actions, and arguments in agent interactions. The proposed semantics also highlights the connection between social commitments and arguments. The ultimate goal is to develop logic-based protocols for a unified framework of pragmatics and semantics in agent communication.
41588	4158842	Haptic teleoperation of a mobile robot: a user study	The article discusses the issue of controlling a mobile robot through shared autonomy. The robot has an onboard controller for obstacle avoidance, while the operator uses a haptic probe to indicate desired speed and turning. The robot's sensors provide obstacle-range data, which is converted into forces and reflected to the operator's hand through the probe. This haptic feedback, along with visual information from a front-facing camera, helps the operator navigate the robot effectively. Experiments with both virtual and real environments have shown that this added haptic feedback improves operator performance and presence, resulting in fewer collisions and increased distance from obstacles, without a significant impact on navigation time.
41588	4158845	Effects of haptic feedback, stereoscopy, and image resolution on performance and presence in remote navigation	The goal of teleoperation is to successfully complete a task in a remote environment while also providing a sense of being present in that environment. This is known as "presence" or "telepresence." Haptic feedback, which involves delivering tactile sensations to the user's hand, is a crucial element in achieving this sense of presence and improving task performance. In this paper, a method for using haptic feedback to teleoperate a mobile robot is described. The results of an experiment showed that haptic feedback significantly improved both task performance and user-felt presence. Stereoscopic images also had a positive impact on both factors, but high-resolution images only improved user-felt presence. However, when considering task performance, haptic feedback was found to be a more significant factor than image resolution and stereoscopy. 
41589	4158942	Probabilistic Principal Surfaces for Yeast Gene Microarray Data Mining	Recent technological advances have led to the production of large data sets across various scientific research fields. While each field has its own specific methods for analyzing data, there is a need for a more general approach to data analysis. In response, the concept of Probabilistic Principal Surfaces (PPS) has been proposed as a versatile tool for data visualization and clustering. PPS offers flexibility and can be applied to a wide range of data-rich fields. A real-life example of PPS being used to analyze yeast gene expression levels from microarray chips is also discussed to demonstrate its potential in data mining applications.
41589	4158925	A multi-step approach to time series analysis and gene expression clustering	The rapid increase in gene expression data requires the use of automated tools for processing and interpreting the data. A new machine learning data mining framework has been developed, consisting of a non-linear PCA neural network for feature extraction and probabilistic principal surfaces combined with an agglomerative approach for clustering gene microarray data. The method has a user-friendly visualization interface and can handle noisy data with missing points. It does not require any prior assumptions and can automatically determine the number of clusters in the data. The effectiveness of the method is confirmed by its successful application to the cell-cycle dataset and detailed analysis. The software is available upon request from the corresponding author and supplementary data can be found online.
41590	4159035	Spectral indexes of quality, diversity and stability in fuzzy clustering.	In this article, the authors discuss the issue of quality assessment in clustering and propose new methods to measure the effectiveness of clusterings. They focus on using fuzzy paradigms to provide more flexibility in evaluating clusterings. The first approach involves analyzing the co-association matrix to evaluate individual clusterings. The second approach compares the co-association matrices of different clusterings using established partition comparison indexes. The authors also suggest using indexes from spectral graph theory to assess clustering stability and diversity in ensembles of clusterings. By incorporating these methods, a more comprehensive assessment of clustering quality can be achieved. 
41590	4159027	An Experimental Comparison of Kernel Clustering Methods	This paper compares the performance of different kernel clustering methods on various data sets. These methods use central clustering and incorporate fuzzy clustering and kernel machines. The data sets cover different application domains and sizes. The paper also discusses techniques for validating results. The findings suggest that clustering in kernel space is generally better than standard clustering, but no method stands out as consistently superior.
41591	4159122	Vector quantization and fuzzy ranks for image reconstruction	Clustering and vector quantization are both methods used to organize and group data. Clustering uses a Voronoi partition, while vector quantization is a different technical problem. However, they have similarities in their approaches to creating a codebook and incorporating fuzzy concepts in their algorithms. The use of fuzzy concepts can also be applied in physical vector quantization systems, such as Neural Gas with a fuzzy rank function. This method can be used to improve the quality of images in lossy compression and reconstruction with vector quantization. 
41591	4159118	A survey of kernel and spectral methods for clustering	Clustering algorithms are powerful tools used in various fields to analyze data structures. This paper focuses on two recent approaches to the partitioning clustering problem: kernel and spectral methods. These methods are able to create nonlinear boundaries between clusters. The paper provides a comprehensive survey of these methods, which are essentially the kernel version of traditional clustering algorithms like K-means and SOM, and the spectral clustering method which is based on spectral graph theory and involves optimizing an objective function. The paper also includes a proof that these two methods share the same mathematical foundation. Additionally, fuzzy kernel clustering methods, which are extensions of kernel K-means, are discussed.
41592	4159271	Phase-based features for motor imagery brain-computer interfaces.	Motor imagery (MI) brain-computer interfaces (BCIs) allow individuals to control devices using their thoughts. Most MI BCIs use power features in the mu or beta rhythms, but some also use phase synchrony measures like the phase-locking value (PLV). In this study, researchers compared the performance of different phase-based features, including instantaneous phase difference (IPD) and PLV, in controlling a MI BCI. They found that IPD was a strong control signal, with stable phase relations between channels over time. Offline and online trials showed high accuracy for differentiating between MI classes using IPD, with results ranging from 84% to 99%. This was higher than the accuracy achieved using amplitude features, which ranged from 70% to 100%.
41592	4159212	Sensory threshold neuromuscular electrical stimulation fosters motor imagery performance.	Motor imagery (MI) has been extensively studied as a way to improve motor learning and restore motor functions. However, the reliability of MI brain patterns for many subjects is still a challenge. To address this, researchers have suggested using somatosensory feedback instead of visual feedback to enhance MI brain patterns. In this study, a new feedback method called sensory threshold neuromuscular electrical stimulation (St-NMES) was proposed. This method does not cause muscular contractions and does not interfere with the recorded MI brain pattern. The study found that St-NMES was more effective in inducing desynchronization over sensorimotor areas and enhancing MI brain connectivity patterns compared to visual feedback. It also improved classification accuracy and stability in novice subjects without causing any detectable artifacts. This suggests that St-NMES could be a promising feedback method for improving MI performance and could be used in online applications for brain-machine interfaces. 
41593	4159338	Extraction of Network Topology From Multi-Electrode Recordings: Is there a Small-World Effect?	The study of large-scale neural activity presents challenges for data analysis. To address this, the researchers propose a method for reconstructing functional networks from recorded spike trains. This involves using generalized linear models to estimate effective interactions between neurons, incorporating self-history, input from other neurons, and external stimuli. The resulting graph is analyzed for small-world and scale-free properties. The study found that cortical networks lack scale-free behavior but exhibit small-world structure. However, this structure is overestimated when only a localized sub-sample of neurons is used. This bias may affect previous experiments using multi-electrode recordings. 
41593	4159361	Towards a theory of cortical columns: From spiking neurons to interacting neural populations of finite size.	Neural population equations, specifically neural mass or field models, are commonly used to study brain activity on a larger scale. However, the relationship between these models and individual neurons is not well understood. This study aims to bridge this gap by deriving an equation for multiple interacting populations using a microscopic model of randomly connected generalized integrate-and-fire neurons. Each population consists of 50-2000 neurons of the same type, but different populations account for different neuron types. The resulting stochastic population equations reveal how spike-history effects in single-neuron dynamics interact with finite-size fluctuations on the population level. These equations are then used to efficiently simulate a cortical microcircuit containing eight different neuron types, allowing for the prediction of spontaneous and evoked responses. This approach provides a general framework for modeling neural population dynamics and offers a useful tool for analyzing cortical circuits and computations. 
41594	4159421	Definition, detection, and recovery of single-page failures, a fourth class of database failures	The traditional failure classes of system, media, and transaction failures have been expanded to include a fourth class called single-page failures. This class covers failures in reading data pages correctly, even after attempts at correction in lower system levels. To efficiently recover from these failures, a new data structure called the page recovery index is needed, which can be maintained transactionally with the same number of log records as current efficient logging and recovery methods. Detection and recovery of single-page failures can be done quickly enough to only cause a delay in data access, without the need to abort the transaction.
41594	4159461	Master-detail clustering using merged indexes	Merged indexes are B-trees that combine multiple traditional indexes and organize their records based on a common sort order. They are commonly used in relational databases to cluster related records, such as orders and order details. This approach shifts denormalization from the logical level of tables and rows to the physical level of indexes and records, resulting in improved performance and reduced I/O costs for joining related tables. To enable the design of merged indexes, a strict separation between B-trees and indexes is necessary, and this paper provides algorithms for data layout, concurrency control and recovery, update operations, adding and removing indexes, and enforcing relational integrity constraints. With these capabilities, merged indexes offer the potential to bring the benefits of master-detail clustering to traditional databases.
41595	4159514	Dual Generative Models For Human Motion Estimation From An Uncalibrated Monocular Camera	We have developed a new method for estimating human walking movements using images from a single camera. This method involves creating two models that represent the variability of walking patterns in terms of both kinematics and appearance. We also introduce a way to combine these models and a new algorithm for tracking and estimating walking in real-time. We tested our method on two different datasets and found positive results. Our approach has the potential to improve the accuracy of gait analysis and could be useful in various applications such as motion tracking and biometrics.
41595	4159510	Optimal Power Allocation for Distributed Detection Over MIMO Channels in Wireless Sensor Networks	The paper discusses the challenges of optimizing detection performance in distributed detection systems with wireless sensor networks where communication between sensors and a fusion center is imperfect. The use of a multiple-input/multiple-output (MIMO) channel model and the J-divergence as a performance criterion are proposed. The paper also studies power allocation schemes with individual and total transmitter power constraints on the sensors, finding a tradeoff between communication channel quality and local decision quality. A weighted water-filling algorithm is proposed for the case of orthogonal channels, which has shown to consume significantly less power compared to an equal power allocation scheme while achieving the same performance. Simulation results demonstrate the effectiveness of the proposed power allocation scheme.
41596	4159623	The development of community members' roles in partnership research projects: An empirical study.	Over the past 3 years, the authors and their colleagues have worked with 11 nonprofit community groups to help them improve their control over information technology. Through a research project, the authors focused on informal learning methods that could benefit the groups and implemented them with key community representatives. In this article, the authors reflect on the progress of two individuals from different organizations in terms of their information technology skills and abilities. They identify four roles that individuals may take on in relation to technology-consumers, planners, doers, and sustainers-and discuss the methods used to promote informal learning. The authors also offer implications for future research on informal learning in community settings.
41596	4159622	Participatory design in community computing contexts: tales from the field	With the increasing integration of technology into our daily lives, there is hope that it can be used to achieve positive outcomes in communities such as increasing access to local information, promoting civic engagement, and facilitating collaboration and communication. However, most studies on community computing portray community members as passive users of existing systems rather than active contributors to the design process. The Civic Nexus project aims to change this by involving community groups in a three-year participatory design project to increase their ability to solve local problems using cutting-edge technology. This approach allows community members to take control of the design process and maintain the technology infrastructure. The paper discusses the challenges faced by designers in engaging in participatory design in community computing settings, based on their experiences working with three community groups.
41597	415971	Distance realization problem in Network Tomography: A heuristic approach	This paper suggests a heuristic method for solving the distance realization problem in Network Tomography, where the goal is to estimate a network's internal structure and link performance using end-to-end measurements. The distance realization problem involves reconstructing a graph or topology from a matrix of pairwise distances between terminal nodes. While there are efficient algorithms for tree realization, finding the optimal realization for a general graph is proven to be NP-hard. The proposed heuristic approach involves three stages: finding a tree realizable distance matrix, constructing a tree, and adjusting for differences between the original and tree realizable distance matrices. It also aims to increase the network's betweenness-centrality measure while meeting the distance constraints.
41597	4159737	Control Languages Associated With Tissue P Systems	We propose a method to link a language to the actions of a tissue P system. Each rule is given a label from an alphabet, and only rules with the same label or an empty label can be used in a transition. This assigns a string to each successful computation, called the control word. The collection of all control words from computations in a tP system creates the control language. We examine the control languages of tP systems and compare them to finite, regular, context-free, context-sensitive, and recursively enumerable languages.
41598	4159812	CARNA - alignment of RNA structure ensembles.	New algorithmic advancements have made it easier to compare RNA sequences using the gold standard of simultaneous alignment and folding. However, this approach has limitations as it only compares RNAs with a single consensus structure. To address this issue, a web server has been introduced that allows for multiple alignment of RNAs with multiple conserved structures and pseudoknots. The server uses base pair probability dot plots to represent input and output information, providing flexibility in the input and allowing for visual analysis of results. The server uses advanced algorithms to optimize all structural similarities in the input simultaneously, making it more efficient than conventional approaches. It performs on par with general-purpose RNA alignment tools and is freely available at http://rna.informatik.uni-freiburg.de/CARNA.
41598	4159815	Structator: fast index-based search for RNA sequence-structure patterns.	The secondary structure of RNA molecules plays a crucial role in their function and is often more conserved than their sequence. As a result, finding RNA molecules in databases requires matching both sequence and structure. However, current tools for this task have limited efficiency and cannot take advantage of complementarity constraints. To address this, a new method using affix arrays – a type of index data structure – has been developed. This approach allows for efficient bidirectional pattern search and incorporates a novel chaining method to remove irrelevant matches. In benchmark tests, this method was found to be much faster than previous methods, making it suitable for searching large RNA databases. The software, called Structator, is available for free.
41599	4159975	A Concurrent G-Negotiation Mechanism for Grid Resource Co-allocation	To achieve the vision of the Grid, it is important to have a strong co-allocation of resources for computationally intensive applications. This can be challenging as resource providers and consumers may have different needs and goals. This work proposes a novel concurrent mechanism that coordinates multiple negotiations between a consumer and multiple resource providers, and manages commitments during the negotiation process. The mechanism includes a utility-based coordination strategy and three commitment strategies, and simulations show that it outperforms existing models in terms of utility, negotiation speed, and success rate. This work highlights the importance of efficient co-allocation in realizing the potential of the Grid.
41599	4159957	Concurrent Negotiation and Coordination for Grid Resource Coallocation	The Grid vision requires efficient resource allocation to support computationally intensive applications. This can be challenging due to different requirements from both resource providers and consumers. This paper proposes a concurrent negotiation mechanism for Grid resource allocation, which involves managing commitments through negotiations and coordinating multiple negotiations between consumers and providers. The paper introduces a utility-oriented coordination strategy, three classes of commitment management strategies, and negotiation protocols. Experiments were conducted to compare the proposed strategies with others in different resource market types, and the results showed that the proposed strategies achieved higher utility and success rates. It was also found that the three classes of commitment management strategies performed differently in different resource market types.
41600	4160035	Trace semantics via determinization.	This paper discusses trace semantics in coalgebra theory and compares two different approaches that have been used to study trace semantics in recent years. The first approach uses final coalgebras in Kleisli categories and has some limitations, while the second approach utilizes final coalgebras in Eilenberg-Moore categories. The paper presents a systematic study of these two approaches and shows that they are equivalent in cases where both can be applied. This provides a better understanding of trace semantics and its applications in various types of transition systems.
41600	4160057	Dijkstra and Hoare monads in monadic computation	Recently, Dijkstra and Hoare monads have been introduced as a way to capture weakest precondition computations and pre- and post-conditions in program verification, using a theorem prover. In this article, a more general description of these monads is given in a categorical setting. The program semantics is viewed as a triangle of computations, state transformers, and predicate transformers. By instantiating this triangle with different computational monads, the Dijkstra monad associated with the monad T can be defined. Abstract definitions of the Dijkstra and Hoare monad are then provided, with the requirement of a suitable categorical predicate logic on the Kleisli category of the underlying monad. It is shown that there are maps of monads (Hoare) ¿ (State) ¿ (Dijkstra), all parametrised by a monad T, when this structure exists.
41601	41601232	Behavioural biometrics: a survey and classification	This study focuses on behavioural biometrics, which involves identifying individuals based on their unique skills, style, preferences, knowledge, and strategies in daily tasks like driving, talking on the phone, or using a computer. The authors review current research and analyze the features used to describe different types of behaviour. They also compare the accuracy of various behavioural biometric methods for user verification and discuss potential privacy concerns that may arise from their use. Overall, this study provides a comprehensive overview of the current state and potential future implications of behavioural biometrics.
41601	4160182	Direct And Indirect Human Computer Interaction Based Biometrics	This paper discusses the current state of human computer interaction (HCI) based biometrics, including both direct and indirect approaches. Direct HCI biometrics use characteristics such as abilities, style, preference, knowledge, or strategy to identify users while they work with a computer. Indirect HCI biometrics involve monitoring users' behavior through computer software actions. The paper examines current research and analyzes the types of features used to describe HCI behavior. It also compares accuracy rates for different HCI-based biometric approaches and addresses privacy concerns. The authors present the results of experiments using direct and indirect HCI-based behavioral biometrics for intrusion detection.
41602	4160278	A linear time algorithm to list the minimal separators of chordal graphs	Kumar and Madhavan developed a linear time algorithm for finding minimal separators in chordal graphs. This paper presents a new algorithm that also has a linear time complexity. However, unlike Kumar and Madhavan's algorithm, this one can work with any type of PEO, such as Lex BFS, instead of requiring a specific type (MCS PEO) to be computed first. This makes it more versatile and useful for chordal graphs with different properties.
41602	4160243	A High Girth Graph Construction and a Lower Bound for Hitting Set Size for Combinatorial Rectangles	The article presents two main findings. The first is a deterministic algorithm that creates a graph with a girth of logk(n) + O(1) and minimum degree of k - 1, using the number of nodes n and edges e = ⌊nk/2⌋ as inputs. These graphs are expanders for subsets with a size of nδ, where δ is either n or k. The second finding is a lower bound of m/8Ɛ for the size of hitting sets for combinatorial rectangles with a volume of Ɛ. This is an improvement from the previous lower bound of Ω(m+1/Ɛ+log(d)). The known upper bound for the size of the hitting set is m poly(log(d)/Ɛ).
41603	4160339	A Grammar-Guided Genetic Programming Framework Configured For Data Mining And Software Testing	Genetic Programming (GP) is a powerful technique used to solve various problems, but most researchers create customized GP tools for specific problems. These tools often require significant changes to be adapted to new domains. This paper introduces Grammar-Guided Genetic Programming (GGGP) as a solution to this limitation. The authors present a framework called Chameleon, which can be easily configured to solve different problems. The effectiveness of Chameleon is demonstrated through its use in two domains not commonly addressed by existing literature: mining relational databases and software testing. The results show that using the GGGP approach leads to more versatile GP frameworks and can be beneficial in diverse domains.
41603	4160326	Grammatically Based Genetic Programming for Mining Relational Databases	The paper discusses the importance of knowledge discovery in enterprise information systems and the need for intelligent tools for automated data mining. It explores the use of Grammatically Based Genetic Programming (GGP) for mining relational databases, specifically for classification. GGP is a powerful induction technique that can be used for various problems and can also be used to induce classifiers. This approach also uses grammars for knowledge representation, which allows for complex structures and relations to be represented. A framework called GPSQL Miner was developed using this approach, which utilizes SQL features for fast access to data. Results from experiments on various databases show that this approach is effective and efficient.
41604	4160440	Cost Tradeoffs in Graph Embeddings, with Applications (Preliminary Version)	An embedding of a graph G in a graph H is a one-to-one mapping of the vertices of G to vertices of H. The cost of this embedding can be measured by the dilation-cost, which is the maximum distance between adjacent vertices in G, and the expansion-cost, which is the ratio of the size of H to the size of G. This paper presents three cases where minimizing one of these costs results in a significant increase in the other. For example, there is an embedding of n-node complete ternary trees in complete binary trees with low dilation-cost but high expansion-cost. Similar tradeoffs are shown for generic binary trees, which can efficiently embed all n-node binary trees with minimal dilation-cost, but high expansion-cost. This concept is applied to computational systems with computation trees, such as Turing machines and context-free grammars.
41604	4160493	Cost Trade-offs in Graph Embeddings, with Applications	An embedding of a graph G in a graph H is a way of associating the vertices of G with the vertices of H, where each vertex in G corresponds to a unique vertex in H. There are two measures of the cost of an embedding: dilation-cost, which is the maximum distance between adjacent vertices in G mapped to H, and expansion-cost, which is the ratio of the size of H to the size of G. This paper discusses situations where minimizing one cost leads to a significant increase in the other. For example, embedding complete ternary trees in complete binary trees with dilation-cost 2 and expansion-cost O(n~), but any embedding with a lower expansion-cost must have a much higher dilation-cost. Similar trade-offs are shown for generic binary trees and their ability to efficiently embed all n-node binary trees. This has implications for computational systems with a "computation tree" concept, such as Turing machines and context-free grammars.
41605	4160528	Online Ensemble Learning for Imbalanced Data Streams.	Cost-sensitive learning and online learning are both extensively studied, but there has been limited research on dealing with both issues simultaneously. To address this challenge, a new learning framework is proposed in this paper. The framework combines online ensemble algorithms with batch mode cost-sensitive bagging/boosting algorithms. This bridges two separate research areas and introduces a batch of theoretically sound online cost-sensitive bagging and boosting algorithms. Unlike other online cost-sensitive learning algorithms, the proposed algorithms have guaranteed convergence under certain conditions and have been validated with benchmark data sets. This demonstrates the effectiveness and efficiency of the proposed methods.
41605	4160565	Online Boosting Algorithms For Anytime Transfer And Multitask Learning	Transfer learning and multitask learning are two common problems that have been extensively studied, resulting in a plethora of models and algorithms. However, most existing approaches assume that data from different domains are given all at once, which is not the case in many real-world scenarios. This is particularly important when dealing with limited data, as in the case of detecting new seizures in patients with epilepsy. To address this issue, the authors propose two novel online boosting algorithms for transfer learning and multitask learning, which are able to adapt to sequential data and leverage knowledge from other domains. Experimental results demonstrate their effectiveness in standard benchmarks and in detecting new seizures with minimal previous data.
41606	4160674	Sketching freeform meshes using graph rotation functions	The article introduces a new method for creating free-form meshes with consistent topology. The 2D curve is interpreted as the projection of the 3D curve with the least curvature, and a topology-consistent strategy is used to trace simple faces on the interconnected 3D curves. This allows the system to automatically identify 3D surfaces. Delaunay triangulation is then applied to the boundary curves of the faces, and harmonic interpolation is used to compute the shape of the triangle mesh. Real-time algorithms are provided for curve generation and surface optimization. The incorporation of topological manipulation enhances the feasibility and benefits of automatically generated models in geometrical modeling.
41606	4160634	Laplacian-based Design: Sketching 3D Shape	This paper introduces an interactive modeling system for creating free-form surfaces using 2D sketches. The system uses an algorithm to find the 3D curve with minimum curvature that corresponds to the 2D sketch. The 3D surfaces are then automatically identified and triangulated using Delaunay triangulation. The shape of the triangular surface mesh is computed using harmonic interpolation, solving Laplacian equations. Experimental results show the effectiveness of the system on different types of sketches. Keywords include curvature, interpolation, free-form surfaces, and sketch.
41607	4160745	Semantic metadata mediation: XML, RDF and RuleML	This paper discusses the challenges of dealing with different types of information, such as data, metadata, and knowledge, in a decision-making system. The focus is on integrating heterogeneous metadata and proposing a mediation model that can reconcile the differences in structure and meaning. The ultimate goal is to provide users with a transparent view of the metadata from various sources, including XML, RDF, and RuleML models. This is achieved by using ontology to manage both structural and semantic heterogeneity. The proposed mediation architecture allows for the decomposition of complex queries into specific queries for each source, followed by the recombination of the results. 
41607	4160748	Merging ontology by semantic enrichment and combining similarity measures	This paper introduces a new method for merging OWL ontologies by enriching initial ontologies with semantic information. The goal is to reconcile and combine stored information in a decisional system, including data, metadata, and knowledge, through mediation. The approach involves enriching initial ontologies with a set of metadata that annotates concepts with synonyms and homonyms, using WordNet or expert input. This creates a thesaurus for each local ontology, which is then used to build a global thesaurus. The method also includes computing semantic similarity between concepts in the ontologies, using a combination of syntactic, lexical, structural, and semantic techniques. The result is a merged ontology generated from a correspondence matrix. 
41608	4160832	Initial Assessment of Architectures for Production Systems	This article discusses the use of production systems in artificial intelligence and expert systems and their limitations in terms of speed. The PSM project aims to address this issue by exploring the use of hardware support for production systems. The project's initial findings suggest that the most effective architecture for production systems is one with a small number of fast processors. The article also discusses the Rete algorithm used in production systems and the need for hardware solutions due to limited potential for speed improvements through software techniques. Different methods for speeding up production systems are also explored.
41608	4160850	Efficient sparse matrix factorization on high performance workstations—exploiting the memory hierarchy	The performance of workstation-class machines has significantly improved in recent years, with affordable options now offering 10-20 MIPS and 1-5 MFLOPS. These machines rely on a small amount of high-speed cache memory to achieve their higher performance. This paper focuses on the problem of Cholesky factorization on a high-performance workstation and identifies the cost of moving data between memory and the processor as the main limitation. Two techniques are proposed to address this issue: reducing the number of memory references and improving cache behavior. Experiments on a DECstation 3100 using benchmarks from the Harwell-Boeing Sparse Matrix Collection show that the resulting factorization code is almost three times faster than SPARSPAK. The paper concludes that these techniques are crucial for effectively using high-performance workstations for large numerical problems.
41609	4160950	Regularity and Conformity: Location Prediction Using Heterogeneous Mobility Data	Mobility prediction is crucial for providing proactive location-based services and intelligence for businesses and governments. Previous studies have shown that human mobility is highly predictable and influenced by social conformity. However, current prediction methods do not effectively combine these factors and struggle with incorporating diverse mobility data. To address these limitations, a new hybrid model is proposed that integrates both regularity and conformity of human mobility, and also incorporates a gravity model to learn location profiles from heterogeneous data. The model is evaluated using various city-scale datasets and outperforms existing approaches in terms of accuracy and percentile rank. Results also indicate that human mobility predictability varies over time, with weekdays being more predictable than holidays and predicting unvisited locations being more challenging on weekdays. 
41609	41609138	Who Will Reply to/Retweet This Tweet?: The Dynamics of Intimacy from Online Social Interactions.	This paper explores the dynamic nature of friendships through online social interactions. Previous studies have focused on social ties as binary relations or fixed values, but this paper takes into account factors like reciprocity, temporality, and contextuality. The authors propose a model that uses machine learning to predict repliers and retweeters on a microblog-based social network, taking into account user-level and tweet-level features. They collected a dataset of 73.3 million interactions between friends and found that their model outperforms existing models in predicting these interactions. This has implications for applications in online social networks.
41610	416109	Inferring common origins from mtDNA	In this article, the authors discuss how human migratory events can be traced through the study of DNA variations. By analyzing non-recombinant mtDNA and Y-chromosome sequences, it is possible to identify migrations that occurred between 30,000-70,000 years ago. Coalescence theory allows researchers to trace these sequences back to a common ancestor and use genetic drift mutations as markers to infer past migration and founder events. However, most mutations seen today are relatively recent and not useful for studying deep ancestry. The authors present results from analyzing 1737 mtDNA sequences and using principal component analysis and unsupervised ensemble clustering to identify substructure within haplogroups. They also introduce a new algorithm to mitigate sample size bias and reveal unbiased population events. The analysis shows that the African clades have the greatest heterogeneity and suggests that the M and N haplogroups originated from separate migrations out of Africa. The authors also identify and label branches of the mtDNA tree with high reliability and provide detailed SNP patterns for each haplogroup.
41610	4161034	Weighted pooling--practical and cost-effective techniques for pooled high-throughput sequencing.	The cost of sequencing large cohorts of individuals is still too expensive, despite the decreasing cost of sequencing technology. To address this issue, sophisticated pooling designs have been suggested that can identify carriers of rare alleles in large cohorts with fewer pools, resulting in significant cost reduction. These designs use combinatorial pooling, where each individual is either present or absent in a pool. A more efficient and cost-effective approach is to use 'weighted' designs, where individuals donate different amounts of DNA to the pools, allowing for the identification of both the number and identity of carriers. This weighted design is competitive in accuracy with combinatorial designs for rare variants and superior for common variants. It can also be incorporated into existing designs to improve accuracy. Ultimately, weighted designs have the potential to facilitate the detection of common alleles and can be a crucial component in large-scale sequencing projects.
41611	4161156	A clustering algorithm for radial basis function neural network initialization	The paper proposes a new algorithm, Output-Constricted Clustering (OCC), for initializing Radial Basis Function Neural Networks (RBFNN). OCC first roughly divides the output into partitions and then clusters the data within each partition based on input complexity. The algorithm introduces the concept of separability, which helps determine the appropriate number of sub-clusters within each partition. This results in better initialization for RBFNNs and improved approximation performance compared to existing methods. The effectiveness of OCC is demonstrated through several examples. 
41611	4161143	A structure evolving learning method for fuzzy systems	This paper introduces a new method for improving fuzzy systems called structure evolving learning. The algorithm works by reducing errors in a 'greedy' manner, starting with a simple fuzzy system and gradually adding more terms and rules. The main features of the algorithm include automated determination and control of fuzzy terms, evenly distributing error to improve accuracy, and using triangular membership functions for transparent and interpretable models. Three benchmark examples are provided to demonstrate the effectiveness of the proposed algorithm. 
41612	41612103	G-ToPSS: fast filtering of graph-based metadata	RDF (Resource Description Framework) is becoming increasingly popular for representing metadata. One application of RDF on the Web is RSS (RDF Site Summary), which has gained significant popularity. However, the current operation of RSS systems is not efficient for large-scale distribution of content. To address this issue, the authors have developed G-ToPSS, a scalable publish/subscribe system that is suitable for applications dealing with high-volume content distribution from multiple sources. G-ToPSS utilizes ontology to provide additional information about the data and also supports RDFS (RDF Schema) class taxonomies. The authors have implemented and tested G-ToPSS, and the results show its superior scalability compared to other alternatives.
41612	4161286	Optimized cluster-based filtering algorithm for graph metadata	The paper discusses the importance of efficient graph-based metadata filtering in the current state of the internet, with the abundance of information and RSS offerings. Matching graph-based documents can be costly due to the complexity of the language, and a centralized architecture is not suitable for large scale dissemination. To tackle these challenges, the authors propose a cluster-based publish/subscribe system with two indexing algorithms for workload distribution and filtering. They also introduce an optimized graph matching algorithm to speed up constraint evaluation for subscriptions. The results of experiments show that the system can support a million subscriptions on a cluster of 5-20 nodes and has linear scalability with the number of nodes. 
41613	4161343	Multivariate selection of genetic markers in diagnostic classification.	In the field of gene expression data analysis, machine learning models face unique challenges due to the large number of variables compared to the number of cases. Identifying relevant genes or groups of genes that can accurately classify a disease is crucial. While many machine learning algorithms have been developed for classification, gene selection has not been thoroughly explored. This study compares several algorithms for selecting gene markers for classification using the simple and efficient supervised learning algorithm of logistic regression. The results from 10 different datasets show that a conditionally univariate algorithm is a viable option for quickly identifying gene markers for disease. Furthermore, logistic regression shows similar performance to more complex algorithms and has a reasonable gene selection process. The algorithm is also easily accessible online, making it a useful tool for future research and development.
41613	4161358	Privacy-preserving heterogeneous health data sharing.	This paper proposes a new algorithm for privacy-preserving data publishing in healthcare, which takes into account both relational and set-valued data. It uses a probabilistic generalization approach and adds noise to ensure e-differential privacy. The disclosed data can be used effectively for decision tree induction classification. The algorithm outperforms existing solutions and is scalable, but may have limitations when dealing with large health databases. Overall, it allows for the disclosure of sensitive data in a differentially private manner while retaining important information for analysis. 
41614	4161441	WASP: Scalable Bayes via barycenters of subset posteriors.	Bayesian methods have been touted for their potential in handling big data, but their full potential has not been achieved due to the lack of scalable computational algorithms. To address this issue, a new approach called the Wasserstein posterior (WASP) has been proposed. This approach involves running a posterior sampling algorithm in parallel on different machines for subsets of a large data set, and then combining these subset posteriors using the Wasserstein barycenter through a highly efficient linear program. This results in an atomic form of the estimate, making it easier to estimate posterior summaries of functionals of interest. The WASP approach allows for easy scaling of smaller data sets to handle massive data, and theoretical justification is provided for its posterior consistency and algorithm efficiency. Examples of its application in complex settings such as Gaussian process regression and nonparametric Bayes mixture models are also presented.
41614	416142	Nonparametric Bayes Classification and Hypothesis Testing on Manifolds.	This article focuses on predicting a categorical response variable by using features on a general manifold, such as the surface of a hypersphere. The authors propose a kernel mixture model for the joint distribution of the response and predictors, with the kernel expressed in product form and dependence induced through an unknown mixing measure. They provide conditions for estimating both the joint distribution and the conditional distribution of the response, using a Dirichlet process prior and von Mises-Fisher kernels when the manifold is a unit hypersphere. Bayesian methods are developed for efficient posterior computation. The article also discusses Bayesian nonparametric methods for testing differences in distributions between groups on the manifold and provides computational methods for calculating the Bayes factor. The proposed methods are evaluated through simulations and applied to spherical data.
41615	4161539	A generalized maximum entropy approach to bregman co-clustering and matrix approximation	Co-clustering is a data mining technique used in various applications such as text clustering, microarray analysis, and recommender systems. A recent approach has been proposed that applies to empirical joint probability distributions. However, there is a need for a more generalized co-clustering framework that can handle different types of matrices and constraints. In this paper, the authors introduce a framework that allows for the use of any Bregman divergence in the objective function and considers various conditional expectation based constraints. This approach is based on the minimum Bregman information principle, which is a generalization of the maximum entropy principle. The methodology also includes new algorithms and incorporates previously known clustering and co-clustering algorithms.
41615	4161572	Approximation Algorithms for Bregman Co-clustering and Tensor Clustering	In recent years, there have been advancements in solving the Euclidean k-means problem, such as Bregman clustering, co-clustering, and tensor clustering. However, these methods are also NP-hard, making it difficult to find optimal solutions. Researchers have developed approximation algorithms for k-means and k-medians, but there are none for Bregman co-clustering and tensor clustering. This paper introduces the first guaranteed methods for these types of clustering, including an approximation factor for tensor clustering with arbitrary metrics. Experiments have shown that these methods are effective and have practical applications.
41616	4161631	Distributed Computing with Adaptive Heuristics.	This article discusses the use of distributed computing to study dynamic environments where computational nodes follow adaptive heuristics. These heuristics are simple rules of behavior, such as "best replying" and minimizing "regret", that have been extensively studied in game theory and economics. The article explores when these simple dynamics can converge to an equilibrium in asynchronous computational environments, and presents a non-termination result for a broad class of heuristics with bounded recall. The implications of this result in various applications, including game theory, circuit design, social networks, routing, and congestion control, are also discussed. Additionally, the article examines the computational and communication complexity of asynchronous dynamics and suggests further research in both distributed computing and game theory.
41616	4161653	Distributed computing with rules of thumb	The authors discuss their recent research on dynamic environments where computational nodes follow simple rules of behavior, such as "best replying" and minimizing "regret." They focus on understanding when convergence to an equilibrium point is guaranteed in asynchronous interactions, as seen in Internet protocols and large-scale markets. The paper presents a non-convergence result and its implications in various applications, including routing, congestion control, game theory, social networks, and circuit design. The authors also explore the relationship between this result and classical nontermination results in distributed computing theory, as well as the impact of scheduling on convergence and the complexity of asynchronous dynamics. They also make observations on the effects of asynchrony on no-regret dynamics.
41617	4161759	Handling Missing Values in Rough Set Analysis of Multi-Attribute and Multi-Criteria Decision Problems	Rough sets are a useful tool for analyzing decision problems involving objects described in a data table with condition and decision attributes. However, in practical applications, the data table often contains missing values. To address this, an extension of the rough set methodology has been proposed. This extension applies to both the classical rough set approach based on indiscernibility relations and the new approach based on dominance relations. These adapted approaches consider missing values as directional statements comparing a subject to a referent object with no missing values. The resulting rules are robust as they are supported by at least one object with complete data in the corresponding attributes or criteria.
41617	4161732	Variable Consistency Model of Dominance-Based Rough Sets Approach	DRSA, or Dominance-based Rough Set Approach, is an extended rough set model that is used to consider preference-orders. The rough approximations within DRSA are based on the consistency in the sense of dominance principle, which states that objects with not-worse evaluations than a referent object should not be assigned to a worse class. However, this strict principle can lead to inconsistencies that make it difficult to find patterns in large data sets. To address this issue, a relaxation of the dominance principle called variable-consistency model (VC-DRSA) is proposed. This model allows for some inconsistent objects to be included in the lower approximations, with the level of consistency controlled by an index. The use of VC-DRSA is demonstrated through an example of customer satisfaction analysis for an airline company.
41618	4161829	Study on a Location Method for Bio-objects in Virtual Environment Based on Neural Network and Fuzzy Reasoning	The difficulty in accurately determining the position of a bio-object in a complex and uncertain environment is a major challenge in designing a manipulator location system. To address this issue, a research study focused on developing an accurate location method using binocular stereo vision in a virtual environment. An experimental platform was established to capture images of the bio-object and environment using binocular stereo vision. Errors in the handling process and imaging were identified and addressed through the use of a neural network and a training system. Fuzzy rule sets were also extracted and modeled to account for the various factors in a natural environment. A fuzzy reasoning model was created based on the experience of agricultural experts to improve the accuracy and intelligence of the location system. A location simulation system was developed using VC++ and EON SDK to automatically simulate the process of a picking manipulator locating a bio-object.
41618	416187	H.264/AVC Video Encoder Implementation Based on TI TMS320DM642	This paper discusses the development and optimization of the H.264/AVC "baseline" encoder on the TMS320DM642 DSP hardware platform. This is a challenge due to the high computational complexity of H.264/AVC, and the limited on-chip memory space available. The paper focuses on optimizing memory space allocation, data transfer strategies, and pixel interpolation in motion compensation. The use of ping-pong buffering and bi-linear interpolation techniques were found to improve the coding performance while maintaining real-time encoding of video sequences with QCIF resolution. However, there was a slight decrease in PSNR compared to the "x264" source program. Overall, the improved encoder showed better coding efficiency and performance compared to previous standards.
41619	4161977	STR3: A path-optimal filtering algorithm for table constraints.	Constraint propagation is a crucial aspect of Constraint Programming (CP) and involves using filtering algorithms to enforce Generalized Arc Consistency (GAC) on various types of constraints. Recent advancements in GAC algorithms for extensional constraints have focused on manipulating tables directly during search. One such approach is Simple Tabular Reduction (STR), which maintains tables of constraints and their relevant tuples. In this paper, a new GAC algorithm called STR3 is proposed, which is designed specifically for enforcing GAC during backtrack search. STR3 has the advantage of avoiding unnecessary traversal of tables, making it optimal along any path of the search tree. Experimental results show that STR3 is competitive with other state-of-the-art GAC algorithms. 
41619	416198	Improving the lower bound of simple tabular reduction	Simple Tabular Reduction (STR) is a filtering technique used to enforce Generalized Arc Consistency (GAC) on positive table constraints. The current version, STR2, has a limitation where the lower bound is equal to the number of domain values in the problem, making it less efficient. This is due to the need to justify each value every time STR2 is called. To address this issue, a new version has been designed that incorporates watched tuples. This revised algorithm has shown better performance on problems with frequent small changes and can be more than twice as fast on structured problems.
41620	4162041	Universal 2-State Asynchronous Cellular Automaton With Inner-Independent Transitions	This paper introduces a new type of cellular automaton, a computational model in which cells can have only two states and are updated asynchronously. The update function for each cell takes into account the states of neighboring cells at different distances. This automaton possesses the property of inner-independence, meaning that a cell's state only depends on its neighborhood and not its previous state. This is significant because previous research on inner-dependence has only been done in synchronous cellular automata. The model's universality is proven by constructing three circuit primitives that are universal for a specific class of circuits. This new automaton has potential applications in classical spin systems.
41620	4162014	Embedding universal delay-insensitive circuits in asynchronous cellular spaces	Asynchronous Cellular Automata (ACA) are cellular automata that allow for random and independent updates of cells. These unpredictable behaviors are often dealt with by simulating a timing mechanism to force synchronicity, but this leads to an increased number of cell states. This paper presents a more efficient approach using a 5-state ACA with von Neumann neighborhood and rotation- and reflection-symmetric transition rules. This allows for efficient computation by embedding Delay-Insensitive circuits, which can handle arbitrary delays without affecting correct operation. This approach not only proves the computational universality of the ACA, but also utilizes its parallelism without signals running indefinitely in the absence of input.
41621	4162112	Reversible computing and cellular automata—A survey	Reversible computing is a new approach to computing that aims to reflect the reversible nature of physical processes. This paper explores the concept of reversible computing, discussing how computation can be done in a reversible system, how a universal reversible computer can be built using reversible logic elements, and how these elements are linked to reversible physical phenomena. The paper also highlights the differences between reversible and conventional computing systems, and how even simple reversible systems or logic elements can have universal computational capabilities. Different models of reversible computing, including reversible logic elements, reversible Turing machines, and reversible cellular automata, are also discussed in this survey/tutorial paper.
41621	4162143	Reversible Computing Systems, Logic Circuits, and Cellular Automata	Reversible computing is a type of computing that is based on the principle of physical reversibility and is expected to play a significant role in future computing systems that utilize microscopic physical phenomena for logical operations. This article discusses the theoretical aspects of reversible computing, including the implementation of a reversible computer as a reversible logic circuit, the composition of reversible logic circuits using reversible logic elements, and the realization of reversible logic elements in physically reversible systems. Despite the constraint of reversibility, it is possible to construct universal reversible computers using simple reversible primitives. These systems have a unique and distinct way of carrying out computations compared to conventional computing systems.
41622	4162253	Proof planning with multiple strategies	Proof planning is a technique used in theorem proving that involves a knowledge-based planning process instead of a blind search. This approach utilizes mathematical knowledge at a human-level of abstraction and employs methods and control rules to create an abstract proof plan that can be expanded using tactics. The paper proposes more flexible refinements to this method by adding a strategic level of control through meta-reasoning. This allows for better cooperation between problem-solving strategies and is implemented in the Multi system. The effectiveness of this approach is demonstrated through several large case studies.
41622	4162241	Flexibly Interleaving Processes	We discuss the challenges faced in constructing proof plans based on analogies, which can make it difficult or costly to find solutions for complex problems. These issues arise due to the fixed order of matching, reformulation, and replay in case-based reasoning, as well as the limited combination of planning from first principles and the analogy process. To address these problems, we propose interleaving matching and replay, and incorporating case-based planning and planning from first principles. Additionally, we suggest a more versatile approach of using different planning strategies to solve a wider range of problems and improve efficiency.
41623	416239	Resource management using multiple feedback loops in soft real-time distributed object systems	The paper presents a Resource Management System for a distributed object system that operates in real-time. The system uses a three-level feedback loop to manage resources, which includes a profiling algorithm to monitor resource usage, a least laxity scheduling algorithm to schedule tasks, and hot spot and cooling algorithms to balance loads by allocating and migrating objects. This system consists of a single Resource Manager for the entire distributed system, as well as a Profiler and a Scheduler on each processor. By utilizing these components, the Resource Management System can effectively manage resources and ensure efficient operation of the distributed object system.
41623	4162366	Total ordering algorithms.	The authors of this study have developed new algorithms for ordering messages in a distributed system that can handle a certain number of faulty processors. These algorithms are designed for use in a local area network using broadcast communication, such as Ethernet or token ring. They have been proven to be partially correct and have a probabilistic termination, but it was also demonstrated that there is no algorithm that can guarantee termination. The complexity of these algorithms is compared in the study.
41624	4162458	Maturity Model for Liquid Web Architectures.	Liquid Web applications are designed to seamlessly adapt to different connected devices and flow between them according to the user's attention. Unlike traditional centralized architectures, where the application's data and logic are stored on a single Web server, Liquid software requires decentralized or distributed architectures to achieve this mobility between clients. This is achieved by breaking down the application's architecture into layers, using the Model View Controller design pattern. A maturity model is proposed, which outlines different levels of application deployment and synchronization across multiple devices. The goal of this model is to help developers understand and control the design of Web applications that follow the liquid user experience paradigm, and provide a gradual path for existing applications to evolve.
41624	4162438	Atomic distributed transactions: a RESTful design.	The REST architectural style is good for interactions between clients and a single server, but it cannot guarantee reliable interactions for more complex scenarios that involve multiple servers. In this paper, we propose a lightweight design for combining RESTful services in a transactional manner. This design, based on the Try-Cancel/Confirm (TCC) pattern, does not require any changes to the HTTP protocol. It assumes that resources are designed according to the TCC pattern and ensures that they are not aware of the transaction. The responsibility for achieving atomicity is delegated to a coordinator, which offers a RESTful API for managing the transaction.
41625	4162537	Team Knowledge and Coordination in Geographically Distributed Software Development	Coordination plays a crucial role in software development as it leads to various benefits such as cost savings, shorter development cycles, and better integration of products. Previous research has focused on team knowledge as the key factor in coordination, but this has primarily been studied in real-time collocated tasks. This study aims to fill the gap by investigating how team knowledge affects coordination in geographically distributed software work. The findings reveal that software teams have three main types of coordination needs- technical, temporal, and process- which vary depending on the individual's role. The study also shows that geographic distance has a negative impact on coordination, but this can be mitigated by shared team knowledge and presence awareness. Additionally, the results highlight the importance of shared task knowledge in coordination among collocated team members. The study proposes further research in this area to gain a better understanding of the impact of team knowledge on coordination in software development.
41625	4162542	Influence of social and technical factors for evaluating contribution in GitHub	Open source software is often seen as a meritocracy, where decisions are based solely on technical skill. However, research suggests that there is a complex social structure at play in these environments. Platforms like GitHub make these social relationships transparent, allowing developers to use both technical expertise and social connections when evaluating contributions. A study on open source software contribution in GitHub found that project managers consider both technical value and social connections when evaluating pull requests. They also take into account the number of comments on a pull request and the submitter's prior interaction with the project. Established projects tend to be more selective in accepting pull requests. This shows that developers consider both technical and social factors when evaluating contributions to open source software.
41626	41626140	A Bayesian Approach to Joint Feature Selection and Classifier Design	This paper introduces a Bayesian approach for simultaneously learning an optimal nonlinear classifier and identifying relevant predictor variables. Using heavy-tailed priors, the approach promotes sparsity in both the utilization of basis functions and features. These priors act as regularizers for the likelihood function, which rewards good classification on training data. An expectation-maximization (EM) algorithm is derived to efficiently compute a maximum a posteriori (MAP) point estimate of the parameters. The algorithm is an extension of existing sparse Bayesian classifiers, which are similar to support vector machines. Experiments using kernel classifiers show effective feature selection and high classification accuracy on various data sets.
41626	41626119	Variational Bayes for continuous hidden Markov models and its application to active learning.	This paper introduces a varitional Bayes (VB) framework for learning continuous hidden Markov models (CHMMs) and explores its application in active learning. Unlike traditional training procedures, VB-based training provides a full posterior estimate of the model parameters, which is crucial for small training sets as it indicates the accuracy of the model. The paper considers three active learning algorithms within this framework and presents experimental results for both synthetic and measured data. The results show that active learning methods can greatly reduce the amount of required labeling compared to random selection, making them useful for reducing model-parameter uncertainty in CHMMs.
41627	4162726	Dual greedy polyhedra, choice functions, and abstract convex geometries	The concept of dual greedy systems and their associated polyhedra, known as dual greedy polyhedra, have been studied by various researchers for different applications. These systems involve maximizing a linear objective function by finding tight inequalities in a greedy manner. Faigle and Kern, as well as Kruger, have looked at dual greedy systems in the context of antichains and partially ordered sets. Kashiwabara and Okamoto have studied them for extreme points of abstract convex geometries. Frank has proposed a related dual greedy algorithm for lattice polyhedra. This paper explores the relationships between dual greedy systems, substitutable choice functions, and abstract convex geometries. It also investigates the submodularity and facial structures of dual greedy polyhedra and considers an extension of this type of polyhedra. 
41627	4162712	A note on Faigle and Kern’s dual greedy polyhedra	In their recent paper, U. Faigle and W. Kern have expanded upon their previous work and that of M. Queyranne, F. Spieksma, and F. Tardella by demonstrating that a dual greedy algorithm can be used to solve a system of linear inequalities with {:0,1}-coefficients defined by antichains of a poset and a submodular function on the set of ideals. Additionally, they have found that this problem can be classified as a submodular flow problem, making it easily solvable using their greedy algorithm. This discovery has important implications for solving similar problems in the future.
41628	4162852	The independent even factor problem	This paper presents a solution to the independent even factor problem, which is a generalization of both the matching problem and the matroid intersection problem. The authors establish a min-max formula for odd-cycle-symmetric digraphs, which includes the Tutte-Berge formula for matchings and the min-max formula of Edmonds (1970) for matroid intersection. They also propose a combinatorial algorithm that efficiently finds the maximum independent even factor in these digraphs, and this algorithm is a combination of two existing algorithms. The running time of the algorithm is O(n4Q), and it also provides a common generalization of two other decomposition methods. This algorithm gives a constructive proof of the min-max formula and introduces a new operation on matroids.
41628	4162817	On the Kronecker Canonical Form of Mixed Matrix Pencils	A mixed matrix pencil is a type of matrix pencil that contains both fixed constants and independent parameters. This paper discusses how the indices of nilpotency of the Kronecker canonical form for a mixed matrix pencil can be described using matroids. This result can also be used to develop an algorithm for determining the rank of a power product of a square mixed matrix.
41629	4162969	Voyagers and voyeurs: supporting social data analysis	In recent years, there has been a rise in online services that use interactive visualizations to collect and analyze data. This has led to the emergence of social data analysis, where experts and non-experts can collaborate and interact with data. This approach has various applications, such as promoting political transparency, enhancing business intelligence, and facilitating citizen science. However, in order to fully realize the potential of social data analysis, there is a need for further innovation in the design of collaborative data management systems. The speaker will discuss recent efforts to use the internet as a platform for collective data creation, management, and analysis, and explore how these systems can be designed to foster social collaboration in data management and exploration. 
41629	4162970	Enterprise Data Analysis and Visualization: An Interview Study	Organizations rely on data analysts to use various tools and techniques to improve customer engagement, streamline operations, increase production, inform decision-making, and combat fraud. However, there has been limited research on how these analysts work within the social and organizational context of companies. To gain a better understanding, the authors conducted interviews with 35 data analysts from 25 organizations in different industries. The results reveal the process of industrial data analysis and how organizational factors affect it. The study identifies common challenges and barriers to using visualization tools and suggests design implications and research opportunities for improving visual analysis in the enterprise context.
41630	4163036	Microdiversity on Rician fading channels	In this study, the performance of an L-branch equal gain combiner on both slow and nonselective Rician fading channels is analyzed and compared to matched filter receivers with coherent phase shift keying and noncoherent frequency shift keying modulation. The performance is evaluated based on the probability distribution of the signal-to-noise power ratio (SNR) and the average bit error rate (BER). Results using maximal ratio combining and selection diversity combining are presented for comparison. The study also investigates the effects of gain unbalance between branches of the combiner on the SNR distribution and BER. The paper suggests that equal gain combiners may be a feasible alternative to maximal ratio combiners. In addition, the study presents an efficient method for computing the distribution of sums of Rician random variables, which may be useful for other problems involving Rician fading. The suitability of using a Nakagami model to represent Rician fading environments is also examined and a formula for determining the corresponding parameters is provided. The results may be applicable to both microcellular and mobile satellite fading channels. 
41630	4163025	SER of M-ary NCFSK with S + N Selection Combining in Nakagami Fading for Integer m	The study focuses on the performance of M-ary orthogonal noncoherent frequency-shift keying (NCFSK) with N branch signal-plus-noise (S + N) selection combining (SC) in Nakagami-m fading. Both independent, identically distributed (i.i.d) and independent, nonidentically distributed (i.n.d) diversity branches are considered, and two S + N SC receiver structures are examined. The performance of S + N SC is compared to that of classical SC and square-law combining (SLC) receivers, taking into account the modulation order, fading parameter, and number of diversity branches. Results show that in i.n.d fading channels, classical SC outperforms S + N SC for small SNR values, while S + N SC has better performance for moderate to large SNR values. Increasing the diversity order also increases the performance gap of S + N SC over classical SC and SLC in both i.i.d and i.n.d Nakagami-m fading channels.
41631	4163135	A Cellular Learning Automata-based Algorithm for Solving the Coverage and Connectivity Problem in Wireless Sensor Networks.	In wireless sensor networks, redundant nodes are common due to the likelihood of failures and the need for a long lifetime. To address this, a distributed algorithm is proposed which uses cellular learning automata to select a minimal subset of active nodes that cover the entire network area while also maintaining connectivity. Each node has a learning automaton that decides whether it should be active based on its remaining energy and its neighbors' situations. The algorithm also determines the transmission range of sensor nodes to ensure connectivity. The time and space costs are compared to existing algorithms and simulation results show its effectiveness, especially in high failure and energy depletion scenarios.
41631	4163181	A New Learning Automata-Based Sampling Algorithm For Social Networks	In recent years, studying social networks has become increasingly important in various fields, including finance and communication. However, due to the large amount of data and privacy concerns, there is limited access to complete network data. As a result, network sampling has emerged as a way to study real networks. This paper proposes a new sampling algorithm, based on a distributed learning automata (DLA) called extended DLA (eDLA), for complex social networks. The algorithm is evaluated on several test networks and compared to existing sampling algorithms in terms of accuracy and statistical tests. The results show that the eDLA-based algorithm outperforms other methods, with a 26.93% improvement in average Kolmogorov-Smirnov value for degree distribution.
41632	4163236	A Verification Mechanism for Secured Message Processing in Business Collaboration	In order to ensure secure message processing in business collaborations, a verification mechanism based on access policies is crucial. This is because incorrect role assignments or unqualified roles in collaborating organizations can lead to authorization policy conflicts and unreliable business collaboration. To address this issue, a role authorization model called Role-Net has been developed using Hierarchical Colored Petri Nets (HCPNs). This model allows for the specification and management of role authorization in business collaborations. A property called Role Authorization Based Dead Marking Freeness has also been defined using Role-Net to verify the reliability of business collaborations according to partners' authorization policies. Additionally, an algebraic verification method for secure message processing has been introduced. 
41632	4163253	TiCoBTx-Net: A Model to Manage Temporal Consistency of Service-Oriented Business Collaboration	Business collaboration involves sharing information and connecting business processes between organizations. Time-critical processes within and across organizations can become unreliable due to temporal inconsistency, where processes cannot follow agreed-upon schedules. To ensure temporal consistency in service-oriented business collaboration, a model called Timed Choreographical Business Transaction Net (TiCoBTx-Net) is proposed. This model, based on Hierarchical Colored Petri Net, allows individual business participants to specify and manage temporal consistency. A set of formalized temporal policies are checked in TiCoBTx-Net to enforce consistency at both design and runtime. A verification mechanism is also developed to identify any inconsistencies. The paper also provides details on implementing this mechanism.
41633	4163316	A probabilistic strategy for temporal constraint management in scientific workflow systems	Scientific workflow systems require timely completion of workflows, which is managed through temporal constraints. These constraints are set during workflow creation and updated during execution. However, current methods for setting constraints are user-specified and do not consider system performance, leading to frequent violations. Additionally, constraint updating is not well studied but is crucial for workflow management tasks. To address these issues, a probabilistic strategy utilizing a probability-based temporal consistency model is proposed. This strategy includes a negotiation process for setting coarse-grained constraints and a probability time deficit/redundancy propagation process for updating fine-grained constraints during execution. A case study demonstrates the effectiveness of this strategy. 
41633	416335	A context- and role-driven scientific workflow development pattern	Scientific workflow execution requires collaboration and intensive computing efforts, which differs from traditional process-centric workflows with fixed specifications. This poses challenges for traditional workflow development strategies in managing dynamic contexts and defining roles. To address this, the concept of application context spectrums is introduced to distinguish different profiles of scientific workflow development. A role enactment strategy is then proposed to enable workflow execution in specific application contexts. This approach enhances the validity of scientific workflow development by clearly identifying the relationship between computational subjects and objects in the system. A context- and role-driven development pattern is also proposed for executing scientific workflows on the Grid. A case study is presented to demonstrate the effectiveness of these methods. This content is copyrighted by John Wiley & Sons, Ltd.
41634	4163456	Energy Restrained Data Dissemination In Wireless Sensor Networks	Wireless sensor nodes can move within a designated area and communicate with nearby nodes within protocol limits. Since all communication in sensor networks is wireless, a peer-to-peer protocol is used between nodes. Some key considerations for sensor network design include increasing bandwidth needs, fast data retrieval, and efficient data transfer over wireless networks to meet the diverse needs of users. Traditional routing protocols do not address power management concerns, but an energy-efficient routing scheme is crucial. The proposed energy-saving information dissemination scheme is discussed in this paper, with experimental results showing significant energy savings compared to previous methods. 
41634	4163431	Reliable data transmission in mobile ad hoc sensor networks	The paper introduces a new routing scheme called RM-IDLF for mobile ad hoc sensor networks. This approach aims to efficiently transport information from source to sink while minimizing energy usage at both node and system levels. It is resource-aware and data-centric, integrating power economics by using local information within a sensor cloud and coordinating with neighboring nodes. The scheme uses metadata descriptors called 'labels' to communicate with other nodes and trade labels to create paths for information dissemination. The performance of RM-IDLF is evaluated for different node densities and compared to existing routing schemes. Results from a C++ simulator show significant energy savings compared to previous methods. 
41635	4163532	Pattern-Based Query Answering	Many users struggle to access information stored in databases due to a lack of knowledge of schemas and structured query languages. Previous efforts have focused on keyword-based searches, with précis queries generating multi-relation database subsets instead of individual relations. However, current approaches have limitations such as relying on user-provided weights and constraints and difficulty capturing different query semantics and user preferences. To address these issues, the proposed pattern-based approach utilizes a repository of précis patterns, which are enriched with tuples from the database to generate logical database subsets. This allows for more efficient and accurate retrieval of information from databases.
41635	416359	Comprehensible answers to précis queries	Users without experience with databases often struggle to access information stored in them due to the complexity of schemas and query languages. To address this issue, there have been efforts in both commercial and research settings to create keyword-based search methods. One such method is the use of précis queries, which generate logical subsets of multi-relation databases instead of individual relations. These subsets include not only directly related items but also those that are indirectly related. Previous studies have recognized the need to provide meaningful answers to novice users and have proposed translating précis query results into narrative form. This paper presents a semi-automatic approach to translating relational outputs of précis queries into a synthesis of results. The method involves using a template mechanism to generate a narrative précis, making use of reusable templates.
41636	4163620	Modeling and language support for the management of pattern-bases	Information overloading is a growing concern in modern web-based information systems. One solution to this problem is using knowledge extraction methods to produce concise representations of data, known as patterns. However, there has been little focus on creating a unified environment for representing and querying different types of patterns. This paper proposes a Pattern-Base Management System (PBMS) to address this issue. The paper includes a formal definition of the logical foundations for managing patterns, along with a formalism for pattern specification and safety restrictions. Additionally, the paper introduces predicates for comparing patterns and query operators. 
41636	4163657	CineCubes: Aiding data workers gain insights from OLAP queries	The paper discusses a system, CineCubes, that enhances query answering by providing a short data movie with insights into the results of an OLAP query. The process involves the user submitting a query, which is then complemented by additional queries and executed. The results are analyzed using highlight extraction algorithms to identify interesting patterns and values. The system then presents the results in a visual format accompanied by text and audio commentary, creating a "movie" for each query result. This movie is returned to the user as a PowerPoint presentation. This method aims to improve the user's understanding of the query results and provide a more interactive and engaging experience. 
41637	4163778	A Geodesic Active Contour Framework for Finding Glass	This paper discusses the challenge of detecting glass objects in images due to their appearance being influenced by the background. Instead of using a simple "glass or not" classification, the authors propose a more nuanced approach using binary criteria to guide the segmentation process. This entails combining measures of similarity between regions made of the same material and differences between regions made of different materials into an objective function. The geodesic active contour framework is then used to minimize this function and label pixels accordingly. The effectiveness of this approach is demonstrated through qualitative and quantitative experimental results. 
41637	4163790	On recognizing and positioning curved 3-D objects from image contours	The article discusses a method for connecting the shape of image contours to models of curved 3D objects, which can be used for object recognition and positioning. The object models are made up of surface patches and their intersection curves, which are commonly used in computer-aided geometric design and computer vision. The image contours in question are those created by the projection of surface discontinuities and occluding contours. Using elimination theory, the implicit equation of these contours can be constructed for objects observed under different projections. This equation is then parameterized based on the object's position and orientation in relation to the observer. This approach can also be applied to parameterized models and has been successfully tested on real images. 
41638	4163860	Illumination Cones for Recognition under Variable Lighting: Faces	This paper discusses the challenge of recognizing objects in images that have varying illumination. To address this issue, the authors propose an appearance-based method that uses a small set of training images to generate a representation called an "illumination cone." This representation can model the complete set of images of an object with Lambertian reflectance map under any combination of point light sources. The method is tested on a dataset of 660 images of 10 faces and outperforms existing methods. This method is an implementation and extension of a previous illumination cone representation proposed in another paper, and it also takes into account cast shadows. 
41638	4163862	What is the set of images of an object under all possible lighting conditions?	This paper discusses the relationship between an object's appearance, viewpoint, and lighting conditions. The authors explore the set of images that can be produced from an object under varying lighting conditions, and how this set can be represented as a convex polyhedral cone. They prove that the dimension of this cone is equal to the number of distinct surface normals, and that it can be constructed from three carefully chosen images. This concept has potential applications in object recognition. The authors also provide empirical evidence to support their findings.
41639	4163988	Ensembles of Restricted Hoeffding Trees	The success of simple classification methods has shown that complex attribute interactions are not always necessary for accurate predictions. This article proposes building an ensemble of Hoeffding trees, each limited to a small subset of attributes, to exploit this phenomenon in data streams. Exhaustive ensembles are created to consider all possible attribute subsets, and a weighting mechanism using sigmoid perceptrons is used to combine the trees and improve accuracy. The perceptrons' learning rate is set using the change detection method for data streams, and ensemble members are reset when they no longer perform well. Experiments show that this approach outperforms bagging for data streams, but at the expense of runtime and memory usage. Additionally, the proposed stacking method can further improve the performance of a bagged ensemble.
41639	4163989	Extremely Fast Decision Tree Mining for Evolving Data Streams	Real-time industrial applications produce a large volume of data every day, requiring fast and efficient methods to process it. Decision trees are popular in these applications because they are easy to visualize and understand, and when combined in an ensemble, they are a powerful machine learning method. This paper presents STREAMDM-C++, a C++ system that implements decision trees for data streams and has been used at Huawei. Unlike traditional decision trees, streaming decision trees can adapt to changes in the data over time. STREAMDM-C++ is easy to extend and includes more powerful ensemble methods and adaptive decision trees. A comparison with VFML, the current state-of-the-art implementation in C, shows that STREAMDM-C++ is faster and uses fewer resources.
41640	4164012	New Applications of the Incompressibility Method.	The incompressibility method is a simple but effective way of proving mathematical statements. It has been successfully applied in various fields. In order to showcase its strength and beauty, we present new straightforward proofs using this method.
41640	4164081	Average-Case Analysis of Algorithms Using Kolmogorov Complexity	The average-case complexity of algorithms is a challenging problem in computer science. Recently, the use of Kolmogorov complexity has proven to be a useful tool for analyzing this complexity. In this paper, the incompressibility method is introduced as a way to analyze average-case complexity. The method is demonstrated through several examples and is shown to be both powerful and simple. The paper also presents bounds for the average-case number of stacks and queues. Overall, the incompressibility method is a valuable approach for understanding the average-case complexity of algorithms.
41641	4164146	Efficient Algorithms for the Closest String and Distinguishing String Selection Problems	The paper discusses three problems: the closest string problem, the farthest string problem, and the distinguishing string selection problem. These problems have various real-world applications, such as motif detection and genetic probe design. The closest string problem involves finding a center string that is within a certain Hamming distance from all input strings. The farthest string problem involves finding a center string that is at least a certain Hamming distance away from all input strings. The distinguishing string selection problem involves finding a center string that is close to one group of strings and far from another group. The paper presents improved fixed parameter algorithms for each of these problems, with running times of O(Ln + nd 23.25d ), O(Ln + nd 2^{3.25d_b}), and O(Ln + nd 2^{3.25d_b}) respectively.
41641	4164153	A three-string approach to the closest string problem	The closest string problem involves finding a new string that is within a certain distance to a given set of strings. This problem is known to be NP-hard and an approximation scheme has been developed to solve it. However, for small distances, parameterized algorithms have been developed. A new approach called the 3-string approach has been introduced in this paper, which improves the previous best algorithm for binary strings. This approach has also been extended to arbitrary alphabet sizes and has been shown to be better for small alphabets, such as DNA strings. The time complexity for this new algorithm is O(nL + nd1.612d(α2 + 1 - 2α-1 + α-2)3d), where α = 3√√|Σ| - 1 + 1.
41642	4164277	Selective Hardening in Early Design Steps	Hardening a circuit against soft errors should be done early in the design process to minimize costs. A successful approach is to target specific parts of the circuit that are critical and likely to cause system dysfunction if an error occurs. However, determining critical areas can be difficult without complete information. A gate-level selection strategy, without considering electrical or timing details, is proposed to identify these critical spots. The effectiveness of this strategy is confirmed using a new SER estimator. Despite using limited information, the results show a significant reduction in soft error susceptibility, surpassing other known topological strategies in terms of hardware and protection. 
41642	416427	On the Reliability Evaluation of SRAM-Based FPGA Designs	Field Programmable Gate Arrays (FPGAs) offer many benefits, making them widely used in various industries from consumer products to astronautics. However, their high susceptibility to soft errors, caused by the dense embedded SRAM cells, requires careful evaluation of their reliability. This is crucial in designing highly reliable systems, giving a competitive advantage in today's market. This paper introduces a mathematical model that can assess and improve the reliability of SRAM-based FPGAs. The model takes into account the device's failure rate, single-event upset (SEU) rate, and design characteristics, assuming a non-redundant design. It provides a time-dependent probability of correct operation, aiding in identifying weak areas and assisting CAD tools in enhancing reliability.
41643	416437	Typing on an Invisible Keyboard.	In order to save screen space, researchers have explored the idea of an invisible keyboard for mobile devices. Despite some challenges with accuracy and key overlaps, users were able to recall key positions and improve their typing speed with practice. By adapting the spatial model, the invisible keyboard was found to be 11.5% faster than simply hiding the keyboard. A 3-day user study showed that with practice, the typing speed on an invisible keyboard could reach a practical level, approaching that of a regular visible keyboard. Overall, the research suggests that an invisible keyboard with an adapted spatial model could be a viable and promising option for text entry on mobile devices.
41643	4164385	Performance and User Experience of Touchscreen and Gesture Keyboards in a Lab Setting and in the Wild	Two popular mobile text entry methods, Smart Touch Keyboard (STK) and Smart Gesture Keyboard (SGK), were studied for their performance and user experience. The first study was a lab-based experiment with ten sessions, while the second study used a new evaluation method called the experience sampling method (ESM). Participants in the ESM study installed an app on their phones for four weeks, which periodically recorded their text entry performance and user experience during daily activities. Results showed an average speed of 28 to 39 words per minute (WPM) and error rates of 1.0% to 3.6%. Touchscreen input, especially with SGK, had higher error rates. Both methods have strengths and weaknesses, and individual preferences play a role. STK is more effective for focused two-thumb typing, while SGK is better for one-handed typing in mobile situations. Users tend to prefer SGK after being exposed to both methods. Combining lab and real-world studies is the most reliable way to assess current and future text entry technologies.
41644	416446	Integrated Grasp Planning and Visual Object Localization For a Humanoid Robot with Five-Fingered Hands	This paper introduces a framework for grasp planning with a humanoid robot arm and a five-fingered hand, specifically for grasping objects in a kitchen environment. The framework utilizes an object model database that contains descriptions of all possible objects in the robot's workspace. Two modules, an offline grasp analysis system and a real-time stereo vision system, use this database to determine the best grasp for each object. The offline system uses simulation and CAD models, while the stereo vision system locates objects in real-time using appearance-based and model-based methods. These components are integrated in a controller architecture to achieve manipulation tasks for the robot. 
41644	4164410	Unions of balls for shape approximation in robot grasping	Future service robots will have to be able to grasp and manipulate a wide range of objects with different shapes and sizes. This is a difficult task because it involves considering multiple factors such as hand movements, object properties, and forces. Searching for stable grasps in this high-dimensional space is not feasible, so a new approach is needed. In this paper, a novel grasp planning method is proposed that uses a representation of an object's geometry called the medial axis. This allows for the evaluation of local symmetry properties and the generation of candidate grasps that are likely to be successful. These grasps are then tested for force-closure using a set of heuristics. The algorithm is demonstrated through simulations using a humanoid robot's hand. 
41645	41645106	Segmentation and learning of unknown objects through physical interaction	This paper presents a new approach for a humanoid robot to segment and learn about unknown objects without any prior knowledge of the objects or environment. The only requirements are that the object has a smooth surface with distinctive visual features and moves as a rigid body. The robot uses its visual and manipulative abilities to push hypothetical objects and gather information, allowing for successful segmentation and learning of the object's appearance from multiple angles. This model enables robust object recognition in cluttered environments.
41645	4164542	Combining Appearance-Based And Model-Based Methods For Real-Time Object Recognition And 6d Localization	The goal of image-based object recognition and localization is still difficult to achieve, so it is necessary to use different approaches for different problems. These approaches include global appearance-based, model-based, histogram-based, and local feature-based techniques. This paper focuses on recognizing and localizing solid-colored objects in 6D for real-time use on a humanoid robot. While model-based methods are efficient for objects with 3D lines and planes, they are not suitable for most real-life scenarios. On the other hand, appearance-based methods can work with any object geometry but are not often used for 6D localization. The proposed system combines the advantages of both approaches and can automatically recognize and localize solid-colored objects in 6D in real-time.
41646	416462	On the Convexity Number of Graphs	Convexity is a property of a set of vertices in a graph, where the set contains all vertices that lie on the shortest paths between any two vertices in the set. The maximum number of vertices in a convex set that does not include all vertices in the graph is known as the convexity number. The problem of determining whether the convexity number of a given bipartite graph is greater than or equal to a given integer is shown to be NP-complete. Additionally, the study of graphs with small convexity numbers reveals necessary extension properties and their relationship with upper bounds on the convexity number.
41646	4164623	On defensive alliances and strong global offensive alliances	This article discusses the complexity of determining defensive and strong global offensive alliances in graphs. The authors prove that it is NP-complete to determine if a given 6-regular graph contains a defensive alliance of a certain size. They also show that calculating the strong global offensive alliance number of a graph is APX-hard for cubic graphs and NP-hard for chordal graphs. They improve upon previous results by showing that the strong global offensive alliance number of a graph with minimum degree at least 2 is at least 3/4 of the total number of vertices. Lastly, they present a formula for calculating the strong global offensive alliance number of a graph based on its degree.
41647	4164735	Learning spectral graph segmentation.	The authors propose a graph learning algorithm for spectral graph partitioning that enables supervised learning of graph structures using labeled data. The algorithm uses gradient descent in the space of all feasible graph weights and involves computing the derivatives of eigenvec- tors with respect to the graph weight matrix. The authors show that these derivatives can be computed in an exact analytical form using the theory of implicit functions and demonstrate that the gradient converges exponentially fast in a simple case. In the context of image segmentation, the authors also demonstrate how to incorporate high level object prior into a shape detection process.
41647	4164719	Object recognition using boosted discriminants	The authors propose a method for object discrimination by learning efficient "codes" for each object class. These codes are built incrementally by constructing discriminants that focus on pairs of training images that are difficult to classify. The discriminants are chosen to partition the objects into two well-separated groups, and an objective criteria is used to find the optimal discriminant and partition. The optimization process alternates between solving two generalized eigenproblems. This method can be biased to focus on difficult pairs, allowing for sequential selection of discriminants. The approach is validated on a face discrimination task and shows favorable performance compared to other methods.
41648	4164830	Recognition of probe cographs and partitioned probe distance hereditary graphs	A graph G is a probe graph of a given class of graphs if it can be partitioned into two sets, one of which is an independent set, and then embedded into a graph of that class by adding edges. If the partition is part of the input, then G is a partitioned probe graph. This article presents a polynomial-time algorithm for recognizing partitioned probe distance-hereditary graphs, using a new data structure. The running time of the algorithm is ${O}(\mathfrak\it{n}^2)$, where $\mathfrak\it{n}$ is the number of vertices in the input graph. It also shows that both partitioned and unpartitioned probe cographs can be recognized in ${O}(\mathfrak\it{n}^2)$ time.
41648	416484	On the recognition of probe graphs of some self-complementary classes of perfect graphs	This paper discusses the recognition of probe graph classes, where a graph G is considered a probe graph of a given class if its vertices can be partitioned into probes and nonprobes, and G can be extended to a graph of the class by adding edges between certain nonprobes. The paper presents polynomial-time recognition algorithms for probe cographs, probe P4-reducible graphs, probe P4-sparse graphs, and probe splitgraphs. This work has practical applications in various fields such as bioinformatics and network analysis.
41649	4164915	Run-Time Support for Distributed Sharing in Typed Languages	DOSA is a new run-time system that efficiently supports shared objects in a typed programming language. It is able to distinguish between pointers and data at run-time, which allows for efficient fine-grained sharing through VM support. This eliminates false sharing and improves performance for fine-grained applications. Unlike previous systems, DOSA's approach allows it to perform equally well on both fine-grained and coarse-grained applications. Its architecture also allows for unique optimizations that are not possible in traditional shared memory systems. 
41649	416497	Software versus hardware shared-memory implementation: a case study	This study compares the performance of software-supported shared memory on a general-purpose network to hardware-supported shared memory on a dedicated interconnect. The results are based on executing a set of application programs on a SGI 4D/480 multiprocessor and on TreadMarks, a distributed shared memory system. TreadMarks performs similarly to the 4D/480 for applications with moderate synchronization, but the difference grows with increased synchronization frequency. For applications with high memory bandwidth requirements, TreadMarks outperforms the SGI 4D/480. For larger problems, a hardware implementation using a directory-based protocol on a dedicated interconnect shows better scalability compared to a software implementation on a general-purpose network. A combined approach using both software and hardware shows similar performance to the hardware implementation for applications with low to moderate synchronization rates and good locality. However, synchronization still remains a bottleneck for all approaches. 
41650	4165050	Synthesis Of Low-Cost Parity-Based Partially Self-Checking Circuits	The article discusses a method for creating efficient partially self-checking multilevel logic circuits with low-cost concurrent error detection (CED). By selecting a subset of inputs and disabling CED when these inputs meet certain criteria, the CED circuitry can be optimized. This methodology is particularly effective for targeting common faults and has shown to significantly reduce error rates in logic circuits. The approach is ideal for applications where cost-effective CED solutions are desired. Experimental results demonstrate the success of this method in achieving a more reliable and cost-efficient logic circuit design.
41650	4165031	Masking timing errors on speed-paths in logic circuits	This paper addresses the concern of timing errors caused by design marginalities and circuit aging in logic circuits. A low overhead solution is proposed to mask these errors on speed-paths within the circuits. By synthesizing a non-intrusive error-masking circuit with sufficient timing slack, timing errors can be masked at the outputs of the logic circuit. This circuit can also collect runtime information to predict wearout and aid in system debugging. Simulation results for various benchmark circuits and modules demonstrate the effectiveness of this solution, achieving 100% masking of timing errors on all speed-paths within 10% of the critical path delay with an average area and power overhead of 16% and 18%, respectively. 
41651	4165123	Smart-Card Implementation of Elliptic Curve Cryptography and DPA-type Attacks	This paper examines the effectiveness of smart-card implementations of elliptic curve cryptography in protecting against side-channel attacks, specifically those using differential power analysis (DPA) and its variations. The use of random curve isomorphisms is a promising method for preventing DPA attacks, but its implementation must be carefully considered. The paper presents several generalized DPA attacks against improperly implemented curve isomorphisms, including a second-order attack against an additive variant and a refined attack against a more general variant. Additionally, the paper provides a precise analysis of second-order DPA attacks, which is relevant for future research in this area. 
41651	4165122	Memory-efficient fault countermeasures	Boscher, Naciri, and Prouff proposed an efficient way to protect against fault attacks in a right-to-left binary exponentiation algorithm in 2007. This method was later expanded upon by Baek in 2010 for 2w-ary algorithms. In this paper, the authors modify and improve upon these algorithms by creating new error detection methods and reducing the required memory without sacrificing security or performance. This results in a full register of memory being gained compared to previous implementations, making these methods well-suited for applications where memory space is limited, such as smart-card implementations of exponentiation-based cryptosystems.
41652	4165235	Discovery and hot replacement of replicated read-only file systems, with application to mobile computing	The "hot replacement" mechanism described in this content allows for the replacement of files in a read-only file system while it is still mounted, without the user being aware. This can improve fault-tolerance and performance. The mechanism monitors the latency of operations on the file system and automatically seeks a replacement file system if the latency degrades. This is particularly useful for mobile computers that may move over a wide area, leading to variable response times and potential problems with latency, failures, and scalability. If a mobile client moves through regions with partial replicas of common file systems, this mechanism can provide increased fault tolerance and more consistent performance.
41652	4165219	A Versatile and User-Oriented Versioning File System	File versioning is a useful technique for keeping track of changes made to files over time. It has many applications, including backups, disaster recovery, and monitoring for security breaches. However, modern systems do not have an automatic and easy-to-use file versioning system. Existing backup solutions are slow and inflexible, often lacking backups for the most recent changes. Online disk snapshotting systems offer more detailed versioning but still do not capture the most recent changes. Furthermore, existing systems do not give individual users control over versioning policies. To address these issues, a lightweight user-oriented versioning file system called Versionfs was designed. It works with any file system and allows for customizable versioning policies. It creates versions automatically and transparently while maintaining Unix semantics. User-level utilities allow for configuring and enforcing default policies, as well as viewing, controlling, and recovering files and their versions. Performance evaluations showed minimal overhead for users under normal workloads.
41653	4165348	Image Retrieval Using Maximum Frequency of Local Histogram Based Color Correlogram	Color histogram and color Correlogram are two commonly used techniques for image indexing in content-based image retrieval (CBIR). While color histogram provides a global representation of the color distribution in an image, it is not robust to large changes in appearance and may yield similar results for different images with the same color distribution. On the other hand, color Correlogram takes into account both the color distribution and spatial information of pixels, making it a more efficient method for CBIR. The proposed algorithm in this paper uses a combination of histogram value divisions and maxima of frequencies to generate a Correlogram, which is then used to calculate the distance between query and database images. The algorithm is tested on a large database of images and shows promising results. 
41653	4165336	Feature extraction through generalization of histogram refinement technique for local region-based object attributes	Content based image retrieval (CBIR) is a method used to retrieve digital images from large databases based on their content. However, this method has limitations as it relies solely on histogram analysis of image intensity values, which can be insensitive to small changes in images and lead to similar histograms for different images. To address this, a new method called Histogram Refinement is proposed, which divides the pixels into classes based on their intensity values and calculates additional regional properties to better describe the image's structure. These region-based features are then used for image retrieval. This research was published in the International Journal of Imaging Systems and Technology in 2011.
41654	4165439	Deriving Production Rules for Incremental View Maintenance	Database systems use production rules to automatically maintain derived data, such as views. However, writing these rules can be a difficult and ad-hoc process. To simplify this task, a facility has been developed where a user can define a view using an SQL select expression. The system then automatically generates set-oriented production rules that maintain a materialization of the view. These rules are triggered by operations on the view's base tables and generally perform incremental maintenance by modifying the materialized view according to changes made to the base tables. However, for some operations, more significant recalculations may be needed. Algorithms have been created to analyze the view definition and determine when efficient maintenance is possible based on key information.
41654	4165442	Storing auxiliary data for efficient maintenance and lineage tracing of complex views	 gorithm is too expensive to be applicable in practice, but the heuristic algorithms we propose are efficient and produce near-optimal solutions, and (2) auxiliary views can significantly improve the efficiency of view maintenance and lineage tracingThe view maintenance process in a data warehouse can become complicated and inefficient as views become more complex. Storing auxiliary views in the warehouse can reduce this complexity and improve efficiency, as well as aid in answering lineage tracing queries. This paper addresses the problem of selecting which auxiliary views to materialize in order to minimize view maintenance and lineage tracing costs. The study focuses on relational views with aggregation operators and presents multiple selection algorithms. Experiments using the TPC-D benchmark and synthetic data show that the proposed heuristic algorithms are efficient and produce near-optimal solutions, demonstrating the potential for auxiliary views to greatly improve the efficiency of view maintenance and lineage tracing.
41655	4165562	Shape Based Detection and Top-Down Delineation Using Image Segments	The authors propose a segmentation-based detection and figure-ground delineation algorithm that primarily relies on the shape of objects instead of their appearance. The algorithm takes in an image and its bottom-up hierarchical segmentation and uses shape and color information to find a "coherent whole" or collection of segments that consistently vote for an object. A novel top-down figure-ground segmentation process is then applied to accurately delineate the object's boundaries by letting voting segments compete for interpreting each semantic part. This method can accurately detect and segment objects with complex shapes and handle occlusions, achieving results comparable to existing state-of-the-art methods. 
41655	4165531	Clustering Appearances of 3D Objects	Our method presents a way to cluster images of 3D objects without the need for supervision. It does this by dividing the images into groups that make up smooth and parallel surfaces in the space of all images. This method also uses sequences of images to improve the accuracy of the clustering. Because our method uses a non-Euclidean similarity measure, we have developed algebraic techniques to analyze the surfaces without first converting the images to a Euclidean space. This has been demonstrated by successfully applying our method to a large image database.
41656	4165653	Genetic Programming for data classification: partitioning the search space	Genetic Programming is a method used to evolve decision trees for data classification, but it often leads to large search spaces. To address this issue, several techniques from machine learning are used to refine and reduce the search space sizes for decision tree evolvers. These refinement methods have been found to improve the classification performance of the algorithms.
41656	4165617	Error-backpropagation in temporally encoded networks of spiking neurons	SpikeProp is a supervised learning rule designed for a network of spiking neurons that uses the timing of individual spikes to encode information. This rule overcomes the challenges of thresholding and allows for complex non-linear classification in fast temporal coding, similar to traditional error-backpropagation. Experiments were conducted on the XOR-problem, along with other benchmark datasets, to demonstrate the effectiveness of this algorithm. It was found that temporal coding requires fewer spiking neurons compared to instantaneous rate-coding. This suggests that SpikeProp is a more efficient approach for networks of spiking neurons. 
41657	4165718	Reverse engineering distributed algorithms	Distributed systems are complex and difficult for humans to understand, making informal reasoning unreliable. To address this, formal tools are needed for building and maintaining these systems. The authors propose a formal approach called "coarsement" for reverse engineering distributed systems, where an implementation is transformed into a high-level specification through a series of intermediate steps. This method adds structure to the system by breaking it down into layers, making it easier to understand and reason about. The authors demonstrate the effectiveness of this approach by applying it to a real-world distributed algorithm for message passing in a network.
41657	4165734	Safety Analysis in Formal Specification	Formal methods are useful tools for specifying and verifying the functionality of a system, especially for safety-critical systems. However, they do not always integrate well with more informal safety analysis techniques. This is where action systems, a formal approach to distributed computing, can be beneficial. With a strong mathematical foundation, action systems allow for rigorous reasoning about the correctness and behavior of the system being developed. This paper aims to demonstrate how safety analysis results can be incorporated into an action system specification using composition operators, in order to create robust and safe controllers for the system. 
41658	4165828	Automatic Detection of Feature Interactions Using Symbolic Analysis and Evolutionary Computation	Phorcys is a new approach for detecting unwanted failures caused by overlapping, conflicting behavior in independently-developed features, known as feature interactions. It uses both symbolic analysis and evolutionary computation at the requirements level to identify these failures, rather than looking for specific unwanted interactions. This allows for more comprehensive detection and provides guidance for mitigation strategies. Phorcys is the only technique currently available to use this combination of methods for detecting failures caused by feature interactions. The approach is demonstrated through its application to an automotive braking system with multiple subsystems. 
41658	4165821	Towards the detection of partial feature interactions	Discrete feature interactions occur when conflicting features affect the functionality of a system, leading to unsatisfactory results. However, adaptive systems can mitigate this uncertainty by combining partially satisfied or unsatisfied requirements. This results in satisficed features, which may still be partially unsatisfied due to feature interactions. In contrast to discrete interactions, these interactions lead to a proportional trade-off between conflicting objectives, rather than a complete failure. This paper introduces the concept of partial feature interactions, which allow for trade-offs between conflicting features. It also proposes a method to reduce the dimensionality of these trade-offs to assess their severity and identify optimal trade-offs. An example from the automotive domain is used to demonstrate the application of this method.
41659	4165933	The Agent Reputation and Trust (ART) Testbed Architecture	The ART Testbed is a new initiative that aims to establish a testbed for technologies related to agent reputation and trust. It serves as a competition platform for researchers to compare their technologies and also provides tools for customizable experiments. The testbed focuses on the art appraisal domain, where agents gather opinions from other agents to accurately value paintings for clients. This paper provides an overview of the domain problem and explains the implementation architecture of the testbed, including its Game Server, Simulation Engine, Database, User Interfaces, and Agent Skeleton. 
41659	4165939	The agent reputation and trust (ART) testbed	The ART Testbed initiative aims to create a testbed for agent reputation and trust technologies. It serves as both a competition platform for researchers to compare their technologies and a set of tools for customizable experiments. In the artwork appraisal domain, agents can purchase opinions and reputation information from other agents to accurately appraise paintings for clients. The testbed also offers data collection tools for storing, downloading, and replaying game data for experimental analysis. Overall, the ART Testbed is a valuable resource for researchers in the field of agent reputation and trust.
41660	4166092	Reconfiguration of Vibro-tactile Feedback Based on Drivers' Sitting Attitude	The use of sensory modalities such as vision and hearing in vehicles has increased due to the development of assistance systems, but this has also led to cognitive overload for drivers. Adding the sense of touch as an interaction channel can help alleviate this overload in a natural and non-distracting way. However, haptic feedback is affected by both the driver and environmental conditions, such as different body sizes and seating positions. To address these issues, a vibro-tactile seat with sensors and actuators can dynamically adjust the haptic output to ensure consistent perception for all drivers. This allows for a more intuitive and effective use of haptic technology in vehicles.
41660	41660105	Subliminal vibro-tactile based notification of CO2 economy while driving	The reduction of carbon dioxide emissions is a significant topic being discussed by society and government. Lower emission values would greatly impact automotive manufacturers as road transport contributes to about one fifth of CO2 emissions in the European Union. The individual driver could also be affected by regulations, as technology allows for the creation of a "personal carbon dioxide profile" that would calculate emissions from various forms of transportation and charge the individual accordingly. However, individuals are often not aware of their carbon dioxide consumption and how to drive efficiently. To address this issue, a vibro-tactile notification system integrated into cars is proposed to help drivers reduce emissions while driving. Real driving experiments have shown that drivers are more conscious of their driving efficiency and emit less carbon dioxide when receiving tactile feedback.
41661	4166159	Neural Animation and Reenactment of Human Actor Videos.	The proposed method for generating realistic animations of real humans involves using a video sequence and a controllable 3D template model of the person. This approach significantly reduces production costs compared to traditional methods that require a high-quality 3D model. A neural network is trained to translate simple synthetic images of the human model into realistic images. The network is trained using a combination of motion tracking and synthetic rendering to generate realistic imagery. This method can be used for reenactment and editing of existing videos. The results outperform other learning-based human image synthesis techniques. 
41661	4166132	Neural Rendering and Reenactment of Human Actor Videos.	The proposed method aims to generate realistic animations of human characters through the use of a video sequence and a controllable 3D template model. This approach reduces production costs and allows for realistic editing of existing videos. The technique involves training a neural network to translate synthetic images of the 3D model into realistic human imagery. The network is trained using tracked motion data and artist-designed skeleton motion. The results of the method outperform current state-of-the-art techniques in human image synthesis. This approach offers a cost-effective and efficient solution for generating video-realistic animations of humans under user control.
41662	416627	Improving multi-objective code-smells correction using development history	This paper presents a new approach to automated software refactoring recommendation, using a multi-objective optimization problem. The approach takes into account both the change history of the system and past refactorings applied to similar contexts. It is validated on five medium and large size software systems and compared to three state-of-the-art approaches and two existing metaheuristic algorithms. The proposed approach aims to improve the quality of software systems by minimizing code-smells, utilizing development history, and preserving construct semantics. The non-dominated sorting genetic algorithm (NSGA-II) is used to find the best trade-offs between these objectives. The results show that the approach is effective, with a high percentage of code-smells fixed and suggested refactorings being semantically coherent when the change history is incorporated.
41662	4166236	Concerned About Separation	The separation of concerns is an important concept in software development that helps manage complexity. There are various approaches to modularizing software based on different concerns, such as subject-oriented programming, composition filters, and aspect-oriented programming. However, there are fundamental issues that need to be addressed, including defining what a concern and an aspect are, which concerns can be separated, and which aspects can be combined. To better understand these issues, the focus should be on the semantics of separation of concerns rather than the mechanics of aspect-oriented software development. A proposed conceptual framework based on transformational view can provide a unified view of different aspect-oriented techniques and address the issue of separability.
41663	4166346	Vector bin packing with multiple-choice	In multiple-choice vector bin packing, we have n items that can be selected in different D-dimensional forms. There are T bin types, each with a cost and size. The goal is to pack the items in bins with the lowest overall cost. This problem is useful for scheduling in networks with guaranteed QoS, but has other applications too. We present an algorithm that approximates the solution with a cost of about ln D times the optimum. It runs in polynomial time when D=O(1) and T=O(logn). This expands on previous results for regular vector bin packing. We also present a PTAS for the multiple-choice version of multidimensional knapsack, where the goal is to pack a maximum weight set of items in one bin.
41663	4166343	Admission control with advance reservations in simple networks	The admission control problem involves finding a feasible schedule for connection requests in a network, with the goal of maximizing the total weight. This problem is studied in two simple topologies: the line and the tree. A 12c-approximation algorithm is proposed for the line topology, where c is the maximum number of requests on a link at any time. This result has implications for the rectangle packing problem. An O(logt)-approximation algorithm is also presented for the tree topology, where t is the size of the tree. In the loss minimization version, the goal is to minimize the weight of unscheduled requests. A c-approximation algorithm is given for this problem in the tree topology, based on a generalization of set cover.
41664	4166457	Directional Coherence Interpolation For Three-Dimensional Gray-Level Images	The paper introduces a new 3D gray-level interpolation method, called Directional Coherence Interpolation (DCI). This approach improves the visual quality of 3D rendering compared to traditional interpolation methods by utilizing directional image-space coherence. DCI estimates the maximum coherence directions (MCD) from local image intensity and incorporates a smoothness term. To enhance efficiency and robustness, a pyramidal search strategy is also utilized. This approach can incorporate image shape and structure information without the need for explicit object boundary representation. Experiments on synthetic and real medical images show that the proposed method is accurate and efficient in handling general object interpolation. 
41664	4166411	An efficient large deformation method using domain decomposition	This paper discusses an approach for efficiently simulating large deformations of flexible objects in computer graphics. The method uses the linear elasticity model and a finite elements method, with a domain decomposition approach to handle the large deformations. The authors also propose two methods to handle the large force-displacement matrix, which can be too large for densely tessellated objects. The first method compresses the matrix using spatial coherence, while the second method only pre-computes the force-displacement vectors for boundary vertices and uses Cholesky factorization to solve for internal vertices. Experimental results demonstrate the effectiveness and fast performance of this approach for interactive user manipulations on complex objects.
41665	4166513	Modeling and rendering of metallic patinas	This paper discusses the importance of weathering in image synthesis and presents an approach for modeling and rendering metallic patinas. Patinas are a film or incrustation on a surface caused by material removal, addition, or chemical alteration. The approach uses a layered structure and a collection of operators to simulate patina development, taking into account the object's geometry and environmental factors. The Kubelka-Munk model is used to represent the reflectance and transmission of light through the layers, resulting in a realistic simulation of the time-dependent appearance of metals. The approach is demonstrated with copper models. The paper falls under the CR categories of Computer Graphics: Three-Dimensional Graphics and Realism, and Computer Graphics: Methodology and Techniques.
41665	4166520	Sketching with projective 2D strokes	Freehand sketching has been a popular artistic medium for conceptual design due to its ability to quickly capture and convey design ideas. A new sketching approach has been developed to support the early stages of design by maintaining the fluidity of traditional freehand drawings. This approach bridges the gap between 2D drawing programs with fixed views and 3D modeling programs with flexible views. The application utilizes a projective representation of points on a unit sphere, allowing for the creation of unique re-projections from an initial perspective sketch. The user interface includes a virtual camera, projective grids, and the ability to overlay sketches with other images. Additionally, the system can align hand-drawn sketches with computer-generated ones, allowing for a seamless integration of both methods.
41666	416668	Formalization of resilience for constraint-based dynamic systems	Researchers in various fields are interested in creating systems that can withstand and recover from large-scale unexpected events. Current methods for evaluating resilience either focus on qualitative aspects or specific domains. This paper introduces a comprehensive computational model that can represent a wide range of constraint-based dynamic systems. Inspired by existing literature, the authors propose a parameterized property that captures the key elements of resilience regardless of the application domain. This new resilience property is used to assess the resilience of constraint-based dynamic systems.
41666	41666106	Modeling of Resilience Properties in Oscillatory Biological Systems using Parametric Time Petri Nets, Supplementary Information	This paper discusses the benefits of using parametric time Petri nets for analyzing the dynamic behavior of biological oscillatory systems, specifically focusing on their resilience properties. These properties are important for understanding how living organisms, such as the mammalian circadian rhythm, can adapt to changes in their environment. By formalizing these properties through parametric TCTL, the authors demonstrate how changes in environmental conditions can be managed to ensure the resilience of living organisms. They also show how this technique can be applied to a simplified model of the circadian clock, highlighting the significance of this approach for modeling dynamic biological systems.
41667	4166768	A fixpoint characterization of abductive logic programs	A new approach to understanding abductive logic programs is presented, where the belief models are defined as the fixpoint of a disjunctive program created through a program transformation. This transformation combines both negative and positive hypotheses in a consistent manner. The method is extended to cover abductive extended disjunctive programs and allows for a parallel bottom-up model generation process for computing abductive explanations from various types of programs with integrity constraints. This approach simplifies the process of finding abductive explanations and can be applied to a wide range of programs.
41667	4166770	On the equivalence between disjunctive and abductive logic programs	The paper discusses the relationship between disjunctive and abductive logic programs. It shows that the generalized stable model semantics of abductive logic programs can be translated into the possible model semantics of disjunctive programs and vice versa. It also proves that abductive disjunctive programs can be expressed as abductive logic programs under the possible model semantics. Additionally, the paper examines the use of disjunctive stable model semantics and its equivalence to the possible model semantics. This study provides a deeper understanding of the connection between these two types of logic programs and expands the potential applications of both.
41668	4166821	Dynamic Logic for Plan Revision in Agent Programming	This paper introduces a dynamic logic specifically designed for the agent programming language 3APL, which allows agents to have beliefs and plans that can be revised during execution. This makes it difficult to analyze plans using standard propositional dynamic logic. The proposed dynamic logic is tailored to handle plan revision and has a sound and complete axiomatization. It also discusses how this logic can be extended to non-restricted plans and provides examples of proofs using the logic. The paper also explores the connection between proving properties of 3APL agents and proving properties of procedural programs.
41668	4166848	Dynamic logic for plan revision in intelligent agents	The paper introduces a dynamic logic designed specifically for 3APL agents, which have beliefs and plans that can be revised during execution. Traditional methods of analyzing plans, such as structural induction, do not work for 3APL agents due to their ability to revise plans. The proposed dynamic logic is tailored to handle this aspect and a sound and complete axiomatization is provided. 
41669	416696	Admission control and scheduling for QoS guarantees for variable-bit-rate applications on wireless channels	Ensuring quality of service (QoS) over unreliable wireless channels is a major challenge for future applications. A model has been proposed to describe QoS requirements based on four criteria: traffic pattern, channel reliability, delay bound, and throughput bound. This model has been extended to handle variable bit rate applications and a sharp characterization of schedulability has been obtained. The results can be applied to various wireless applications, including video streaming, VoIP, and wireless sensor networks. Two main issues in QoS over wireless are admission control and scheduling. A necessary and sufficient condition for feasibility of a set of variable bit-rate clients has been analytically derived based on the model. Two scheduling policies have been proposed and shown to be optimal, and can be easily implemented on the IEEE 802.11 standard. Simulation results support the theoretical findings.
41669	41669132	Queueing systems with hard delay constraints: a framework for real-time communication over unreliable wireless channels	The text discusses the challenges of using wireless networks to serve real-time flows and proposes a model that considers delay bounds, unreliable channels, and throughput requirements. The feasibility of meeting client requirements is determined, and an efficient algorithm for admission control is proposed. The text then explores a scenario where clients have elastic throughput requirements but hard delay bounds, and formulates it as a utility maximization problem. A bidding game is suggested as a solution, where clients and the access point compete for resources. The access point's strategy can be implemented using a simple on-line scheduling policy, regardless of channel reliabilities. 
41670	416705	Weak Unit Disk and Interval Representation of Planar Graphs.	The article discusses a problem involving representing vertices of a planar graph using unit balls, where the balls representing adjacent vertices must intersect if their corresponding edge is labeled as "near." The problem is proved to be NP-hard, but it is shown that all series-parallel graphs can be represented in this way. The problem is also shown to be equivalent to a variant of graph coloring when considering the line. Examples of girth-4 planar and girth-3 outerplanar graphs that cannot be represented with unit intervals are given, but it is shown that all triangle-free outerplanar graphs and graphs with maximum average degree less than 26/11 can be represented. This provides a simple proof for the representability of planar graphs with large girth.
41670	4167034	Contact Representations of Sparse Planar Graphs.	CCA-representations are a way of representing graphs using circular arcs, where each edge is represented by an endpoint of one arc touching the interior of another. A graph is considered (2,k)-sparse if it has at most 2s-k edges in every s-vertex subgraph, and (2,k)-tight if it has exactly 2n-k edges, where n is the number of vertices. It is known that every graph with a CCA-representation is planar and (2,0)-sparse. It has also been shown that for k≥3, every (2,k)-sparse graph has a CCA-representation. However, the question of CCA-representability for (2,k)-sparse graphs with 0≤k≤2 is still open. This article provides a partial answer by presenting CCA-representations for various subclasses of planar (2,0)-sparse graphs. It is also shown that finding a CCA-representation for a plane (2,0)-tight graph with maximum degree 5 is an NP-complete problem. Additionally, a simple algorithm is described for representing plane (2,0)-sparse graphs with wedges using two circular arcs or straight-line segments.
41671	41671130	Approximate sequencing for variable length tasks	Czumaj et al. analyzed the Variable Length Sequencing Problem (VLSP) in their paper, which focused on efficient information gathering on the Web. They proved that VLSP is NP-complete and provided a polynomial time algorithm for a limited version and an approximation algorithm for a slightly less restricted version. However, the difficulty of the problem was further clarified by showing it is NP-complete to approximate within a factor nk for any fixed integer k. It was also proven that finding the optimal solution is NP-hard, even when the jobs follow a periodic property. To address the NP-hardness of VLSP, the paper introduces an optimal version that maximizes the number of completed tasks and presents an approximation algorithm with a factor of 2 and a polynomial time algorithm for a special case with restricted types of tasks.
41671	41671144	On the complexity of crossings in permutations	This article discusses the problem of crossing minimization for a set of permutations, where a crossing represents a mismatch between elements. The goal is to find a common permutation that minimizes the number of crossings, which is known as the Kemeny optimal aggregation problem in voting and social science theory. The problem can be phrased as a crossing minimization problem for bipartite graphs, and the authors contribute the max version, which minimizes discrimination against any permutation. They prove the NP-hardness of the problem for four permutations and establish a 2-2/k-approximation algorithm. The max version is also shown to be NP-hard and has a 2-approximation algorithm. The article also discusses the solution for two permutations and the open problem for three permutations.
41672	4167242	Coordinating Self-interested Planning Agents	This article discusses planning problems where multiple agents must work together on a set of tasks. Each agent is responsible for a subset of tasks and must create their own plan. However, since the agents are non-cooperative, they do not want to revise their individual plans when combining them into a joint plan. The article presents a formal framework for studying the computational aspects of this coordination problem and identifies factors that contribute to its complexity. An example of applying this framework to multi-modal logistic planning is also provided.
41672	4167216	Plan-Coordination Mechanisms and the Price of Autonomy	Task-based planning problems in multi-agent systems require agents to jointly plan for a set of tasks. However, due to task interdependencies, agents cannot make plans independently without risking conflicts. To address this, a plan-coordination mechanism is needed to ensure a conflict-free and feasible plan. This paper discusses the challenges of designing such a mechanism, including deciding if one is necessary, designing an arbitrary one, and designing an optimal (minimal) one. While finding an optimal mechanism is difficult, the paper presents an algorithm that can find a non-trivial one. It also explores the trade-off between autonomy and performance in using a coordination mechanism, with a case study on multi-modal transportation. The paper concludes by discussing the potential use of these mechanisms in a broader context and recent extensions dealing with temporal planning.
41673	4167376	Effectively closed sets and graphs of computable real functions.	This paper compares the computability and complexity of a continuous real function with its graph. It also analyzes the computability and complexity of functions on subspaces of the real line, such as the Cantor space, the Baire space, and the unit interval. Four types of effectively closed sets are defined based on the recursive enumerability of sets of closed or open intervals with nonempty or empty intersection with the set. The relationships between these four types of effectively closed sets are studied, both in general and for closed sets that are graphs of continuous functions. 
41673	416732	On speedable and levelable vector spaces	This paper examines the lattice of recursively enumerable (r.e.) subspaces of a recursively presented vector space V∞ in the context of complexity-theoretic speed-up properties. These properties, such as speedable, effectively speedable, levelable, and effectively levelable, were introduced by Blum and Marques. The authors study the relationship between an r.e. basis A for a subspace V of V∞ and V in relation to these properties. They show that if A or V is speedable (effectively speedable), then V is levelable (effectively levelable). They also provide comparisons between the lattice of r.e. subspaces and the lattice of r.e. sets with respect to these properties, including the existence of supermaximal spaces that are nonspeedable in all possible r.e. degrees. 
41674	4167457	A Light-Weight Framework for Bridge-Building from Desktop to Cloud.	In recent years, there has been a growing use of computational techniques in scientific research, allowing for virtual experiments to be conducted through computer modeling. This trend has expanded from large-scale projects to smaller ones, and has the potential to foster collaboration between researchers. The development of web service and cloud technology further supports this trend, but there are challenges in terms of accessibility for non-experts. To address these issues, a framework has been developed that allows for the deployment of web services on cloud infrastructure without modifications or technical knowledge. This framework has been tested and proven useful in image processing and multi-disciplinary optimization.
41674	4167455	Matchmaking Support for Dynamic Workflow Composition	This paper discusses the challenges of accurately and generally identifying services for both service description and discovery. Traditional discovery systems have focused on domain-specific techniques using single sources of knowledge, making maintenance and extension difficult. The authors propose a generic brokerage framework that uses plug-in components, which are web services themselves. This framework has been successfully used in the KNOOGLE project for discovering Grid services and integrating with the Taverna workflow system. The broker is domain-independent and uses multiple user-specified matchmaker plug-ins to provide domain-specific knowledge. A user-specified selection policy is then applied to determine the final choice of service. This approach allows for a comprehensive packaging of brokerage functionality through the use of supplied and user-defined matchers and selection policies. 
41675	4167517	Propagative hough voting for human activity recognition	Hough-transform based voting has been successfully used for object and activity detection, but it can struggle with limited training data. To address this, a new method called propagative Hough voting is proposed for activity analysis. Instead of individual feature voting, random projection trees (RPT) are used to match feature points in a high-dimensional feature space. These trees can index unlabeled feature points in an unsupervised way and propagate label and spatial-temporal information from training samples to testing data. This method does not rely on human detection and tracking, making it robust to variations in activity patterns. It outperforms state-of-the-art techniques on two benchmarked activity datasets, even with limited training data.
41675	4167551	Locality Versus Globality: Query-Driven Localized Linear Models for Facial Image Computing	Conventional subspace learning methods and recent feature extraction techniques prioritize globality for designing algorithms that can accurately classify images. In this paper, the authors propose a local approach to subspace learning, which incorporates locality criteria in the sample, feature, and learning spaces. This approach, measured by the discriminating power coefficient (DPC), leads to improved classification accuracy and computational efficiency. The authors also present a multimodal localized piecewise subspace learning framework that can be applied to various subspace learning methods. This framework unifies global and local learning methods and addresses the issue of numerical difficulty in large pattern classification tasks. The proposed query-driven locally adaptive (QDLA) mixture-of-experts model is shown to be effective, robust, and fast for large and complex datasets in face recognition and head pose estimation. 
41676	4167643	Constrained clustering with local constraint propagation	This paper discusses the problem of multi-class constrained clustering, where pairwise constraints are used to specify which data points belong to the same or different clusters. The authors propose a new algorithm, called Local Constraint Propagation (LCP), which can effectively incorporate the influence of these constraints onto the unconstrained data. LCP not only reveals the structures of the clusters, but also takes into account the influence of all pairwise constraints on each data point. The effectiveness of LCP is demonstrated through experiments on image segmentations.
41676	416764	Improving constrained clustering via swarm intelligence.	This paper introduces a new algorithm, called the constrained ant clustering algorithm, which mimics the behavior of ant colonies in the real world. The algorithm incorporates a heuristic walk mechanism based on random walk to handle constrained clustering problems that involve pairwise must-link and cannot-link constraints. The performance of this approach is evaluated on synthetic and real datasets and is found to be more effective compared to existing methods such as Cop-Kmeans and ant-based clustering algorithm. These results demonstrate the potential of our algorithm in solving constrained clustering problems.
41677	4167737	ATL with Strategy Contexts: Expressiveness and Model Checking	This study focuses on the alternating-time temporal logics ATL and ATL(star) with the addition of strategy contexts, which allow agents to commit to strategies during formula evaluation. This is in contrast to plain ATL and ATL(star) where strategies can be reset. The use of strategy contexts is shown to have significant expressive power, as it makes the extended logics ATL(sc) and ATLs(c)(star) equally expressive. Despite this, the study also proves that the model-checking problems for these logics remain decidable. This is achieved through the development of a tree-automata-based algorithm for model-checking ATL(sc)(star) on n-player concurrent game structures.
41677	4167762	Satisfiability Of Atl With Strategy Contexts	Recent developments in temporal logic have resulted in the introduction of various extensions of ATL, a logic used to express properties of multi-agent systems. These extensions include ATL(sc), which incorporates strategy contexts, and Strategy Logic, which allows for first-order quantification over strategies. However, the added expressiveness of these logics comes at a cost: model-checking becomes non-elementary and satisfiability is undecidable. This paper presents proofs that show satisfiability can be decided in certain cases, such as when the game is turn-based or when there is a bound on the number of available moves for agents. However, when strategy quantification is restricted to memoryless strategies, undecidability returns.
41678	4167828	Analyzing Program Analyses	A complete static analysis of a program means that no errors or imprecisions arise when querying its behavior in both the concrete and abstract interpretations. This is important for gaining confidence in static analysis alarms. The completeness class of an abstraction refers to the set of programs for which the abstraction is complete. It is not recursively enumerable, and a stratified deductive system is used to prove the completeness of program analyses. The main sources of incompleteness are assignments and Boolean tests, and the proof system has two layers - one generic and one abstraction-specific. The second layer can be instantiated with different abstract domains, such as Intervals and Octagons, to provide necessary and sufficient conditions for completeness. 
41678	4167836	Generalizing the Paige--Tarjan algorithm by abstract interpretation	The Paige and Tarjan algorithm is commonly used to compute the finest state partition that is a bisimulation on a Kripke structure. This is equivalent to strong preservation of CTL or Hennessy-Milner logic. The algorithm can be analyzed from an abstract interpretation perspective, allowing for reasoning on strong preservation for different languages and models. A generalized version of the algorithm, called GPT, can be used to compute the minimal refinement of an abstract interpretation-based model that strongly preserves a given language. GPT can also be used to efficiently compute stuttering and simulation equivalence, and can handle new languages by computing the coarsest refinement of a partition that strongly preserves them. This makes GPT a versatile tool for model checking.
41679	4167966	Shallow Circuits with High-Powered Inputs	The paper discusses the importance of a deterministic black-box identity testing algorithm for univariate polynomials, which would have implications for the complexity of the permanent. The current known lower bounds from derandomization of multivariate identity testing are weaker. The paper proposes a new approach by focusing on a specific type of polynomials: sums of products of sparse polynomials with sparse coefficients. This leads to new versions of the Shub-Smale tau-conjecture on integer roots of univariate polynomials. The paper also introduces a new result on reduction to depth 4 for arithmetic circuits, which has implications for the lower bound on the size of depth 4 circuits computing the permanent. A slightly superpolynomial or weaker bound on the number of real roots would suffice to obtain this lower bound.
41679	4167918	Arithmetic circuits: The chasm at depth four gets wider	Agrawal and Vinay's paper shows that polynomials with small arithmetic circuits also have small circuits with only four levels. This is significant for problems like circuit lower bounds and identity testing. The authors demonstrate that starting from polynomial size circuits can result in even smaller depth four circuits. For example, if a polynomial has a circuit of polynomial size, it also has a depth 4 circuit of size n^O^(^n^l^o^g^n^). This is useful for lower bound problems and deterministic identity testing, especially for sparse univariate polynomials. The paper also reproves two results on boolean circuits for languages in LOGCFL and for arithmetic circuits with polynomial size and bounded degree.
41680	4168026	The partitioned exponential file for database storage management	The rate of increase in hard disk storage capacity is growing faster than the rate of decrease in hard disk seek time. This means that the value of a seek is increasing exponentially relative to the value of storage. To address this trend, a partitioned exponential file (PE file) has been introduced as a customizable storage manager for various types of data. The PE file is designed for use in environments with high update loads and concurrent analytical queries, such as in long-running scientific applications. The proposed Large Synoptic Survey Telescope will produce massive amounts of data, making the PE file a useful tool. The PE file is organized for sequential I/O, reducing the time required for record insertion or retrieval. Benchmarking experiments have been conducted for T1SM, a PE file customized for multi-attribute data records organized on a single numerical attribute. Various data organizations, including the B+-Tree, LSM-Tree, Buffer Tree, Stepped Merge Method, and Y-Tree, have been tested, with T1SM performing well in many cases. It is particularly effective in handling heavy query workloads and intense insertion streams concurrently, maintaining low query latencies.
41680	4168027	Maintaining a large spatial database with T2SM	Large spatial databases often face the challenge of handling both intense query and update loads simultaneously. This is especially true for data warehouses that contain spatial data, where analytical queries must be answered while new data is constantly being added. Traditional spatial access methods, such as the R-tree, may not be suitable for such environments due to slow update rates and inefficient data organization. In this paper, the authors introduce T2SM, a linear file template designed specifically for spatial data. T2SM utilizes external memory sorts based on the STR algorithm, allowing for efficient query performance and exceptional update rates. Overall, T2SM offers a viable solution for managing intense query and update loads in large spatial databases.
41681	4168126	A bayesian mixture model with linear regression mixing proportions	Classic mixture models have a fixed prevalence of components, making it difficult to learn how data distributions change over time. To address this, we propose models and Bayesian learning algorithms that can infer the temporal trends of mixture components. We demonstrate the effectiveness of our methods by applying them to tracking changes in antibiotic resistance rates in two bacteria. Our results show that our models can accurately identify temporal patterns in antibiotic resistance. This approach has potential for other applications where understanding the evolution of complex data distributions is important.
41681	416818	A practical hierarchical model of parallel computation I. The model	The H-PRAM is a model of parallel computation that combines the favorable properties of the PRAM with a more realistic representation of communication and synchronization costs. It allows for asynchrony by using a hierarchy relation to organize subalgorithms, and restricts communication asynchrony to achieve determinate algorithms. This model is applicable to various existing and proposed parallel architectures and can potentially lead to more efficient algorithms by considering both communication and synchronization simultaneously. The H-PRAM also provides a framework for studying the exploitation of general locality in parallel computing. Results from the PRAM can be transferred to this model. A companion paper demonstrates the use of the H-PRAM by designing and analyzing algorithms for specific computations. 
41682	416823	Impact of interconnection-free biomolecular computing	The article discusses a new model for a computing system that does not rely on traditional interconnections. Instead, it utilizes biodevices and molecular information to perform logical operations in parallel. The system works by distributing logical information through different types of molecules and using enzymes for parallel selection. The potential impact of this interconnection-free computing on highly parallel processing architectures is explored through the design of parallel sorting networks. This innovative approach has the potential to greatly improve the efficiency and speed of computing systems.
41682	4168211	Design of set-valued logic networks for wave-parallel computing	The article discusses a design for SVL networks that addresses issues with interconnection in highly parallel VLSI systems. The concept uses frequency multiplexing to allow for the parallelism of electrical or optical waves in parallel processing. This wave-parallel computing approach enables multiple binary functions to be performed simultaneously using a single module. The article also covers the systematic synthesis of wave-parallel computing systems and discusses the potential implementation of SVL networks. This approach offers a solution to interconnection problems in highly parallel VLSI systems and has the potential to greatly enhance parallel processing capabilities. 
41683	4168352	Reasoning about the transfer of control	DCL-PC is a logic that allows for reasoning about the abilities of agents and coalitions of agents when control is transferred from one agent to another. It is built upon the foundation of CL-PC, a logic for cooperation that uses Boolean variables to represent the choices available to a coalition. DCL-PC adds dynamic logic modalities to reason about control transfer, and can be semantically interpreted in two ways - a direct semantics and a Kripke semantics. The logic is proven to be equivalent in both semantics and has an axiomatization. The computational complexity of model checking and satisfiability in DCL-PC is shown to be PSPACE-complete. Finally, the distinction between first-order and second-order control is explored, with a logical characterisation provided for second-order control.
41683	4168373	Communicating Rational Agents	This article discusses the incorporation of communication in a dynamic/epistemic multi-agent system. The system formalizes the knowledge, abilities, and actions of each agent, as well as the opportunities and results of their actions. Two types of communication are addressed: one where a didactic agent shares its knowledge with all other agents, and one where agents only exchange information upon request. The ability and opportunity to communicate is based on the agents' knowledge and trust relationships. The semantics of communication is defined using epistemic updates, allowing for the modeling of knowledge-producing actions. This approach is compared to other multi-agent epistemic notions, such as belief dependence and distributed/common knowledge, and can be represented within a Kripke model. 
41684	4168473	Toward Intelligent Biped-Humanoids Gaits Generation	This chapter discusses experimental studies on analyzing natural human walking and presents a biologically inspired design for a simplified bipedal locomotion system for humanoid robots. The design is based on human walking analysis and mimics the mechanism and control of human muscles. A hybrid algorithm is proposed for generating walking gaits, which is a novel approach compared to traditional methods that use kinematics and dynamic equations. The algorithm utilizes particle swarm optimization and can be applied to small humanoid robots with at least six degrees of freedom in the knee, ankle, and hip joints.
41684	4168422	Improvement of On-line Recognition Systems Using a RBF-Neural Network Based Writer Adaptation Module	The purpose of this paper was to develop an adaptation module to improve the performance of a recognition system for new users or writing styles. The module, based on a Radial Basis Function Neural Network, is added after the recognition system and adjusts the output to better match the user's desired response. Two training strategies were used - increasing the number of hidden units and adjusting the parameters of the nearest unit. The results of applying this module to two recognition systems showed a significant decrease in error rates and fast adaptation to different handwriting styles. The module also outperformed a strategy of only updating weights of the nearest center. 
41685	4168559	Deliberation in Equilibrium: Bargaining in Computationally Complex Problems	The authors propose a normative theory of interaction, specifically negotiation, between self-interested and computationally limited agents. They focus on a 2-agent setting where each agent has a complex problem and there is potential for collaboration to solve a joint problem. At any time, agents can compute to improve their own solution, their opponent's solution, or the joint solution. At a deadline, the agents must decide whether to implement the joint solution and how to divide its value. The authors present a model for controlling anytime algorithms and analyze the perfect Bayesian equilibria for various scenarios. They also provide algorithms for finding these equilibria.
41685	416855	A truth serum for sharing rewards	In this study, we explore how a group of agents can fairly distribute a joint reward based on their subjective opinions of each other's contributions. We develop a mechanism to gather and combine these opinions, as well as determine each agent's share. The key idea is that agents who trust their peers and truthfully report their opinions will receive a higher expected share. Assuming agents are Bayesian decision-makers and the population is large enough, we prove that our mechanism is incentive-compatible, budget-balanced, and practical. We also suggest ways to ensure individual rationality and fairness.
41686	4168687	LOOM: optimal aggregation overlays for in-memory big data processing	Aggregation is a crucial aspect of processing big data, as it involves distilling information from large datasets. Common operations such as top-k matching and word count rely on fast aggregation, but existing frameworks like MapReduce do not explicitly consider or optimize it. In recent "online" approaches to big data analysis, where data is stored in main memory across nodes, the impact of aggregation time on overall job completion time becomes even more significant. To address this, LOOM is a system designed for efficient big data aggregation within analysis frameworks. It supports two-phased computations and uses heuristics to construct optimized aggregation overlays, resulting in improved performance in microbenchmarks and real-world scenarios. 
41686	4168618	Multicast with aggregated deliveries	The paper discusses the concept of Multi-Delivery Multicast (MDMcast) in distributed systems with crash-stop failures. This involves aggregating multiple messages according to process-specific criteria and delivering them atomically. While existing systems and models only consider unicast or focus on efficiency, this paper presents a model for MDMcast that includes both conjunctions and disjunctions. It also introduces a generic predicate grammar for expressing these conjunctions and disjunctions. The paper shows that a total order on individual messages is necessary for processes to achieve agreement on delivered messages, and presents algorithms implementing MDMcast on top of Total Order Broadcast (TOBcast). The paper also formalizes additional properties for MDMcast and shows how the algorithms implement these properties.
41687	4168718	Efficient Tree Layout in a Multilevel Memory Hierarchy	The problem of organizing a tree or trie in a hierarchical memory to minimize block transfers during a search operation is explored. Previous research has shown optimal but complex algorithms when the block-transfer size is known. A simple greedy algorithm is proposed that is almost optimal, while a more flexible but slightly less efficient algorithm is also presented. The latter is extended to the cache-oblivious setting where the block-transfer size is unknown, providing a solution for a multilevel memory hierarchy. The query performance of this layout is only slightly worse than the optimal layout with known block size.
41687	416877	Optimal Cache-Oblivious Mesh Layouts	A mesh is a graph that divides space into regions for various applications. One important aspect is the mesh update, where each vertex updates based on neighboring values. The performance of a mesh update depends on the layout in memory, with good data locality resulting in faster updates. This paper presents a way to find a memory layout that guarantees optimal performance for any set of memory parameters. It also gives two algorithms for finding these cache-oblivious layouts, one with a runtime of O(|G|log 2|G|) and the other with a runtime of O(|G|log |G|log log |G|). Both algorithms use in-order traversals of fully balanced or relax-balanced decomposition trees to obtain the layout.
41688	41688102	Fast Counting with Bounded Treewidth	The treewidth of a structure can greatly impact the difficulty of solving a problem, with a constant treewidth making it more tractable. Courcelle's Theorem states that any property defined by Monadic-Second Order (MSO) sentences can be solved efficiently if the treewidth is bounded. This has been extended to counting problems, but the MSO description is not an algorithm. New approaches, such as monadic datalog, have been developed to create efficient algorithms for decision problems based on Courcelle's Theorem. In this paper, the authors apply this approach to fundamental counting problems in logic and artificial intelligence.
41688	4168824	Clique-Width and Directed Width Measures for Answer-Set Programming.	Disjunctive Answer Set Programming (ASP) is a powerful way of programming that deals with complex decision problems. Identifying and developing efficient algorithms for tractable fragments is important to complement the existing ASP systems. Problems can become easier to solve if a certain parameter is bounded by a fixed number, known as fixed-parameter tractable (FPT) problems. While there are existing FPT results for ASP, parameters related to directed or signed graphs have been overlooked. This paper shows that directed width measures do not lead to FPT results, but signed clique-width can be used to develop a novel dynamic programming algorithm that is FPT, making it the first for reasoning problems beyond SAT. This parameter is more general than treewidth. 
41689	4168927	Chromatic Adaptation Performance Of Different Rgb Sensors	This paper compares the performance of different chromatic adaptation transforms (CAT) and transforms based on RGB primaries in imaging systems. The studied CATs include von Kries, Bradford, Sharp, and CMCCAT2000, while the RGB primaries investigated are ROMM, ITU-R BT.709, and "prime wavelength" RGB. A von Kries model is used to linearly scale post-adaptation cone responses with illuminant dependent coefficients. The evaluation is done using 16 sets of corresponding color data and three error prediction metrics. The results show that traditional CATs, Sharp CAT and CMCCAT2000, perform best, but some transforms based on RGB primaries also exhibit good chromatic adaptation behavior. This suggests that white-point independent RGB spaces can be defined for image encoding, assuming the adequacy of the linear von Kries model.
41689	416898	Spherical Sampling and Color Transformations	This paper introduces a novel spherical sampling method for finding optimal sensors in trichromatic color applications. Unlike other optimization techniques, this method guarantees finding a global minimum and allows for retaining a set of possible solutions. The technique was used to identify all possible RGB sensors that meet a specific chromatic adaptation transform (CAT) behavior, using the CIE DeltaE(94) error criterion on Lam's corresponding color data set. The results show that there are multiple sensors that meet the criterion, and that commonly used sensors like Bradford, Sharp, and CMCCAT2000 are not the only options. 
41690	4169031	On Merging Strategy-Proofness	Merging operators are used to combine the beliefs and goals of a group of agents from each individual member's beliefs and goals. When an agent has a preference for the outcome of the merging process, they may try to manipulate it by lying about their true beliefs and goals. This can lead to a more favorable outcome for that agent, but it also raises questions about the fairness and accuracy of the merged result. To ensure a fair process, it is important to have strategy-proof operators that are not susceptible to manipulation. This paper examines the strategy-proofness of various merging operators, including those based on models and formulas, and considers different scenarios such as the number of agents and integrity constraints.
41690	416902	Conflict-Based Merging Operators.	This paper focuses on propositional belief merging, where the goal is to combine the beliefs of a group of agents into one coherent set from different individual beliefs. The authors propose a family of merging operators based on the concept of conflict between interpretations, rather than distance. This family includes many existing operators and the paper presents comparison relations and logical properties of these operators. The authors also discuss the key problem in this setting and introduce a well-studied family of merging operators based on distance. Overall, this paper presents a more general approach to belief merging and provides a thorough analysis of the proposed operators.
41691	4169153	Iterated belief change: a transition system approach	The transition system approach is a method used to understand how an agent's beliefs change as they take actions. Some actions lead to belief revision and others lead to belief update, but the interaction between these two processes can be complex. To better understand this interaction, a set of basic rules has been developed. Additionally, a new operator has been introduced that helps explain how revisions and updates can work together in a logical way. This approach provides a plausible interpretation for the evolution of beliefs through alternating sequences of revisions and updates.
41691	4169194	Belief Change and Cryptographic Protocol Verification	Cryptographic protocols are used to exchange information in hostile environments, and often have the goal of changing participants' beliefs. Epistemic logics have been used to verify these protocols, but formal belief change operators have not been incorporated. This paper introduces a new approach to protocol verification by combining a monotonic logic with a non-monotonic belief change operator. This allows participants to retract incorrect beliefs and postulate the most plausible explanation for new information. The paper makes two main contributions: extending belief change techniques to cryptographic protocol verification and introducing a specific model of belief change suitable for this context. The authors argue that this combination of belief change and protocol verification can benefit both fields. The paper provides background on logical protocol verification, discusses the limitations of existing belief change operators, and presents a simple approach to protocol verification using formal belief change operators. Overall, the paper highlights the importance of considering belief change in the context of protocol verification.
41692	4169221	Model Comparison to Synthesize a Model-Driven Software Product Line	The current method for developing software product lines is focused on feature and variability modeling. However, there is potential for automatic assistance in identifying commonalities and variabilities within a set of models in a given domain. This paper presents a generic approach for synthesizing a software product line using model comparison. The approach uses EMF Compare, a generic model comparison tool, to detect differences between existing potential product models and Common Variability Language (CVL) to specify variability. The developer can then use the results to automatically induce a preliminary product line model and make further enhancements. An example of a train control product line is used to illustrate the approach.
41692	4169218	Adding Standardized Variability to Domain Specific Languages	This article discusses how a shared language for variability can enhance the capabilities of Domain Specific Languages (DSLs). DSLs are often used to express variability in a particular domain or family of systems. By incorporating variability into the language itself, all possible models within that language can be represented. The authors explore the idea of separating variability from the base modeling language, making it possible to express variability in both small DSLs and more general languages like UML. This approach allows for standardization of the variability language and removes the need for DSLs to have built-in variability mechanisms.
41693	416930	Dynamically Optimizing High-Dimensional Index Structures	High-dimensional query processing involves optimizing the logical page-size of index structures, which is crucial for efficient performance. However, the optimal page-size is not only determined by static schema information, but also dynamic factors like the number of objects in the database and the degree of clustering and correlation in the data set. To address this issue, a method called DABS-tree has been proposed, which dynamically adapts the page size during insert and delete operations. This is achieved by consulting a cost model before splitting or merging pages. This approach has been shown to outperform the X-tree and sequential scan by factors of up to 4.6 and 6.6, respectively, through experimental evaluation. The DABS-tree algorithm also ensures that all restructuring operations are locally restricted.
41693	4169346	A cost model for query processing in high dimensional data spaces	In recent years, multimedia databases have become increasingly important in various fields such as medicine, CAD, geography, and molecular biology. One important area of research in this field is similarity search in large data sets. The most commonly used approach is the feature approach, where important characteristics of objects are transformed into high-dimensional feature vectors. This allows for similarity search to be conducted through a neighborhood search in feature space. To manage these feature vectors, multidimensional index structures are typically used. Optimization techniques such as blocksize optimization, data space quantization, and dimension reduction can greatly improve query processing. To accurately determine optimal parameters, a cost model has been developed for index structures such as the R*-tree and X-tree. This model takes into account issues specific to high-dimensional data spaces, such as boundary effects, and uses the concept of fractal dimension to account for correlated data.
41694	4169424	CoCo: coding cost for parameter-free outlier detection	The question of how to automatically identify outliers in a data set is a common one in various fields such as economics, biology, and medicine. However, current methods for outlier detection have several limitations, including the need for specific parameter settings and assumptions about the data distribution. To address these issues, a new technique called CoCo is proposed. CoCo uses a data compression approach to identify outliers, without relying on specific data distributions. The technique also includes a new algorithm for outlier detection and has been tested on both synthetic and real-world data, showing promising results. The source code and data sets used in the experiments are available for further use.
41694	4169440	Mining Interaction Patterns among Brain Regions by Clustering	Functional magnetic resonance imaging (fMRI) is a non-invasive way to study brain function, but the large and complex data it produces requires efficient data mining techniques. Recent findings suggest that the brain is organized into modules, and to better understand how different regions of the brain interact, a new clustering technique called interaction K-means (IKM) has been proposed. This approach models each subject as a multivariate time series, with each dimension representing the fMRI signal in a different part of the brain. Unlike previous methods, IKM focuses on the interactions between these time series to group them into clusters with similar patterns. The effectiveness and efficiency of IKM has been demonstrated through extensive experiments on benchmark data and real fMRI studies, showing its potential to contribute to our understanding of normal brain function and psychiatric disorders.
41695	416950	Strategic Port Graph Rewriting: an Interactive Modelling Framework	Strategic port graph rewriting is a method used to implement visual modelling tools in order to simplify the process of specifying and programming complex systems. It involves representing a system using an initial graph and a set of rules, along with a user-defined strategy for applying these rules. This approach has adapted traditional operators from term rewriting languages to work with graph rewriting, as well as introducing new constructs for graph traversal and managing rewriting positions. The language has a formal semantics and is implemented through the PORGY tool, which allows for graph transformation and visualization. 
41695	4169538	A strategy language for graph rewriting	We have developed a formal way to define a graph-based programming language, which involves a set of rules to rewrite graphs, a strategy to control the rule application, and an initial graph to be rewritten. We have modified traditional operators from term rewriting languages to work with graph rewriting and added new features for graph traversal and management. This language is used in PORGY, a tool for graph transformation and visualization. 
41696	41696100	Boosting Saliency in Color Image Features	Salient point detection is a method used to identify distinctive events in images. This is typically done by analyzing the local differential structure of images, focusing on the shape of the local neighborhood. However, most detectors only consider luminance information, ignoring the potential of color information. To address this, a new algorithm called color saliency boosting has been developed, which takes into account both shape and color distinctiveness. This is achieved by analyzing the statistics of color image derivatives and transforming them to have equal impact on saliency. This method can be applied to existing feature detectors and has been shown to significantly improve the information content of images.
41696	4169619	Evaluation of Intensity and Color Corner Detectors for Affine Invariant Salient Regions	Global features are often used to describe the content of an image, but they are limited in capturing all the different characteristics of the image. To address this issue, local computation of image information is necessary. This can be achieved by using salient points to represent local information, which leads to more discriminative features. This study builds upon an existing affine invariant local feature detector, but extends it with the intensity based SUSAN corner detector, and also incorporates color information into the detection process. The results show that the extended algorithm outperforms the original one in terms of invariance and distinctiveness of the regions, while also being more computationally efficient. 
41697	4169718	A CP framework for testing CP	The success of constraint-based modeling languages has highlighted the need for better software engineering practices, particularly in the testing phase. This paper presents a testing framework that automates test case generation for constraint programming. The framework follows a general development process that involves starting with a simple declarative constraint model and refining it using various techniques such as constraint reformulation and symmetry-breaking. The proposed process uses the initial model as a reference for identifying errors and generating practical test cases. The framework has been implemented in a tool called CPTEST, which has been used to detect errors in well-known benchmark programs. This approach aims to address the potential introduction of faults during the refinement stage of constraint program development.
41697	4169750	A Framework for the Automatic Correction of Constraint Programs	Constraint programs, written in high-level languages such as OPL, COMET, ZINC, or ESSENCE, are increasingly being used in important business applications. Like any critical program, they need to be thoroughly tested and corrected to avoid financial losses. This paper introduces a framework for automatically correcting constraint programs, considering the unique characteristics and common errors of these programs. The framework has been implemented in the testing platform CPTEST for OPL programs. Through the use of mutation testing, the results of experiments demonstrate that well-known OPL constraint programs can be corrected automatically using this framework.
41698	4169822	MINLP Based Retrieval of Generalized Cases	The use of generalized cases has been found to be beneficial in finding flexible and configurable products, such as reusable components in electronic design automation. This paper focuses on the issue of similarity assessment and retrieval in case bases that contain both traditional and generalized cases. Previous approaches were limited to continuous domains, but this paper expands the scope to include mixed, continuous and discrete domains. The problem is viewed as a nonlinear optimization problem and is extended to a mixed integer nonlinear optimization problem, which is a current area of research in mathematical optimization. The paper also presents two optimization-based retrieval methods that improve response time by using a pre-existing index structure. This is an important development as many real-world applications require mixed domains for case descriptions.
41698	4169853	Generalized Cases: Representation and Steps Towards Efficient Similarity Assessment	In certain areas of case-based reasoning, the traditional idea of cases as individual points in a problem-solving space is not suitable. In order to address this, a new concept called generalized cases is introduced. These cases represent a broader range of experience and can be represented using constraints. This requires a new method for measuring similarity between cases, and while fuzzy constraint satisfaction or non-linear programming could be used, a more efficient algorithm is proposed. This allows for a more flexible and accurate representation of cases in case-based reasoning systems.
41699	4169935	SemEval-2015 Task 10: Sentiment Analysis in Twitter	The paper discusses the 2015 SemEval shared task on Sentiment Analysis in Twitter, which was the most popular sentiment analysis task at the time with over 40 teams participating. The task consisted of five subtasks, including two previous tasks (expressed sentiment by a phrase in a tweet and overall tweet sentiment) and three new tasks (sentiment towards a topic in a single tweet, overall sentiment towards a topic in a set of tweets, and degree of prior polarity of a phrase). This competition aimed to predict sentiment in various contexts on Twitter and had a high level of participation. 
41699	4169967	Large-Scale Goodness Polarity Lexicons for Community Question Answering	The authors propose a new approach to ranking comments in community question answering (cQA) by applying the concept of sentiment analysis. By identifying specific vocabulary used in good and bad comments, they create a polarity lexicon to classify comments as either good or bad. This lexicon is built using pointwise mutual information in a semi-supervised manner, starting with a small number of initial seeds. The results of their evaluation show a significant improvement over a strong baseline and achieve state-of-the-art performance on a relevant task. 
41700	4170042	Assessing attractiveness in online dating profiles	Online dating is a popular way for people to meet potential partners, but little research has focused on how users perceive each other through their personal profiles. A study examined how users rate attractiveness in online dating profiles, including factors like extroversion and trustworthiness. It found that the photograph in a profile was the strongest predictor of overall attractiveness, but the free-text section also played a significant role. Other qualities, such as age and education, also affected attractiveness ratings but in different ways for men and women. Surprisingly, the fixed-choice elements of a profile had no impact on perceived attractiveness.
41700	417009	Supporting exploratory text analysis in literature study.	WordSeer is a software system designed for exploring and analyzing literary texts. It aims to support the entire cycle of literature study, including reading, interpretation, exploration, and understanding. While there are many tools available for reading and interpreting texts, WordSeer focuses on the neglected areas of exploration and understanding. It is based on the concept of sensemaking, which involves analyzing large datasets in an open-ended way. The system integrates algorithmic text processing with interactive features for interpretive and exploratory tasks. It currently offers tools for grammatical search, contextual similarity determination, visualization of word context patterns, and organizing source material for comparison and hypothesis building. An example of its use is shown through an analysis of language differences between male and female characters in Shakespeare's plays. Future plans include incorporating additional sensemaking tools to aid in comparison, exploration, grouping, and pattern recognition.
41701	4170142	Digital fountains and their application to informed content delivery over adaptive overlay networks	In this study, we focus on optimizing the transfer of large files across adaptive overlay networks. We propose using a digital fountain approach, which offers reliability and flexibility for connection migration and parallel transfers while also being resilient to packet loss. We discuss recent advancements in coding for digital fountains and how they have been effective in reliable multicast and parallel downloading. In the context of collaborative transfers, we address the issue of overlapping encoded symbols acquired by peers and present algorithms for efficient estimation, summarization, and reconciliation of these symbols while minimizing messaging and computation. Through simulations and experiments, we show the benefits of our approach for content delivery in overlay networks.
41701	4170170	Digital fountains: a survey and look forward	Digital fountains are a type of erasure coding used for network communication. This approach changes the traditional method of receiving an ordered stream of packets to one where the user only needs to receive enough packets to retrieve the desired data. This simplifies data delivery, especially for large data sets or when distributing to many users. However, there are barriers to the widespread adoption of digital fountains, and we explore whether these challenges can be addressed. 
41702	4170234	Top-k closest pairs join query: an approximate algorithm for large high dimensional data	This paper introduces a new algorithm for efficiently calculating the top k closest pairs join query of two large and high dimensional data sets. The algorithm has a worst case time complexity of 0(d^2 nk) and a space complexity of 0(nd). It guarantees a solution within a factor of 0(d^{1 + \frac{1}{\tau }}) of the exact one, where \tau\in \{ 1,2, \ldots ,\infty \} represents the Minkowski metrics L_\tauof interest and d is the dimensionality. The algorithm utilizes a space filling curve to establish an order between the points and performs a limited number of sorts and scans of the data sets. Experimental results show that the algorithm performs similarly to an exact algorithm in low dimensional spaces, can significantly reduce the data set size in high dimensions under certain conditions, and still returns a solution close to the exact one.
41702	4170230	Fast Outlier Detection in High Dimensional Spaces	This paper introduces a new definition of distance-based outliers, which considers the sum of distances from a point's k nearest neighbors. These outliers have the highest weight values. To efficiently compute these weights, the algorithm uses a Hilbert space filling curve to find the k nearest neighbors for each point. The algorithm has two phases, with the first phase providing an approximate solution in a short amount of time. The second phase then returns the exact solution by examining a small fraction of the data set. Experimental results show that the algorithm is efficient and accurate, scaling linearly with the size and dimensionality of the data set. 
41703	41703225	Comparison of transform coding techniques for two-dimensional arbitrarily shaped images	This paper discusses the potential for advanced multimedia video services to include irregularly shaped image segments in addition to traditional rectangular images. This is particularly relevant for TV weather reports, where chromo-key techniques and video analysis are used to create these irregularly shaped segments. The paper focuses on efficient ways to code these segments using transform techniques, specifically looking at two approaches: brute force transform coding and shape-adaptive transform coding. The former involves filling in the uncovered areas with redundant data, while the latter adapts the transform basis or coefficient calculation based on the shape of the segment. The paper proposes a new adaptive transform based on the same principle as the DCT, and analyzes the tradeoff between compression performance, computational complexity, and codec complexity for different coding schemes. Simulation results show that more complicated algorithms can improve quality, but at a higher cost, while a simple mirror image extension technique can also improve quality without any additional overhead. The paper's contributions include efficient problem formulations, new transform coding techniques, and analyses of tradeoffs. 
41703	41703258	A secure and robust digital signature scheme for JPEG2000 image authentication	This paper introduces a new method for digitally signing JPEG2000 images to verify their authenticity. The method uses a unique concept called lowest authenticable bit rates (LABR) to protect the image's authenticity during transcoding. The scheme involves signature generation and verification, error correction coding, and watermark embedding and extracting. The invariant features of the image are generated and signed by the sender's private key, resulting in a small crypto signature. Error correction coding is used to handle any changes to the features caused by transcoding. Watermarking is used to store the check information for error correction. The proposed solution can be easily integrated into the JPEG2000 codec and is compatible with Public Key Infrastructure. The paper also evaluates the security and robustness of the proposed solution.
41704	41704100	An EM algorithm for shape classification based on level sets.	The paper suggests an EM approach to classify shapes in a database into different classes and simultaneously determine the best shape contours for each class. The level set function is used as the shape descriptor and it is assumed that for each class, there is an unknown underlying level set function that describes the contour of the shapes within that class. The level set function for each shape in the database is considered as a noisy measurement of the underlying function. The EM algorithm uses this measurement model to determine the class labels and estimate the shape contours. This algorithm is efficient, simple, and accurate and is applied to two medical applications.
41704	417041	3D statistical shape models to embed spatial relationship information	This paper discusses the creation of 3D statistical shape models of knee bones and their use in a segmentation system for MRI images. The models incorporate spatial information between the bones and cartilages to facilitate automated segmentation. The approach involves using a point distribution model optimization framework and a parameterized surface extraction algorithm to generate the models. The study uses a database of segmented knee images to create 3D statistical shape models of the patella, tibia, and femoral bones. These models are then used to embed spatial relationship information and aid in automating segmentation algorithms for the cartilages.
41705	4170599	Delineating white matter structure in diffusion tensor MRI with anisotropy creases	Geometric models of white matter architecture are becoming increasingly important in neuroscience studies that use diffusion tensor imaging. The most commonly used method for creating these models is fiber tractography. However, for some types of analysis, it may be more useful to look at the first and second derivatives of diffusion anisotropy. This approach involves identifying ridges and valleys in the tensor field, which correspond to regions of high and low anisotropy. These features, known as anisotropy creases, can be used to extract a skeleton of major white matter pathways. This technique has been applied to real diffusion MRI data, and the results have been visualized alongside fiber tractography to confirm their relevance.
41705	4170592	Anisotropy creases delineate white matter structure in diffusion tensor MRI	Current methods for extracting models of white matter architecture from diffusion tensor MRI typically use fiber tractography. However, analyzing the first and second derivatives of diffusion anisotropy could offer an alternative approach. Anisotropy creases, which are areas of locally extreme anisotropy, can be identified by their orthogonal gradient to one or more eigenvectors of the Hessian. These creases could serve as a basis for extracting a skeleton of white matter pathways, with ridges of anisotropy representing fiber tracts and valleys representing interfaces between different tracts. A crease extraction algorithm has been proposed and tested on a diffusion tensor dataset, with visualization alongside tractography confirming its anatomical relevance.
41706	4170658	Simultaneous truth and performance level estimation (STAPLE): an algorithm for the validation of image segmentation.	The challenge of accurately assessing image segmentation techniques has been an ongoing issue. This is important because these algorithms often have limited precision and accuracy. While human raters have been the most accepted method for segmentation, there is variability between raters. Automated algorithms have been sought to reduce this variability, but they must be evaluated to ensure effectiveness. However, obtaining a known true segmentation for clinical data is difficult. Using a collection of segmentations by raters is a useful alternative, but the most appropriate measure to compare them has not been determined. The Expectation-Maximization algorithm for Simultaneous Truth and Performance Level Estimation (STAPLE) can address these challenges by estimating the true segmentation and performance levels using a collection of segmentations, incorporating a prior model and spatial constraints. This makes it a valuable tool for evaluating image segmentation techniques in clinical imaging data.
41706	4170643	Segmentation of brain tissue from magnetic resonance images.	Medical image segmentation is a difficult task due to the complexity of the images and the lack of fully accurate models for the anatomy. The brain, in particular, is a challenging structure to segment and is crucial for various studies and applications. A new method is proposed to segment brain tissue from magnetic resonance images by combining three existing techniques from computer vision. These techniques have been customized for brain tissue segmentation, resulting in a more robust method. The method has been parallelized and tested on a large database of brain scans, with results validated against expert segmentations. 
41707	4170732	Simulation of Corticospinal Tract Displacement in Patients with Brain Tumors	In order to plan a surgical strategy for a brain tumor, it is important to understand the spatial relationship between the tumor and the corticospinal tracts. However, manually outlining these tracts from MRI scans is time consuming and impractical. To address this issue, a method has been developed to automatically retrieve structural information by registering a standardized brain atlas to the individual patient's tumor anatomy. This involves segmenting the skin and brain in the patient's MRI scan and then registering a normal brain atlas to the patient's brain using various techniques. The resulting spatial correspondence is used to map the corticospinal tracts from the atlas onto the patient's brain. This method has been tested on 5 patients with different types and locations of tumors and has shown to be accurate in comparison to manual segmentation. This approach allows for the visualization of complex anatomical information with minimal user interaction, making it a useful tool for surgical planning.
41707	4170742	Capturing Brain Deformation	Neurosurgeons face a critical challenge during surgery to remove as much tumor tissue as possible while preserving healthy tissue and minimizing disruption to critical anatomical structures. In order to achieve this, intraoperative image processing algorithms have been developed to enhance the surgeon's ability to visualize and navigate the brain. These algorithms take into account the brain's nonrigid shape changes during surgery and use preoperative data to project onto the intraoperative configuration. This allows for real-time tracking of brain changes and improved visualization of critical structures. The use of patient-specific data such as fMRI and DTI has also been shown to greatly enhance the surgeon's ability to locate critical structures during surgery. This real-time augmented visualization is crucial for quick surgical decision making. 
41708	41708108	3D Image Matching Using a Finite Element Based Elastic Deformation Model	The authors propose a new method for calculating the deformation field between 3D images. The approach minimizes the squared differences between the images while taking into account the physical properties of the objects represented. The objects are modeled as elastic bodies and the method differs from optical flow methods in three ways: it considers the physical properties of the objects, provides information about the deformed objects, and calculates a global solution instead of local solutions. This is achieved through a finite-element approach, which requires the objects to be meshed. A specialized tetrahedral mesh generator has been developed for this purpose. The method has been successfully tested on simulated and medical data, including muscle exercise imaging and ventricular deformation in multiple sclerosis. 
41708	4170823	Capturing Brain Deformation	The main challenge for neurosurgeons during surgery is to remove as much tumor tissue as possible while preserving healthy tissue and minimizing disruption to critical anatomical structures. In order to aid in this process, researchers have developed intraoperative image processing algorithms to help surgeons visualize and navigate the changing shape of the patient's brain during surgery. This involves using preoperative data to create a biomechanical simulation of brain deformation and then projecting this data onto intraoperative images. By incorporating patient-specific data, such as functional MRI and diffusion tensor MRI, the augmented visualization can greatly enhance the neurosurgeon's ability to locate critical structures during surgery. This process can be done in real-time, providing the surgeon with up-to-date information to aid in decision making.
41709	4170917	Registering UMM Business Collaboration Models in an ebXML Registry	The UN/CEFACT's modeling methodology (UMM) is a framework for creating global choreographies of business processes between organizations. It is important for UMM models to be publicly accessible so they can be used repeatedly and referenced in agreements with trading partners. This paper outlines a mapping of UMM models to the ebXML registry information model (RIM), providing a standardized way for organizations to store and access UMM models. This allows for easier collaboration and implementation of UMM models in inter-organizational business processes. 
41709	417096	Registering a business collaboration model in multiple business environments	Business registries are commonly used to find services offered by potential business partners. However, they can also be used to search for inter-organizational business process definitions that are relevant to one's own business environment. It is important to determine the validity of these definitions in different environments, and to register any adaptations specific to a particular environment. This paper focuses on business process definitions based on UMM business collaboration models, and discusses two approaches for binding a model to a business environment: within the model itself, or in the registry's meta-data. Both approaches are essential for effectively utilizing business registries in finding suitable business partners and process definitions.
41710	417108	On the Complexity of Decidable Cases of Commutation Problem for Languages	The article discusses the complexity of testing the commutation of two languages, specifically when one of the languages is a finite language. This is a problem that has recently gained interest due to progress made in solving it. The complexity of the problem ranges from co-NEXPTIME-complete to P-SPACE complete and co-NP complete, with different types of representations such as nondeterministic automata, regular expressions, and grammars being considered. Interestingly, the complexity does not change when considering a general language instead of a finite one. For the case of two finite sets, polynomial time algorithms are provided that are more efficient than a naive algorithm. However, for deterministic automata, the complexity is more complicated due to the asymmetry of concatenation.
41710	4171029	On the complexity of decidable cases of the commutation problem of languages	The article discusses the complexity of determining whether two languages, X and Y, commute (i.e. if XY = YX). The complexity varies depending on the type of language representation used for X and Y. For example, if X is represented by a context-free grammar and Y is a finite language, the problem is co-NEXPTIME complete. On the other hand, if both X and Y are explicitly given finite languages, the problem is solvable in deterministic polynomial time. Interestingly, the complexity does not change for most cases when the type of Y is the same as X. The problem remains open for deterministic finite automata due to the asymmetry of catenation.
41711	417117	Optimal Rebuilding of Multiple Erasures in MDS Codes.	MDS array codes are commonly used in storage systems because of their efficient encoding and decoding procedures. These codes can correct any number of node failures by accessing the remaining information. However, in practical scenarios, it is more likely to have a specific number of node failures rather than all nodes failing. This raises the question of how much information needs to be accessed to rebuild a certain number of failed nodes. A previous study introduced zigzag codes, which achieve the optimal rebuilding ratio of 1/r for systematic node failures, but require all information to be accessed for parity node failures. This paper presents three results on rebuilding codes, including a fundamental bound on storage size and repair bandwidth, construction of optimal systematic codes for both systematic and parity node failures, and error correction algorithms for zigzag codes. These codes can also be corrected beyond their minimum Hamming distances.
41711	4171161	Low-Complexity Array Codes for Random and Clustered 4-Erasures	A new type of low-complexity array codes has been proposed to fix 4 column erasures. These codes are specifically designed for a new error model that takes into account the characteristics of high-order failures in storage arrays. This model considers the number of erased columns and clusters, without pre-defining cluster sizes, to address the issue of correlated failures. These new codes are capable of correcting nearly all combinations of clustered 4 erasures, which fall into three or less clusters. They are also more efficient than the current best 4-erasure correcting codes in terms of encoding, decoding, and update complexity. 
41712	4171216	Permanent deformation analysis of asphalt mixtures using soft computing techniques	This study compares the effectiveness of two soft computing techniques, multi expression programming (MEP) and multilayer perceptron (MLP), for predicting the rutting potential of dense asphalt-aggregate mixtures. The researchers developed correlations between the flow number of Marshall specimens and various influencing factors, such as aggregate content, bitumen percentage, and voids in mineral aggregate. The correlations were based on a comprehensive experimental database, and the relative importance of each variable was determined. A multiple least squares regression (MLSR) analysis was used to compare the MEP and MLP models. The results were validated through a parametric study and showed good agreement with previous studies. The proposed correlations are considered efficient for evaluating the rutting potential of asphalt mixtures, with MEP-based formulas being more practical for engineering applications.
41712	4171226	Genetic-based modeling of uplift capacity of suction caissons	This study uses classical tree-based genetic programming (TGP), linear genetic programming (LGP), and gene expression programming (GEP) to create new prediction equations for the uplift capacity of suction caissons. The models are based on multiple variables and are developed using an experimental database. Statistical analyses and sensitivity and parametric analyses are conducted to validate the models. The results show that TGP, LGP, and GEP are effective methods for predicting the horizontal, vertical, and inclined uplift capacity of suction caissons. The models perform better than or comparable to existing models in the literature. 
41713	4171352	Firefly Algorithm: Recent Advances and Applications.	Nature-inspired metaheuristic algorithms, particularly those based on swarm intelligence, have been gaining popularity in the past decade. The firefly algorithm, which was introduced about five years ago, has seen a significant increase in literature and application diversity. This paper provides a brief overview of the firefly algorithm and its recent publications. It also delves into the concept of optimality in balancing exploration and exploitation, a crucial aspect for all metaheuristic algorithms. Through a comparison with intermittent search strategy, the paper concludes that metaheuristics like the firefly algorithm perform better than the optimal intermittent search strategy. The article also examines the implications of these algorithms for higher-dimensional optimization problems.
41713	41713103	Bat algorithm based on simulated annealing and Gaussian perturbations	The Bat Algorithm (BA) is a new method for global optimization that uses simulated annealing and Gaussian perturbations to improve its search performance. This modified version, called the Simulated Annealing Gaussian Bat Algorithm (SAGBA), combines the simplicity and efficiency of BA with a faster global convergence rate. The researchers conducted numerical experiments using BA, simulated annealing particle swarm optimization, and SAGBA on 20 test benchmarks. Their results showed that SAGBA outperformed the other two algorithms in terms of convergence and accuracy, demonstrating its ability to improve global convergence. Overall, SAGBA is a promising approach for global optimization problems.
41714	4171431	Papiercraft: A gesture-based command system for interactive paper	Paper is still widely used in active reading and other knowledge-worker tasks due to its convenience and flexibility. However, it becomes difficult to store and access as annotations and cross references accumulate. To address this issue, systems like XLibris simulate paper using tablet PCs, but this approach has limitations. PapierCraft is a gesture-based command system that allows users to manipulate digital documents using paper printouts as proxies. With a digital pen, users can draw gestures on paper to tag, email, copy, and create links to documents. PapierCraft also supports real-time interactions between paper and digital documents. User feedback has been positive, and the article describes the design and implementation of the system.
41714	4171429	Handle Flags: efficient and flexible selections for inking applications	Content selection in pen-based interfaces can be challenging due to the lack of supplementary buttons and the difficulty of making precise selections with a lasso stroke. A new technique called Handle Flags has been developed to address these issues. When the pen is positioned near an ink stroke, Handle Flags are displayed for potential selections that the ink stroke could belong to. By tapping on the handle, users can easily access the desired selection without having to make a complex lasso stroke. Studies have shown that Handle Flags offer significant advantages over traditional techniques and have the potential to improve the usability of pen-based applications.
41715	4171542	A receiver-centric transport protocol for mobile hosts with heterogeneous wireless interfaces	Numerous transport protocols have been developed for use by mobile hosts in wireless environments. These protocols take into consideration the unique characteristics of the last-hop wireless link, such as random errors, varying round-trip times, blackouts, and handoffs. This paper argues that placing the intelligence of the transport protocol at the mobile host adjacent to the wireless link can result in better performance. The proposed protocol, RCP (Reception Control Protocol), is similar to TCP but has improved congestion control, loss recovery, and power management mechanisms. It is also suitable for mobile hosts with multiple interfaces accessing different wireless networks, as it provides scalable congestion control, seamless server migration during handoffs, and efficient bandwidth aggregation. Both simulations and real-world experiments were used to evaluate RCP.
41715	4171513	An end-to-end approach for transparent mobility across heterogeneous wireless networks	This paper discusses the use of multiple wireless interfaces on a mobile host and how it can lead to a seamless experience for the user, even when transitioning between different wireless networks. It is predicted that future wireless networks will support a wide range of technologies, allowing for more choices for mobile users. The proposed solution for a multi-homed mobile host includes features such as end-to-end host mobility without relying on network infrastructure, smooth handoffs between networks, and the ability to use different congestion control methods for each connection. The paper also presents the design and performance of the proposed approach through simulations and field experiments. 
41716	417161	binpac: a yacc for writing application protocol parsers	The process of semantic analysis in network traffic involves parsing the traffic stream to extract high-level protocols. This helps in transforming raw data into structured and meaningful data fields. However, manually constructing protocol parsers is a complex and error-prone task. To simplify this process, a declarative language called binpac has been developed, along with a compiler. This language allows for the efficient and robust creation of protocol parsers. It has been successfully used to build parsers for the Bro network intrusion detection system, replacing some existing analyzers and adding new ones. This allows for easier and more expressive analysis of network traffic using Bro's scripting language. binpac is now available as part of the open-source Bro distribution.
41716	4171615	RPC in the x-Kernel: evaluating new design techniques	The paper discusses the implementation of remote procedure call (RPC) protocols in the x-kernel. This is significant because it utilizes two unique design techniques, virtual protocols and layered protocols, which are made possible by the x-kernel's object-oriented infrastructure. This infrastructure provides a uniform interface to all protocols, allows for late binding between protocol layers, and has a minimal overhead for invoking any given layer. The paper provides concrete examples and experimental results to illustrate the effectiveness of these design techniques in implementing RPC protocols.
41717	4171741	K-automatic discovery in large image databases	This paper presents an unsupervised grouping method for organizing data items, specifically images, in large image databases. The focus is on a partition clustering approach that addresses the challenge of automatically determining the optimal number of clusters (k). While most partition clustering methods require manual input for k, this method offers an experimental solution. While manual input may be useful in certain domains, it is not realistic for general applications and can be time-consuming and unreliable. The paper also includes keywords such as clustering, partition, and data bases.
41717	4171727	PedVed: Pseudo Euclidian Distances for Video Events Detection	This paper presents a new method for automatically generating pseudo Euclidian distances (PED) from motion history images (MHI) to extract efficient image features for video event detection (VED). The method involves using trigonometric treatments on motion history blobs (MHB) to determine the maximum virtual distance a point can travel within a circle with its direction of motion. This PED can be applied in various computer vision applications and is the main contribution of this paper. The authors also propose a PED-based methodology for VED and demonstrate its effectiveness by showing detection results for events in real videos at TRECVID'08.
41718	4171856	Secure Multicast in a WAN	A secure reliable multicast protocol allows a process to send a message to a group of recipients, ensuring that all correct recipients receive the same message even if a minority of processes are malicious. This protocol is useful for building secure distributed services, but its cost typically increases with the size of the system. To reduce this cost, two approaches are presented: a protocol with a cost proportional to the number of tolerated failures, and a method of relaxing the consistency requirement to a probabilistic guarantee, resulting in a constant cost for very large networks. 
41718	4171862	A high-throughput secure reliable multicast protocol	A reliable multicast protocol allows a process to send a message to a group of processes in a way that ensures all honest members of the group receive the same message, even if some members are faulty. This protocol is useful for creating secure multiparty cryptographic protocols and distributed services. Our high-throughput reliable multicast protocol can tolerate malicious behavior from up to one-third of the group members. This is achieved through a unique method of chaining multicasts, which spreads the cost of ensuring agreement over multiple messages. Additionally, our protocol includes a flow-control mechanism that minimizes multicast latency.
41719	4171949	SlickFeel: sliding and clicking haptic feedback on a touchscreen	SlickFeel is a haptic display setup that can provide two types of feedback to a finger on a touchscreen. Sliding feedback allows the user to feel interactive objects on the screen through changes in friction, while clicking feedback replicates the sensation of pressing a button. This technology has been demonstrated through two scenarios - a simple button-click scenario where the user can feel the position of buttons and a simulated key-click when pressing on them, and a haptically-enhanced thumb-typing scenario where two thumbs can type without looking at the screen. SlickFeel has been integrated with a Kindle Fire tablet, making it compatible with existing mobile touchscreen devices.
41719	4171963	Effects of Multimodal Error Feedback on Human Performance in Steering Tasks	This study looks at how "error feedback" impacts user performance in steering tasks. The experiment tests different types of feedback, such as visual, auditory, and tactile, both alone and in combination. The findings reveal that feedback has a significant effect on the accuracy of steering tasks, but not on the time it takes to complete them. Tactile feedback was found to be the most effective in promoting accuracy. The paper provides valuable insights into the role of error feedback in steering tasks and offers suggestions for designing multimodal feedback systems in the future. 
41720	41720149	Browsing around a digital library	In the future, working in a digital library will feel qualitatively different than it does today. Instead of browsing through collections, readers and writers will engage in context-directed browsing. This will be made possible by structures created through automatic analysis of the library's contents, including full-text books and journals. Text mining techniques will be utilized to enhance the browsing experience. While present digital libraries are similar to traditional libraries, the digital library of the future will offer a more dynamic and personalized experience for users.
41720	4172038	Customizing digital library interfaces with Greenstone	Digital libraries are collections of information that are focused on a specific topic or theme and are organized to make information easily accessible. They typically include a description of how the information is organized. Greenstone is a software that helps users quickly create collections of information, ranging from a few minutes to several hours depending on the size and complexity of the collection. The software allows for customization of document formats, searchable indexes, and browsing structures. Users can also personalize the interface, translate it into different languages, and incorporate their own widgets and browsing mechanisms. Greenstone's design allows for flexibility and the addition of new facilities as needed.
41721	4172159	Information Based Distributed Control for Biochemical Source Detection and Localization.	The paper presents improvements to the Direction of Gradient (DOG) algorithm for detecting a biochemical source with moving sensors. The DOG algorithm is modified to be used as a distributed control scheme for a mobile sensing network, with more efficient numerical procedures replacing the original maximum likelihood estimation. Simulations show that these modifications simplify the algorithm and improve its performance. The algorithm uses the Cram´er-Rao bound (CRB) to minimize the expected location estimation error, and two methods are provided to approximate the ML estimates in a computationally cheaper manner. The motion control algorithm also allows for variable step size, unlike the original paper. The proposed modifications allow the DOG algorithm to be implemented on a mobile sensor network in a distributed manner, improving performance compared to a single sensor. 
41721	4172120	Projection Matrix Optimization for Sparse Signals in Structured Noise	The problem of estimating a signal corrupted with structured noise is addressed by mapping measurements to a lower dimensional space through a projection matrix. A method is proposed to optimize the design of this matrix, reducing processing time and error in estimating unknown parameters of the sparse model. Tunable parameters can affect performance, but a variant of the algorithm is also introduced that does not require tuning. Synthetic data is used to analyze algorithm performance and robustness against model parameter errors. A radar application with real clutter data is also used to demonstrate the effectiveness of the method. 
41722	4172253	Multiframe temporal estimation of cardiac nonrigid motion	The system described is designed for accurately tracking the motion of the left ventricular endocardial wall in image sequences. It uses an adaptive transversal filter and contour-based description of the object's boundaries to model motion trajectories across multiple frames. The system takes in a set of correspondences between contours and quality measures as input and derives relationships and models between frames. The system's output is validated by comparing computed trajectory estimates with the trajectories of physical markers implanted in the LV wall. Results show that a multiframe temporal model without spatial periodicity constraints provides the best performance with the least computational cost, while a multiframe spatiotemporal model offers the best performance at a higher computational expense.
41722	4172250	Estimating cardiac motion from image sequences using recursive comb filtering	The article introduces a framework for analyzing the movement of the left ventricular (LV) endocardial wall using a harmonic estimation approach. It also presents a method for computing flow vectors, which utilizes shape-based correspondences to create smoother and more accurate results. A recursive filter is then used to incorporate this relationship and temporal trends in the analysis. The study compares the trajectories of MR contrast markers implanted in the LV wall with those estimated by the filter, showing that 2-D analysis is a useful tool for developing algorithms for the 3-D problem of cardiac motion.
41723	4172354	On the Scope of the Universal-Algebraic Approach to Constraint Satisfaction	The universal-algebraic approach is a useful method for studying the complexity of constraint satisfaction problems (CSPs). It has previously been applied to CSPs with finite or infinite omega-categorical templates. This paper presents a characterization of CSPs that can be formulated with a finite or omega-categorical template. The approach relies on the fact that in these structures, a relation is primitive positive definable if and only if it is preserved by the polymorphisms of the structure. The paper also introduces results that can be used to study the computational complexity of CSPs with arbitrary infinite templates, showing that every CSP can be formulated with a template where a relation is primitive positive definable if and only if it is first-order definable and preserved by the infinitary polymorphisms. This has applications in describing and analyzing the computational complexity of CSPs, including a description of CSPs that can be solved in polynomial-time and general hardness criteria based on the absence of certain polymorphisms. 
41723	417234	Schaefer's theorem for graphs	Schaefer's theorem is a result that classifies Boolean constraint satisfaction problems into six classes, which can be solved in polynomial time, or are NP-complete. This concept has been extended to the propositional logic of graphs, where the input consists of a set of variables and statements about them, and the question is whether the statements are satisfiable in a graph. This extension results in 17 classes of graph formulas, which can be solved in polynomial time, or are NP-complete. This is achieved through a universal-algebraic approach, using structural Ramsey theory. The computational problems are formulated as constraint satisfaction problems (CSPs) and classified using the countably infinite random graph. This classification also produces independent mathematical statements.
41724	417245	Experiments with a robotic computer: body, affect and cognition interactions	RoCo is a robotic computer that moves its monitor in response to the user's postural movements. A study was conducted to investigate whether the computer's posture can influence the user's posture and affect, and if this affects their performance on a task. The study found that RoCo's posture was associated with the user's posture and affect, leading to increased persistence in problem solving and a higher level of comfort. This supports new theories linking physical posture with affect and cognition. The results suggest that a computer's posture can influence the user's body state and improve task measures.
41724	4172489	MeBot: a robotic platform for socially embodied presence	Telepresence involves using technology to feel as though one is present at a distant location. Telerobotics is a specific type of telepresence that involves robots. This paper discusses the design and evaluation of a telepresence robot that allows for social expression. The researchers hypothesized that a robot that can use gestures, body pose, and proxemics in addition to audio and video communication would lead to a more engaging and enjoyable interaction. The MeBot platform was created through an iterative design process, along with supporting systems and control interfaces. A human subject study was conducted to measure the effects of expressiveness, and the results showed that a socially expressive robot was more engaging, likable, and led to more psychological involvement and cooperation than a static one.
41725	4172538	Une description probabiliste de la communication parlée entre homme et machine	This paper introduces a formal structure for describing human-computer spoken communication within spoken dialogue systems. This structure utilizes probability to describe the interaction and information processing within each module of a dialogue system. The paper also suggests potential uses for this framework in the design of dialogue systems.
41725	4172522	Consistent Goal-Directed User Model For Realisitc Man-Machine Task-Oriented Spoken Dialogue Simulation	Designing a spoken dialogue system is a complex task due to the many factors that need to be considered. This makes it difficult to rapidly design and reuse previous work. As a result, the application of machine learning methods to optimize dialogue strategies has gained attention in the past decade. However, methods like reinforcement learning require a large amount of training data, which is time-consuming and expensive to obtain in the case of spoken dialogues. To overcome this, dialogue simulation techniques are being used to expand existing datasets. This paper presents a user modeling technique for realistic simulation of goal-directed spoken dialogues, which is consistent and takes into account the interaction history and the user's goal. 
41726	4172649	Automated Design of Cryptographic Hash Schemes by Evolving Highly-Nonlinear Functions	In recent years, several vulnerabilities have been discovered in commonly used cryptographic hash functions like MD4, MD5, and SHA-1. This has led to the recommendation of switching to more secure options like SHA-256 and Whirlpool. However, there is concern that these schemes may also be vulnerable to similar attacks due to their similarities with MD4. To address this, a new approach using Genetic Programming for automated design of cryptographic block ciphers and hash functions is presented. One such scheme, named Wheedham, is proposed and compared with other options like SHA-512 and Whirlpool in terms of security and speed. The results show that this automated approach can produce competitive and secure schemes.
41726	4172652	Secure electronic payments in heterogeneous networking: new authentication protocols approach	Recent research has focused on maintaining transparent heterogeneous networking using the all-IP concept. For example, the global standardization initiative now includes inter-working between 3G cellular systems and wireless LAN. Smart cards have been identified as powerful devices that can provide strong authentication at lower layers of the protocol stack. This research proposes a new model and scenario for secure electronic payment in this environment, taking into consideration the impact on trust relationships. A set of authentication requirements is also provided. Additionally, a new approach using end-to-end layer 2 authentication protocols is proposed, incorporating improvements that align with this context.
41727	4172731	DroidNative: Automating and optimizing detection of Android native code malware variants.	Mobile malware development has primarily targeted the Android platform, with a focus on creating variants that can evade detection by traditional signature-based detectors. These variants often use obfuscation techniques and are embedded in native code, making them difficult to detect with static analysis tools. In response to this threat, a new system called DroidNative has been proposed, which uses specific control flow patterns to identify and detect malware in both bytecode and native code. When tested on a dataset of 5490 samples, DroidNative achieved a high detection rate of 93.57% and a low false positive rate of 2.7%. Compared to other academic and commercial tools, DroidNative showed a significantly higher detection rate of 99.48% when tested with traditional malware variants.
41727	41727111	Using Failure Information Analysis to Detect Enterprise Zombies.	The authors propose a new method, called "failure information analysis," for detecting malware and other unusual activity in enterprise network traffic. Their focus is on identifying self-propagating malware like worms and botnets. They conducted an empirical study using real malware traces and found that the failure patterns of these traces differed significantly from those of normal applications. They then developed a prototype system, called Netfuse, which uses machine learning and clustering techniques to automatically detect and isolate suspicious failure patterns. Their evaluation using multiple malware traces showed that Netfuse is effective in identifying infected hosts and could be a valuable addition to current defense mechanisms. 
41728	4172831	Rewrite method for theorem proving in first order theory with equality	This paper details an extension of the term rewriting approach to first order theorem proving, specifically in the theory of first order predicate calculus with equality. This allows the term rewriting method to be as powerful as the combination of paramodulation and resolution. The approach was originally described in Hsiang & Dershowitz 1983.
41728	4172840	Rewrite Methods for Clausal and Non-Clausal Theorem Proving	In this paper, a new set of rewrite rules for Boolean algebra is introduced to provide efficient and complete proof strategies for first order predicate calculus. These methods can utilize lemmas in proofs and can incorporate canonical systems as rewrite rules, making them useful for program verification, data type specification, and programming language design. Preliminary results show that these methods are more space-efficient than conventional resolution strategies.
41729	4172912	Generalized Minimum Noise Subspace For Array Processing.	The paper introduces the generalized minimum noise subspace (GMNS) method, which is an extension of the minimum noise subspace (MNS) method used for blind channel identification. GMNS allows for faster and parallel computation of signal and noise subspaces, even with a fixed number of parallel computing units. The paper presents different batch and adaptive algorithms for implementing GMNS and compares its computational complexity and accuracy with other standard subspace methods through simulated and real-life experiments in radio astronomy. The results show that GMNS strikes a good balance between computational efficiency and subspace estimation accuracy. 
41729	417294	COMMON POLE ESTIMATION WITH AN ORTHOGONAL VECTOR METHOD	The article discusses the challenge of estimating common poles in multi-channel setups used in biomedical analysis. The proposed solution is a new subspace algorithm based on Orthogonal Vector Methods, called root-MUSIC, which can accurately estimate damped components at a similar cost to traditional methods. The effectiveness of the algorithm is demonstrated through an example, showing its efficiency in low signal-to-noise ratio situations.
41730	4173034	Performance analysis of MUSIC and Pencil-MUSIC algorithms for diversely-polarized array.	The paper discusses the MUSIC and Pencil-MUSIC methods, proposed by Hua in 1993, for estimating 2D angles and polarizations using crossed dipoles. It provides an asymptotical analysis of these methods and derives first order perturbation expressions for their variances. These expressions are then used to compare the performances of the two methods. The paper also presents a comparison between the Pencil-MUSIC method and the angle-only Pencil-MUSIC method proposed by Cheng and Hua in 1993. Simulation results show good agreement with theoretical results. 
41730	4173055	Asymptotic properties of 2-D MUSIC estimator and comparison to 2-D MP estimator	In this study, the MUSIC estimator for two-dimensional frequencies (2-D MUSIC) is analyzed in a one-measurement data model with deterministic phases and additive complex white Gaussian noise. The large sample estimation covariance for 2-D MUSIC is derived and compared to that of the 2-D matrix pencil (MP) estimator. The theoretical estimation variances for both methods are compared with simulated results and the Cramer-Rao Bound (CRB). The paper also provides the most revealing form of the estimation covariance for both methods in the case of a single 2-D sinusoid. These results are applicable for a moderate range of signal-to-noise ratios (SNR).
41731	417311	Randomized Shellsort: a simple oblivious sorting algorithm	The paper introduces a randomized version of the Shellsort algorithm that guarantees a running time of O(n log n) and a high probability of successfully sorting any input permutation. This algorithm has potential applications in designing efficient privacy-preserving computations using secure multi-party computation. By converting it to a Las Vegas algorithm, it becomes the first version of Shellsort with a proven running time of O(n log n) and a high probability of success.
41731	4173132	Randomized Shellsort: A Simple Data-Oblivious Sorting Algorithm	The article discusses a randomized version of the Shellsort algorithm that guarantees a running time of O(n log n) and a high probability of sorting any given input permutation. This algorithm has potential applications in designing efficient privacy-preserving computations using secure multiparty computation. Additionally, the algorithm can be converted to a Las Vegas equivalent with a provable running time of O(n log n) and high probability of success.
41732	4173280	An eigenspace projection clustering method for inexact graph matching	The paper discusses a method for solving inexact graph matching by using a technique called eigenspace renormalization projection clustering (EPC). This involves projecting the vertices of a pair of graphs into a joint eigenspace and then using a form of relational clustering to determine their correspondence. One advantage of this method is its ability to match graphs with different numbers of vertices. The authors demonstrate the effectiveness of this approach through the use of shock graph-based shape matching and also explore its application to random graphs. Initial results show promise for this method as a more objective way to evaluate graph matching.
41732	4173247	Inexact Multisubgraph Matching Using Graph Eigenspace and Clustering Models	This paper discusses how inexact multisubgraph matching can be solved by projecting vertices and their connections into the eigenspaces of graphs and using associated clustering methods. It highlights the limitations of current eigenspectra methods and showcases the effectiveness of full eigenspace methods for filtering computationally intensive problems. The paper also includes examples of how this method can be applied to shape matching, information retrieval, and natural language processing. Overall, the paper provides a solution for efficiently solving inexact multisubgraph matching problems using eigenspace methods. 
41733	417333	Towards Training-Free Refinement for Semantic Indexing of Visual Media.	The current method of indexing visual media based on content analysis involves combining individual concept detectors and post-processing their outputs. However, due to limited and imprecise training data, training-based methods struggle to accurately capture relationships between concepts. This paper proposes a training-free refinement algorithm, TFR, that uses concept detection results and incorporates global and temporal neighbourhood information to enhance semantic indexing. Additionally, ontological concept relationships can be integrated into the model for further improvement. Experiments on two datasets show the effectiveness of this approach.
41733	417339	Semantically Smoothed Refinement for Everyday Concept Indexing.	This paper proposes a semantically smoothed refinement algorithm for concept detection in visual lifelogging using wearable cameras. It leverages concept correlations to improve accuracy, as semantic concept pairs tend to co-occur in images. The algorithm uses externally modeled concept relationships from a user experiment, rather than training data, to adjust initial concept detection results. Experiments show the effectiveness of the algorithm and the extracted correlations. This is especially useful for lifelogging, where images often contain a variety of concepts that can challenge detection performance. By exploiting concept correlations, the proposed algorithm enhances the accuracy of everyday concept detection for wearable camera recordings.
41734	4173471	Mining diversity on networks	Many large-scale networks have recently emerged in various fields, but a crucial measure of a participant's diversity in the network has been overlooked in previous studies. This measure, known as diversity, measures the diversity of connections a node has with its peers. This paper presents a thorough analysis of the concept of diversity, outlining two criteria that capture its meaning and proposing a simple definition that can be easily incorporated. The paper also introduces an efficient algorithm for ranking top-k diversity in dynamic networks. Experiments on synthetic and real datasets yield significant results, with highly diverse nodes being identified as intuitive.
41734	417347	Item-Level Social Influence Prediction with Probabilistic Hybrid Factor Matrix Factorization.	Social influence is a crucial driving force behind the constantly evolving structure and behaviors of social networks. Previous studies have mainly focused on analyzing social influence at the network or topic level. However, this paper aims to predict social influence at a more detailed level by looking at specific items. This is achieved by estimating a user-post matrix, where each entry represents the strength of social influence a particular user has on a web post. To address challenges such as sparsity and complexity, the authors propose a method called Probabilistic Hybrid Factor Matrix Factorization (PHF-MF) which incorporates prior knowledge on both user and web post dimensions. The effectiveness of this method is demonstrated through experiments on a real world online social network.
41735	4173518	A transformational approach to compiling Sisal for distributed memory architectures	This paper discusses ways to efficiently execute array computation on Distributed Memory Architectures using compiler-directed program and data transformations. By translating a subset of Sisal, a single-assignment language, into a linear algebraic framework, the authors show that it is possible to reduce load imbalance and non-local memory access. They propose a new test to identify load imbalance and a method for data alignment to reduce non-local access. The paper also presents three criteria for partitioning and a systematic approach for mapping data and computation to processors. Additionally, a new pre-fetching procedure is introduced to prevent redundant non-local accesses.
41735	4173524	A Complete Compiler Approach to Auto-Parallelizing C Programs for Multi-DSP Systems	This paper introduces a new approach for auto-parallelizing compilers on embedded DSPs, which have been previously unsuccessful due to complex memory models and pointer arithmetic. The approach combines pointer conversion and modulo elimination techniques to enable parallelization. It also includes a data transformation method that exposes the location of partitioned data, and an address resolution mechanism for efficient programs without message passing. An optimization is also presented to exploit remote data locality and local memory bandwidth. This approach was tested on two benchmark suites and showed an average speedup of 3.78 on four Analog Devices TigerSHARC TS-101 processors. 
41736	4173647	Optimal Data Acquisition for Statistical Estimation.	In this paper, we examine the problem of a data analyst purchasing data from strategic agents in order to accurately estimate a statistic. The agents incur private costs to reveal their data, which can be correlated with their data. Our focus is on linear unbiased estimators, and we develop a mechanism that is both individually rational and incentive compatible. This mechanism minimizes the worst-case mean-squared error of the estimation, while also adhering to a budget constraint. We provide a closed-form solution for this optimal mechanism and also extend our findings to the case of acquiring data for parameter estimation in regression analysis, where private costs can correlate with the dependent variable but not the independent variables.
41736	4173613	Incentives and Efficiency in Uncertain Collaborative Environments.	The article discusses collaborative systems in which users contribute to multiple projects and are rewarded based on their contributions in each individual project. This is a model for online social computing systems and scientific co-authorship. The study shows that simple local sharing rules can effectively approximate the maximum value produced, even when there is incomplete information about the users' abilities and effort constraints. In most cases, the efficiency of this system is close to 95% at equilibrium. However, when players have a cost for their effort, there is a threshold effect where the efficiency is a constant fraction of the optimal when the cost is strictly convex and decreases with the number of players if the cost is linear. 
41737	41737104	Medium access control with channel state information for large sensor networks	Traditionally, random access protocols have been designed with simple models for the physical layer. However, a new reception model has been introduced that incorporates the channel states of transmitting users and allows for multiple simultaneous successes. This model assumes that each user has access to their own channel state and proposes a variant of Slotted ALOHA protocol for medium access where the transmit probability is based on the channel state. The maximum stable throughput of a finite user system is derived and optimal schedulers are obtained for simple reception models. The concept of asymptotic stable throughput is also introduced, which is the maximum stable throughput as the number of users increases. The use of channel state information in schedulers is studied for sensor networks, specifically in the context of CDMA networks. This application shows the benefits of using channel state information to vary the transmit probability.
41737	4173795	Anonymous Networking Amidst Eavesdroppers	This work focuses on addressing the issue of security against packet timing based traffic analysis in wireless networks. The authors propose a measure of anonymity for routes in eavesdropped networks using information-theoretic equivocation. To improve network performance while maintaining anonymity, they design scheduling and relaying techniques for networks with orthogonal transmitter directed signaling. The performance is measured by the total rate of packets delivered, with constraints on latency and medium access. The authors present results for two scenarios: single relay with independent scheduling and multihop network with randomized relay selection. The proposed strategies are shown to be equivalent to an information-theoretic rate-distortion function.
41738	41738121	A computational model for watermark robustness	Multimedia security schemes often use a combination of cryptography and information hiding techniques, such as steganography or watermarking. These schemes have various applications, such as dispute resolution, proof of ownership, and fingerprinting. To ensure their effectiveness, formal security definitions are necessary. However, existing definitions for information-theoretic and computational security cannot be directly applied to watermarking schemes due to their differences. Additionally, current definitions for watermark security have conceptual flaws. In this paper, the authors propose a formal framework and definitions for watermark robustness, the key security property of watermarking schemes. These definitions address the limitations of previous proposals and allow for compatible security proofs with cryptographic definitions. 
41738	4173838	Unsafe exposure analysis of mobile in-app advertisements	In recent years, there has been a significant increase in smartphone sales, leading to a vast number of available apps. These apps offer various features that attract consumers, while also providing financial benefits for developers through direct sales or embedded ad libraries. However, these ad libraries pose potential privacy and security risks. A study was conducted on the Android platform, where 52.1% of the collected 100,000 apps contained representative in-app ad libraries. A system called AdRisk was developed to identify these risks, examining if the ad libraries upload private information or download untrusted code from remote servers. The results showed that most ad libraries collect private information, some for legitimate purposes like location, while others excessively collect data like call logs and installed apps. Some even use unsafe methods to fetch and run code from the internet, posing serious security risks. This highlights the need for better regulation of ad libraries in Android apps.
41739	4173977	Towards security policy decisions based on context profiling	The growing use of personal mobile devices has led to the creation and consumption of valuable and sensitive data by ordinary users. However, without proper access control policies, this data can be disclosed in unintended ways. While some applications and services offer security and privacy policies, users may not understand or be able to adjust them to their needs. This paper suggests that context information, such as a device's location, can be used to infer appropriate access control policies. Three usage scenarios are described to support this argument, and the concept of "familiarity" is proposed as a measure to determine suitable policy settings. The paper also shares the results of a study using context observations from two participants' devices over a period of time.
41739	4173979	Usable mobile security	In today's world, mobile security is becoming increasingly important as we rely more on our mobile devices for various tasks. However, achieving usable security on these devices is a challenging task. This is due to various problems such as small screens, limited user attention, and the need for convenience. In order to address these issues, it is crucial to design user-friendly and secure solutions for mobile devices. We highlight the importance of considering the unique characteristics of mobile devices in developing such solutions. By doing so, we can ensure that mobile security is not only effective, but also usable for the average user. 
41740	4174025	A visual token-based formalization of BPMN 2.0 based on in-place transformations	This paper presents a formalization of the Business Process Model and Notation (BPMN) standard, which defines the execution semantics for process instances. Existing formalizations are incomplete and require mappings to other languages, while this formalization uses in-place graph transformation rules that directly update the models. The formalization is documented visually using BPMN syntax and has been extensively verified. It can be used to complement the existing standard and identify potential issues, providing a useful tool for the maintainers of the standard and vendors to verify conformance.
41740	4174014	Semantics and analysis of business process models in BPMN	BPMN is a standard used for capturing business processes in the beginning stages of systems development. However, the variety of elements included in BPMN can lead to models with semantic errors. These errors can be costly and difficult to correct in the early phases of development. To address this issue, a mapping from BPMN to Petri nets, a formal language with efficient analysis techniques, has been proposed. This mapping has been implemented as a tool to enable the static analysis of BPMN models. The formalisation process also revealed flaws in the BPMN standard specification.
41741	4174111	Improved IPsec performance utilizing transport-layer-aware compression architecture.	The growth of internet communication has led to a need for better security measures for transmitting information over public networks. The Internet Protocol Security (IPsec) standard was developed to address this issue. However, IPsec solutions often result in slower processing times and larger packet sizes. To improve both security and performance in IPv6 networks, a new compression architecture called Efficient Secure Pipe (ES_PIPE) has been proposed. Unlike traditional IPsec compression methods, ES_PIPE is transport-layer-aware and can compress different types of packets. Experimental results show that ES_PIPE reduces transmission time by 20-30% and improves the efficiency of encryption and decryption processes by 20%.
41741	4174125	Digital evidence collection for web image access.	The increasing amount of multimedia data being shared online has raised concerns about unauthorized duplication of copyrighted images and the spread of undesirable material. A new approach has been proposed to detect the dissemination of pre-selected JPEG images, which are the most commonly used format for images on the Internet. This approach collects and stores packet-level features of the target image in a database, and compares these features with those of packets passing through a designated router to identify any transmissions of the target image. Results from experiments show that this approach is more efficient than existing methods and could be used for monitoring JPEG streams on the Internet to detect and gather evidence of unlawful transmissions.
41742	4174245	Design of Length-Compatible Polar Codes Based on the Reduction of Polarizing Matrices	Length-compatible polar codes are a type of polar codes that can be used for various lengths using a single encoder and decoder. This paper presents a method for constructing length-compatible polar codes by reducing the 2n × 2n polarizing matrix proposed by Arikan. The conditions for a reduced matrix to support a polar code of a given length are analyzed and used to construct these codes through codeword-puncturing and information-refreezing. These codes have low complexity and can be encoded and decoded similarly to a polar code of length 2n. The results show that these codes provide a performance gain of 1.0 - 5.0 dB compared to randomly punctured codes when using successive cancellation decoding.
41742	417428	A combining method of quasi-cyclic LDPC codes by the chinese remainder theorem	This paper introduces a method for creating QC-LDPC codes with large lengths by combining smaller QC-LDPC codes using the Chinese remainder theorem. The resulting codes have a higher girth than their component codes. The method is applied to array codes, resulting in a family of high-rate regular QC-LDPC codes with no 4-cycles. Simulation results demonstrate that these codes perform similarly to random regular LDPC codes.
41743	4174399	Compute-and-forward: Harnessing interference with structured codes.	The use of centralized encoder and decoder in communication systems involves a channel matrix, which represents a set of linear equations that can be transformed into parallel channels. In multiuser networks, interference is seen as creating linear equations of codewords, and the goal of the receiver is to collect a complete set of such equations. A new relaying technique, called compute-and-forward, uses structured codes to reliably compute functions over channels. This allows the relays to efficiently recover linear functions of codewords without fully decoding them, thus removing the effects of noise at the relay. This technique has been applied to a Gaussian relay network with interference and has shown to achieve better rates compared to other techniques in certain situations.
41743	4174313	Compute-and-Forward: Harnessing Interference Through Structured Codes	This paper presents a new approach, called compute-and-forward, for wireless networks that utilizes interference to achieve higher communication rates between users. The idea is for relays to decode linear equations of transmitted messages based on their observed channel coefficients, instead of treating interference as noise. These equations are then sent to the destinations, which can recover their desired messages with enough equations. This approach uses nested lattices and algebraic codes to ensure reliable decoding of integer combinations of codewords. The encoding and decoding process involves mapping messages from a finite field to a lattice and then back to equations over the field. This method can be used even without channel state information from the transmitters.
41744	41744210	The SPRIGHT algorithm for robust sparse Hadamard Transforms	This paper discusses the problem of computing a K-sparse N-point Hadamard Transform (HT) from noisy time domain samples, where K is sub-linearly scaled to N. The SPRIGHT algorithm is proposed to recover the sparse HT coefficients in a stable manner, even with the presence of additive Gaussian noise. It is shown that the K-sparse HT can be reconstructed from noisy samples with a small error probability and the same sample and computational complexity as in the noiseless case. Numerical experiments also confirm the small big-Oh constants in the complexity of the SPRIGHT algorithm.
41744	41744152	A robust R-FFAST framework for computing a k-sparse n-length DFT in O(k log n) sample complexity using sparse-graph codes	The Fast Fourier Transform (FFT) is the most efficient way to compute the Discrete Fourier Transform (DFT) of an arbitrary n-length signal, with a computational complexity of O(n log n). However, if the DFT of the signal has only k non-zero coefficients (where k <; n), a new algorithm called FFAST (Fast Fourier Aliasing-based Sparse Transform) can be used to compute the DFT even faster. This algorithm works by inducing sparse graph codes in the DFT domain through a Chinese-Remainder-Theorem-guided sub-sampling operation. It then uses a simple iterative decoder to compute the DFT using only O(k) time-domain samples and O(k log k) computations, without any noise. This paper extends the FFAST algorithm to handle white Gaussian noise, showing that the noise robust algorithm R-FFAST can compute the DFT using only O(k log n) noise-corrupted time-domain samples and O(n log n) computations. Simulation results show that this algorithm works well for signals with an approximately sparse Fourier spectrum, such as MR images.
41745	417456	A trace semantics for Petri nets	The concept of trace is extended to apply to Petri nets, resulting in a poset of generalized traces associated with each net. The trace languages defined by Petri nets are characterized, and it is shown that this characterization also applies to Winskel's general event structures and stable event structures. This means that in this framework, stable event structures, general event structures, and Petri nets form a hierarchy in terms of expressive power.
41745	417454	Local Event Structures and Petri Nets	This paper explores the question of what type of event structures can accurately represent the behavior of general Petri nets. The authors propose a new class of event structures, called OL-event structures, which are able to represent Petri net behavior as long as auto-concurrency is excluded. Despite this limitation, the OL-event structure semantics is a significant extension of the existing event structure semantics for 1-safe Petri nets. Furthermore, the relationship between prime event structures and 1-safe Petri nets can also be extended to this new setting, as long as the Petri nets being studied do not exhibit auto-concurrency.
41746	4174653	Getting Rid Of Links In Hierarchical Radiosity	Hierarchical radiosity with clustering is a popular algorithm for calculating global illumination in complex environments. However, it has been difficult to use for intricate scenes due to the large number of transport coefficients (links) that need to be stored between surfaces. In this paper, a modified shooting method is proposed to solve the radiosity equation without the need for storing links. This reduces memory consumption significantly, although computation time may increase. The error behavior of this new method is compared to the traditional gathering approach, and the relationship between global error and local error threshold is analyzed. Caching can also be used to reduce computation time.
41746	4174630	Three Point Clustering for Radiance Computations	This paper discusses the progress made in accelerating global illumination computation for diffuse environments and the use of clustering to handle complex scenes. However, for non-diffuse scenes, Monte-Carlo sampling methods are still preferred due to the long computation times of non-diffuse finite-element approaches. The paper proposes a novel clustering approach for radiance computations using a line space hierarchy, which efficiently represents light propagation and reflection between non-diffuse surfaces and clusters. It also explains the rendering equation, which is used to approximate the solution for all global illumination algorithms, and how it describes the reflection properties of surfaces and their emission of light. The paper will use a three-point parameterization of the rendering equation.
41747	4174770	Warped K-Means: An algorithm to cluster sequentially-distributed data	Many data-generating devices, such as motion sensors and eye trackers, produce large amounts of sequential data. This data often needs to be compressed for classification, storage, and retrieval purposes. Traditional clustering algorithms are not effective for this type of data as they do not consider the sequential nature of the data. To address this issue, a new method called Warped K-Means (WKM) has been developed. This algorithm incorporates a hard sequentiality constraint in the clustering process, resulting in improved accuracy and reduced computational cost. WKM has been successfully applied to various tasks, including human activity segmentation and classification, outperforming other state-of-the-art techniques.
41747	4174718	Two Different Approaches for Cost-Efficient Viterbi Parsing with Error Correction	The article discusses the problem of Error-Correcting Parsing (ECP) using a complete error model and a Finite State Machine (FSM). This issue is crucial in Linguistic and Speech Processing, particularly in Syntactical Pattern Recognition where data is often distorted or noisy. The Viterbi algorithm can be used for ECP by extending it to a trellis diagram with the same number of states as the FSM. However, this process can be computationally complex and not suitable for real-time pattern recognition tasks. The article suggests two approaches to efficiently implement ECP: adapting an existing algorithm or using a depth-first sort of the FSM states to process deletion transitions. Experiments were conducted to compare the effectiveness of these techniques.
41748	41748102	Face Matching and Retrieval Using Soft Biometrics	Soft biometric traits, such as gender and facial marks, are not distinct enough on their own for accurate face recognition. However, when combined with face matching scores, they can improve overall accuracy. In certain applications, like visual surveillance, soft biometric traits are even more valuable in cases where a face is partially covered or in a non-frontal pose. These traits can also differentiate between identical twins and provide descriptive evidence in court cases. A method has been developed to automatically detect facial marks using active appearance models and blob detection. Experiments using various databases have shown that incorporating soft biometric traits can enhance the performance of face recognition systems.
41748	4174819	Demographic Estimation from Face Images: Human vs. Machine Performance	Demographic estimation involves automatically determining a person's age, gender, and race from their face image. This has various potential applications, but it remains a difficult problem due to the variation in facial appearances among individuals within the same demographic group. In this paper, a framework is presented for automatic demographic estimation using a boosting algorithm and a hierarchical approach. Quality assessment is also incorporated to identify low-quality face images. The proposed approach outperforms existing methods on several face image databases and a human perception study using crowdsourced data provides further insights into this challenging problem.
41749	4174936	KoNKS: konsensus-style network koordinate system	A network coordinate system assigns virtual coordinates to nodes in a network based on their distance from each other, allowing for accurate estimation of network latency without direct communication between nodes. These systems have many practical applications, but can also be vulnerable to attacks where malicious peers report false coordinates. Previous secure schemes have been developed to protect against these attacks, but a new type of attack has recently been discovered. To address this issue and prevent an "arms race" of constantly developing new defenses, a new network coordinate system called KoNKS has been proposed. KoNKS aims to achieve consensus among nodes by limiting the influence of malicious peers and preventing them from appearing closer than they actually are. It has been shown to be as accurate as previous systems and has low processing and communication overhead.
41749	417495	From weak to strong watermarking	A watermarking scheme aims to embed a mark in a digital object, making it difficult for an adversary to remove without damaging the object. While there has been much research on creating and breaking watermarking schemes, little attention has been given to formally defining their security goals. This study introduces a new definition of security for watermarking schemes based on complexity theory. It addresses shortcomings of previous definitions and shows that security under this new definition also implies security under previous ones. Additionally, the study proposes two weaker security conditions that align with practical goals and demonstrates how schemes meeting these conditions can be enhanced to meet the new definition.
41750	417503	Secure Group Communication Using Robust Contributory Key Agreement	Contributory group key agreement protocols are useful for small peer groups as they generate group keys based on contributions from all group members. These protocols are resistant to various attacks and offer strong security properties such as key independence and perfect forward secrecy. This paper introduces a robust contributory key agreement protocol that can handle any sequence of group changes. It is based on Group Diffie-Hellman and uses a group communication system with Virtual Synchrony semantics. The protocol provides both Virtual Synchrony and Group Diffie-Hellman security properties in the presence of node failures, network partitions, and other events. A secure group communication service, Secure Spread, was implemented using this protocol. It is compared to a centralized group key management protocol with similar security properties to demonstrate its practicality.
41750	4175011	Exploring Robustness in Group Key Agreement	The importance of secure group communication in dynamic environments and over unsecured networks is highlighted in this paper. Key agreement is a crucial aspect of providing security services for such systems. However, most existing contributory key agreement protocols do not have the ability to handle failures and changes in group membership during execution, especially nested or cascaded events like partitions. This paper introduces a robust contributory key agreement protocol that can withstand any sequence of events while maintaining group communication membership and ordering guarantees. This protocol is the first of its kind and addresses a significant gap in group communication security.
41751	4175121	Metric Learning Based Structural Appearance Model for Robust Visual Tracking	The success of a visual tracker depends heavily on its ability to accurately model appearance. Recently, there has been a growing interest in using sparse representation for appearance modeling. However, most existing methods use reconstruction errors to compute the observation likelihood, which can lead to poor performance when faced with significant appearance variations. This paper proposes a new approach that uses a metric learning based structural appearance model to better match different appearances. The structural representation is obtained through multiscale max pooling of weighted local sparse codes. An online multiple instance metric learning algorithm is then used to learn a discriminative and adaptive metric, reducing the impact of misaligned training examples. The proposed method, implemented within a Bayesian inference framework, outperforms existing methods in terms of accuracy and effectiveness, as demonstrated through comprehensive experiments on challenging image sequences.
41751	4175155	Fast human action classification and VOI localization with enhanced sparse coding	Sparse coding, a technique used to encode natural visual signal into a sparse space for visual codebook generation and feature quantization, has been successfully applied in image classification tasks. However, its potential in video analysis has not been fully explored. The complexity of characterizing human actions with spatial and temporal variations poses challenges to traditional sparse coding methods. To address this, the paper proposes an enhanced sparse coding scheme that learns a discriminative dictionary and optimizes the local pooling strategy. The paper also presents a coarse-to-fine framework for localizing actions in videos using the sparse coding representations. This framework utilizes a Spatial Temporal Pyramid Matching method to obtain candidate volumes of interest and a multi-level branch-and-bound approach to refine them. Experimental results show that the proposed approach reduces computational cost while maintaining comparable accuracy to state-of-the-art methods on benchmark datasets. 
41752	417523	Single Image Dehazing with White Balance Correction and Image Decomposition	Dehazing a single image is a difficult task due to its ambiguous nature. Existing dehazing methods often make assumptions and use prior knowledge to improve the results, but they rarely consider the impact of the imaging process itself, such as white balance and metering. This can result in color distortion and under-exposure in foggy scenarios. To address these issues, a new approach is proposed in this paper which corrects white balance and decomposes the image into two components, reflex lightness and ambience illumination. These components are then used to improve the dehazing process and produce a high quality image. Experimental results show that this method outperforms current state-of-the-art methods in terms of contrast and color accuracy.
41752	4175236	Video Object Segmentation with Occlusion Map	Video segmentation is the process of separating foreground objects from the background in a video. However, this becomes challenging when using hand-held cameras, as most existing approaches rely on assumptions about the background scene or camera motion. To address this, some methods use depth and motion cues, while others utilize interactive image segmentation algorithms. In this paper, the authors propose a new approach that leverages the latest advances in interactive image segmentation. They use an occlusion map derived from an optical flow-based method as the "seeding" input for the interactive segmentation algorithm. This allows for accurate extraction of foreground objects, even in cases where only partial movements are present. Experimental results demonstrate the effectiveness and efficiency of this approach. 
41753	4175352	Combined Retrieval Strategies for Images with and without Distinct Objects	The paper discusses the design of an all-season image retrieval system that can handle images with and without distinct objects. The system uses a neural network trained with the BPTS algorithm to pre-classify images into distinct-object or no-distinct-object categories based on visual contrasts and spatial information. For images with distinct objects, an attention-driven retrieval strategy is used, while images without distinct objects are processed with a fusing-all retrieval strategy. The combination of these two approaches results in improved performance. 
41753	4175337	Tree structures with attentive objects for image classification using a neural network	This paper presents a new method for classifying images using a neural network model that incorporates attentive objects in a tree structure. These attentive objects are extracted from segmented images using an attention-based algorithm. The paper explores three different combinations of tree structures and finds that the "image + attentive objects" structure is the most effective in terms of classification accuracy and computational time. The neural network is trained using the back propagation through structure (BPTS) algorithm. Results show that this structure outperforms a state-of-the-art tree structure from previous research. Overall, the proposed method offers a promising approach for image classification.
41754	4175452	Quantifying the Benefits of SSA-Based Mobile Code	This paper discusses the use of Static Single Assignment (SSA) form as an alternative transportation format for Java code, and its potential benefits for just-in-time compilers. The authors integrated support for SSA-based code representation into Jikes RVM, a Java execution environment, and compared its performance to traditional JVM bytecode. Results show improvements in both compilation time and code quality, suggesting that SSA-based representations offer advantages for just-in-time compilation. This study validates the potential of SSA form as a superior alternative to virtual machines for high-performance Java code generation.
41754	417546	Java bytecode verification via static single assignment form	Java Virtual Machines (JVMs) use a traditional iterative dataflow analysis method to perform bytecode verification, ensuring type safety as temporary variables are not statically typed. However, a new verification mechanism has been presented that transforms JVM bytecode into Static Single Assignment Form (SSA) and propagates definitions directly to uses, allowing for type checking at control flow merge points in a single pass. This alternative algorithm has been implemented as a prototype and is faster than the standard JVM bytecode verifier. Furthermore, it also has the added benefit of generating SSA as a side effect, which can be useful for subsequent dynamic compilation stages.
41755	4175590	Video-Based Self-positioning for Intelligent Transportation Systems Applications.	Traffic congestion is a common issue in urban areas, leading to increased research in automatic traffic management systems and congestion pricing. A crucial aspect of these systems is lane prediction and self-positioning of vehicles on the road. To address this, a new problem of vehicle self-positioning using dashboard camera footage is introduced. This involves predicting the number of lanes and localizing the vehicle within those lanes. A model is formulated that uses the video as a key observation, making it more effective than existing low-level vision-based techniques. The model takes into account the number of lanes and vehicle position as parameters, making use of high-level semantic knowledge. This approach is tested on real-world videos and proven to be successful.
41755	4175552	Bridging the semantic gap in sports video retrieval and summarization	The ''semantic gap'' between user expectations and automatically extracted content descriptions is a major challenge in media management systems. This issue is addressed in the sports domain through a framework for indexing and summarizing sports broadcast programs. The framework uses a high-level model of sports broadcast video based on the concept of an event, and includes automatic event detection algorithms for different types of sports. Event modeling and detection help to reduce the semantic gap by providing basic semantic information from media analysis. Additionally, a new approach is proposed that combines rich textual metadata with event segments to bridge the gap completely. This is implemented in an MPEG-7 compliant browsing system for semantic retrieval and summarization of sports video.
41756	4175651	Mapping deontic operators to abductive expectations	Deontic concepts and operators, which represent norms, have been widely used in legal reasoning and normative multi-agent systems. The EU-funded SOCS project has developed a language for specifying agent interactions in open multi-agent systems, with a declarative and operational semantics. The SOCS framework uses this specification directly for verification. This paper proposes mapping deontic operators to expectations in the SOCS social framework, supported by a similarity between their abductive semantics and Kripke semantics. The aim is to use the computational machinery of the SOCS framework for specifying and verifying systems using deontic operators.
41756	4175635	Verifiable agent interaction in abductive logic programming: The SCIFF framework	SCIFF is a framework designed to specify and verify interactions in open agent societies. It uses a language with a semantics based on abductive logic programming and a new proof procedure called SCIFF for reasoning with expectations in dynamic environments. This article presents the declarative and operational semantics of the SCIFF language, as well as the termination, soundness, and completeness results of the SCIFF proof procedure. It also showcases how SCIFF can be applied in the multiagent domain. 
41757	4175715	1600 faults in 100 projects: automatically finding faults while achieving high coverage with EvoSuite	Automated unit test generation techniques typically focus on either identifying violations of automated oracles or producing comprehensive test suites. However, the use of search-based testing (SBST) in conjunction with automated oracles has not been extensively explored, as it becomes less efficient with a large number of testing targets. This paper presents a search-based approach, implemented in the EvoSuite tool, that successfully achieves both objectives at once. An empirical study using EvoSuite on 100 open source projects revealed that SBST is able to efficiently detect faults while also producing comprehensive test sets for any chosen coverage criterion. The study also showed that EvoSuite was able to detect twice as many failures related to undeclared exceptions compared to traditional random testing methods, indicating its potential for uncovering real faults in software projects. This highlights the need for further research in improving automated oracles and for incorporating tools like EvoSuite into software development processes to ensure clean program interfaces.
41757	4175765	A Survey on Unit Testing Practices and Problems	Unit testing is a widely used practice in software development where test cases are written alongside regular code. Popular automation frameworks like JUnit for Java have made it easier to execute unit tests frequently and automatically. However, software engineering researchers believe that there is still room for improvement and are exploring advanced techniques such as automated unit test generation. To better understand the needs of developers, a survey was conducted among 225 software developers from 29 countries, covering different programming languages. The results of the survey confirm the importance of unit testing in software development and highlight the potential for research in automating unit testing. The survey also sheds light on areas that require further research, such as maintaining unit tests, and the effectiveness of online marketing research platforms in conducting software engineering surveys.
41758	4175821	Tree transducers with external functions	This paper discusses the computational abilities of two types of tree transducers: macro tree transducers and attributed tree transducers. These transducers allow for the invocation of external functions during rewriting processes, making them more powerful than traditional transducers. The main finding of the paper is that macro tree transducers with external function calls can be characterized in terms of attributed tree transducers with external function calls. Additionally, the paper defines two classes of tree functions that can be generated using these transducers, and proves that these classes are equal to the class of primitive recursive tree functions. This demonstrates the strength of these transducers in terms of computational power.
41758	417581	The translation power of top-down tree-to-graph transducers	The article introduces a new type of syntax-directed translation device called top-down tree-to-graph transducer. This device is similar to the usual top-down tree transducer, but its rules use hypergraphs instead of trees. The focus is on tree-generating top-down tree-to-graph transducers, which can translate trees into different objects. The result of the translation is a hypergraph representing a tree, which can be interpreted in various algebraic structures. These transducers have more transformational power than traditional top-down tree transducers, as the translation of a subtree can depend on its context. The article also proves that tree-generating top-down tree-to-graph transducers are equivalent to macro tree transducers and are closed under regular look-ahead. 
41759	4175926	Ensembles of nested dichotomies for multi-class problems	Nested dichotomies are a statistical method used in logistic regression to solve complex classification problems with multiple categories. They are represented as binary trees and involve breaking down the problem into simpler dichotomies. This approach is more accurate than directly applying other classification techniques like C4.5 or logistic regression to multi-class problems. In the standard approach, the choice of the tree is based on domain knowledge, but an alternative is to treat all trees as equally likely and create an ensemble classifier. This approach produces even more accurate results and also generates class probability estimates. It is a useful method for applying binary classifiers to multi-class problems.
41759	4175956	Building Ensembles of Adaptive Nested Dichotomies with Random-Pair Selection.	A system of nested dichotomies is a way to break down a multi-class problem into smaller binary problems by dividing the classes into two subsets and training a binary classifier for each split. While ensembles of nested dichotomies with random structure have been successful, using a more advanced method for selecting class subsets can improve accuracy. The method of random-pair selection is investigated and compared to other methods in terms of effectiveness. Results show that this method outperforms others in some cases and is equal in others. The related software can be found at a specific URL.
41760	4176037	Ramsey number of paths and connected matchings in Ore-type host graphs.	The Schelp conjecture states that in any 2-coloring of a graph with 3n-1 vertices and minimum degree at least (3n-1)/4, there is a monochromatic path of length 2n. This conjecture has been extended to graphs with an Ore-type condition, where the sum of the degrees of any two non-adjacent vertices is at least half of the total number of vertices. A new proof has been proposed, showing that a connected matching of size n can be found in any 2-coloring of such graphs. This result has been used to prove an asymptotic version of the conjecture for paths.
41760	4176031	Monochromatic bounded degree subgraph partitions	In this article, the authors examine a sequence of graphs, denoted as F, where each graph F n has a maximum degree of Δ and is composed of n vertices. They prove that there exists a constant C, independent of Δ, such that any 2-edge-colored complete graph can be divided into at most 2 C Δ log Δ vertex disjoint monochromatic copies of graphs from F. If each graph in F is bipartite, then the bound can be improved to 2 C Δ, which is the best possible result up to the constant C. This has implications for the partitioning of complete graphs into monochromatic copies of smaller graphs.
41761	4176156	A Lagrangian Approach To Dynamic Interfaces Through Kinetic Triangulation Of The Ambient Space	This paper presents a new Lagrangian approach for modeling dynamic interfaces between different materials in two-dimensional simulations. The approach maintains a two-dimensional triangulation that includes the one-dimensional description of the interfaces, allowing for topology changes to be detected through face inversions. Each face is labeled with the type of material it contains, and the triangulation and labels are updated consistently during deformation using kinetic data structures. This approach offers an alternative to other popular methods, such as the level set and particle level set methods, and has been shown to be reliable in difficult situations. Numerical experiments demonstrate the efficiency of the approach, and potential for extension to three dimensions is discussed.
41761	4176170	Anisotropic Delaunay Mesh Generation	Anisotropic meshes are triangulations of a given domain that have elements elongated in specific directions. These triangulations are useful for interpolating functions and solving PDEs. A new approach to generating anisotropic meshes is proposed, using the concept of anisotropic Delaunay meshes. These meshes have vertices with simplices that are Delaunay for the associated metric field. The refinement algorithm takes a domain and metric field as input and produces an anisotropic mesh with elements sized and shaped according to the metric field. This method can be used in any dimension and is a simple way to generate anisotropic meshes. 
41762	4176267	Non-linear book manifolds: learning from associations the dynamic geometry of digital libraries	The traditional approach to designing virtual libraries is to replicate the physical space they are based on. However, a new paper suggests automatically capturing the actual space of the books and creating a non-linear book "manifold." This raises questions about whether the focus should be on the books themselves or the users. Experiments on a real digital library show that the user-focused approach is a strong competitor to the book-focused one. Additionally, the geometric layers of the manifold provide significant benefits for retrieval and visualization. For example, the topological layer allows for more effective "Manifold association rules," which outperform conventional rules. This approach improves various aspects of association rule mining, including computational efficiency and accuracy.
41762	41762107	k-variates++: more pluses in the k-means++.	The paper introduces a generalization of the commonly used k-means++ seeding method called k-variates++, which allows for the sampling of general densities instead of just discrete points. This results in a bias+variance approximation bound that is more accurate and less dependent on noise. The k-variates++ method is also shown to be applicable to various clustering frameworks and can be used for differential privacy. The paper demonstrates the effectiveness of k-variates++ through experiments and comparisons with other state-of-the-art methods. This new approach is more efficient and has better approximation properties, making it a valuable tool for clustering algorithms.
41763	4176355	Boosted Density Estimation Remastered.	There has been an increase in the use of iterative approaches for boosted density estimation and sampling. However, these methods often rely on heavy assumptions on the iterates, which can be unrealistic or difficult to verify. This contrasts with the original boosting theory, where weaker assumptions would suffice. In this paper, it is shown that a weak learner, in the original boosting theory sense, is all that is needed to achieve boosting for density estimation. The convergence rates presented in the paper are better and rely on weaker assumptions compared to existing methods. Additionally, the model fit is shown to belong to exponential families, and a new variational characterization of $f$-divergences is obtained. Experimental results demonstrate improved performance compared to AdaGAN, with smaller architectures. 
41763	4176320	Learning Games and Rademacher Observations Losses	The popular logistic loss used in supervised learning has been found to be equivalent to optimizing the exponential loss over sufficient statistics about the class, known as Rademacher observations (rados). This equivalence can be extended to other example/rado losses with necessary and sufficient conditions, including exponential, mean-variance, linear hinge, ReLU, and unhinged losses. This also reveals a connection to regularized learning, where regularizing the loss over examples is equivalent to regularizing the rados. This leads to simple and powerful rado-based learning algorithms for sparsity-controlling regularization, exemplified by a boosting algorithm for the regularized exponential rado-loss. Experiments show that this regularization consistently improves performance and can even outperform state-of-the-art example-based learning on small sets of rados. Additionally, this work connects regularization to differential privacy and shows its effectiveness in protecting against privacy breaches. 
41764	4176423	Index coding via linear programming	Index Coding is a topic that has gained attention recently due to its applications in video-on-demand and wireless communication. It encodes information as an undirected graph and the key parameter is the broadcast rate, which is the average cost of communication per bit. Previous bounds on this rate were derived indirectly from other Index Coding capacities, but these did not provide much insight into the behavior of the rate. This study introduces a new hierarchy of linear programs that directly analyze the broadcast rate and improve upon previous bounds. This also leads to the discovery of new graphs with significant differences in their broadcast rates and the first nontrivial approximation for the rate. The proofs utilize tools from extremal graph theory.
41764	4176478	Lexicographic Products and the Power of Non-linear Network Coding	A new technique has been developed for creating large gaps between parameters in network coding and index coding problems. This involves using linear programs to establish separations between combinatorial and coding-theoretic parameters, and then using hyper graph lexicographic products to amplify these separations. By combining the dual solutions of these products, it is possible to construct hard instances with polynomially large gaps. This method has been applied to a wide range of linear programs, leading to the discovery of previously unknown polynomial separations. Notably, a polynomial gap has been found between linear and non-linear network coding rates, and a connection between matroids and index coding has been used to establish a similar separation between linear and non-linear index coding rates. Finally, an index coding problem has been constructed with a polynomial gap between the broadcast rate and the trivial lower bound, where no gap was previously known. 
41765	4176524	Secure Hash-and-Sign Signatures without the Random Oracle.	A new signature scheme is proposed that is secure against chosen message attacks, assuming a variant of the RSA conjecture. Unlike other schemes, it does not rely on signature trees and instead uses the "hash-and-sign" paradigm. The cryptographic hash function used is well defined and reasonable, rather than being modeled as a random oracle. The security proof is constructed in three steps: first, a random oracle model is used, then it is replaced with a hash function that meets strong computational assumptions, and finally, it is shown that these assumptions are reasonable based on standard intractability assumptions.
41765	4176557	UMAC: Fast and Secure Message Authentication	UMAC is a message authentication algorithm that is significantly faster than current methods and has been proven to be secure. It uses a new universal hash-function family, NH, and can effectively utilize SIMD parallelism. UMAC does not introduce any new heuristic primitives, but instead relies on standard cryptographic primitives chosen by the user. The security of UMAC is rigorously proven, making it difficult to forge authenticated messages. Unlike traditional MACs, UMAC is parallelizable and can take advantage of increasing parallelism in machines. It is seen as a practical solution for message authentication in the future.
41766	4176630	Imitation learning with hierarchical actions	In this paper, the authors discuss the power of imitation in learning new skills through observing a mentor. They propose a new model for goal-based imitation that utilizes action hierarchies and relies on sample trajectories of the mentor's actions, rather than requiring explicit mentor actions. The model is tested on a large-scale grid world task and shows rapid learning by combining hierarchical actions to achieve subgoals and reach a desired goal state. This approach of hierarchical imitation can significantly improve learning, especially in large state spaces, compared to learning without a mentor or without an action hierarchy.
41766	4176614	Goal-Based Imitation as Probabilistic Inference over Graphical Models	This paper discusses how humans are skilled at learning by imitating others, and how this ability progresses in children from simple body movements to goal-based imitation. The problem of goal-based imitation can be solved by inferring goals and choosing actions using a probabilistic graphical model. The paper outlines algorithms for planning actions to achieve a goal state using probabilistic inference and how this can aid in learning goal-dependent policies through feedback from the environment. The graphical model is then applied to goal-based imitation in a maze navigation task, showing how an agent can infer and imitate the goals of an observed teacher even with uncertainty and incomplete demonstrations. This research will be featured in the 2006 Advances in NIPS conference.
41767	4176729	Identity based threshold ring signature	Threshold ring signature schemes allow a group of t entities to automatically recruit n – t entities to create a publicly verifiable signature on behalf of the entire group while maintaining the anonymity of the actual signers. This is useful for ad-hoc groups like mobile ad-hoc networks. The paper introduces an identity based threshold ring signature scheme that is secure and compatible with trusted authorities. It is the first of its kind and is also the most efficient ID-based and threshold ring signature scheme in terms of required pairing operations.
41767	4176722	Two improved partially blind signature schemes from bilinear pairings	A blind signature scheme is a way for a signer to provide a digital signature without being able to see the message or the resulting signature. A partially blind signature allows some parts of the message to be revealed to the signer, while a threshold blind signature requires a predetermined number of signers to produce a signature. This paper introduces a threshold partially blind signature scheme and an ID-based partially blind signature scheme, both using bilinear pairings and proven to be secure in the random oracle model. This is the first discussion of these schemes by the authors.
41768	4176854	Scalable Web Content Attestation	The web is a major platform for sharing information, but recipients have no way of verifying the integrity of the content they receive. This paper introduces the Spork system, which uses the Trusted Platform Module (TPM) to tie the web server's integrity state to the content delivered to browsers. This allows clients to verify that the content is coming from a functioning server. The paper discusses the design and implementation of Spork and its Firefox validation extension, as well as the challenges of scaling the delivery of mixed static and dynamic content using TPM hardware. Empirical analysis shows that Spork can handle a large number of clients and deliver integrity-measured content with manageable overhead. 
41768	4176899	Establishing and Sustaining System Integrity via Root of Trust Installation	Integrity measurements are used in distributed systems to determine the trustworthiness of potentially compromised remote hosts. However, current techniques only verify the identity of software and do not provide ongoing status updates, leaving systems vulnerable if not carefully managed. To address this issue, a Root of Trust Installation (ROTI) is proposed as a foundation for high integrity systems. This trusted system installer verifies the integrity of trusted computing base software and data, allowing for comprehensive integrity verification. The ROTI solves the problem of determining a trusted system state in a heterogeneous environment and enables remote parties to verify integrity guarantees. The paper discusses the implementation of a custom ROTI installer in a distributed security architecture called Shamon and shows that strong integrity guarantees can be efficiently achieved with limited administrative overhead.
41769	4176967	Chinese Microblog Sentiment Classification Based on Deep Belief Nets with Extended Multi-Modality Features	This paper introduces a DBN model and a multi-modality feature extraction method for Chinese micro blogging sentiment classification. The model incorporates traditional features for document classification as well as comments from micro blog posts. The integration of these features is represented as an input vector for the DBN. The DBN model consists of stacked layers of RBMs, which can learn hidden structures from probability distribution samples of the data. A Class RBM layer is added for final sentiment classification. The results show that this deep learning method outperforms traditional models such as SVM or NB, highlighting the effectiveness of DBN for short-text sentiment classification with the proposed feature extension method.
41769	41769131	Multi-Strategy Based Sina Microblog Data Acquisition For Opinion Mining	Sina microblog is a popular social media platform that contains emotional and important opinions from its participants. However, accessing this data for analysis and mining has become increasingly difficult due to commercial and security concerns. In response, the paper proposes a platform that combines multiple strategies and resources to collect data from Sina microblog. By using a combination of API and web crawler, sentiment analysis and opinion mining can be efficiently performed. This allows for the development of applications such as hot word searching, opinion mining, and sentiment analysis. The proposed solution is effective and provides a straightforward approach to accessing and analyzing microblog data.
41770	4177080	Tractable Bayesian Learning of Tree Belief Networks	The paper introduces decomposable priors, a type of prior for tree belief nets that allows for tractable Bayesian learning with complete observations. This means that the posterior can be analytically determined in polynomial time. The tractability is achieved through two main results: the integration of factored distributions over spanning trees in a graph, and the use of compactly parametrized product of Dirichlet distributions as tree parameter priors. These results not only enable exact Bayesian learning, but also allow for a new class of tractable latent variable models where the likelihood of a data point is calculated through an average of different tree structures. 
41770	4177045	Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network	Computational chemistry faces the challenge of predicting the outcomes of organic reactions due to the large number of atoms involved. Current methods use reaction templates, but they have limitations. In this paper, a template-free approach is proposed, which first identifies the reaction center and then enumerates potential products. The candidates are scored using a Weisfeiler-Lehman Difference Network, which captures high-order interactions between changes in the molecule. This approach outperforms template-based methods and has significantly faster running time. The accuracy of the model is also comparable to that of domain experts. This method shows promise for efficient and accurate prediction of organic reaction outcomes.
41771	4177136	Effective Category and Measure in Abstract Complexity Theory (Extended Abstract)	The Operator Speed-up, Gap, and Compression Theorems have been strengthened by utilizing an efficient form of the Baire Category Theorem. This results in more powerful versions of these theorems. Additionally, it has been proven that all complexity classes of recursive predicates have a negligible amount of effective measure in the space of recursive predicates. Conversely, the class of predicates with a complexity above a given recursive threshold has a significant amount of recursive measure in the class of recursive predicates.
41771	4177112	Relativized topological size of sets of partial recursive functions	In [1], a recursive topology was introduced for unary partial recursive functions and recursive versions of Baire topological concepts were defined. These tools were used to determine the size of certain classes of partial recursive functions. It was shown that measured sets and complexity classes are recursively meagre, while sets of all p.r. functions and recursive functions are sets of recursively second Baire category. This paper extends these results by using the Baire notions relative to the topological spaces induced by sets of p.r. functions. This approach strengthens previous results and also yields new ones, revealing significant distinctions between "local" and "global" topological size for many sets of p.r. functions.
41772	4177254	Imprecise probabilistic query answering using measures of ignorance and degree of satisfaction	Conditional probabilistic logic programming involves answering a query using either a probability interval or a precise probability obtained through the maximum entropy principle. While the former can be uninformative and the reliability of the latter can be questionable with imprecise prior knowledge, this paper proposes methods to measure if a probability interval or a single probability is sufficient for answering a query. These methods include measuring the ignorance of a probabilistic logic program with respect to a query, which reflects the reliability of a precise probability for the query, and measuring the probability that the exact probability falls within a given interval. These measures are shown to satisfy certain properties and are demonstrated through a case study.
41772	41772102	Optimization of dialectical outcomes in dialogical argumentation.	Informal arguments can often be imprecise, leaving the audience unsure of the intended structure. To address this uncertainty, probabilistic argument graphs can be used to model the different possible subgraphs of an argument. By assigning probabilities to each subgraph, we can determine the likelihood of certain arguments being included or excluded from an extension. This approach can also be used to create an argumentation lottery, allowing the audience to assess the expected utility of a debate and the presenter to choose the most effective arguments. A logic of dialectical outcomes is used to represent and reason with these outcomes, and an empirical evaluation is conducted to assess the usefulness of argumentation lotteries in dialogical argumentation. 
41773	417736	The Hardness and Approximation Algorithms for L-Diversity	There are two main categories of solutions for preserving privacy when publishing data: theoretical and heuristic. The theoretical category guarantees low information loss, but is limited to the k-anonymity principle which has been shown to be vulnerable to privacy attacks. On the other hand, heuristic solutions may incur high information loss in some cases, but have been shown to perform well on real data. While many heuristic algorithms have been developed for advanced privacy principles, this study focuses on the popular l-diversity principle. The study shows that finding optimal l-diverse generalizations is a difficult problem, but proposes an approximation algorithm with a non-trivial bound on information loss. Experiments with real data support the effectiveness and efficiency of the proposed solution.
41773	4177364	Range search on multidimensional uncertain data	This article discusses the use of probability density functions in uncertain databases, where each object is associated with a likelihood of appearing in a specific location in a multidimensional workspace. The focus is on two types of range retrieval – nonfuzzy and fuzzy – which return objects within a specified search region with a minimum probability. The concept of "probabilistically constrained rectangle" is introduced to efficiently filter out non-qualifying data. The U-tree index structure is proposed to reduce query overhead. Theoretical analysis confirms the effectiveness of these techniques, which are supported by experiments.
41774	4177440	Process attributes in bio-ontologies.	Biomedical processes are important for understanding the functioning of organisms and are often included in biomedical terminologies and ontologies. However, accurately representing the attributes of these processes, such as rates and regularities, has been a challenge. To address this issue, a design pattern has been proposed that is compatible with upper ontology frameworks. It is based on the idea that process attributes can be described by how repeated parts of a process contribute to the overall process. This approach has been applied to modelling heart beating processes and has shown that logical definitions of process attributes are feasible. However, the expressivity of description logic languages limits the ability to fully define these attributes, indicating the need for primitive entities in ontology frameworks. Overall, this highlights the importance of formal upper-ontology frameworks for ensuring consistency and interoperability in representing biomedical processes and their attributes.
41774	4177480	Proposed actions are no actions: re-modeling an ontology design pattern with a realist top-level ontology.	Ontology Design Patterns (ODPs) are tools used in ontology engineering to solve recurring design problems. They aim to improve the process by increasing flexibility, re-usability, and predictability. In this paper, the authors analyze ODP repositories and their relationship with upper-level ontologies. They specifically look at the Action ODP from the NeOn repository and compare it to the BioTop upper ontology. The authors propose a re-design of the Action ODP, aligning it with the principles of realist ontologies. This re-design clarifies the ontological commitment of ODP classes and distinguishes between classes of real entities and information artifacts. The authors demonstrate that an expressive upper-level ontology, such as BioTop, can effectively represent complex ODPs. They also argue that embedding ODPs into a top-level ontology can provide a predetermined structure and improve the ontology engineering process. Finally, the authors contrast the advantages of embedded upper-level structures with the potential problems caused by "self-sufficient" ODPs.
41775	4177525	A comparison of multiobjective evolutionary algorithms with informed initialization and kuhn-munkres algorithm for the sailor assignment problem	This paper compares the performance of two multiobjective evolutionary algorithms, NSGA-II and SPEA2, with informed initialization on large instances of the Sailor Assignment Problem for the United States Navy. The informed initialization includes special solutions from an extended version of the Kuhn-Munkres algorithm, which is typically used for solving single valued linear assignment problems. The extension allows it to be used for single objective instances of the sailor assignment problem. The results of the evolutionary algorithms are compared to the Kuhn-Munkres extension, providing a benchmark for their performance.
41775	4177561	Genetic algorithms for the sailor assignment problem	This paper discusses the use of genetic algorithms in solving the United States Navy's Sailor Assignment Problem (SAP). The SAP involves assigning n sailors to m jobs in a way that maximizes desirability and minimizes cost. The paper compares the effectiveness of genetic algorithms to the Gale-Shapley algorithm and presents evidence that the former can produce high-quality solutions at a lower cost. It also examines the use of genetic algorithms in generating multiple solutions for a human decision maker, known as a detailer, and demonstrates that it can provide a diverse set of good solutions. Overall, the paper highlights the potential of genetic algorithms in solving complex real-world problems.
41776	41776107	Finding Associations in Composite Data Sets: The CFARM Algorithm	This paper introduces a composite fuzzy association rule mining mechanism (CFARM) for identifying patterns in datasets with composite attributes, which are attributes that can have multiple values following a common schema. The focus is on generating fuzzy association rules based on the properties of these composite attributes, specifically in the context of analyzing nutrients in grocery data sets. The paper begins with a background and related work review, followed by a formal definition of CFARM concepts. The algorithm is then described and evaluated using both real and synthetic data sets. Overall, CFARM proves to be effective in identifying patterns in datasets with composite attributes.
41776	417766	On Extraction Of Nutritional Patterns (Nps) Using Fuzzy Association Rule Mining	This paper proposes a framework for using market basket data to generate Nutritional Patterns (NPs) and a method for analyzing them using Fuzzy Association Rule Mining. The data is first filtered and converted to Recommended Dietary Allowance (RDA) values, and then to a fuzzy database. Analysis can be done on either normal association rules or the fuzzy database. A prototype support tool is used to extract NPs and determine the nutritional content in each item of an association rule. The paper presents tests and measures to show the effectiveness of the approach, and concludes with experimental results and a discussion on evaluating the framework. 
41777	4177729	On the Portability of Prolog Applications	The lack of portability in Prolog programs has been a major concern for programmers. Despite the ISO standard set in 1995, it has not fully addressed the issue for larger applications. In response, YAP and SWI-Prolog have developed a compatibility framework since 2007 to allow code to run on various Prolog systems without migration. The implementation and effectiveness of this framework have been evaluated through its use in several libraries and a significant application. This approach aims to make Prolog programming more convenient and efficient by reducing the need for adaptation and migration of programs.
41777	4177757	The YAP Prolog System	YAP is a Prolog system created in the 1980s and has been continuously developed since then. This paper highlights three significant contributions of YAP to the Logic Programming community. The first is its efficient Prolog engine, achieved through various techniques. Secondly, YAP offers a unique dynamic indexing mechanism, called the just-in-time indexer (JITI), which improves upon the indexing algorithms used in most Logic Programming systems. Lastly, YAP has successfully integrated both or-parallelism and tabling in one system, making it a valuable contribution to the field. These advancements make YAP a notable and influential Prolog system in the Logic Programming community.
41778	4177823	3D Object Modeling and Recognition from Photographs and Image Sequences	The chapter introduces a method for representing rigid 3D objects using local affine-invariant descriptors and spatial relationships between their surface patches. This approach combines geometric constraints from different views with a normalized representation of appearance to aid in object modeling and recognition. It is applied in two domains: photographs, where models are created and recognized from a small number of images, and video, where dynamic scenes with multiple moving objects are segmented and matched using 3D models for efficient indexing and retrieval. This approach offers a new method for handling complex and cluttered scenes in both photography and video.
41778	4177883	3D object modeling and recognition using affine-invariant patches and multi-view spatial constraints	The paper introduces a new way of representing three-dimensional objects by using image patches and their spatial relationships. This representation is combined with multi-view constraints and a normalized appearance to enable the creation of accurate 3D models from multiple images and their recognition from a single photograph. This approach does not require a separate segmentation step and can be used in complex scenes. The paper also presents initial results of modeling and recognition using this method.
41779	417790	Labelled natural deduction for a bundled branching temporal logic	We present a complete and reliable method for reasoning in a bundled branching temporal logic called BCTL*, which is a modified version of CTL*. Unlike CTL*, BCTL* does not have the limit-closure property in its validity semantics. We offer both classical and intuitionistic versions of our labelled natural deduction system for the until-free version of BCTL* and analyze the intuitionistic system to show that derivations can be reduced to a normal form. This allows us to prove the consistency of the system using purely syntactical methods for both the intuitionistic and classical versions. 
41779	417794	A History of Until	The temporal operator "until" is known for its difficulty in being both existential and universal simultaneously. This poses a challenge in creating deduction rules for this operator. However, in this paper, a new operator "@?" is introduced to capture the "history" of until, or the universal quantification over time instants. This allows for the development of deduction systems for temporal logics with the until operator. A labeled natural deduction system for a linear-time logic with the new history operator is presented, and it is shown to be sound and complete when translated to linear temporal logic with until. 
41780	4178035	Matrix partitions of perfect graphs	The M-partition problem involves determining if a given graph G can be divided into m parts based on the rows and columns of a symmetric m by m matrix M, with specific adjacency rules based on the values in M. This problem is a generalization of graph coloring and homomorphisms and is often studied in perfect graphs. The paper focuses on M-partitions in perfect graphs and identifies a class of "normal" matrices for which the problem can be solved using a finite set of forbidden induced subgraphs. However, there are also matrices that do not fall into this class and can result in more complex M-partition problems. Some of these problems can be solved in polynomial time, while others are NP-complete and difficult to classify. 
41780	417807	List matrix partitions of chordal graphs	The article discusses the M-partition problem, which involves partitioning a chordal graph into independent sets, cliques, or arbitrary sets based on certain conditions specified by a symmetric matrix. The authors prove that many M-partition problems are solvable in polynomial time for chordal graphs, even with the presence of lists. However, there are also M-partition problems that remain NP-complete for chordal graphs. The article also explores forbidden subgraph characterizations for graphs that can be partitioned according to certain conditions. The authors highlight the improvements that can be made for the class of chordal graphs, rather than just perfect graphs. Overall, the article presents new findings and expands upon previous research on partitioning chordal graphs.
41781	417819	Combined Discriminative Training For Multi-Stream Hmm-Based Audio-Visual Speech Recognition	This paper explores the use of discriminative training for a multi-stream hidden Markov model (HMM) based audio-visual speech recognizer (AVSR). The authors propose joint training of the two streams, in contrast to previous approaches that trained each stream separately. Experiments on a 20-speaker one-hour test set show a 22% relative gain in AVSR performance compared to models with separate training, and a 50% relative gain over the maximum-likelihood models. On a noisy test set, there is a 21% relative gain over models with separate training, representing a 30% improvement over the maximum-likelihood baseline. These results demonstrate the effectiveness of discriminative training for AVSR.
41781	417814	Front-end feature transforms with context filtering for speaker adaptation	In this study, the effectiveness of feature-space maximum likelihood linear regression (FMLLR) as a speaker adaptation technique is explored. The full-rank square matrix of FMLLR is extended to a non-square matrix that takes into account neighboring feature vectors in order to estimate the adapted central feature vector. By optimizing the objective function, the correlation between feature context is used to filter and transform features. Comparisons are made with conventional FMLLR, which only considers the current feature vector. Experiments conducted on automobile data with varying speed conditions show a 23% improvement in word error rate with context filtering, compared to conventional FMLLR on noisy 60mph data with adapted ML model. Additionally, improvements of 7% and 9% are seen over discriminatively trained FMMI and BMMI models.
41782	4178247	A Fuzzy Logic Approach to Wrapping PDF Documents	This paper addresses the challenge of wrapping PDF documents, which is necessary for effective text data management. The proposed method utilizes fuzzy logic to handle the uncertainty inherent in the structure and presentation of PDFs. A bottom-up hierarchical approach is used, where a PDF wrapper is defined by specifying group type definitions that dictate the desired structure of groups of tokens. These definitions include fuzzy conditions based on spatial and content predicates. An algorithm for wrapper evaluation has been developed and implemented in a visual wrapper generation system. Experimental results have shown the system's accuracy and applicability to PDF documents from different domains.
41782	4178252	The Lixto data extraction project: back and forth between theory and practice	The Lixto project is a research project and commercial enterprise that focuses on developing software for extracting data from websites and defining web services. The project uses a logic-based framework for wrapping and has achieved theoretical results on the expressive power and complexity of its languages. The Lixto system allows for easy visual wrapper specification and practical aspects of wrapping. The project has also contributed to the study of complexity in query languages for trees. The Lixto Transformation Server is used for streaming integration of data extracted from web pages, which is important for complex web services. The project has industrial applications and potential for future research.
41783	4178334	Querying Graph Databases	Graph data is becoming increasingly popular as a way to represent various types of databases. Many recursive queries in relational databases can be expressed as graph traversals. This paper introduces a language for searching through graph-like databases using extended regular expressions. The language allows for the creation of a partial order on the paths used for searching, as well as the ability to nondeterministically cut off low priority tuples. An algebra for partially ordered relations and an algorithm for computing path queries are also presented. This language has potential applications in hypertext databases, such as the World Wide Web.
41783	4178348	Bounded programs: a new decidable class of logic programs with function symbols	The use of function symbols in logic programming is important but can make common inference tasks impossible to solve. To address this issue, recent research has focused on identifying classes of logic programs that limit the use of function symbols while still ensuring solvability of common inference tasks. These classes, known as termination criteria, provide conditions for programs to have a finite number of stable models with finite size. This paper introduces a new class of programs, called bounded programs, which guarantees this property and is broader than existing termination criteria. The paper also discusses the correctness, expressiveness, and complexity of bounded programs.
41784	4178438	Web Communities: Models and Algorithms	Recent research has focused on improving web search engines by developing new techniques to enhance recall and precision. However, few studies have addressed the problem of identifying web communities. Most approaches use spectral or hierarchical algorithms, which overlook the fact that web communities are social networks with unique statistical properties. This paper analyzes web communities by studying the evolution of hubs and authoritative pages. By understanding how authors behave in relation to popular pages, the paper proposes a method for identifying relevant properties for specific topics. Experiments have confirmed the effectiveness of this model and technique. 
41784	4178410	STED: a system for topic enumeration and distillation	The popularity of search services on hyperlinked data has increased due to the vast amount of available information and the difficulty in finding relevant documents. Traditional term-based search engines are not efficient for this purpose, as the ranking of results depends on the precision of the query. Current research focuses on topic distillation, where documents related to the query topic are identified, regardless of the exact query string. Topic distillation algorithms first create a base set of relevant pages, then use an iterative process to find authoritative pages. This paper presents STED, a system for topic distillation and enumeration of Web documents. The system analyzes the structure of the base set and uses statistical methods to identify authoritative pages. Experiments have shown the effectiveness and efficiency of the system.
41785	4178549	Analyzing and Predicting Emoji Usages in Social Media.	Emojis are a popular form of graphical expression used in social media to convey emotions. They go beyond text and improve communication effectiveness. Machine learning advancements now allow for automatic composition of text messages with emojis. However, understanding and predicting emoji usage is a complex task. To address this, a dataset of emojis and tweets was created and their usage was systematically studied. This led to the development of a multitask multimodality gated recurrent unit (mmGRU) model, which leverages text, image, and user information to predict emoji categories and positions. The model showed significant improvements in accuracy (+9.0% for category and +4.6% for position). Case studies were also conducted to further understand the usage of emojis in social media.
41785	4178530	Grading the Severity of Mispronunciations in CAPT Based on Statistical Analysis and Computational Speech Perception.	Computer-aided pronunciation training (CAPT) technologies use automatic speech recognition to identify mispronunciations in second language (L2) learners' speech. This paper proposes a principle-based method for grading the severity of these mispronunciations, based on auditory perception. A computational method is developed to measure the perceptual distance (PD) between two spoken phonemes, which is then used to calculate the auditory confusion between a learner's native language (L1) and the target language (L2). The results show that PD is a reliable indicator of mispronunciations in L2 learning and can be used to prioritize corrective feedback for learners. Overall, this approach can enhance the effectiveness of CAPT systems for Chinese learners of English. 
41786	4178660	Emotional talking agent: System and evaluation	This paper presents a system for creating emotional audio-visual speech for a 3-D talking agent using the PAD emotional model. A GMM-based method is used to predict changes in acoustic features based on the PAD values, and a parametric framework is developed for emotional facial expression synthesis. Perceptual evaluations were conducted to understand the impact of vocal and facial expressions on emotion and to assess the effectiveness of the emotional talking agent in human-computer speech communications. The study found that the combination of audio and visual modalities has a strong reinforcing effect on emotion perception and that users prefer a multimodal interface for better comprehension of emotion. The results also demonstrate the success of the PAD-based emotional talking agent synthesis system.
41786	4178631	Facial Expression Synthesis Using PAD Emotional Parameters for a Chinese Expressive Avatar	Facial expressions are a crucial aspect of face-to-face communication as they convey nonverbal information and emotions that go beyond speech. This paper proposes a method for synthesizing facial expressions using a Chinese talking avatar. A layered parametric framework is designed using PAD emotional parameters, which describe human emotional states in three dimensions. A new parameter, Partial Expression Parameter (PEP), is introduced to represent facial movements in specific regions. This serves as a middle ground between low-level Facial Animation Parameters (FAPs) and high-level PAD emotional parameters. A pseudo facial expression database is created by cloning real human expressions onto the avatar and annotating them with corresponding PAD scores. An emotion-expression mapping model is then trained on this database to map PAD values to PEP configurations. Perceptual evaluation shows that this approach effectively captures human perception of synthetic expressions.
41787	4178740	Hermes: agent-based middleware for mobile computing	Hermes is a middleware system that allows for the design and execution of activity-based applications in distributed environments. It supports mobile computation and allows application domain experts to focus on designing activity workflow without needing to consider the distributed environment. A context-aware compiler is responsible for generating mobile agents from a workflow specification. Hermes has a component-based, agent-oriented system with a three-layer software architecture and can be customized for specific application domains by adding domain-specific component libraries. The middleware layer, compilers, libraries, services, and other tools make up a general programming environment that has been proven successful in both industrial control and bioinformatics applications. In these applications, mobile agents are used for tasks such as product tracing, self-healing, data collection, service discovery, and simulating biological systems. 
41787	4178722	An agent-based matchmaker	Service discovery is the process of finding and accessing available resources and services in large distributed systems such as the Web. In order to effectively utilize these services, a matchmaker is needed to filter and select the most suitable ones. The matchmaker is a software entity that monitors service availability and maintains a database of useful information for using services. In this paper, an architecture for an agent-based matchmaker is proposed, which uses a quality model to ensure the proper selection of services. A case study in the biomedical domain is presented, where a multi-agent system is developed with a Bio-certifier to support the service discovery process.
41788	4178872	Horizon-Independent Optimal Prediction with Log-Loss in Exponential Families	. The focus of this paper is on the study of online learning using regular parametric models and logarithmic loss.2. Hedayati and Bartlett (2012b) found that a Bayesian prediction strategy with Jeffreys prior and sequential normalized maximum likelihood (SNML) are optimal if the latter is exchangeable and the optimal strategy can be calculated without knowing the time horizon beforehand.3. The authors investigated which families have exchangeable SNML strategies and found that for one-dimensional exponential families, the exchangeability can only occur for the Gaussian, Gamma, and Tweedie exponential family of order 3/2.4. This paper provides a solution to an open problem and highlights the importance of exchangeability in online learning.5. Keywords: SNML Exchangeability, Exponential Family, Online Learning, Logarithmic Loss, Bayesian Strategy, Jeffreys Prior, Fisher Information.
41788	41788104	Minimax Fixed-Design Linear Regression	A linear regression game is played where the learner predicts a real-value based on known covariates and the adversary reveals a label, resulting in a squared error loss. The goal is to minimize regret in terms of linear predictions. It is shown that for various constraints on the adversary's labels, the optimal strategy is linear, similar to ordinary least squares. Predictions are based on all covariates, with a focus on future ones to minimize regret. Two constraint families are studied, with a strategy that is adaptable and does not require knowledge of the constraint set. The minimax regret is explicitly expressed for these games, with a worst case scenario showing a regret of O(d\log T) with no dependence on covariate scaling.
41789	4178965	Constructive cryptography --- a new paradigm for security definitions and proofs	Constructive cryptography is a new approach to defining and proving the security of cryptographic schemes. It involves constructing a resource with certain security properties from another, weaker resource. This is done through the use of a simulator, which allows for composability – meaning that a protocol created by combining multiple secure steps is also secure. This is different from traditional game-based and attack-based definitions of security. Constructive cryptography allows for a deeper understanding of security notions and the modular and composable design of protocols. It also separates the understanding of cryptography's goals from the technical definitions and proofs, making it useful for teaching and protocol design. 
41789	4178999	Tight security proofs for the bounded-storage model	The bounded-storage model is a way to ensure secure encryption and key-agreement by assuming that an adversary's storage capacity is limited. In this model, a cipher can be proven secure even if the adversary has unlimited computational power as long as their storage is bounded by a certain number of bits. The legitimate sender and receiver can generate a long one-time pad that the adversary has essentially no information about. Previous results in this model were not optimal, but a new paper proves the first non-restricted security result by utilizing the full potential of the model. This means that the secret key can be short, the one-time pad can be very long, and the adversary only needs to store a moderate amount of information to maintain security. The proof is also optimally strong, with the storage bound being essentially optimal.
41790	4179014	W3QS - A System for WWW Querying	W3QL is a high level language similar to SQL that allows users to access data and services on the World-Wide Web. It is declarative, meaning that a query is used to specify a graph to be matched with portions of the WWW. This includes nodes corresponding to web pages and edges representing hypertext links. Complex conditions can be applied to nodes and their relationships, and W3QL can also utilize existing search services like AltaVista. The language is also extensible, allowing users to use their own data analysis tools. W3QS is a system that manages W3QL queries and can be accessed through the WWW or a programming interface. Multiple interfaces are available, including graphic interfaces and templates for commonly used queries.
41790	4179024	Evaluating very large datalog queries on social networks	The article discusses the concept of users contributing both data and rules to a Web 2.0 application, such as a social network. These rules can automatically manage various tasks, such as adding connections, based on a user-defined query. The authors introduce the Query Network model, which is a graph-based model that uses Datalog rules to define connections. They also discuss the challenges of handling large, recursive Datalog programs and propose algorithms to effectively evaluate these query networks. The results of experiments with synthetic and real datasets demonstrate the usefulness of their methods. Extensions to the model are currently being developed and tested.
41791	4179111	Online control of enumeration strategies via bat algorithm and black hole optimization.	Constraint programming is a problem-solving approach that models problems as a series of variables and constraints. These variables have a range of possible values and the solving process involves assigning values to them to find potential solutions. The efficiency of this process depends on the selection of an enumeration strategy, which determines the order in which variables and values are chosen. This is a difficult task as the behavior of strategies is unpredictable and varies depending on the problem. To address this, recent research has focused on using a combination of strategies and evaluating them during the search process. This is known as online control of enumeration strategies, which can be seen as an optimization problem. In this paper, two new systems based on nature-inspired metaheuristics are proposed: bat algorithm and black hole optimization. These approaches show improved performance compared to previous work on online control, as demonstrated through experiments on various benchmarks.
41791	417918	Efficient Parallel Sorting for Migrating Birds Optimization When Solving Machine-Part Cell Formation Problems	The Machine-Part Cell Formation Problem (MPCFP) is a difficult optimization problem that involves grouping machines and parts into cells in order to minimize intercell movements and allow for independent operation. Previous research has used a variety of techniques, including linear programming and nature-inspired metaheuristics, to solve this problem. This paper presents a parallel version of the Migrating Birds Optimization metaheuristic, which is based on the V-Flight formation used by migrating birds to save energy. By incorporating parallel procedures, the proposed approach improves the performance of the sorting processes involved in the metaheuristic. Computational experiments on 1080 benchmarks show promising results, with the proposed method able to reach the global optimum in all instances and significantly reducing solving time compared to a nonparallel approach.
41792	417925	Hyper-heuristics: a survey of the state of the art.	Hyper-heuristics are a set of methods that aim to automate the development of heuristic techniques for solving difficult computational search problems. This research field focuses on creating more widely applicable search methodologies. The term "hyper-heuristic" was first used in 2000 to describe techniques for choosing heuristics in combinatorial optimization, but the idea of automating heuristic design dates back to the 1960s. Hyper-heuristics refer to methods for selecting or generating heuristics to solve computational search problems. There are two main categories of hyper-heuristics: heuristic selection and heuristic generation. Unlike traditional approaches, hyper-heuristics operate on a search space of heuristics rather than directly on the search space of solutions. This paper provides an overview of the origins, types, and current research trends in hyper-heuristics, as well as potential directions for future research.
41792	4179225	Monte Carlo hyper-heuristics for examination timetabling.	Automating neighbourhood selection using heuristics is a complex task, and hyper-heuristics aim to provide a general framework for solving different problem instances. The key goal is to extend the generality to solve problems from different domains. The heuristic design process is a major challenge, and recent studies have explored ways to introduce learning mechanisms for better performance. A study compared different hyper-heuristics for solving capacitated examination timetabling problems, including a combined approach with a learning heuristic selection and simulated annealing move acceptance method. Results showed that simulated annealing with reheating has potential, while the learning hyper-heuristic using simulated annealing performed poorly due to weaknesses in the rewarding mechanism and utility values. The best alternative for simulated annealing with reheating for examination timetabling is the previously proposed choice function.
41793	4179337	Toward the automatic control of robot assembly tasks via potential functions: the case of 2-D sphere assemblies	The authors present a method for controlling automated assembly tasks using artificial potential functions. They focus on generating actuator commands for a robot manipulator to move rigid-body parts from disassembled to assembled configurations. The approach is applied to 2D sphere assemblies and a primitive constructive theory is presented for controlling this class of tasks. Preliminary computer simulations show promising results for the proposed approach.
41793	4179312	Automatic assembly planning and control via potential functions	The article discusses a method for automating assembly planning and control using artificial potential functions. It focuses on a specific type of task, 2D sphere assemblies, and presents a theory for planning and controlling these tasks. Computer simulations show that the approach can achieve high levels of performance.
41794	41794133	Determining trust in media-rich websites using semantic similarity	The growth of multimedia content on the Web has made it an integral part of people's lives. However, it is important for users to be able to determine the trustworthiness of the information they access. This is especially crucial since users typically rely on search engines like Google or Yahoo! to provide them with information. While these search engines may prioritize popular content, it may not always be trustworthy. To address this issue, a new method has been proposed to determine the trustworthiness of websites based on the similarity of their multimedia content with established and trusted websites. This method allows for dynamic computation of the trust level of websites across different domains, reducing reliance on traditional user feedback. The experimental results demonstrate the effectiveness of this approach.
41794	4179470	Association-based dynamic computation of reputation in web services	Web services are selected and composed based on their reputation, which is typically determined by user feedback. However, this approach faces challenges such as low incentive for providing ratings and bias towards positive or negative ratings. To address these issues, a new method is proposed that calculates the reputation of a web service based on its association with other web services. This is done by considering the frequency with which they have been used together. This approach offers a dynamic and time-based approach to reputation computation. The experimental results show the effectiveness of this method. 
41795	4179524	Exploring the underlying structure of haptic-based handwritten signatures using visual data mining techniques.	This paper discusses the analysis of haptic-based handwritten signatures using visual data mining techniques. The aim is to create virtual reality spaces using nonlinear transformations and optimizing for minimal information loss. The study compares the effectiveness of genetic programming and classical optimization methods in constructing these visual spaces using large haptic datasets. Different distance functions and classifiers are also explored to determine their impact on the representation accuracy and discrimination power of the visual spaces. Results show that both methods are able to effectively represent the relationships between haptic data objects and their classes, with classical optimization outperforming genetic programming in terms of mapping error and discrimination power. 
41795	4179527	Revocable handwritten signatures with haptic information	Virtual environments are becoming increasingly popular due to advancements in hardware and software for 3D applications. Haptic devices, which allow for more immersive interaction with virtual environments, are also gaining popularity. As more users connect to these environments through the internet, the need for secure authentication of their identity arises. One emerging field of research is identifying users through their interaction with haptic devices. However, this method relies on the uniqueness of stored templates, which can pose a risk if compromised. Unlike passwords and pins, biometric templates are irrevocable. This paper explores ways to introduce revocability to biometric templates using haptic information for user authentication.
41796	41796117	Efficient Routing in Networks with Long Range Contacts	Long Range Contact graphs are defined by an underlying network topology and an extra link connecting each node to a distant neighbor, chosen randomly according to a probability distribution. These graphs are a good model for the small world phenomenon, and we investigate their use in greedy routing, a distributed routing protocol. We give bounds on the expected number of steps required for routing in a ring augmented with links chosen using the r-harmonic distributions, showing a 驴(log2 n)-bound for the 1-harmonic distribution. This suggests that the model of Kleinberg can be simplified by using a ring instead of a mesh. We also compare the performance of different distributions in terms of diameter and routing. Finally, we show how to define a probability distribution for any network and study the performance of greedy routing in the augmented network. 
41796	4179682	On Recognizing a String on an Anonymous Ring	The paper discusses the problem of recognizing if a binary string is the same as the input of a ring of processors. Previous algorithms did not consider local patterns in the string. The paper presents new upper and lower bounds for the complexity of string recognition, with near optimal bounds for periodic strings and an optimal algorithm for Kolmogorov random strings. This shows that most strings can be recognized with Θ(n log n) bits of communication. The paper also uses Kolmogorov complexity theory to prove the upper bound, instead of its traditional use for lower bounds.
41797	4179765	Demarcation of bacterial ecotypes from DNA sequence data: A comparative analysis of four algorithms	Microbiologists in a variety of fields, such as systematics, epidemiology, and biotechnology, would benefit from being able to identify closely related but ecologically distinct populations of bacteria. To aid in this task, several algorithms have been developed. This paper examines the effectiveness of four of these algorithms in correctly identifying ecotypes from sequence data, including information on the habitats where the bacteria were isolated. The algorithms were tested on synthetic sequences and data from Bacillus strains isolated from Death Valley. The results showed that one algorithm, Ecotype Simulation, performed significantly better than the others, but was also the least efficient.
41797	4179749	Wireless Autonomous Robot Evacuation from Equilateral Triangles and Squares	This article discusses the problem of evacuating a group of robots from an unknown exit point on the perimeter of an equilateral triangle or square with side length 1. The robots can move at a speed of 1 and communicate wirelessly with each other. The goal is to minimize the evacuation time, and it is a difficult problem even for simple shapes like equilateral triangles. Optimal evacuation trajectories are designed for two robots in equilateral triangles and for starting positions on the perimeter of squares. The article also explores the optimal number of robots needed for efficient evacuation and how starting positions can affect the evacuation time. A simple algorithm is shown to be asymptotically optimal for a large number of robots starting from the center of the shape.
41798	4179851	An evaluation of intelligent agent based innovation in the wholesale financial services industry	The success of wholesale financial services hinges on the development of flexible e-business models and strategies, as well as innovative systems for knowledge management and customer relationship management. Australian corporations in this sector face challenges and the paper proposes a research model to assess the impact of emerging intelligent agent technologies. This will help firms achieve international competitiveness by investigating and monitoring the use of these technologies and identifying successful approaches for adoption. The integration of e-business strategy, finance, agent architectures, and knowledge technologies offers a new solution to the challenges faced by the wholesale financial services industry. Agent architectures allow for dynamic and responsive systems that can adapt to the highly competitive global financial environment.
41798	4179865	Representation = Grounded Information	The grounding problem is a major challenge in Artificial Intelligence. The ability to create and manage representations is crucial for an intelligent system. However, the existing literature on this topic is unclear and has not made significant progress. This paper proposes that a representation is grounded information, providing much-needed clarity and a new perspective for analyzing the grounding problem. By understanding the relationship between information and representation, we can gain a deeper understanding of intelligence and use it to design and build autonomous intelligent systems that are capable, intentional, and purposeful.
41799	41799155	Discriminative Density Propagation for 3D Human Motion Estimation	The article describes a new algorithm for estimating 3D human motion in videos using a discriminative approach. This means it does not rely on a predictive observation model, but instead uses a large motion capture database and a 3D human model to synthesize training data. The algorithm learns to predict conditional state distributions for 3D body pose tracking, without the need for a generative 3D model. It also improves mixing and initialization of generative inference algorithms. The paper presents three main contributions: establishing density propagation rules, proposing algorithms for learning multimodal state distributions, and demonstrating the effectiveness of the method in real and motion capture-based test sequences.
41799	41799120	Fast algorithms for large scale conditional 3D prediction	The success of using discriminative learning approaches for 3D reconstruction depends on the ability to efficiently train predictive algorithms with enough representative examples. Recent research has shown that sparse conditional Bayesian mixture of experts (cMoE) models are suitable for this task as they can provide contextual 3D predictions and handle depth ambiguities and occlusion. However, training these models is challenging and limited to small datasets. This paper presents a new large-scale algorithm, fBME, which combines forward feature selection and bound optimization to train probabilistic BME models using larger datasets (100,000+ examples) in a much faster manner. The effectiveness of this approach is demonstrated through experiments on the HumanEva dataset.
41800	4180034	Interconnect agnostic checkpoint/restart in open MPI	HPC applications at scale must be able to handle faults in order to utilize current and future HPC systems. MPI level transparent checkpoint/restart fault tolerance is a popular choice for developers who do not want to modify their code. However, previous MPI implementations have struggled to support a wide range of interconnects, especially shared memory. This paper introduces a new method for implementing checkpoint/restart coordination algorithms that is independent of the interconnect type. This allows applications to be checkpointed on one set of interconnects and restarted on a different set, adapting to changes in the cluster environment. Performance results for HPC applications using this approach are also presented.
41800	41800126	The Design and Implementation of Checkpoint/Restart Process Fault Tolerance for Open MPI	This paper discusses the need for fault tolerance in modern high-performance computing (HPC) applications and the limitations of existing MPI implementations in incorporating such capabilities. The authors present their design and implementation of an infrastructure for checkpoint/restart fault tolerance within the Open MPI project. This design includes extensible frameworks that can support distributed checkpoint/restart and provide fault tolerance services without sacrificing performance, robustness, or flexibility. While the current implementation includes some checkpoint/restart mechanisms, the authors encourage experimentation with alternative techniques within a production-quality MPI implementation. By incorporating fault tolerance into Open MPI's component architecture, this infrastructure aims to enable the full utilization of larger computing platforms for HPC applications.
41801	4180116	Computing pure strategy nash equilibria in compact symmetric games	This article discusses the complexity of computing pure strategy Nash equilibria (PSNE) in symmetric games with a fixed number of actions. The authors focus on "compact" representations, where the number of players can be exponential in the representation size. They show that determining the existence of PSNE in general is an NP-complete problem when utility functions are represented as arbitrary circuits. However, for games with two actions, a PSNE always exists and can be found in polynomial time. They then introduce a specific compact representation for piecewise-linear utility functions and provide polynomial-time algorithms for finding a sample PSNE, counting the number of PSNEs, and finding social-welfare-maximizing equilibria. This representation is extended to parameterized families of symmetric games, allowing for efficient methods of answering questions about the family without solving each game individually. 
41801	4180155	Symmetric games with piecewise linear utilities	This article discusses the complexity of computing pure strategy Nash equilibria (PSNE) in symmetric games with a fixed number of actions and compactly represented utilities. The representation is able to describe symmetric games with exponentially many players. The general case, where utility functions are represented as arbitrary circuits, is shown to be NP-complete. However, for games with two actions, there is always a PSNE and a polynomial algorithm is provided for finding one. The article then focuses on a different representation of utility, piecewise-linear functions, and shows that it has favorable computational properties. Specifically, polynomial-time algorithms are given for counting the number of PSNE and finding a sample PSNE, if one exists.
41802	4180265	Boundary graph grammars with dynamic edge relabeling	eNCE graph grammars are a type of graph grammar that generate graphs with edge labels, in addition to node labels. These edge labels, along with the NCE feature, introduce new properties to the grammar. This is particularly evident in boundary eNCE (B-eNCE) grammars, which do not have the context-sensitive feature of "blocking edges" found in other eNCE grammars. B-eNCE grammars also have a Chomsky normal form and a Greibach normal form, and their languages can be characterized in terms of regular tree and string languages. It is proven that the class of (boundary) eNCE languages is larger than the closure of the class of (boundary) NLC languages under node relabelings. Similar results are found for linear eNCE grammars.
41802	4180234	A comparison of boundary graph grammars and context-free hypergraph grammars	Context-free hypergraph grammars and boundary graph grammars with bounded nonterminal degree have equal power in generating sets of graphs and hypergraphs. However, arbitrary boundary graph grammars have more graph generating power than context-free hypergraph grammars, but the same hypergraph generating power. To prove this, various normal forms for boundary graph grammars are provided. Additionally, the class of boundary graph languages is shown to be closed under edge contraction, with the label of the edge determining whether it should be contracted or not.
41803	4180324	Offline/realtime traffic classification using semi-supervised learning	The task of identifying and organizing network traffic based on application type is difficult due to the constantly changing nature of applications, particularly those that aim to go undetected. Traditional methods such as using port numbers are becoming less effective, leading to a reliance on deep packet inspection. However, this approach is resource-intensive, leading researchers to propose a new method of classifying traffic based on unique flow characteristics. This paper introduces a semi-supervised classification method that can handle both known and unknown applications, using a small number of labeled and a larger number of unlabeled flows for training. The evaluation of this approach on real network traffic shows high accuracy rates and the need for retraining only when there are significant changes in network usage patterns. Prototype systems are also implemented to demonstrate the feasibility of this method. 
41803	4180329	A comparative analysis of web and peer-to-peer traffic	P2P applications have become very popular and are now the main contributor to Internet traffic, surpassing Web applications. This has led to an extensive analysis of P2P traffic, comparing it to Web traffic and discussing its implications. The study also looks at the two main types of P2P traffic, BitTorrent and Gnutella, in order to gain a better understanding of how they function. The results of this analysis can be used to generate synthetic workloads, gain insights into P2P applications, and develop network management strategies. It is suggested that new models are needed for Internet traffic, and flow-level distributional models are presented for both Web and P2P traffic for use in network simulation and emulation experiments. 
41804	4180433	(s|qu)eries: Visual Regular Expressions for Querying and Exploring Event Sequences	In this paper, the authors introduce (s|qu)eries, a visual query interface for analyzing event sequence data using regular expressions. Many domains rely on finding patterns in event sequences to gain insights, but current systems have limitations in their expressiveness, workflow, and scalability. (s|qu)eries addresses these issues by providing a touch-based interface that allows users to build and visualize complex queries in an approachable way. This supports iterative and exploratory workflows, as well as debugging. The system was validated through interviews with data scientists who regularly analyze event sequence data. 
41804	4180422	Reflections on how designers design with data	In recent years, non-computer science designers have been creating popular data visualizations using design tools and environments. To better understand their challenges and perspectives, professional designers were interviewed and observed working with data in both lab and team settings. From these observations, patterns emerged and several themes were identified that offer a new perspective for creators of visualization tools and address known engineering problems. This research aims to improve the development of tools intended for designers working with data. 
41805	4180521	Graph Isomorphism, Color Refinement, and Compactness.	Color refinement is a widely used technique to prove that two given graphs are not isomorphic. It is considered to be a highly efficient method, but it does not work for all graphs. A graph is said to be amenable to color refinement if the procedure successfully distinguishes it from any non-isomorphic graph. Babai et al. (1980) have demonstrated that random graphs have a high probability of being amenable. It has been further determined that the applicability of color refinement can be precisely determined by recognizing amenable graphs in \({O((n+m)\log n)}\) time, where n and m are the number of vertices and edges in the input graph.
41805	418054	Approximate graph isomorphism	This article discusses optimization versions of Graph Isomorphism, which involves finding a bijection between two graphs that maximizes the number of matches between edges and non-edges. The authors provide a nO(logn) time approximation scheme that can achieve a constant factor of α-approximation. This is achieved by combining a previous approximation algorithm with a simple averaging algorithm. The article also examines the minimization problem of mismatches and proves its NP-hardness for any constant factor of α. Additionally, it is shown that it is also NP-hard to approximate the maximum number of edges mapped to edges beyond a factor of 0.94. The authors also explore these problems for bounded color class graphs, which surprisingly turn out to be harder than the uncolored case in terms of approximation. 
41806	4180661	The Trouble with Long-Range Base Pairs in RNA Folding	RNA prediction accuracy is hindered by long-range base pairs, and a recent study analyzed the distribution of these pairs in known RNA structures. Surprisingly, long-range base pairs are actually overrepresented in these structures, challenging the idea that they are systematically overpredicted. Instead of kinetic effects, the study suggests that a modification to the expected accuracy model for RNA secondary structures could improve prediction. This modification includes a span-dependent penalty, which has shown to improve maximum expected accuracy structure predictions. The prevalence of long-range base pairs also suggests that RNA structures do not follow the polymer zeta property, which has implications for the performance of sparsified RNA folding algorithms.
41806	4180622	Alignment of RNA base pairing probability matrices	This article discusses the importance of structure-based alignments in RNA bioinformatics and presents a new method for computing these alignments. Functional RNA molecules often have similar secondary structures but different sequences, making traditional sequence-based alignments inadequate. The method described uses base pairing probability matrices, which incorporate information on the energetics of the sequences, to compute alignments. These alignments can be extracted using a simplified version of Sankoff's algorithm. The programs used for this method are available for download and there is also a web server for easy access. This new approach allows for more reliable multiple alignments and can greatly benefit subsequent data analysis methods. 
41807	418072	Three-dimensional modeling from two-dimensional video	The paper introduces a surface-based factorization method for recovering the 3-D structure of a rigid object from a 2-D video sequence. The method uses polynomial patches to describe the object's shape and parametric 2-D motion models to track its projections in the image plane. By factorizing a rank 1 matrix, the method is able to simultaneously estimate the 3-D shape and motion parameters. This approach is an extension of the original factorization method by Tomasi and Kanade (1992) and simplifies the tracking process by focusing on regions with a single set of motion parameters. Experimental results demonstrate the effectiveness of this method in real-life video sequences.
41807	4180789	Factorization as a Rank 1 Problem	Tomasi and Kanade introduced a method for recovering 3D structure from 2D video by using an SVD to approximate a matrix that is rank 3 in a noiseless situation. This paper presents a reformulation of the problem by utilizing the known x and y coordinates of each feature in frame 1. This allows for the computation of the 3D shape and motion by a simple factorization of a matrix that is rank 1 in a noiseless situation. The method also allows for the use of fast algorithms with a large number of features and frames. Furthermore, the paper shows how to incorporate confidence weights for the feature trajectories without any additional computational cost. This is achieved by rewriting the problem as the factorization of a modified matrix.
41808	4180834	Towards a Compositional SPIN	This paper discusses the use of automated assume-guarantee verification with learning in the SPIN tool. This technique can complement existing state-reduction methods, allowing SPIN to handle larger systems. The authors present a "light-weight" approach for evaluating the benefits of learning-based assume-guarantee reasoning within SPIN, and experimented with different versions of this technique. They also introduce a new heuristic for generating component assumptions when their environment is unavailable. The effectiveness of learning-based assume-guarantee reasoning is demonstrated through an example of a resource arbiter for a spacecraft. While this approach currently has performance overheads, the authors believe it can be integrated into SPIN for improved memory savings.
41808	4180844	Assume-guarantee verification of source code with design-level assumptions	Model checking is an automated method for determining if a system meets certain requirements. This technique can be hindered by the "state explosion" problem, but a solution is proposed through the integration of assume-guarantee verification in different stages of system development. During design, abstract models are used to establish key properties, and techniques have been developed to improve the scalability of model checking by automatically breaking down the verification task. These design artifacts are also used to guide implementation and enhance the reasoning of source code. By using assumptions generated during the design phase, the verification of the actual system implementation can be similarly decomposed. This approach has been successfully demonstrated on a NASA application, where it helped identify and correct a safety property violation.
41809	4180926	Exploiting Simd Parallelism With The Cgis Compiler Framework	Desktop PCs now have various parallel processing units, making it a challenging task for programmers to develop applications that take advantage of this parallelism. To do so efficiently, programmers need to have in-depth knowledge about the hardware. CGiS is a data-parallel programming language that simplifies this process by providing a unified abstraction for both graphics processing units (GPUs) and vector processing units of CPUs. The CGiS compiler framework virtualizes the differences in capability and accessibility, allowing for easy mapping of the abstract programming model to these targets. This approach has been successfully applied to CPUs and has now been extended to SIMD instruction sets. Experimental results have shown that real-world applications can be easily implemented with CGiS and produce efficient code.
41809	4180914	The CGiS compiler—a tool demonstration	CGiS is a programming language that allows general purpose programmers to tap into the parallel processing power of GPUs. In this paper, the authors discuss the concept behind CGiS and its compiler framework, and demonstrate its practical applications. With CGiS, programmers can take advantage of the high performance capabilities of GPUs for a wide range of tasks beyond just graphics processing. This language has the potential to greatly enhance the efficiency and speed of various computing applications, making it a valuable tool for developers. 
41810	4181025	Pegasos: primal estimated sub-gradient solver for SVM	We introduce a new stochastic sub-gradient descent algorithm for solving the optimization problem in Support Vector Machines (SVM). Our algorithm requires a smaller number of iterations to achieve a desired level of accuracy compared to previous methods. The number of iterations also scales linearly with the regularization parameter and the runtime is dependent on the number of non-zero features in each example. This makes our algorithm ideal for learning from large datasets, particularly in text classification problems. It can also be applied to non-linear kernels, but in this case, the runtime is dependent on the size of the training set. Our algorithm shows a significant increase in speed compared to previous SVM learning methods.
41810	4181076	Efficient projections onto the l1-ball for learning in high dimensions	The article discusses efficient algorithms for projecting a vector onto the l1-ball, which is useful for online learning in sparse feature spaces. Two methods are presented, with the first achieving exact projection in O(n) expected time and the second working on perturbed vectors in O(k log(n)) time. The algorithms are shown to be effective in batch and online learning tasks, outperforming interior point methods commonly used in optimization. Additionally, in online settings, gradient updates with l1 projections are found to perform better than the exponentiated gradient algorithm, producing models with high levels of sparsity.
41811	4181155	A Relation-Based Schema for Treebank Annotation	This paper introduces a new annotation schema for treebanks, focusing on its use in creating a corpus of Italian sentences. The schema differentiates between arguments and modifiers, providing a more precise representation of predicate-argument structure and subcategorization. The accuracy of the schema relies on the methods used to define the relations, which consist of three components: morpho-syntactic, functional, and semantic. The authors provide evidence for the effectiveness of these tripartite structures through examples encountered while developing the Italian treebank.
41811	4181116	Digital heritage and avatars of stories	The annotation of digital heritage faces a challenge when it comes to abstract concepts in a cross-media context, particularly in dramatic media such as screenplays, performances, TV series, video games, and feature films. These different forms of media share the same primary notion of story, with each format serving as an avatar of this concept. To address this issue, a collaborative annotation system has been developed to describe the dramatic qualities of media heritage, using story elements such as agents, goals, and conflicts. This system includes an ontology-based schema and a web platform for annotation and visualization of the metadata, allowing for cross-comparisons in drama studies. The system was tested on a comparison between story representation and performance timelines to demonstrate its effectiveness.
41812	4181238	A posteriori error estimates for nonconforming finite element methods for fourth-order problems on rectangles	The a posteriori error analysis for conforming finite element discretisations of the biharmonic problem for plates is well-established, but nonconforming discretisations are easier to implement in practice. However, the a posteriori error analysis for the Morley plate element is unique because two edge contributions from an integration by parts cancel out. This property is not present in popular nonconforming finite element schemes such as the nonconforming rectangular Morley, incomplete biquadratic, and Adini finite elements. To address this issue, a new methodology using a conforming discrete space on macro elements is introduced to prove the reliability and efficiency of an explicit residual-based error estimator. This technique is successfully applied to the Morley triangular finite element, with numerical experiments confirming its reliability and efficiency on uniform and graded tensor-product meshes.
41812	418126	Convergence analysis of an adaptive nonconforming finite element method	The authors present a new adaptive nonconforming finite element method that improves the accuracy of finite element approximations and ensures convergence. This method is specifically analyzed for the lowest order Crouzeix-Raviart elements and is proven to have linear convergence as the number of refinement levels increases. The proof relies on discrete local efficiency and quasi-orthogonality properties, without the need for solution regularity or duality arguments. Additionally, this method does not require specific mesh designs to be monitored, as data control is used instead. Overall, this approach offers a more efficient and reliable way to refine finite element approximations.
41813	4181324	Architectural evolution of FamiWare using cardinality-based feature models	Ambient Intelligence systems are constantly evolving with the introduction of new devices and technologies. To manage these changes, a software product line engineering process has been developed for FamiWare, a family of middleware for ambient intelligence environments. This process uses cardinality-based feature models and clonable features to handle the structural variability of these systems. The process automates the management of feature changes and propagates them to the FamiWare middleware code. It is able to handle thousands of features, which would be impossible to manage manually. The process also calculates the effort needed to make changes in customized products. Two operators have been defined to assist with these tasks. The approach has been validated through case studies, showing that it can successfully handle changes in FamiWare configurations with thousands of features.
41813	4181378	Software product line evolution with cardinality-based feature models	Feature models are commonly used to represent the variability in a Software Product Line family. This article suggests using cardinality-based feature models and clonable features to model and manage the structural variability in pervasive systems, which consist of a diverse range of devices. However, the use of clonable features can increase the complexity of configurations, making manual evolution difficult. To address this, the authors propose a model-driven development process to automatically propagate changes from an evolved feature model to existing configurations. This process also allows for the calculation of effort required for evolution changes in customized products. Two operators are defined to assist with this process. The approach is validated by showing that it can generate configurations for a product family with a large number of cloned features.
41814	4181437	Inference with separately specified sets of probabilities in credal networks	We have developed new methods for making predictions in credal networks, which are graphs that represent relationships between variables using sets of probabilities. These networks reflect strong independence between variables. Our approach involves using separate sets of probabilities and we have proven that making predictions in polytrees is a difficult problem. To address this issue, we have developed techniques that make the process more efficient by examining the separability of these sets of probabilities. This can greatly reduce the computational effort required, especially in polytrees. 
41814	4181438	Evidence Propagation in Credal Networks: An Exact Algorithm Based on Separately Specified Sets of Probability	Probabilistic models and graph-based independence languages are commonly used together in artificial intelligence research, with Bayesian networks being a prime example. Credal networks are a type of graphical structure that combine graphs with sets of probability measures. A new algorithm for evidential reasoning in credal networks has been developed, simplifying the computation of upper and lower probabilities by utilizing strong independence and separate sets of probability measures. This algorithm is especially effective for polytree structures. A strategy for approximate reasoning in multi-connected networks using conditioning is also discussed. 
41815	4181562	General lower bounds for evolutionary algorithms	Evolutionary optimization, specifically genetic optimization, is a widely used framework for optimizing problems. It is known for being user-friendly, robust, and not requiring derivatives. However, it is also known to be slow. Recent research has shown that the convergence rate of popular evolution strategies cannot be faster than linear, and the constant in the linear convergence decreases quickly as the dimension increases. This limitation is not only present in evolution strategies, but also in comparison-based algorithms like the Hooke & Jeeves algorithm and the simplex method. However, using the full ranking information of the population instead of just selections can lead to faster convergence rates, potentially providing superlinear convergence results. 
41815	418159	Analysis of Different Types of Regret in Continuous Noisy Optimization.	The performance measure and analysis of an algorithm is important in determining its effectiveness. This is often done by studying the convergence rate of the algorithm, which measures how close the approximated optimum is to the real optimum. The concept of Regret is commonly used in assessing an algorithm's performance, both in bandit literature and optimization algorithms. In the evaluation of noisy algorithms, approximations of Regret are often used. This article discusses two types of approximations of Simple Regret, used for evaluating algorithms in noisy optimization. The Approximate Simple Regret, commonly used in optimization testbeds, is shown to be ineffective in estimating the Simple Regret convergence rate. A new approximation, called Robust Simple Regret, is also discussed and its advantages and disadvantages are shown through specific algorithms and the noisy sphere function.
41816	418162	3D Warp Brush: Interactive Free-Form Modeling on the Responsive Workbench	The 3D warp brush is a new method for creating shapes in a virtual reality environment. It operates on triangle meshes and combines the efficiency of explicit mesh representations with implicit modeling operators. The brush has an adjustable area of influence and offers different warp functions such as drag, explode, and whittle. A unique aspect is the ability to convert meshes into 3D warp brushes in real-time. The use of a Responsive Workbench and two-handed interaction makes it easy for users to modify surfaces and create desired shapes. Examples of models created with this method showcase its effectiveness.
41816	41816110	Interactive Visualization of Very Large Medical Datasets using Point-based Rendering	Visualizing high-resolution volumetric medical datasets is difficult with current graphics hardware only supporting up to 512 3 data points. A new method has been developed that allows for interactive texture-based volume-rendering of higher-resolution datasets without sacrificing image quality or frame-rates. This approach utilizes an out-of-core point-based rendering method, where data is pre-processed and streamed from disk when needed. The high density of data points allows for pure point-based rendering, resulting in interactive frame-rates even for datasets exceeding 512 3 resolution. This approach also enables real-time changes to the displayed values, allowing for interactive exploration of important features and contours. By using points instead of textured volumes, the amount of data transferred to the graphics hardware is reduced, and an optimized data-organization further decreases update times. Overall, this method offers a way to interactively display high-resolution datasets without any loss of detail.
41817	4181777	Attribute generation based on association rules	A decision tree is a useful tool for accurately classifying data and keeping the tree size small. To improve the performance of a decision tree, new attributes and values can be added during data pre-processing. However, current methods for creating new attributes are time-consuming and require prior knowledge of the data. To address this issue, a new approach is proposed that extracts knowledge on relevant attributes through association rules from the training data. The new attributes and values are then generated based on these rules. Experiments show that this approach is effective in improving the performance of decision trees.
41817	41817101	Classifier construction by graph-based induction for graph-structured data	This paper discusses the use of a machine learning technique called Graph-Based Induction (GBI) for extracting patterns from graph-structured data. GBI is known for its efficiency due to its greedy search approach. However, a traditional decision tree cannot be constructed for data that is not explicitly expressed in attribute-value pairs. To address this issue, the authors propose a method called Decision Tree - Graph-Based Induction (DT-GBI) which constructs a classifier by using attributes extracted on the fly from GBI. The effectiveness of DT-GBI is demonstrated through testing on a DNA dataset, where the data is transformed into graph-structured form and attributes are extracted by GBI to construct a decision tree. The results show the effectiveness of DT-GBI in constructing a classifier for graph-structured data. 
41818	4181856	Intelligent Social Learning	The paper discusses social learning, which is the process by which individuals acquire new information through exposure to others in a common environment. This can occur through social facilitation, where individuals are influenced by the presence of others, or through imitation. The paper presents a general definition of social learning and analyzes the cognitive processes involved in social facilitation and imitation. It also highlights the lack of a systematic understanding of social learning and proposes a continuum of cognitive complexity for these processes. The paper draws upon a cognitive model of social action and discusses how this approach can be useful in agent systems applications.
41818	4181849	From Simulation to Theory (and Backward)	Recently, there has been a surge in interest in Agent Based Generative Social Simulation (ABGSS), which is a type of agent-based modeling and simulation that utilizes the generative paradigm. This paper supports the idea of weak ABGSS, which states that while social effects can be explained by growing them, this is not sufficient. The authors argue that ABGSS must be informed by two theoretical complements: a bottom-up theory of local rules and their impact on macroscopic effects, and a theory of downward causation that explains how local rules are influenced by the effects they contribute to. Three examples of social phenomena are discussed in relation to this twofold thesis: the witness effect, Schelling's segregation model, and the minority game.
41819	418196	Pairing the Volcano	Isogeny volcanoes are mathematical structures composed of elliptic curves connected by l-isogenies. Researchers have previously developed algorithms for navigating these graphs, but they were unable to predict the direction of a step before taking it. This led to a trial-and-error approach, resulting in many unnecessary steps. In this paper, the authors propose a more efficient method for finding points of order l on the volcano that generate a horizontal or ascending isogeny. Their method is faster than previous ones, and they also explore the case of 2-isogeny volcanoes and derive a new invariant. Their benchmarks show that their algorithm is faster than previous methods for computing the endomorphism ring of an elliptic curve.
41819	4181945	Another Approach to Pairing Computation in Edwards Coordinates	This paper discusses the use of Edwards curves in elliptic curve cryptography, which have greatly reduced the cost of addition on these curves. The authors present new explicit formulae for implementing pairing operations in Edwards coordinates, which have similar performance to Miller's algorithm in Jacobian coordinates. The method is especially useful for cryptographic protocols that use Edwards curve implementations, and is faster than a recent proposal for computing pairings on supersingular curves using Edwards coordinates. Overall, this research shows the potential for Edwards curves to be a valuable tool in elliptic curve cryptography.
41820	4182044	An intelligent agent-based scheme for vertical handover management across heterogeneous networks.	The advancements in technology have led to the widespread use of mobile devices and innovative solutions for various applications and services. As the demand for constant and seamless connectivity increases, network operators are required to integrate different types of wireless and cellular networks. This integration necessitates the ability for users to freely move between networks, known as vertical handover. However, due to the involvement of various technologies, the issue of vertical handover needs to be addressed. Several solutions have been proposed but most lack intelligence and adaptability, resulting in packet loss and delay. This paper suggests an intelligent cooperative agent based approach using a knowledge plane to address this problem. Agents are introduced in mobile devices and access points to collect information from the environment and make handover decisions. A selection function is also introduced to choose the best network for handover. The proposed approach is validated through simulations.
41820	4182033	Autonomous agents for autonomic networks.	 This paper discusses the use of multi-agent technology to introduce autonomic behavior in network control and management. The authors first describe the autonomic communication paradigm and the need for new architectures. They then compare and contrast classical and autonomic networks. After discussing related works combining AI and telecommunication networks, the authors explain the characteristics of agents and multi-agent systems that can provide autonomic behavior in networks. The paper also looks at various proposals in the literature and describes a network-oriented platform with agent capabilities. An application of this architecture to Ospf weight setting optimization is presented, followed by details of two experiments on Ospf weight setting in a small and large scale network.
41821	4182190	Aavp: An Innovative Autonomic Architecture For Virtual Network Piloting	The Internet has experienced significant growth in recent years, leading to the need for new technologies and applications. However, the size and scale of the Internet has also created challenges, making it necessary to design a Next Generation Internet. One solution is network virtualization, which allows for different network architectures and protocols to be used on a shared physical infrastructure. While this offers many advantages, it also adds complexity to network systems. To address this, an autonomic computing framework is proposed, which allows for self-management of virtual resources. Testing on a real test-bed has shown the effectiveness of this framework in maintaining a desired quality of service.
41821	418218	A Survey of Autonomic Network Architectures and Evaluation Criteria.	Autonomic network management is a promising approach to simplify and reduce the cost of managing network infrastructures. This involves removing the human administrator from the network control loop and allowing the network to perform management tasks on its own. This has gained attention from both academia and industry due to its potential for automating management systems. In this paper, the authors provide an overview of autonomic architecture proposals and evaluation metrics. They also identify new criteria that are important for these architectures. Finally, the authors compare different existing autonomic architectures and discuss their strengths and weaknesses in terms of network management and performance. 
41822	4182222	Service management for multi-operator heterogeneous networks	The emergence of heterogeneous radio networks, which aim to provide ubiquitous services, requires a comprehensive management approach. In this paper, we propose an integrated management approach for end-to-end service management in a multi-operator environment. Our solution can be easily integrated into existing web-based management platforms, making it a flexible and scalable option. This approach allows for efficient management of services across various networks, ensuring a seamless experience for users. 
41822	4182220	Managing multi-configurable terminals	This paper discusses the need for increased management capability in the era of software defined architecture for terminals. The authors propose an architecture for configuring multi-technology terminals, as future wireless networks will offer a wide range of services over a heterogeneous infrastructure. This shift towards a service-oriented view requires a different approach to configuration management, particularly for software-oriented radio receivers. The authors suggest using IETF proposed management paradigms, including information models and a Java-based management architecture. This will allow for more flexibility in configuring these receivers, which will now perform most functions in software rather than hardware. 
41823	4182347	A CIM Extension for Peer-to-Peer Network and Service Management	Peer-to-peer (P2P) networks and services are becoming more prevalent in the networking industry, particularly for enterprise solutions. As a result, there is a need for an open management approach that ensures quality of service (QoS) parameters are met. This paper focuses on the design of a management framework for P2P networks and services. A management information model has been developed, based on the standard CIM model, to capture the various functional, organizational, and topological aspects of the P2P model. This model is essential for effectively managing P2P networks and services.
41823	4182399	Leveraging Countermeasures As A Service For Voip Security In The Cloud	Cloud computing has become a major factor in the integration of various services, particularly Voice over Internet Protocol (VoIP) services. Although there have been concerns about the security of cloud-based VoIP services, they also offer new opportunities for enhancing security. To address this, a risk management strategy is proposed for VoIP cloud, utilizing outsourced security countermeasures. The architecture and components of this solution are presented, with a focus on the Session Initiation Protocol (SIP). Mathematical modeling is used to support the approach, and different treatment strategies for implementing countermeasures are described. Extensive simulation results demonstrate the effectiveness of these strategies in maintaining a low risk level, with a potential cost increase of up to 7% or an additional risk of up to 12%. The strategies can also be combined to achieve a balance between cost and performance.
41824	418247	Multipub: Latency And Cost-Aware Global-Scale Cloud Publish/Subscribe	Topic-based pub/sub is a popular way to communicate between loosely connected entities in distributed systems. To handle varying communication demands, pub/sub services can be deployed in the cloud and distributed across multiple regions for faster dissemination. However, the design and deployment of such middleware can impact communication latency and cloud costs. To address this, the paper proposes MultiPub, a flexible middleware that can dynamically reconfigure the communication layer to achieve a predefined maximum latency while minimizing costs. Experiments show that MultiPub can effectively reduce latency and save costs compared to traditional approaches. 
41824	4182441	Publish/subscribe network designs for multiplayer games.	This paper discusses the need for a scalable and low latency messaging middleware for massively multiplayer online games (MMOGs). Three different pub/sub-driven designs are presented, each accounting for the highly interactive and massive nature of these games. These designs use different pub/sub approaches and serve varying responsibilities, including integrating game functionality. The authors implement, evaluate, and compare the designs in a MMOG prototype called Mammoth, and show the effectiveness of pub/sub while highlighting trade-offs between the designs. Real-world results demonstrate the importance of considering message types and frequency in MMOG networking engines.
41825	4182529	Development of Fault-Tolerant Software Systems Based on Architectural Abstractions	This paper explores the use of architectural abstractions in developing fault-tolerant software systems. These abstractions represent different aspects of fault tolerance, such as error detection and handling, and can be instantiated into concrete components and connectors. By formally specifying the structural and behavioral properties of these abstractions, the process of verifying and validating software architectures can be automated. The paper discusses how these abstractions, along with a recursive process, can aid in the modelling and analysis of fault-tolerant software systems. The effectiveness of this approach is demonstrated through a case study of a critical real-time application. Overall, the use of architectural abstractions can help manage the complexity of incorporating fault tolerance into systems. 
41825	4182541	Architecting fault tolerance with exception handling: verification and validation	This paper discusses the challenges of integrating untrusted software components into a dependable system and the potential for architectural mismatches. It proposes an architectural abstraction based on exception handling to structure fault-tolerant systems, which includes components and connectors that promote untrusted software into idealized fault-tolerant elements. This approach is considered within a formal software development approach, using formal methods for representing the architecture's structure and behavior. The proposed approach also includes formal specification and verification for analyzing exception propagation and verifying dependability properties. Additionally, the paper describes how formal models can be automatically generated from UML diagrams and used for generating test cases to assess the correctness of the source code. The feasibility of this approach was evaluated on an embedded critical case study.
41826	4182626	Fifth workshop on software engineering for large-scale multi-agent systems (SELMAS)	As technology continues to advance, software is becoming more and more embedded in our daily lives, leading us towards a future of ambient computing. Multi-agent systems (MAS) are a key technology in creating and managing large-scale distributed systems. In recent years, there has been a focus on improving the design and implementation of MAS through software engineering research. However, ensuring the reliability of these large MAS remains a challenge. The Fifth Workshop on Software Engineering for Large-Scale Multi-Agent Systems (SELMAS 2006) aims to bring together experts from various fields to discuss the latest technologies and approaches for creating dependable MAS. 
41826	4182660	Rigorous development of fault-tolerant agent systems	Agent systems are complex distributed systems that operate in unreliable communication environments. However, these systems often have high reliability requirements. To ensure system correctness and design effective fault tolerance mechanisms, we need to develop methods that integrate fault tolerance in the development process. This paper presents a formal approach for developing fault tolerant location-based mobile agent systems using stepwise refinement in the Event B framework. This approach allows for the gradual introduction of implementation details while preserving correctness. It also enables a formal representation of important abstractions, such as scopes, roles, locations, and agents, in the development process. The result is a fault tolerant agent system with guaranteed inter-consistency and inter-operability. 
41827	4182717	A Batch-Authenticated and Key Agreement Framework for P2P-Based Online Social Networks.	Online social networks (OSNs) like Facebook and MySpace have become popular platforms for people to share their interests with friends. However, the security and privacy of these networks are major concerns. To address this issue, a security framework has been proposed for authenticating multiple users simultaneously in P2P-based OSNs. The framework includes three batch authentication protocols that use different cryptographic systems such as one-way hash function, ElGamal proxy encryption, and certificates. These protocols offer various advantages, such as lower computational cost, exchange of more information among users, and guaranteeing non-repudiation of transactions. The proposed framework does not require a centralized authentication server, making it suitable for extending OSNs with batch verifications. The security of the protocols has been formally proven against passive adversaries and impersonator attacks. They also meet important security requirements such as mutual authentication, reputation, community authenticity, non-repudiation, and flexibility. This makes the framework an appropriate choice for P2P-based OSNs.
41827	4182718	A signaling system using lightweight call sessions	The emergence of different types of access devices and networks has created a need for a communication network that can integrate telephony and data services across these networks. While call setup has been addressed by protocols such as Session Initiation Protocol (SIP), there is a lack of support for maintaining and controlling calls after setup. To address this, a new signaling protocol has been developed that offers scalability, high availability, and flexibility for creating new call processing services. The protocol is compatible with existing call setup protocols and simplifies the implementation of basic call services, including multi-device calling and service handoff between different networks. The protocol also separates control (signaling information) from data, making it easier to implement. It has been successfully implemented in a testbed with access to various networks.
41828	4182835	Near-Optimal Evasion of Convex-Inducing Classifiers	Classifiers are commonly used to identify and prevent miscreant activities. In this study, we examine how an adversary can manipulate a classifier by making strategic queries in order to avoid detection at a low cost. We build upon previous research by Lowd and Meek (2005) and extend it to include convex-inducing classifiers. Our algorithms are able to generate undetected instances with minimal cost using a polynomial number of queries, without needing to reverse engineer the decision boundary. This research sheds light on the vulnerabilities of classifiers and provides insights for improving their effectiveness against malicious manipulation.
41828	4182849	Optimal ROC Curve for a Combination of Classifiers	Our study introduces a novel approach for combining binary classifiers, using the Neyman-Pearson lemma as a theoretical foundation. Through this, we present a method for finding the optimal decision rule for a combination of classifiers, which results in the best possible Receiver Operating Characteristic (ROC) curve. Our approach improves upon previous methods and can be applied to any number of Boolean rules. We also prove that our method is optimal in this space and that the Boolean AND and OR rules are always part of the optimal set when classifiers are independent. Additionally, we provide a sufficient condition for another existing method to be optimal.
41829	4182968	Evaluating tax sites: an evaluation framework and its application	In recent years, governments have been implementing e-services to provide faster and more advanced services to their citizens. One important aspect of these e-services is tax filing and payment, and this paper examines the key factors that contribute to a successful tax website. The goal is to create an evaluation framework that takes into account the needs of citizens, which can be used by both government officials and website developers to improve the quality of services provided to citizens. This framework has the potential to enhance the overall user experience and efficiency of tax websites.
41829	4182960	Evaluating mobile banking portals	Mobile banking, or m-banking, is becoming more widespread globally. In order to design effective m-banking services, a comprehensive framework is needed. This article introduces the Mobile Banking Evaluation Framework (MoBEF), consisting of 164 criteria divided into six categories: interface, navigation, content, offered services, reliability, and technical aspects. MoBEF can be used by both customers to assess potential bank's m-banking services and by bank managers to enhance their offerings. The framework was used to evaluate the m-banking portals of 30 major banks worldwide, revealing strengths and weaknesses, particularly in the area of offered services. The article provides suggestions for improvement.
41830	4183077	Multimodal representation, indexing, automated annotation and retrieval of image collections via non-negative matrix factorization	The availability of large image collections on the internet has led to the incorporation of non-visual data such as text descriptions, comments, ratings, and tags. These additional data can enhance the analysis of image content and improve its performance. This paper introduces a new method, based on non-negative matrix factorization, to create multimodal representations of images that combine visual features with text information. This approach identifies common factors in different types of data and integrates them into the same representation space. The effectiveness of this method was evaluated in image indexing and search tasks, and the results showed significant improvement compared to other methods using singular value decomposition.
41830	4183021	A Genetic Niching Algorithm with Self-Adaptating Operator Rates for Document Clustering	The proposed Genetic algorithm aims to cluster documents by evolving candidate cluster representatives using an evolutionary multimodal optimization algorithm. This involves evolving both the representative solutions and the genetic operator rates simultaneously. The evolving population is made up of sparse index and variable length vectors, and specialized genetic operators are used for this representation. These operators balance exploitation and exploration in searching for optimal document cluster prototypes, with the most specialized being the Sparse Top-K-Addition operator. This approach takes into account associated terms within a local context, resulting in improved cluster prototypes. Experimental results on two document data sets show the effectiveness of this approach.
41831	4183114	Finding a Sun in Building-Free Graphs	A recent study showed that determining whether a graph contains a sun is a difficult problem. However, for building-free graphs, this can be solved in O(min{mn 3, m 1.5 n 2}) time and the sun can be found in the same time. Building-free graphs include perfect graphs such as Meyniel graphs, as well as imperfect graphs. This class also encompasses other graph classes, making it a useful generalization. Additionally, a vertex elimination method is presented for (building, gem)-free graphs, which are a type of distance hereditary graphs and a subset of (building, sun)-free graphs.
41831	4183127	Weakly Triangulated Comparability Graphs	The class of weakly triangulated comparability graphs and their complements are similar to interval graphs and chordal comparability graphs. By transforming these graphs into chordal bipartite graphs, problems such as recognition and independent set can be solved efficiently in O(n2) time. The number of weakly triangulated comparability graphs is $2^{\Theta ( n {{\log}^2} n)}$, and algorithms have been developed to compute transitive closure and transitive reduction in O(n2loglogn) time if the underlying undirected graph is a weakly triangulated comparability graph.
41832	4183261	Perception of Material from Contact Sounds	Contact sounds play a crucial role in how we perceive materials in virtual environments. Researchers studied the relationship between material perception and factors that influence the creation of contact sounds. They found that an auditory decay parameter, which reflects the sound's rate of fading, strongly influenced how people perceived an object's material. In experiments where participants were asked to compare the similarity of synthesized sounds based on material or length, the decay rate had a greater impact than the sound's fundamental frequency. This was further confirmed in a study where participants were asked to categorize sounds into different material types. The results suggest that a simplified model of material can be used in virtual auditory environments, with the decay rate being a key factor in creating realistic material perception.
41832	4183271	Precomputed acoustic transfer: output-sensitive, accurate sound generation for geometrically complex vibration sources	Simulating realistic sounds from vibrating objects is difficult due to the complex diffraction and interreflection effects involved. These wave phenomena have been largely disregarded in computer graphics because of the high cost and complexity of computing them at audio rates. However, a new algorithm has been developed for real-time synthesis of sound radiation from rigid objects. This algorithm precomputes the linear vibration modes of an object and relates them to their sound pressure fields using numerical acoustics. Low-order multi-pole sources are then used to approximate each transfer function, with a low-memory and multilevel randomized algorithm for optimized placement. At runtime, the algorithm can quickly sum contributions from each mode's equivalent sources to simulate different interaction sounds and global effects. This allows for efficient and dynamic control over simulation costs and sound quality. The algorithm has been successfully demonstrated in various physically based animations.
41833	4183338	Breaking up is hard to do: An evaluation of automated assume-guarantee reasoning	Finite-state verification techniques can be hindered by the state-explosion problem, where the number of states in a system grows exponentially. Assume-guarantee reasoning is a proposed approach to mitigate this issue by breaking down a system into smaller subsystems and analyzing them individually. This allows for a more efficient and cost-effective verification process. However, assumptions about the environment in which a subsystem operates must be provided, which has traditionally been a difficult manual task. Recent advances in automatically generating these assumptions have prompted a study to compare assume-guarantee reasoning with monolithic verification. The results showed that in only a few cases, assume-guarantee reasoning was able to verify larger systems than monolithic verification, highlighting the need for further research and experimental evaluation in this area.
41833	418331	Efficient composite data flow analysis applied to concurrent programs	FLAVERS is a tool that verifies properties of concurrent systems using composite data flow analysis. While it is one of the few techniques that can handle large scale systems, it can still be expensive to use. This paper compares the cost of two versions of FLAVERS for solving composite data flow analysis problems. The first version, product-based, is more straightforward while the second, tuple-based, reduces analysis space requirements at the expense of analysis time. Through experiments on actual concurrent programs, it is shown that the tuple-based version is comparable in time to the product-based version but requires significantly less space for large composite data flow problems.
41834	4183492	The right algorithm at the right time: comparing data flow analysis algorithms for finite state verification	Finite state verification is becoming increasingly important in proving properties about software. In this process, analysts go through different modes of operation, such as exploratory, fault finding, and maintenance. During the exploratory mode, inconsistencies are often found due to flaws in the properties or the software itself. This leads to the analyst shifting to a fault finding mode, where they need counter example traces to determine the cause of the inconsistency. As the system becomes more stable, maintenance mode is entered and re-verification is needed to ensure consistency. It is believed that having different algorithms optimized for each mode of use would be most beneficial for the analyst.
41834	418345	Modeling and analyzing faults to improve election process robustness	This paper presents a method for improving the robustness of election processes through continuous process improvement. Using the Little-JIL process definition language, a detailed model of the election process is created. From this model, a fault tree is automatically generated to identify potential failures that could lead to undesirable events. Through fault tree analysis, combinations of failures are identified and the process model is iteratively improved to increase its resilience. The approach is demonstrated on the Yolo County election process, specifically focusing on the ballot counting process and addressing identified single points of failure. Proposed process modifications successfully remove these single points of failure.
41835	4183548	Scalable fault tolerant protocol for parallel runtime environments	The use of high performance computing platforms is increasing in order to tackle larger and more complex problems. This has led to a need for parallel runtime environments that can adapt to changing hardware and library requirements. This paper focuses on the design of a scalable and fault tolerant protocol for communication within these environments. The protocol supports message transmission across multiple nodes and includes a self-healing topology to prevent failures. Formal protocol verification has been conducted for both normal and failure cases. Multiple routing algorithms have been implemented, with the rule-based variant being the most effective for damaged or incomplete topologies.
41835	4183569	Reliability Analysis of Self-Healing Network using Discrete-Event Simulation	The use of high performance computing platforms is increasing to handle larger and more complex problems, but with more components comes a higher chance of failure. Therefore, software scalability and fault-tolerance are crucial in this field. Reliability analysis, specifically using discrete-event simulation, is necessary to ensure software reliability in the event of failure. In this study, the reliability of a self-healing network designed for parallel runtime environments is analyzed using discrete-event simulation. The network is designed to support message transmission and protect against failures. The results demonstrate the effectiveness of using discrete-event simulation to study network behavior and different parameters under failure conditions.
41836	4183669	Identifying redundancy and exposing provenance in crowdsourced data analysis.	The presented system allows analysts to use paid crowd workers to explore data sets and build upon their insights. Crowd workers can easily perform basic analysis tasks, but may generate redundant explanations. To efficiently use the crowd's work, analysts must be able to identify and consolidate redundant responses and determine the most plausible explanations. The paper demonstrates various techniques to assist analysts in using crowdsourced explanations, such as utilizing multiple workers to detect redundancies and capturing explanation provenance through tasks and an interface for filtering and selecting the most plausible explanations.
41836	4183687	Tools for placing cuts and transitions in interview video	Our tools aim to assist editors in placing cuts and creating transitions in interview videos. The interface provides a text transcript linked to the raw footage, allowing for easy identification and deletion of unwanted segments. The system also analyzes the audio and visual features to suggest suitable cut locations. Hidden transitions are generated using a graph-based algorithm that considers the specific features of interview footage. A new data-driven technique is used to set the timing of these transitions. Additionally, the tools offer a one-click method for removing filler words and inserting natural pauses. These tools have been successfully applied to edit various interviews and can also be used to quickly combine multiple takes of an actor's narration.
41837	418375	On Regulatory and Organizational Constraints in Visualization Design and Evaluation.	Problem-based visualization research offers clear guidance for understanding and designing for user needs, but lacks guidance on other external factors that can impact visualization design and evaluation. This gap can leave researchers and practitioners vulnerable to unexpected constraints that may affect the validity of evaluations or result in project failure. This article focuses on two types of external constraints, regulatory and organizational, and discusses how they can impact visualization design and evaluation. The authors suggest strategies for identifying, mitigating, and evaluating these constraints using a design study methodology, and provide a healthcare case study as an example. By incorporating external constraints into visualization design and evaluation, researchers and practitioners can improve the effectiveness and success of their solutions, especially in industries where these constraints are prevalent.
41837	4183754	Matches, Mismatches, and Methods: Multiple-View Workflows for Energy Portfolio Analysis	The analysis and monitoring of energy performance in large building portfolios is a difficult task due to limitations in current analysis tools. To address this issue, a visualization design study was conducted, taking into account the specific data and tasks involved. The study resulted in generalizable design choices for representing time-oriented data and considerations for workflow design. These designs address scalability, view coordination, and the limitations of line charts for derived and aggregated data. The study also provides guidelines for familiarity and trust in visualizations and highlights the importance of methodological considerations in design studies. The designs were successfully implemented into an energy analysis software application that will be used by thousands of energy workers. 
41838	4183837	A process calculus for Mobile Ad Hoc Networks	The @w-calculus is a process calculus used to formally model and reason about Mobile Ad Hoc Wireless Networks (MANETs) and their protocols. It accurately captures key features of MANETs, such as the ability of nodes to broadcast messages within their transmission range and move in and out of other nodes' transmission range. The @w-calculus separates a node's communication and computational behavior from its physical transmission range. The calculus has a formal operational semantics and is a conservative extension of the @p-calculus. It also has a symbolic semantics and a corresponding notion of symbolic bisimulation equivalence. The @w-calculus is useful for developing and analyzing formal models of MANET protocols.
41838	4183810	Local Model Checking and Protocol Analysis	This paper discusses a model-checking algorithm for a specific fragment of the modal mu-calculus that has been implemented in the Concurrency Factory. It also examines the application of this algorithm to a real-time communication protocol, RETHER, developed at SUNY Stony Brook. The results of the model-checking show that RETHER successfully provides guaranteed bandwidth to real-time nodes without affecting non-real-time nodes. The algorithm also identified an alternative design for RETHER with potentially lower run-time overhead, which was also verified through local model checking. This highlights the benefits of formal verification in uncovering interesting design alternatives.
41839	4183956	Digital stereo edges from zero crossing of second directional derivatives	The facet model is used for step edge detection. It involves analyzing the pixel values in a neighborhood and interpreting them relative to the underlying gray tone intensity surface. A pixel is marked as a step edge if there is a negatively sloped zero crossing of the second directional derivative in the direction of a nonzero gradient at the pixel's center. To determine this, the underlying gray tone intensity surface is estimated using a functional form with discrete orthogonal polynomials up to degree three. This allows for easy computation of the appropriate directional derivatives. Compared to other operators, the zero crossing of the second directional derivative performs the best for edge detection, followed by the Prewitt gradient operator, and the Marr-Hildreth zero crossing of the Laplacian operator performs the worst.
41839	41839114	Shape estimation from topographic primal sketch	This paper explores a method for estimating the shape of an object using topographic labeling. This labeling indicates the peaks, pits, ridges, valleys, and other surface features of a gray tone image of a three-dimensional object. By analyzing the patterns of these labels, information about the object and its illumination can be captured. The feasibility of using this technique is tested through analytical and experimental studies on images of mathematically generated surfaces with varying illumination directions. The results demonstrate the potential for accurately determining three-dimensional shape from two-dimensional images. Additionally, a proposed scheme for partially classifying the object's surface is presented, with preliminary results shown.
41840	4184039	A Methodology and Toolkit for Deploying Reliable Security Policies in Critical Infrastructures.	The rapid development of Information and Communication Technologies (ICT) has led to the emergence of intelligent and autonomous systems in critical infrastructures. These systems are being integrated into various domains such as transportation, communication, finance, commerce, and healthcare through different types of networks. However, ensuring the security and protection of sensitive data and services in these critical infrastructures is a major challenge. To address this issue, the authors propose a methodology for deploying and monitoring trusted access control policies. This methodology incorporates formal and semiformal techniques to specify, verify, implement, reverse-engineer, validate, assess risk, and optimize access control policies. To facilitate this process, the authors introduce their system SVIRVRO. An example is provided to demonstrate the effectiveness of this methodology.
41840	4184026	An Ontology Regulating Privacy Oriented Access Controls.	Access Control is a crucial aspect of data protection in today's open and complex environments, such as the Internet and cloud computing. With the growing concern for privacy and data protection regulations, it is important for access control models and languages to incorporate these requirements. However, few proposals have addressed this issue. This paper suggests using a formal ontology approach to incorporate privacy compliance in access control policies. This approach aims to simplify the expression of legal requirements and facilitate their automation and enforcement during execution. This includes addressing the interoperability of different types of information and referencing text laws in a novel way. 
41841	418411	On the Differential Geometry of 3D Flow Patterns: Generalized Helicoids and Diffusion MRI Analysis	The article discusses the use of dense locally parallel 3D curves in medical imaging, computer vision, and graphics. These curves can be found in various structures such as white matter fibre tracts, textures, fur, and hair. The authors propose a differential geometric approach to characterize these structures by examining the behavior of the associated 3D frame field and its curvature functions. They also introduce the use of a generalized helicoid model as an osculating object and establish a connection between its parameters and the curvature functions. This allows for the creation of parametrized 3D vector fields that can approximate these patterns. The approach is applied to the analysis of diffusion MRI data, showcasing the benefits of considering the full differential geometry.
41841	4184145	Geometric heat equation and nonlinear diffusion of shapes and images	Visual tasks often require a hierarchical representation of shapes and images in different scales. Various smoothing techniques have been proposed, such as Gaussian smoothing and anisotropic diffusion, resulting in scalespace representations. A new method based on local curvature for shapes and images is proposed. This method, known as curvature deformation smoothing, is a form of the reaction-diffusion framework and has desirable properties such as preserving inclusion order, eliminating extrema and inflection points, and decreasing total curvature. It is also applicable to images by viewing them as iso-intensity level sets and reassembling them after smoothing. This approach has been successfully implemented and demonstrated on various types of images. The mathematical soundness of this method has been justified through recent results, and a generalization has extended the entropy scale space for shapes to one for images.
41842	4184278	The Hamilton-Jacobi Skeleton	The eikonal equation and its variations are important in computer vision and image processing, as it is used for continuous versions of mathematical morphology, stereo, and shape-from-shading. Its numerical simulation can be challenging due to the formation of singularities, but there are classical approaches from Hamiltonian physics that have not been explored in computer vision. This paper presents a new algorithm for simulating the eikonal equation that has advantages in tracking shocks, as well as an efficient method for detecting shocks by measuring outward flux and applying an energy conservation principle. Several numerical examples, including complex shape skeletons, are provided to illustrate the approach.
41842	4184260	Hamilton-Jacobi Skeletons	The eikonal equation and its variations are important in computer vision and image processing. It is used in mathematical morphology, stereo, shape-from-shading, and dynamic theories of shape. However, simulating it can be difficult due to the formation of singularities, and most methods rely on level set methods. In this paper, the authors review a Hamiltonian formulation of the eikonal equation which has advantages in detecting singularities. They apply this formulation to Blum's grassfire flow and use a measure of the outward flux to distinguish between singular and non-singular points. This measure is combined with a homotopy preserving thinning process, resulting in a robust and efficient algorithm for computing skeletons in 2D and 3D. Several examples are provided to illustrate the approach.
41843	4184340	Tubular Surface Segmentation for Extracting Anatomical Structures From Medical Imagery	This work presents a model and algorithm for automatically extracting tubular structures from medical imagery. The model is able to fit various anatomical structures, such as fiber bundles in the brain and blood vessels in computed tomography angiograms. The extraction of these structures has potential benefits in the diagnosis of diseases such as schizophrenia and cardiovascular issues. The proposed tubular model has advantages over existing methods, including fewer degrees of freedom and a more efficient representation in 4-D. The algorithm can also detect and evolve branches of tubular trees. Testing on datasets shows promising results when compared to expert segmentations. 
41843	4184389	Finsler active contours.	This paper discusses a new method for segmenting images using directional information within the conformal active contour framework. By introducing a scalar conformal factor based on image data, the method is able to more accurately identify points of interest, such as edges. This factor is chosen based on position and is considered isotropic. The paper shows that this approach, when combined with a Finsler metric, results in a well-defined minimization problem that can be solved using calculus of variations or dynamic programming. The effectiveness of this technique is demonstrated through examples of extracting roads, blood vessels, and neural tracts from different types of imagery.
41844	41844131	Visual Attention Accelerated Vehicle Detection in Low-Altitude Airborne Video of Urban Environment	The airborne vehicle detection system aims to reduce collisions and traffic congestion caused by the increasing number of vehicles. Unlike stationary systems, airborne systems mounted on unmanned aircrafts or satellites have a wider view angle and higher mobility. However, detecting vehicles in aerial videos is challenging due to complex scenes and platform movement. To address this, a new two-stage method is proposed, consisting of attention focus extraction and vehicle classification. The first contribution is a fast attention focus extraction algorithm that detects candidate vehicle regions for quicker computation. The second contribution is a simple and efficient hierarchical classification process using the AdaBoost learning algorithm to reduce false alarms. Experimental results show that this method outperforms other algorithms in terms of detection rate and false positive rate while meeting real-time application requirements.
41844	4184435	Rapid pedestrian detection in unseen scenes	The paper presents a new rapid adaptive method for pedestrian detection using a cascade classifier with ternary pattern. The method utilizes three strategies: dynamic adjustment of key parameters, efficient optimization based on prior knowledge, and the use of ternary detection pattern to increase speed. The experiments were conducted on two datasets, AHHF and NICTA, and showed that the proposed method outperforms existing methods in quickly adapting to new scenes for pedestrian detection.
41845	4184524	Does JavaScript software embrace classes?	JavaScript is the dominant programming language for the Web and is used to create various applications such as mail clients and office applications. These applications can contain hundreds of thousands of lines of code and rely on informal class abstractions to manage complexity. However, there has been little research on how these class abstractions are used in JavaScript. To address this gap, a study was conducted on 50 popular JavaScript applications from GitHub. The study identified four types of JavaScript software: class-free, class-aware, class-friendly, and class-oriented. These categories represent varying levels of usage of classes, with class-oriented systems being the least common at 8%. Understanding how these classes are used is important for improving programming environments and structure libraries to better support developers.
41845	4184522	Creating sophisticated development tools with OmniBrowser	Smalltalk is a popular object-oriented programming language that is also known for its comprehensive integrated development environment. While the default tools are sufficient for coding and application development, it can be challenging to customize the environment for new language constructs or create new tools for code navigation. This paper introduces the OmniBrowser, a browser framework that allows for the creation of custom browsers using an explicit metamodel. The framework is implemented in the Squeak Smalltalk environment and has been used to create various browsers, such as a system browser, coverage browser, Duo Browser, and dynamic protocols browser. This paper discusses the framework's features and provides examples of its applications. 
41846	418469	Personalized Web search for improving retrieval effectiveness	Current Web search engines are designed to cater to the needs of all users, without considering the specific requirements of individual users. Personalization of Web search involves customizing the search results based on the interests of each user. A new method is proposed to learn user profiles from their search histories, which are then used to enhance the effectiveness of Web search. The technique involves learning a user profile from their search history and a general profile from a category hierarchy. These profiles are combined to map the user's query to a set of relevant categories, providing context to disambiguate the query words. The search is then conducted using both the user's query and the set of categories. Various algorithms for profile learning, category mapping, and fusion are evaluated, showing that this personalized approach is both effective and efficient.
41846	4184634	Personalized web search by mapping user queries to categories	Web search engines are designed to cater to the general needs of all users, rather than individual users. However, personalization of web search can improve results by taking into account a user's specific interests. To achieve this, a new technique has been proposed that maps a user's query to a set of categories, representing their search intent. This helps to disambiguate words in the query and improve the accuracy of results. The technique involves learning a user profile from their search history and a general profile from a category hierarchy, and combining them to map the query to categories. Several algorithms were evaluated, with the Rocchio-based method being chosen for its simplicity, efficiency, and adaptability. Experimental results show that this personalized approach to web search is both effective and efficient.
41847	418474	A Multi-View Embedding Space for Modeling Internet Images, Tags, and Their Semantics	This paper addresses the issue of modeling Internet images and associated text or tags for various tasks such as image-to-image search, tag-to-image search, and image annotation. The authors utilize canonical correlation analysis (CCA) to map visual and textual features to a latent space and incorporate a third view that captures high-level image semantics. They present two methods for training this three-view embedding: supervised, using ground-truth labels or search keywords, and unsupervised, using semantic themes obtained through tag clustering. To ensure accurate retrieval while maintaining scalability, multiple visual features are combined and explicit nonlinear kernel mappings are used. A specially designed similarity function is used for retrieval, which outperforms the traditional Euclidean distance. The proposed system shows promising results and outperforms baseline methods on large-scale Internet image datasets.
41847	4184740	Scalable near identical image and shot detection	This paper presents two new techniques for detecting near duplicate images and video shots. The first method uses global hierarchical colour histograms and Locality Sensitive Hashing for quick retrieval. The second method utilizes local feature descriptors (SIFT) and approximates set intersections between documents using a min-Hash algorithm. The authors address two definitions of near duplicates: those that are perceptually identical and those that are images of the same 3D scene. They also define near-duplicate shots as those with a high percentage of shared frames. The focus is on scalability for large databases, with both methods requiring minimal data storage. Results are demonstrated using the TRECVID 2006 data set and other videos.
41848	4184834	Approximation algorithms for metric facility location and k-Median problems using the primal-dual schema and Lagrangian relaxation	This article discusses approximation algorithms for two common optimization problems in facility location: the metric uncapacitated facility location problem and the metric k-median problem. These algorithms have a low running time of O(m logm) and O(m logm(L + log (n))) respectively, where m and n are the total number of edges and vertices in a complete bipartite graph of cities and facilities. The main techniques used are a new approach to the primal-dual schema and the application of Lagrangian relaxation to develop efficient approximation algorithms. These algorithms have guarantees of 3 and 6 for the uncapacitated facility location problem and the k-median problem respectively.
41848	4184855	Primal-Dual Approximation Algorithms for Metric Facility Location and k-Median Problems	Our paper introduces new approximation algorithms for two common optimization problems, the uncapacitated facility location problem and the k-median problem. Our algorithms have guarantees of 3 and 6 for these problems, respectively, and are notable for their efficient running time. The algorithms have a time complexity of O(m log m) and O(m log m (L + log(n))), where m and n are the number of vertices and edges in the graph. The key concept behind our algorithms is a novel extension of the primal-dual schema. 
41849	4184969	Secure Overlay Network Design	As Internet security threats continue to increase, new overlay network architectures have been proposed to secure privileged services. These architectures involve a defense perimeter where only servelets are allowed to pass, and end users must be authorized to communicate with access points (APs). The goal is to minimize the number of APs that could be compromised by an attacker. In this paper, the authors present a polynomial-time algorithm for designing the connections between APs and servelets in the average-case model, and also show that a star-shaped design is optimal when the probability of AP failure is at least 1/2. In the worst-case model, they use combinatorial set theory to provide bounds on the number of APs that a perfectly failure-resistant design can support. These results provide a theoretical foundation for practical secure overlay network design. 
41849	4184932	Packing Steiner trees	The Steiner packing problem involves finding the most edge-disjoint subgraphs in a given graph G that connect a set of required points S. This problem is important in VLSI-layout, broadcasting, and has theoretical significance. The paper presents an algorithm with an approximation factor of &verbar;S&verbar;/4 and shows that this is the best possible when there are only 3 terminals. The existence of k edge-disjoint Steiner trees can be determined based on the edge-connectivity of the graph. The fractional version of the problem can be solved using the ellipsoid algorithm, which reduces it to the minimum Steiner tree problem. 
41850	4185058	Allocating online advertisement space with unreliable estimates	The article discusses the problem of optimally allocating online ad space to advertisers with limited budgets. The objective is to find an algorithm that can efficiently use estimated keyword frequencies to make near optimal decisions, while also maintaining a good worst-case competitive ratio. This is important in real-world situations where search engines have stochastic information that can accurately estimate search query frequencies, but may also have unpredictable spikes. The approach is a black-box one, using an oracle to recommend advertisers based on the estimates. The algorithm provides two performance guarantees, which can be adjusted by a parameter, and is applied to two online problems.
41850	4185013	Approximation Algorithms for Computing Maximin Share Allocations.	The study focuses on finding fair allocations for a group of agents and a set of goods, where each agent is guaranteed a certain share of goods. This maximin share is determined by allowing agents to divide the goods into bundles and receiving their least desirable bundle. While these allocations may not always exist, the study proposes a 2/3-approximation algorithm that runs in polynomial time for any number of agents and goods. This improves upon a previous algorithm that only works for a constant number of agents. The new algorithm uses a bipartite graph representation and a probabilistic analysis to achieve better results. Additionally, the study provides positive results for two special cases involving three agents and a limited range of item values.
41851	41851134	Spanners in sparse graphs	The concept of a t-spanner in a graph is introduced, where the distance between any two vertices is at most t times their distance in the original graph. If the t-spanner is required to be a tree, it is called a tree t-spanner. In 1998, Fekete and Kremer proved that the problem of determining if a planar graph has a tree t-spanner is polynomial time solvable for t ≤ 3, but NP-complete for larger t values. They also posed the question of whether the problem is polynomial time solvable for every fixed t ≥ 4. In this work, the authors resolve this open problem and extend the solution to a larger class of sparse graphs. They show that the problem is fixed parameter tractable for planar graphs of bounded treewidth, as well as for a more general class of graphs called apex-minor-free graphs. However, the problem remains NP-complete for K6-minor-free graphs, indicating that the restriction to apex-minor-free graphs is necessary for tractability.
41851	41851132	Kernels for (Connected) Dominating Set on Graphs with Excluded Topological Minors.	This article discusses the development of linear kernels for the Dominating Set and Connected Dominating Set problems on graphs that exclude a fixed graph H as a topological minor. The authors prove the existence of polynomial time algorithms that can output a smaller, H-topological-minor-free graph that has a (connected) dominating set of size k if and only if the original graph has one. This extends the known classes of graphs where these problems admit linear kernels. The authors also introduce a new concept called "protrusions" which aids in the development of these linear kernels. This new approach to protrusions may have applications in other graph problems and algorithmic settings.
41852	4185263	Random classification noise defeats all convex potential boosters	Boosting algorithms, such as AdaBoost and LogitBoost, can be seen as using gradient descent to minimize a potential function based on the margins of a data set. However, a recent study has shown that these algorithms are highly affected by random classification noise. This means that even with techniques like early stopping or weight regularization, these boosters cannot accurately learn a data set with a nonzero random noise rate. This is in contrast with other boosting algorithms that do not fall into the convex potential function framework and can still achieve high accuracy in the presence of random noise. 
41852	4185273	Boosting the Area under the ROC Curve	 with independent noise, there isThe article discusses the concept of boosting weak rankers to achieve a high area under the ROC curve (AUC). It shows that any weak ranker that performs slightly better than random guessing can be boosted to achieve an AUC close to 1. This can be done even in the presence of independent misclassification noise, as long as the weak ranker is noise-tolerant. The article also mentions related work, such as RankBoost and noise-tolerant boosting for classification. Overall, the article highlights the effectiveness of boosting weak rankers to improve AUC in various scenarios.
41853	4185347	Mining the Student Assessment Data: Lessons Drawn from a Small Scale Case Study	This paper discusses a case study on educational data mining (EDM) using data from an online assessment where students received tailored feedback after each question. The focus is on analyzing the effectiveness of the questions and feedback in meeting individual student needs. Despite using traditional data mining techniques such as clustering, classification, and association analysis, it was difficult to obtain meaningful and insightful results due to the small dataset and well-defined problems. This case study highlights the challenges in applying DM to educational data and the need for more advanced approaches. 
41853	4185311	Using mobile and web-based computerized tests to evaluate university students	Mobile learning and testing is a growing trend in education, and this article compares the use of mobile devices for testing to traditional web-based assessment systems. The authors also introduce an authoring tool that allows for adaptable and adaptive computerized tests to be used on a variety of devices, including personal computers, personal digital assistants, and mobile phones. An experiment was conducted with computer science university students, and the results showed that the students were highly motivated and enjoyed using the mobile application for testing. Additionally, there were no significant differences in the results between the different versions of the test. This study was published in Wiley InterScience and can be accessed online.
41854	4185448	Applying SAT Solving in Classification of Finite Algebras	The classification of mathematical structures is important for pure mathematics research, but it is a meticulous task. Automated techniques can aid in this process, with many focusing on the quantitative side such as counting isomorphism classes. However, a bootstrapping algorithm has been developed for qualitative classification, producing theorems that describe unique properties for isomorphism classes. To fully verify the classification, a range of problems must be proven, which can be challenging for traditional automated theorem provers. This paper discusses the use of Boolean satisfiability solving to generate fully verified classification theorems in finite algebra, and presents various methods for encoding these problems to improve the bootstrapping algorithm. Experimental evidence shows the effectiveness of these methods.
41854	4185436	Algebraic Theory Exploration: A Comparison of Technologies	The article discusses a case study comparing different tools for exploring theories in finite algebra. The goal is to compare the performance of model generators, constraint solvers, and satisfiability solvers in generating large algebraic structures with specific properties, such as quasigroups. The study also experiments with techniques that further constrain the problems by pre-computing additional information. Two specific constructions are used, one based on randomization and one based on pre-computing knowledge. Results show that these approaches can increase the solvability horizon and reduce the time needed to find solutions for algebraic structures with desired properties. The study provides insights into the selection of suitable systems for utilizing these techniques. 
41855	4185575	Composite model-checking: verification with type-specific symbolic representations	Automated verification methods based on state exploration have made significant progress in areas such as hardware design, improving testing and validation processes. Symbolic model-checking, which uses compact data structures like Binary Decision Diagrams (BDDs) to check for safety and liveness properties, has been successful in this field. However, these techniques have not been as successful in software systems due to limitations in dealing with infinite-state programs and efficiently representing different variable types. A new approach has been proposed that combines BDD and arithmetic constraint representations to overcome these limitations. This composite model-checking strategy can be extended to other symbolic representations and also includes approximation techniques to address the undecidability of model-checking on infinite-state systems. This approach has been demonstrated to be effective in analyzing software specifications with a mix of booleans, integers, and enumerated types. 
41855	4185573	Compatibility Checking for Asynchronously Communicating Software.	Compatibility is a major concern when building new software using existing components. A set of components is considered compatible if they maintain certain properties, such as being deadlock-free, when combined. However, checking compatibility for systems that use asynchronous communication is a complex and undecidable problem. Asynchronous communication is widely used in software systems, but traditional methods for analyzing them involve limiting the state space. In this paper, a new approach is presented that does not have any limits on the number of participants or message buffer sizes. Instead, it relies on the synchronizability property, which allows for checking compatibility by analyzing a finite part of the system's behavior. A prototype tool has been developed to automate this approach and it has been successfully applied to various examples.
41856	4185613	Unifying Justifications and Debugging for Answer-Set Programs.	Viegas Damasio et al. (2013) introduced a method to construct propositional formulas for logic programs that encode provenance information. These formulas can be used to extract justifications for a given interpretation, but do not provide information on why the interpretation is not an answer-set. In contrast, Gebser et al. (2008) use meta-programming transformations to debug logic programs and identify necessary changes for an interpretation to be an answer-set. The authors propose a unified approach that combines these two methods, using meta-programming transformations to generate answer-set programs that can compute provenance and debugging models directly. This approach is more efficient and generalizes the debugging one, as any error can be traced back to a provenance but not the other way around. The authors developed a proof-of-concept tool to demonstrate the effectiveness of their approach.
41856	418564	Generalizing Modular Logic Programs.	Modularity has been extensively studied in conventional logic programming, but there are limited approaches on how to incorporate it into Answer Set Programming (ASP), a rule-based declarative programming paradigm. One notable approach is Oikarinnen and Janhunen's Gaifman-Shapiro-style architecture, which allows for the composition of program modules and strengthens the splitting set theorem for normal logic programs. However, this approach is limited by restrictive module conditions that are necessary for compatibility with the stable model semantics. This paper presents alternative ways to lift these restrictions, making the framework more applicable and expanding the scope of the module theorem.
41857	4185725	Argumentation based decision making for autonomous agents	This paper proposes an argumentation based framework to help agents make decisions within a modular architecture. The framework is dynamic and takes into account the context in which the agent is operating, allowing for adaptability in a changing environment. To address incomplete information in open environments, abduction has been integrated into the framework. This is especially useful in situations where the agent is facing a dilemma and needs additional information to make a decision. Additionally, a personality theory for agents has been developed within the same framework, adding an element of individuality to the decision making process based on principles from Cognitive Psychology.
41857	4185745	The KGP Model of Agency	The KGP model is a new approach to agency, drawing from the BDI model. It proposes a hierarchical agent architecture with a modular structure to handle reasoning and sensing in a dynamic environment. The model uses Computational Logic (CL) for formal analysis and computational realization, and allows for the independent development of different components of an agent. It also offers context-sensitive control through a CL theory component, allowing agents to adapt to their current circumstances. A prototype of KGP agents has been created, demonstrating the model's potential for designing heterogeneous agents. This model breaks away from conventional one-size-fits-all control of operation and offers greater flexibility in agent design.
41858	4185822	On the Pedagogically Guided Paper Recommendation for an Evolving Web-Based Learning System	This paper discusses a unique recommender system for suggesting papers in a web-based learning environment. The system is based on the observation of learners and their behaviors, allowing the most suitable papers to survive and be recommended. It also introduces a pedagogically layered similarity between read and recommended papers, taking into consideration both interest and pedagogical suitability. The third aspect is the annotation of papers with temporal sequences of learners' behaviors, which helps maintain objectivity and integrity while providing deeper insights into their knowledge levels and enabling 'just-in-time' recommendations for e-learning support.
41858	4185836	Beyond Learners' Interest: Personalized Paper Recommendation Based on Their Pedagogical Features for an e-Learning System	In this paper, the authors discuss the challenges of delivering personalized recommendations for e-learning, which differs from other domains such as e-commerce and news. While user interest is important, pedagogical features like learning goals and background knowledge must also be considered. To address this issue, the authors propose two techniques: model-based and hybrid recommendations, using a pedagogy-oriented similarity measurement. They conducted an experiment with artificial learners and found that the hybrid collaborative filtering technique was effective in reducing computational costs without sacrificing overall performance. They also note that as more learners participate in the learning process, the recommendation system can be continuously updated, making it beneficial for web-based learning systems.
41859	4185981	Investigation of the Social Predictors of Competitive Behavior and the Moderating Effect of Culture.	Research has found that Competition is a strong motivator for changing behavior, but there is limited understanding of its effectiveness and how it is impacted by culture. This study examined the predictors of Competition using three social influence measures: Reward, Social Comparison, and Social Learning. The results showed that Reward has the greatest influence on Competition, followed by Social Comparison, but Social Learning has no significant impact. Culture was also not found to moderate the relationship between these constructs. This suggests that designers of gamified applications can use Reward, Social Comparison, and Competition together to motivate behavior change in both individualistic and collectivistic cultures. 
41859	4185928	Investigation Of Social Predictors Of Competitive Behavior In Persuasive Technology	Research has found that Competition is a powerful motivator for users to engage in a desired behavior in a social setting. However, there is a lack of research on what factors contribute to the persuasiveness of Competition as a motivational strategy in technology. Using a sample of 213 Canadians, a study tested a model that included three socially influential strategies (Social Learning, Social Comparison, and Reward) as predictors of Competition. The results showed that Reward is the strongest predictor of Competition, followed by Social Comparison, but Social Learning is not a significant predictor. Additionally, Social Comparison was found to mediate the influence of Reward on Social Learning and Competition. These findings suggest that designers of persuasive technology can use Reward, Social Comparison, and Competition as effective strategies to increase user engagement.
41860	4186015	Combining Online Algorithms for Acceptance and Rejection	 derive a combined algorithmResource allocation and admission control are essential tasks in communication networks, often needing to be performed in real-time. These problems are typically approached under either a benefit or cost model, seeking to maximize accepted requests or minimize rejected requests, respectively. However, algorithms designed for these different objectives can be vastly different. This study aims to combine algorithms from both models to achieve good results for both objectives. The proposed approach involves using an algorithm with a competitive ratio of cA for accepted requests and cR for rejected requests, resulting in a combined algorithm with a competitive ratio of O(cRcA) for rejection and O(cA) for acceptance. This method can also be extended to a collection of k algorithms.
41860	4186023	Bandits with Movement Costs and Adaptive Pricing.	The article introduces an extension of the Multi-armed Bandit model that considers a cost for switching between actions and incorporates a metric between the actions. The metric is modeled as a complete binary tree, with the distance between two leaves being the size of the subtree of their least common ancestor. This abstracts the scenario where the actions are points on a continuous interval and the switching cost is their distance. A new algorithm is proposed that achieves a regret rate of $widetilde{O}(sqrt{kT} + T/k)$, where $k$ is the number of actions and $T$ is the time horizon. This algorithm can also be applied to bandit learning with Lipschitz loss functions, achieving an optimal regret rate of $widetilde{Theta}(T^{2/3})$. The main application of this algorithm is in an adaptive pricing problem, where a single seller faces a stream of patient buyers with private values and buying windows. With an appropriate discretization of prices, the seller can achieve a regret of $widetilde{O}(T^{2/3})$, outperforming the previous regret bound of $widetilde{O}(T^{3/4})$.
41861	41861126	On profiling blogs with representative entries	This paper explores the challenge of finding relevant blogs in a rapidly growing blogosphere. To address this issue, the authors propose a new problem of profiling a blog by selecting a set of representative entries. This allows for more efficient and accurate searching and matching, as it avoids dealing with the large number of frequently updated and noisy entries in a blog. The proposed entry selection algorithm is guided by three principles - anomaly, representativeness, and diversity - and is found to be highly efficient and effective in classification accuracy. By using fewer than 20 representative entries, comparable accuracy can be achieved compared to using all entries. 
41861	4186178	Online Feature Selection and Its Applications	Feature selection is a crucial aspect of data mining, but most studies focus on batch learning methods. However, online learning has emerged as a more efficient and scalable approach for large-scale applications. To address the issue of high dimensionality and expensive data acquisition, researchers have investigated online feature selection (OFS), where the learner can only access a small and fixed number of features. This presents a challenge of accurately predicting with a limited number of features, as opposed to using all features in traditional online learning. The article explores sparsity regularization and truncation techniques to solve this challenge, and presents novel algorithms for learning with both full and partial input. These algorithms are evaluated on public datasets and applied to real-world problems with promising results.
41862	4186227	Stochastic Convex Optimization with Multiple Objectives.	If you want to change your name on a website or platform, you can use the "Report an Issue" link to make the request. This link is usually located in the support or help section of the website. By clicking on it, you can send a message to the website's customer service team explaining the reason for your name change and providing any necessary documentation. They will review your request and make the necessary changes if approved. This is a simple and efficient way to request a name change on a website or platform.
41862	4186292	Mixed Optimization for Smooth Functions.	If you need to change your name on a platform or website, you can use the "Report an Issue" link to make the request. This link is usually found on the website's help or support page. By clicking on the link, you can submit a request to change your name and provide any necessary documentation or information. Depending on the platform, the name change may need to be approved by the website's team before it is updated. Using the "Report an Issue" link is a simple and efficient way to request a name change without having to go through multiple steps or contacting customer service. 
41863	41863239	A neural network approach to smarter sensor networks for water quality monitoring.	Environmental monitoring is becoming more advanced with the use of large-scale and affordable sensor networks that can operate reliably and autonomously for extended periods of time. However, sophisticated analytical instruments like chemo-bio sensors have limitations in terms of the number of samples they can take. To overcome this, multiple sources of information can be coordinated to maximize their deployment lifetime. In a study conducted at the River Lee in Cork, Ireland, a neural network was used to control the sampling frequency of a phosphate analyzer based on inputs from rainfall radar images and a water depth sensor. The results showed that this approach can effectively improve the efficiency of the sensor network, even with limited training data.
41863	41863248	Sensor node localisation using a stereo camera rig	This paper discusses the use of stereo vision processing techniques to detect and locate sensors in an experimental sensor network. The sensors communicate with a camera through LED patterns, allowing for low-cost and low-power communication. The goal is to use this technology to monitor environmental events using existing CCTV infrastructure. To support this research, a controlled environment was created to simulate pollution events and test the accuracy of the sensor localization. The paper demonstrates the advantages of using a stereo camera rig over a traditional 2D CCTV camera for 3D spatial localization of sensors.
41864	4186476	Estimating Mixture Models via Mixtures of Polynomials.	Mixture modeling is a technique that combines simple models with weighting to make them more expressive. The Expectation Maximization (EM) algorithm is commonly used to update and improve mixture models, but it does not guarantee global convergence due to the non-convex nature of the likelihood function. Method of moments approaches have been developed to provide global guarantees for some mixture models, but they are not applicable to all types of mixture models. In this study, a new framework called Polymom is introduced, which is based on the method of moments and allows for easy derivation of estimation procedures similar to EM. The framework uses a Generalized Moment Problem and solves it using semidefinite optimization and computer algebra. This approach allows for insights and tools from convex optimization and computer algebra to be applied to statistical estimation problems, with promising results shown in simulations. 
41864	418648	Transforming Question Answering Datasets Into Natural Language Inference Datasets.	This article discusses the importance of existing datasets for natural language inference (NLI) in advancing language understanding research. The authors propose a new method for creating NLI datasets by using a sentence transformation model to convert question-answer pairs into declarative statements. This model, trained on a single question answering dataset, can successfully apply to other resources to create a new dataset with over 500k NLI examples (QA-NLI). This dataset displays a diverse range of inference phenomena that are not typically seen in previous NLI datasets. Overall, this approach offers a valuable tool for generating large-scale NLI datasets from existing QA resources.
41865	4186536	Biasing evolving generations in learning classifier systems using information theoretic measures	This paper discusses the use of information-theoretic concepts to improve the evolution process in Learning Classifier Systems. By considering the potential information contained in individuals within each generation, the Sufficiency measure of a rule is used as an indicator of usefulness. This measure is then incorporated into the XCS algorithm during the early stages of each run to guide the selection process. Initial simulations demonstrate that this approach reduces the effort required to solve the 20-input multiplexer problem. Overall, the incorporation of information-theoretic ideas shows promise in improving the efficiency and effectiveness of Learning Classifier Systems.
41865	4186540	Information theoretic fitness measures for learning classifier systems	Genetic algorithms, specifically Learning Classifier Systems, benefit from using multiple fitness functions, including information theoretic measures, during evolution. These measures, such as Sufficiency and Quality, are derived from entropy and mutual information and can evaluate the fitness of rules against data. By incorporating these measures, it is possible to reduce the number of generations needed to achieve peak performance in the algorithm. This is demonstrated through experimental results of evaluating individual rules against data. Different methods of evaluating rules, both with and without using data, are discussed. Overall, incorporating information theoretic fitness measures can improve the effectiveness of genetic algorithms in achieving optimal performance.
41866	4186659	Inter-receiver fairness: a novel performance measure for multicast ABR sessions	In a multicast ABR service, connections are limited to the rate of the bottleneck link in the distribution tree, which can lead to unfairness among receivers with different preferred operating rates. To address this issue, a paper proposes allowing the connection to operate at a higher rate, taking into account each receiver's loss tolerance. A measure for inter-receiver fairness is developed and a technique for determining the optimal rate is presented. The paper also suggests using multiple virtual circuits for a single multicast session to improve fairness. Examples are provided to demonstrate the concept in various network scenarios.
41866	4186648	Selecting among replicated batching video-on-demand servers	A Video-on-Demand (VoD) service offers a variety of videos for customers to choose from, and designers aim to minimize access latency. One strategy involves serving multiple clients with the same video through one multicast stream, which saves server resources and network bandwidth. Another approach is VoD server replication, which allows for a larger number of clients at the cost of additional servers. However, this requires efficient server selection techniques to utilize the increased capacity effectively. This paper explores different selection algorithms for three batching methods: Batching with Persistent Channel Allocation, Patching, and Hierarchical Multicast Stream Merging (HMSM). The results show that combining server replication with appropriate selection techniques can improve the service's capacity and performance.
41867	41867220	The Asymptotics of Ranking Algorithms	The article discusses the problem of supervised ranking, where the goal is to rank sets of candidate items returned in response to queries. Traditional statistical methods for this task require complete rankings from individuals, which is not practical in real-world scenarios. As a result, researchers have focused on modeling partial preference data, such as pairwise comparisons of items. However, the article shows that commonly used methods for this approach are not consistent, even in low-noise settings. To address this issue, the authors propose a new approach based on aggregation of partial preferences and develop empirical risk minimization procedures. These procedures are shown to be consistent through theoretical analysis and experimental results.
41867	4186738	Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates	Classification algorithms such as support vector machines, boosting, and logistic regression use minimum contrast methods to minimize a convex surrogate of the 0-1 loss function. In this study, the authors examine the statistical implications of using such a surrogate by establishing a relationship between the risk assessed by the 0-1 loss and the risk assessed by any nonnegative surrogate loss function. This relationship is based on a variational transformation of the loss function and is applicable even under the weakest condition of Fisher consistency for classification. The authors also present a refined version of this result for low noise scenarios and showcase its application in estimating convergence rates for function classes.
41868	41868132	Efficient message passing interface (MPI) for parallel computing on clusters of workstations	Parallel computing on clusters of workstations and personal computers has great potential due to its utilization of existing hardware and software. Parallel programming environments provide an easy way to express parallel computation and communication, with a recent industrial standard being the Message Passing Interface (MPI). However, existing implementations on clusters suffer from poor performance in the collective communication part due to being built on top of a point-to-point communication layer. In this paper, the authors present an efficient design and implementation of the collective communication part in MPI, optimized for clusters of workstations. This system consists of an MPI-CCL layer and a User-Level Reliable Transport Protocol (URTP), integrated with the operating system via a kernel extension mechanism. Results from performance measurements on a collection of IBM RS/6000 workstations connected via a 10-Mbit Ethernet LAN show that the system's performance is limited by interactions between the kernel and user space rather than the LAN's bandwidth. 
41868	4186813	Accelerating Distributed Computing Applications Using a Network Offloading Framework	In the last 20 years, there has been a lot of research focused on distributed computing. These applications often require frequent network communication, which can slow down overall performance. With the rise of programmable peripheral devices for computers, it is possible to improve the performance of distributed applications by offloading network functions to these devices. However, programming these devices can be difficult and each one has its own unique features. To address this, a new offloading framework called HYDRA has been proposed. This framework allows developers to specify an "offloading layout" for their application, which is enforced during deployment. By using this framework, the performance of distributed algorithms can be greatly improved, as demonstrated by the evaluation of two applications: a distributed total message ordering algorithm and a packet generator.
41869	4186941	Synthesis of Petri Nets with Whole-Place Operations and Localities.	This paper discusses the use of synthesising systems from behavioural specifications as a means of creating implementations that are automatically correct and do not require expensive validation processes. The systems are modelled using Petri nets and the specifications are given in the form of step transition systems. The focus is on synthesising Petri nets with whole-place operations and localities, which can express a wide range of system behaviours such as inhibiting actions, resetting local states, and maximal executions. The synthesis problem has been solved for specific net classes and a general approach has been developed for tau-nets. This paper adapts the synthesis techniques for wpol-nets, based on the concept of a region of a transition system.
41869	4186940	Transition systems of elementary net systems with localities	This paper focuses on investigating transition systems for a type of Petri net called Elementary Net Systems with Localities (ENL-systems). These nets are used to model and analyze globally asynchronous locally synchronous systems, where events can only be executed simultaneously within certain designated groups. The authors introduce a new way of representing the behavior of ENL-systems using a step transition system, where arcs are labeled by sets of events executed concurrently. They also develop a method for characterizing the transition systems that can be generated by ENL-systems under their intended concurrency semantics. This involves defining io-regions, which contain additional information, and using them to establish key relationships between the Petri net and its transition system. The paper also presents two translations between ENL-systems and their transition systems, providing a solution to the synthesis problem of constructing an ENL-system for a given transition system.
41870	4187017	Parallel Shortest Path Algorithms for Solving Large-Scale Instances	This study explores the efficiency of the Delta-stepping parallel algorithm in solving the single source shortest path problem on large-scale graphs with non-negative edge weights. The performance of the algorithm is tested on the Cray MTA-2, a high-end shared memory system with features that aid in parallel implementation of irregular algorithms. The results show significant speedup compared to sequential algorithms for low-diameter sparse graphs. For example, the algorithm can solve a directed scale-free graph with 100 million vertices and 1 billion edges in less than ten seconds on 40 processors, with a speedup of close to 30. These are the first reported performance results for a shortest path problem on realistic graph instances with billions of vertices and edges.
41870	4187031	An Experimental Study of A Parallel Shortest Path Algorithm for Solving Large-Scale Graph Instances	`In this study, we use the A-stepping parallel algorithm to solve the single source shortest path problem with non-negative edge weights on large-scale graphs. Our experiments are conducted on the Cray MTA-2, a high-end shared memory system with features that aid parallel implementation of irregular algorithms. Our implementation shows significant speedup compared to sequential algorithms for low-diameter sparse graphs, such as a directed scale-free graph with 100 million vertices and 1 billion edges taking less than ten seconds on 40 processors. These results are the first of their kind for solving a shortest path problem on realistic graphs with billions of vertices and edges.
41871	418715	DANCo: Dimensionality from Angle and Norm Concentration	Recently, determining the intrinsic dimensionality of a dataset has become increasingly important. However, current methods are unreliable when the dataset has a high intrinsic dimensionality and is nonlinearly embedded in a higher dimensional space. To address this issue, a new robust estimator is proposed in this paper. It utilizes information from normalized nearest neighbor distances and angles between neighboring points, and also provides closed-forms for the Kullback-Leibler divergences of the respective distributions. Experiments on synthetic and real datasets demonstrate the effectiveness and robustness of this algorithm compared to existing methods. 
41871	4187126	IDEA: intrinsic dimension estimation algorithm	The complexity of real life signals makes common signal processing and pattern recognition methods impractical. Researchers have focused on developing algorithms for dimensionality reduction, aided by estimating the intrinsic dimensionality of a dataset - the minimum number of parameters needed to capture all the information. However, existing techniques struggle with noisy data or high intrinsic dimensionality. This paper presents a local intrinsic dimension estimator that utilizes the statistical properties of data neighborhoods. Evaluations on synthetic and real datasets, as well as comparisons with other algorithms, demonstrate the potential of this technique.
41872	4187214	Dialogue games that agents play within a society	This paper discusses the use of argumentation and dialogue games in artificial agent societies in order to manage conflicts and social influences. While there has been significant focus on developing theoretical models for multi-agent systems, there is a lack of understanding of the computational implications and systemic impact of using argumentation in agent societies. To address this, the paper presents a framework for argumentation-based negotiation (ABN) that takes into account the societal structure and provides agents with strategies for resolving conflicts. Experiments show that arguing can be beneficial for agent societies, particularly when resources are limited and agents have imperfect knowledge. Negotiating social influences can also improve agent performance.
41872	4187266	Arguing and negotiating in the presence of social influences	In a society where agents have incomplete information and conflicting influences, they may lack the knowledge, motivation, and capacity to fulfill their commitments. To function cohesively, it is important for these agents to have a way to resolve conflicts and reach a mutual understanding. Argumentation-based negotiation is an effective method for agents to resolve conflicts within a multi-agent society. However, agents need four fundamental capabilities to engage in this type of negotiation: a way to reason in a social context, a method to identify suitable arguments, a language and protocol for exchanging arguments, and decision-making skills to generate dialogues. This paper presents these capabilities and proposes a framework for agents to argue, negotiate, and resolve conflicts within a multi-agent society.
41873	4187344	An Anytime Algorithm for Optimal Coalition Structure Generation	Coalition formation is a crucial aspect of multi-agent systems, where autonomous agents come together to efficiently achieve their individual or collective goals. However, the problem of determining which coalitions to form in order to achieve a specific goal is challenging due to the large number of possible solutions. To address this, an anytime algorithm has been developed that uses a novel representation of the search space and a branch-and-bound technique to efficiently search through sub-spaces. This algorithm has been tested with a new input distribution and has been shown to outperform previous algorithms in terms of solution quality and efficiency. It is also capable of providing a guaranteed solution within a specific bound, even if interrupted before completing its search.
41873	4187320	Decentralized Coordination in RoboCup Rescue	Emergency responders face multiple challenges during major disasters. The number of tasks often exceeds the available resources and deadlines must be met for each task. New tasks may arise and existing ones may disappear, requiring quick adaptation. Forming teams or coalitions from different agencies is crucial for efficient disaster management. These coalitions must be selected and scheduled effectively to save lives and infrastructure. A decentralized approach is preferred to avoid system failures and conserve limited resources. The authors propose a decentralized solution to this coalition formation problem, using a distributed constraint optimization algorithm called Max-Sum. Their algorithm, F-Max-Sum, outperforms other decentralized algorithms in simulations. 
41874	4187434	Using constraints and process algebra for specification of first-class agent interaction protocols	Current approaches to multi-agent interaction involve defining protocols as a set of possible interactions and hard-coding decision-making processes into agent programs. This creates problems such as a strong coupling between agents and protocols, limitations on the protocols agents can use, and the inability to compose protocols at runtime. To fully utilize the potential of multi-agent systems, it is important to have protocols exist as first-class entities that can be inspected, referenced, composed, and shared. This paper proposes a framework, RASA, which treats protocols as first-class entities. It also presents a formal language for specifying agent interaction protocols as first-class entities, allowing for agents to reason about and compose more complex protocols at runtime. 
41874	4187449	On illegal composition of first-class agent interaction protocols	This paper discusses first-class protocols for multi-agent systems, which are executable specifications used by agents at runtime to follow a set of rules. The authors propose a framework called RASA, which allows for the specification of protocols using a formal, executable language and constraints. This enables agents to inspect the protocol at runtime and make decisions based on it, rather than relying on pre-defined decision making mechanisms. The paper explores the implications of protocol composition and presents conditions under which composing existing protocols may result in illegal protocols. The authors provide a precise definition of an illegal protocol and present proof obligations to ensure the legality of protocol compositions.
41875	4187581	Characterizing effective auction mechanisms: insights from the 2007 TAC market design competition	The article discusses the 2007 TAC Market Design competition and categorizes the entries. It then compares these entries and examines the relationship between market dynamics, auction rules, and adaptive strategies through post-tournament experiments. Based on this analysis, the paper speculates on the design of successful auction mechanisms, both in the specific competition and in a broader context.
41875	4187591	What the 2007 TAC Market Design Game tells us about effective auction mechanisms	The paper examines the participants of the 2007 tac Market Design Game and categorizes their entries. It then compares these entries and explores the relationship between market dynamics and the auction rules used. The paper also conducts experiments after the competition to study the adaptive strategies of the participants. Based on this analysis, the paper makes predictions about effective auction mechanisms for both the competition and other scenarios. Overall, the paper provides insight into the design of successful auction mechanisms.
41876	4187647	An Approach to Specifying Coordinated Agent Behaviour	Team oriented programming involves various methods for organizing teams of agents and coordinating their actions to reach specific objectives. The SimpleTeam framework is designed to facilitate this type of programming. It allows for the creation of team plans that outline the tasks of a group of agents or sub-teams working towards a common goal. The framework also includes tools for managing concurrency and handling exceptions. An example application is provided to demonstrate the benefits of this approach in simplifying the design of coordinated behavior in multi-agent systems.
41876	4187630	Coordination in Adaptive Organisations: Extending Shared Plans with Knowledge Cultivation	Agent-based simulation is a useful tool for investigating behavioral requirements, capabilities, and strategies in complex and dynamic situations. It can also be used for training purposes. The paper focuses on the coordination requirements in complex unfolding scenarios, where agents come and go and the organizational structure is constantly changing. The authors argue the need for an extension to the SharedPlans formalism to support the sharing of knowledge about the situation, including team composition and relevant knowledge. This need is supported by a case study of a railway accident and further analysis of coordination and communication among the disaster management team. The authors propose using knowledge cultivation processes to help agents reason about the dynamic organizational structure and changing world state. This requires access to explicit models of organizational knowledge, including team structure, shared beliefs, goals, and plans. A formal representation of this model is presented to illustrate the information needed in an adaptive organization.
41877	4187744	Equilibrium analysis in cake cutting	Cake cutting is an important concept in fair division, where a divisible good needs to be divided among agents with different preferences. The main criteria for fairness are proportionality and envy-freeness, and existing protocols aim to achieve these outcomes if all agents follow the protocol. However, it is not guaranteed that all agents will follow the protocol, leading to a need to study equilibria of cake cutting protocols. The Dubins-Spanier procedure, which ensures a proportional allocation, is studied in this paper, and it is shown that every envy-free allocation can be mapped to a pure Nash equilibrium in the corresponding moving knife game. Furthermore, every pure Nash equilibrium in the moving knife game results in an envy-free allocation of the cake. The game also has an epsilon-equilibrium that is both epsilon-envy-free and independent of the tie-breaking rule.
41877	4187728	Envy-Free Pricing in Multi-unit Markets.	This article discusses the issue of envy-free pricing in multi-unit markets with budgets. The goal is to find a pricing and allocation strategy that meets the demands of buyers within their budget constraints while also achieving a desirable objective. The study focuses on markets with linear valuations and considers scenarios where buyers are either price takers or price makers. A polynomial time algorithm is provided for computing the welfare maximizing envy-free pricing in the price taking scenario, followed by an FPTAS and exact algorithm for a constant number of buyer types. In the price taking model with strategic buyers, the article shows that it is generally impossible to design a strategyproof and efficient mechanism. However, an optimal strategyproof mechanism is provided for markets with common budgets. For markets with general valuations, hardness results are shown for computing envy-free pricing schemes that maximize revenue and welfare, along with fully polynomial time approximation schemes for both problems.
41878	41878107	Inter-organization networks: implications of access control: requirements for interconnection protocol	Inter-Organization Networks (IONs) are formed when two or more distinct organizations connect their internal computer networks. These networks facilitate the exchange of data and resources between organizations, such as cad/cam data between manufacturers and subcontractors, software distribution from vendors to users, and customer input to suppliers' order-entry systems. This paper examines the technical implications of interconnecting networks across organization boundaries. It argues that traditional network design criteria of connectivity and transparency are not sufficient for IONs, as access control is a primary high-level requirement. This requires a scheme based on non-discretionary control to allow organizations to limit connectivity and make network boundaries visible. New interconnection protocols are needed to support these access control requirements, and message-based gateways are a promising solution.
41878	4187839	Controls for Interorganization Networks	Interorganization computer networks facilitate communication between individuals through electronic mail and allow for the exchange of data, software, and documents through file transfer. These networks also allow for input to be made into order-entry or accounting systems through a database query and update protocol. Additionally, shared computational resources can be accessed using an asynchronous message protocol or remote login. However, in these networks, organizations typically limit the resources that are accessible to outsiders and have specific usage-control requirements due to the potential risks associated with external users. This highlights the unique challenges and considerations involved in managing Interorganization Networks (ION's).
41879	4187917	An Integrated View on Rules and Principles	The differences between rules and principles in the field of law have long been acknowledged, with rules appearing to directly lead to their conclusion while principles provide reasons for their conclusions. This has led to a debate over whether rules and principles should be strictly distinguished based on their logical structure. Dworkin argues for a strict distinction, while others, such as Soeteman, disagree. The authors of this paper propose an integrated view in which rules and principles have the same logical structure but behave differently in reasoning. They support this view with a formalization using Reason-Based Logic and provide three ways of reconstructing reasoning by analogy based on their integrated view.
41879	4187944	Dao: a benchmark game	Dao is a compact and engaging game that can be solved quickly on a computer. Despite its small size, its game graph can be stored in internal memory. However, the number of nodes in the game graph is large enough to allow for in-depth analysis. It falls between simple games like Tic-Tac-Toe and Do-Guti and more complex games like Connect-Four and Awari, which are solvable but have too many nodes to fit in memory. This paper delves into the properties of Dao and its solution, highlighting its potential as a benchmark for search enhancements. The authors also provide an example of using Dao to test the size of transposition tables in α-β search.
41880	4188013	Design and analysis of an asymmetric mutation operator	Evolutionary algorithms are a type of randomized search heuristics that use fitness to guide their search. However, in practical applications, problem-specific knowledge can be used to improve the search process. For problems with bit strings of finite length, good solutions tend to have either very few one-bits or very few zero-bits. To address this, a specific mutation operator has been developed and analyzed. This operator is designed to be effective in these situations, but it also has some limitations. Analytical results have been obtained for both specific examples and general function classes to understand the strengths and weaknesses of this mutation operator. 
41880	418809	Comparing global and local mutations on bit strings	Evolutionary algorithms commonly use a global mutation where each bit in a string is flipped independently with a fixed probability. This approach is difficult to analyze, so researchers have explored a local operator that only flips one randomly chosen bit. While most known results suggest that the global approach is at least as efficient as the local one, a recent study found that there is no general rule for when this is true. Under favorable conditions, the local operator can find the optimal solution in polynomial time, while the global operator may take exponentially longer from most starting points.
41881	4188158	Sum-Product Networks: A New Deep Architecture	The complexity of the partition function is a major challenge in graphical model inference and learning. To address this, researchers have developed a new type of deep architecture called sum-product networks (SPNs). These networks are directed acyclic graphs that use variables, sums, and products as nodes. If an SPN is complete and consistent, it can represent the partition function and all marginals of a graphical model. Learning algorithms for SPNs have been proposed using backpropagation and EM, and experiments have shown that they can outperform standard deep networks in terms of speed and accuracy. Additionally, SPNs have potential connections to the structure of the cortex. 
41881	4188138	Discriminative Learning of Sum-Product Networks.	Sum-product networks (SPNs) are a type of deep neural network that can efficiently handle complex models and perform accurate inference. While previous methods for training SPNs have been generative, this paper introduces the first discriminative training algorithms for SPNs. This approach combines the accuracy of generative methods with the representational power and tractability of discriminative ones. It is shown that discriminative SPNs have a wider range of tractable models than generative SPNs. An efficient backpropagation-style algorithm is proposed for computing the gradient of the conditional log likelihood. This paper also addresses the diffusion problem in standard gradient descent by using a "hard" gradient descent approach. Discriminative SPNs are tested on image classification tasks, achieving the best results on the CIFAR-10 dataset with fewer features compared to previous methods. The highest published test accuracy on STL-10 is also reported, using only the labeled portion of the dataset.
41882	4188233	Precoding and Power Optimization in Cell-Free Massive MIMO Systems.	Cell-free Massive multiple-input multiple-output (MIMO) is a wireless communication system that uses a large number of small, low-cost, and low-power access points (APs) connected to a central network controller. This system does not have traditional cellular boundaries and each user is served by all APs at the same time. The basic linear precoding schemes used in this system are conjugate beamforming, zero-forcing, and maximum ratio transmission. This technology has the potential to significantly increase spectral efficiency and enhance coverage and capacity in wireless networks.
41882	4188223	Cell-Free Massive MIMO: Uniformly great service for everyone	Cell-Free Massive MIMO systems use a large number of distributed access points to serve a smaller number of users. Each access point uses local channel estimates to apply conjugate beamforming and transmit data to the users. A closed-form expression for the achievable rate is derived, allowing for the design of an optimal max-min power control scheme to give equal quality of service to all users. The performance of Cell-Free Massive MIMO is compared to a conventional small-cell network, showing that the median throughput is much higher and the system is more robust to shadow fading correlation. The Cell-Free system can provide almost 20 times higher 95%-likely per-user throughput compared to the small-cell system. 
41883	4188311	Autonomous Background Coordination Technology for Timely Sensor Connection in Wireless Sensor Networks	Wireless sensor networks (WSNs) are becoming increasingly popular for collecting data in various military and civil applications. However, in an indoor factory monitoring environment, the constant need for sensor relocation and addition to accommodate changes in production lines can affect the scalability of the network due to limitations of the wireless nodes. To address this issue, a two-tier autonomous decentralized architecture has been proposed, with sensors forming the first layer and routers forming the second layer. The routers form communities to coordinate and share information, allowing for efficient switching of sensor loads and timely expansion of sensors. Simulation results demonstrate that this background coordination technology improves sensor connection time and increases the number of connected sensors in the network.
41883	4188345	Autonomous online expansion technology for wireless sensor network based manufacturing system	Wireless sensor networks (WSNs) are used for data collection in various monitoring environments. In manufacturing systems, sensor addition, relocation, and reorganization are necessary for production line changes, which can lead to high sensor density in certain areas. This can cause capacity constraints and prevent new sensors from connecting to the network. To address this issue, a two-layer autonomous decentralized heterogeneous architecture is proposed, with sensors in the first layer and routers in the second. The routers form communities to switch connected sensors in high density areas to other routers in low density areas, allowing new sensors to join. The community can also expand or shrink as needed. Simulation results demonstrate the effectiveness of this approach.
41884	4188479	Extracting Places and Activities from GPS Traces Using Hierarchical Conditional Random Fields	This paper discusses the importance of learning human behavior patterns from sensor data for accurate activity inference. It proposes a system that uses hierarchically structured conditional random fields to extract a person's activities and significant places from GPS data. Unlike existing techniques, this approach considers high-level context to detect significant places. Experiments show significant improvements and the system is able to accurately estimate a person's activities using data from other individuals. This highlights the robustness and effectiveness of the proposed system in understanding human behavior.
41884	4188417	Location-based activity recognition	Extracting and labeling human activities and significant places from GPS data is crucial for understanding behavior. Our system uses relational Markov networks to represent the hierarchical activity model, considering the context and complex relations between GPS readings, activities, and significant places. FFT-based message passing enables efficient summation over large numbers of nodes. Compared to existing techniques, our approach simultaneously detects and classifies significant locations, resulting in significant improvements.
41885	4188576	Model counting: a new strategy for obtaining good bounds	Model counting is a problem in computer science where we need to calculate the number of solutions to a given propositional formula. It is a more general version of the NP-complete problem of propositional satisfiability, making it both useful and difficult to solve. However, a new approach has been developed that involves adding carefully chosen constraints to the formula in order to reduce the solution space. These constraints are randomly chosen XOR or parity constraints and have been proven to provide accurate model counts with high probability. This method has been shown to be effective for solving difficult combinatorial problems and provides high-confidence guarantees on the quality of the results.
41885	4188592	Short XORs for model counting: from theory to practice	A new method for accurately counting the number of solutions to Boolean formulas was introduced, but it required the use of large random XOR or parity constraints. In reality, shorter XOR constraints are preferred for better performance in SAT solvers. However, through experiments, it has been shown that for certain problem domains, using very short XOR constraints can produce results with the same level of accuracy as using larger constraints. This suggests that the structural properties of the problem may play a role in the effectiveness of different constraint sizes. 
41886	4188646	Rollup: Non-Disruptive Rolling Upgrade with Fast Consensus-Based Dynamic Reconfigurations.	Rolling upgrade is a process of upgrading servers in a distributed system in a progressive manner to minimize service downtime. This requires a well-designed protocol for cluster membership to ensure the system's availability during the upgrade. Existing protocols, such as CoreOS etcd, rely on a primary server for both reconfiguration and data storage, which can cause disruptions when the primary needs to be replaced. The proposed protocol, called Rollup, uses a candidate leader only for reconfiguration and scalable biquorums for service requests, resulting in a fast and non-disruptive upgrade. While Rollup does not provide a complete coordination service, it offers an 8-fold speedup compared to protocols relying on a primary for reconfiguration. This has been demonstrated through theoretical analysis and experiments on physical and virtual machine clusters. 
41886	4188625	Providing view synchrony for group communication services	Building applications in a distributed computing environment is a complex process that involves considering various factors such as message ordering, system failures, and link failures. To make this task easier, group communication services offer building blocks and define properties that are guaranteed by these services. One such property is VS (view synchrony), which ensures that processors sharing the same view of the distributed environment receive the same set of messages or prefixes of the set received by one processor. This paper presents an algorithm, based on the ideas of Cristian and Schmuck, and proves that it satisfies the VS property. The contributions of this paper are demonstrating the feasibility of implementing VS and providing an algorithm for its implementation. It also presents conditional timing properties for group formation under certain stability conditions. 
41887	4188745	Semantics for null extended nested relations	The nested relational model is an extension of the flat relational model that allows for complex objects to be modeled by relaxing the first normal form assumption. Previous research on this model has focused on defining data structures and query language, while integrity constraints have been characterized by subclasses and normal forms. This paper introduces null extended data dependencies to define the semantics of nested relations with null values. A generic null value is used to formalize incomplete information, and a preorder is defined to measure the information content of nested relations. The extended chase procedure is developed to test and infer from null extended data dependencies, and it is shown to generalize the classical chase procedure. This approach also allows for the capture of losslessness in nested relations and extends the semantics of flat relations.
41887	418872	Null inclusion dependencies in relational databases	Functional dependencies and inclusion dependencies are important integrity constraints in relational databases. However, in cases where the database is incomplete and contains null values, additional constraints called null inclusion dependencies are needed. The implication problem for null inclusion dependencies is equivalent to that of inclusion dependencies. A complete and sound axiom system has been developed for null functional dependencies and null inclusion dependencies, and it has been proven that the implication problem for these constraints is decidable and EXPTIME-complete. This is in contrast to the undecidability of the implication problem when there are no null values. This has led researchers to focus on functional dependencies and noncircular inclusion dependencies, which have been shown to have an EXPTIME-complete implication problem. These findings suggest that null inclusion dependencies do not need to be assumed to be noncircular in relational database design.
41888	4188869	Improved equilibria via public service advertising	 guarantees any improvement.The paper discusses the concept of "public service advertising campaigns" as a means to encourage players in natural games to follow low-cost equilibria. However, the assumption that everyone will follow the advice is unrealistic. Instead, the paper considers the question of how effective such campaigns can be if only a fraction of the population actually follows the advice, and only temporarily. The analysis is done for various types of games, including network design, scheduling, and party affiliation games. The results show that for some games, a random fraction of the population following the advice can lead to a significant improvement, while for others, there is a strict threshold that must be met. For certain games, no fraction guarantees any improvement. 
41888	4188820	Reducing mechanism design to algorithm design via machine learning	The article discusses how techniques from machine learning can be used to solve incentive-compatible mechanism design problems in revenue-maximizing pricing situations. The authors propose a reduction method that allows for converting an optimal algorithm for an algorithmic pricing problem into a (1+@e)-approximation for the incentive-compatible mechanism design problem, as long as the number of bidders is sufficiently large. This method is applied to various scenarios, such as auctioning a digital good, discriminatory pricing problems, and item-pricing in combinatorial auctions. The authors also introduce a new type of covering-number bound to address challenges such as discontinuity and asymmetry in the loss function. 
41889	4188967	The Second Rewrite Engines Competition	The Second Rewrite Engines Competition (REC) took place during the 7th Workshop on Rewriting Logic and its Applications (WRLA 2008). Five systems, ASF+SDF, Maude, Stratego/XT, TermWare, and Tom, participated in the competition. The organization and conduct of the competition are discussed and the main results and conclusions are presented.
41889	4188938	Prototyping The Semantics Of A Dsl Using Asf+Sdf: Link To Formal Verification Of Dsl Models	A formal definition of the semantics of a domain-specific language (DSL) is necessary for verifying the correctness of models and transformations used in the DSL. To achieve this, a prototype of the DSL for concurrent, communicating object systems was created. This prototype transforms DSL models into labeled transition systems (LTS), allowing for the use of existing visualization and verification tools with minimal effort. The prototype was implemented using the ASF+SDF Meta-Environment, an IDE for the algebraic specification language ASF+SDF, which enables efficient execution and the ability to read models and produce LTSs without any additional steps.
41890	4189049	SIMULTANEOUS REGISTRATION, SEGMENTATION AND MODELLING OF STRUCTURE IN GROUPS OF MEDICAL IMAGES	In this article, the authors propose an algorithm for extracting information from groups of medical images of the same anatomy. The algorithm simultaneously segments and registers the images, creating a model of their structure and correspondences. This framework models the tissue fractions rather than the expected intensity in each voxel, allowing for decoupling from imaging details. The algorithm compares the current image to a reconstructed image generated from the model tissue fractions and current intensity distributions for each tissue type. The algorithm is described in detail and results are presented from its application to a set of brain MR images.
41890	418909	Combining Local and Global Shape Models for Deformable Object Matching	The article presents a new method for modelling and locating deformable objects by combining global and local shape models. The object is represented as a set of patches with a geometric model of their positions, using a global pose and linear shape model. A Markov Random Field (MRF) model is also used to account for local variations. The process involves an alternating scheme where the MRF selects the best candidates for each point, updating the global pose and shape model. A cascade of increasingly complex models is used for accurate matching to new images. The method is compared to other widely used methods and is shown to achieve better accuracy on standard datasets.
41891	4189154	Automatic interpretation and coding of face images using flexible models	Face images are complex and challenging to interpret due to their high variability. This variability is caused by factors such as individual appearance, 3D pose, facial expression, and lighting. To address this issue, a compact parametrized model of facial appearance has been developed. This model takes into account all sources of variability and includes both shape and gray-level appearance. It is created by analyzing a training set of face images and can be used for tasks like image coding, person identification, 3D pose recovery, gender recognition, and expression recognition. A robust multiresolution search algorithm is used to fit the model to new face images, allowing for the accurate location of facial features and recovery of shape and appearance parameters. This model has been tested on a database of 690 face images with varying 3D pose, lighting, and expression, and has shown good performance in all tasks.
41891	4189151	Combining Local and Global Shape Models for Deformable Object Matching	The authors propose a new method for modelling and locating deformable objects using a combination of global and local shape models. The method involves representing an object as a set of patches with a global pose and linear shape model, along with a Markov Random Field (MRF) model for local displacements. The matching process involves an alternating scheme where MRF inference is used to select the best candidates for each point, which updates the global pose and shape model. This approach is shown to be more accurate than other commonly used methods on standard datasets, and a cascade of increasingly complex models is used to improve performance. The authors also analyze the impact of different model parameters on system performance.
41892	4189260	Timeliness Evaluation of Intermittent Mobile Connectivity over Pub/Sub Systems.	Mobile systems often have intermittent connectivity and asynchronous data transfer, making it challenging to guarantee timely communication between mobile users. To address this, Queueing Network Models (QNMs) can be used to model system performance and provide accurate solutions for metrics such as response time. In this paper, the authors propose a model for end-to-end response time in mobile systems using QNMs and a publish/subscribe middleware. This model incorporates intermittent connectivity by modeling user connections/disconnections with an ON/OFF queueing system. The model is validated through simulations with real-world workload data, showing a deviation of less than 5% from actual performance. This approach can be used to improve the QoS of mobile systems in various application scenarios.
41892	4189272	Queueing Network Modeling Patterns For Reliable And Unreliable Publish/Subscribe Protocols	Mobile IoT applications are often deployed on devices with limited resources and unreliable network connectivity. To support these applications, a Publish/Subscribe interaction paradigm is commonly used, which separates peers in time and space. Middleware protocols and APIs are designed to consider hardware limitations and offer Quality of Service (QoS) features to enable effective application development. However, with the abundance of pub/sub protocols and intermittent connectivity, tuning these applications can be challenging. In this paper, the performance of pub/sub middleware protocols used in IoT is modeled, taking into account reliability and QoS features such as data validity and buffer capacities. This analysis shows the significant impact of QoS on successful interactions, with examples in Traffic Information Management systems. The resulting PerfMP pattern can be customized for different deployments to control QoS policies.
41893	418934	Efficient Subwindow Search: A Branch and Bound Framework for Object Localization	Object recognition systems typically only determine if an object is present or not, but do not provide information on its location. To estimate the location, a sliding window approach can be used, but this is computationally expensive. A new branch and bound scheme is proposed in this paper that efficiently maximizes a variety of quality functions over all possible subimages. This method is faster than exhaustive or sliding window search and allows the use of previously considered too slow classifiers for localization. The resulting systems achieve state-of-the-art performance on various data sets and competitions. 
41893	4189327	An efficient divide-and-conquer cascade for nonlinear object detection.	This article introduces a faster method for evaluating object detection cascades using a divide-and-conquer approach. This method requires fewer evaluations of classifier functions compared to the current state-of-the-art method, resulting in a faster search. The method also incorporates the efficient subwindow search procedure to further improve its speed. This method can be used not only for faster cascade evaluation, but also for efficient branch-and-bound object detection using kernelized support vector machines. Experiments on the PASCAL VOC 2006 dataset demonstrate a more than 50% acceleration compared to standard cascade evaluation. 
41894	4189448	Multiresolution modeling and visualization of volume data based on simplicial complexes	A new method for representing and visualizing scattered volumetric datasets is proposed. These datasets are a sample of a scalar field defined over a three-dimensional domain, which can be visualized as a hypersurface in four dimensions. The method uses a multiresolution model, where the domain is split into tetrahedra and the scalar field is approximated with increasing precision. This is achieved through an adaptive incremental approach based on the coherence of the scalar field. This model allows for efficient extraction of isosurfaces at different levels of resolution, as well as progressive and multiresolution rendering. The method is evaluated on various scattered datasets with positive results.
41894	4189449	Multiresolution Representation and Visualization of Volume Data	The system presented in this content allows for the representation and visualization of scalar volume data at multiple resolutions. It is based on a multiresolution model using tetrahedral meshes with scattered vertices that can be derived from any initial dataset. The model is created through data simplification techniques and stored in a compact data structure for quick access. The system offers interactive visualization at any desired level of resolution through isosurface and projective methods. Users can adjust the quality of visualization to meet the needs of a specific task and hardware platform. Different resolution representations can also be combined to improve performance and interaction through progressive and multiresolution rendering.
41895	4189532	Parallel mining of closed sequential patterns	Sequential pattern mining is an important data mining task with many practical applications. The closed sequential pattern, which contains all the information of the complete pattern set but is more compact, is the most useful variation. However, there is currently no parallel method for mining closed sequential patterns. In this paper, the authors propose a new algorithm, called Par-CSP, for parallel mining of closed sequential patterns on a distributed memory system. Par-CSP uses dynamic scheduling and selective sampling to minimize interprocessor communication and address load imbalance. Experiments on a 64-node Linux cluster showed that Par-CSP performs well on different datasets. 
41895	4189519	A parallel numerical solver using hierarchically tiled arrays	Solving linear systems is a key problem in scientific computing, and parallelization is crucial for handling complex systems. This usually involves writing parallel algorithms on top of a library like MPI. The SPIKE family of algorithms is a well-known parallel solver for linear systems. The Hierarchically Tiled Array (HTA) data type enhances traditional data-parallel array operations by allowing programmers to manipulate tiles directly. The HTA's higher level of abstraction makes it portable across different platforms, and current implementations target both shared-memory and distributed-memory models. This paper presents a proof-of-concept for portable linear solvers, implementing two SPIKE algorithms using the HTA library. The results show that the HTA's abstractions allow for compact, efficient code that can run on different models without modification. Performance is compared to MPI and state-of-the-art linear algebra routines.
41896	4189633	The software model checker Blast	Blast is a tool used for verifying temporal safety properties in C programs. It can either prove that a program satisfies a safety property or provide an execution path that violates it. This is achieved through the use of lazy predicate abstraction and interpolation-based predicate discovery. The paper discusses how Blast has been applied in two case studies - one for statically proving memory safety and the other for generating test suites that guarantee coverage of a specific predicate. The tool has been shown to provide automated, precise, and scalable analysis for C programs.
41896	4189648	The BLAST software verification system	Blast is a verification system that checks the safety properties of C programs. It uses a lazy-abstraction algorithm to integrate automatic abstraction refinement and model checking. The input to Blast is a C program and a safety monitor written in a specification language similar to C. The algorithm can either return an error trace and corresponding test case, or a proof that the program satisfies the safety property. Blast uses interpolation-based decision making and counterexample analysis to construct and refine a parsimonious predicate abstraction of the program. It has successfully verified and identified violations in interface safety, memory safety, race conditions, and file handling properties of large programs. Blast also supports program testing and incremental programming. It can be accessed at http://www.eecs.berkeley.edu/blast.
41897	4189749	Compositional Shape Analysis by Means of Bi-Abduction	The article discusses the challenges of accurately and efficiently analyzing mutable data structures in automatic program verification and introduces shape analysis as a solution. Shape analysis uses a compositional method where each procedure is analyzed independently, and utilizes a restricted form of separation logic to over-approximate data structure usage. The method is based on a generalized form of abduction, called bi-abduction, which jointly infers missing portions of state and untouched portions of state. The article reports on case studies and implementation of the analysis algorithm, showcasing its potential for automation and scalability. The article also makes technical contributions in proof procedures and analysis algorithms, but its main contribution is demonstrating the increased automation possible with abductive inference.
41897	4189747	Program analysis for overlaid data structures	This paper discusses overlaid data structures, which are data structures that contain links for multiple data structures and are used simultaneously. The authors present a static program analysis for these structures, implementing two main ideas. The first is to use sub-analyses to track information about non-overlaid data structures, which are later combined to infer safety properties about the entire overlaid structure. The second idea is to control communication among the sub-analyses using ghost states and instructions to improve efficiency by only transferring necessary information at key points in the program. This analysis has been successfully applied to prove the memory safety of two real-world programs.
41898	4189898	Decidability of inferring inductive invariants.	Induction is a commonly used method for verifying the correctness of hardware and software systems. This involves modeling the system using logical formulas and then using a decision procedure to check if a logical formula serves as an inductive safety invariant for the system. The main challenge in this approach is determining the inductive invariant, also known as invariant inference. This paper focuses on the problem of inferring first-order inductive invariants by restricting the language of candidate invariants. The authors present cases where this problem can be solved in infinite languages, as long as certain restrictions are applied and a suitable well-quasi-order is defined. They also show that there are some cases where this is not possible, and propose a framework for constructing infinite languages while ensuring the decidability of the invariant inference problem. The paper provides examples of applying this approach to programs that manipulate linked-lists and distributed protocols.
41898	4189827	Paxos made EPR: decidable reasoning about distributed protocols	Distributed protocols, such as Paxos, are crucial in many computer systems, but a bug in these protocols can have significant consequences. Efforts have been made to verify these protocols, but it is difficult to check invariants due to the large number of nodes and messages involved. This paper proposes a methodology for automatic verification using effectively propositional logic, a decidable fragment of first-order logic. This approach allows for the identification of bugs and verification of correct protocols. The methodology involves modeling the protocols in general first-order logic and systematically transforming the model to obtain a decidable model and inductive invariant. This approach has been successfully used to verify the safety of Paxos and its variants, including Multi-Paxos, Vertical Paxos, Fast Paxos, Flexible Paxos, and Stoppable Paxos. This is the first work to verify these protocols using a decidable logic and the first formal verification of Vertical Paxos, Fast Paxos, and Stoppable Paxos.
41899	4189962	Relational access to Unix kernel data structures.	State of the art kernel diagnostic tools such as DTrace and Systemtap provide a procedural interface for analyzing kernel tasks. However, this work argues that a relational interface to kernel data structures can offer additional benefits for kernel diagnostics. The Pico COllections Query Library (PiCO QL) Linux kernel module uses a domain specific language to define a relational representation of accessible Linux kernel data structures. This includes a parser to analyze the definitions and a compiler to implement an SQL interface for querying the data structures. Unlike other tools, PiCO QL does not require kernel instrumentation and imposes no overhead when idle. It only accesses relevant kernel data structures to answer the input queries. Examples of Linux kernel queries are provided to showcase PiCO QL's usefulness in identifying system resources, security vulnerabilities, and performance issues.
41899	4189919	How to Train Your Browser: Preventing XSS Attacks Using Contextual Script Fingerprints.	Cross-Site Scripting (XSS) is a common vulnerability in web applications, often referred to as the "buffer overflow of the web." To prevent this type of attack, a script whitelisting approach is proposed. This involves a transparent script interception layer in the browser's JavaScript engine that compares every script to a list of valid ones for the specific site or page being accessed. This method uses contextual fingerprints, which are identifiers for specific elements of a script and its execution context, to avoid false positives. The list of valid scripts can be managed by the website's administrators or a trusted third party. Testing has shown the proposed solution to be effective against real-world attacks, with a minimal browsing performance overhead. However, when script elements are altered or new scripts are added, a new fingerprint generation phase is needed. Short- and long-term experiments have been conducted to study the feasibility of this approach.
41900	41900258	Deep Features For Text Spotting	The aim of this project is to identify text in natural images. This is achieved through two steps: detecting word regions in the image and recognizing the words within these regions. The authors propose a Convolutional Neural Network (CNN) that can perform both tasks efficiently. The CNN has a unique architecture that allows for feature sharing and outperforms previous methods in text detection, character classification, and bigram classification. The authors also make technical changes to traditional CNN architectures, including no downsampling and multi-mode learning. They also present a method for automatically generating annotations from Flickr data. All these components are combined to create a state-of-the-art text spotting system, which is evaluated on two standard benchmarks and shows improvement over previous methods.
41900	41900259	Multiple kernels for object detection	The goal of this research is to create a top-performing object detector by using a state-of-the-art image classifier to search for objects in various image sub-windows. They use multiple kernel learning to find the best combination of exponential kernels, each capturing different features such as edges, visual words, and feature descriptors. To make the classifier more efficient, they propose a three-stage approach that combines linear, quasi-linear, and non-linear SVMs. They also compare three methods for proposing candidate regions and introduce overlap-recall curves to evaluate and optimize performance. The method is evaluated on the PASCAL Visual Object Detection Challenge and outperforms previous methods for most classes.
41901	41901194	Online Spatial Event Forecasting in Microblogs.	Event forecasting using social media data streams has numerous practical applications. While existing methods can accurately predict temporal events like elections and sports, they struggle with forecasting spatiotemporal events such as civil unrest and influenza outbreaks. To address this challenge, the authors propose novel batch and online approaches for spatiotemporal event forecasting on platforms like Twitter. These models take into account the evolving spatial features and their correlations over time. The proposed inference algorithms optimize the model parameters, and a dynamic programming technique is used to calculate the alignment likelihood of tweet sequences. The effectiveness of the approach is demonstrated through experiments in two different domains.
41901	41901119	Multi-Task Learning for Spatio-Temporal Event Forecasting	Spatial event forecasting from social media is a challenging task due to the dynamic nature of features and geographic heterogeneity. Existing approaches such as LASSO regression, dynamic query expansion, and burst detection address some of these challenges but not all. This paper proposes a multi-task learning framework that simultaneously addresses all the challenges by building forecasting models for multiple locations at once. The framework combines static features from a predefined vocabulary with dynamic features from query expansion and balances homogeneity and diversity between them. Efficient algorithms are used for model training and prediction. Experiments on Twitter data from four Latin American countries show the effectiveness of this approach.
41902	4190232	Decoupling function and anatomy in atlases of functional connectivity patterns: Language mapping in tumor patients.	This paper discusses the construction of an atlas that summarizes the functional connectivity characteristics of a cognitive process in a population of individuals. The atlas uses a low-dimensional embedding space derived from a diffusion process on a graph of fMRI time course correlations. It is independent of the anatomical space and can represent functional networks with varying spatial distribution in a population. The atlas is represented by a common prior distribution for all subjects. An algorithm is developed for fitting this model to observed data. The method is successfully applied to a language fMRI study, identifying coherent and functionally equivalent regions across subjects. It is also able to map functional networks from a healthy population to individuals with language network disruptions caused by tumors.
41902	4190241	Learning an atlas of a cognitive process in its functional geometry.	 This paper presents a method for creating an atlas that captures the functional characteristics of a cognitive process, using data from multiple individuals. The atlas is based on a low-dimensional embedding space derived from a diffusion process on a graph of fMRI correlations. It is represented by a common prior distribution for all subjects, and is not directly linked to anatomical space, allowing for variability in functional networks. The authors also provide an algorithm for fitting this model to population data. The results of a language fMRI study showed that this method successfully identified consistent and equivalent regions across subjects. 
41903	4190358	Data Lifecycle Challenges in Production Machine Learning: A Survey.	Machine learning is a crucial tool for extracting knowledge from data and solving complex tasks. However, the accuracy of a machine learned model is heavily influenced by the quality of the data it is trained on. To address this, there is a need for robust processes and tools to analyze, validate, and transform data in large-scale machine learning systems. Based on Google's experience in developing a production machine learning platform, this article discusses the challenges in data management and explores relevant literature from the data management and machine learning communities. The three main areas of focus are data understanding, data validation and cleaning, and data preparation, with an emphasis on the constraints that arise depending on when and by whom the problems are encountered in the model's lifecycle.
41903	4190341	Scaling Datalog for Machine Learning on Big Data	This paper argues for a declarative approach to building data-intensive machine learning systems. Instead of creating new systems for each type of machine learning task, the authors propose using recursive queries to program a variety of systems. This allows for the use of database query optimization techniques and a single data-parallel processing engine. The authors demonstrate this approach by showing how two programming models from the machine learning domain can be captured in Datalog and compiled into optimized physical plans. Experiments on real data show that this approach offers good performance, increased generality, and easier programming. 
41904	419042	On a class of vertices of the core.	For supermodular TU-games, the vertices of the core are the marginal vectors. This also applies to games with a distributive lattice of feasible coalitions, which are determined by a hierarchy among players. A broader class of core vertices, called min-max vertices, can be found by minimizing or maximizing the coordinates of a core element in a given order. A simple formula, applicable for connected hierarchies and under certain restrictions, can be used to calculate these vertices without solving an optimization problem. The conditions for two different orders to produce the same vertex for all games are identified, and it is shown that balanced games with more than four players have core vertices that are not min-max vertices.
41904	4190433	A value for bi-cooperative games	Bi-cooperative games extend the concept of traditional cooperative games by allowing players to participate positively, negatively, or not at all. This paper suggests a method for determining the share of wealth obtained by players based on their participation choices. It is found that the cost allocation rule does not consider a player's contribution when they choose to participate negatively. The value of this approach is illustrated through various examples.
41905	419051	Quasi- and pseudo-inverses of monotone functions, and the construction of t-norms	The article discusses properties of quasi- and pseudo-inverses of non-decreasing real functions. A method for constructing a commutative, fully ordered semigroup on the unit interval is presented, using a given triangular norm T and non-decreasing function f. A similar construction using the pseudo-inverse results in an operation bounded by the minimum, but may violate associativity. Sufficient conditions for creating new t-norms from a given one and a non-decreasing function f are also discussed, along with examples. The article was published in 1999 by Elsevier Science B.V. and is protected by copyright.
41905	419054	Generalized real analysis and its applications	This paper discusses the benefits of using a generalized form of real analysis, known as pseudo-analysis, which utilizes alternative operations instead of traditional addition and multiplication. This approach allows for a unified treatment of nonlinear problems in various fields such as system theory, optimization, and control. The paper introduces important real aggregation functions and a real semiring with pseudo-operations. These operations are also used in fuzzy logic and fuzzy sets, and can be extended to a hybrid probabilistic-possibilistic measure. Pseudo-additive measures, integrals, and convolutions are formed from the real semiring, creating the framework for pseudo-analysis. The paper also presents applications in areas such as large deviation principle, Hamilton-Jacobi equation, and cumulative prospect theory.
41906	419069	HoIP: Haptics over Internet Protocol	This paper introduces a new application layer protocol called Haptics over Internet Protocol (HoIP) that enables low latency communication of haptic data for applications like telesurgery. To maintain stability in the control loop, the protocol has a target round trip delay of 5 ms or lower. HoIP uses adaptive sampling strategies, multithreaded architecture, and UDP implementation to achieve this low latency without compromising on the quality of haptic perception. The paper discusses the frame structure and design choices of HoIP, as well as the implementation and processing delay measurements using real haptic devices and HAPI software. Results show that HoIP has a maximum processing delay of 0.6 ms at both the transmitter and receiver side. 
41906	4190684	Application Layer Congestion Control for Network-Aware Telehaptic Communication.	In order for telehaptic applications to function smoothly in a shared network environment, there must be distinct Quality of Service (QoS) guarantees for different media components. This can be challenging when the network traffic is unknown and constantly changing. To address this issue, a new application layer congestion control protocol called the dynamic packetization module (DPM) has been proposed. DPM is a lossless and network-aware protocol that adjusts the packetization rate based on the level of congestion in the network. It also uses a novel network feedback module to monitor network congestion and communicate it to transmitters with minimal overhead. Through simulations and a real-time telepottery experiment, DPM has been shown to meet QoS requirements and outperform other telehaptic communication protocols. 
41907	4190761	Selecting informative features with fuzzy-rough sets and its application for complex systems monitoring	The use of intelligent pattern recognition applications is hindered by high dataset dimensionality. To address this issue, a redundancy-removing step is often performed, using rough set theory (RST) as a dataset pre-processor. However, RST relies on crisp datasets and can result in loss of important information. To overcome this, a new feature selection technique is proposed that uses a hybrid variant of rough sets and fuzzy-rough sets. This approach retains dataset semantics and allows for the creation of clear and readable fuzzy models. Results from applying this technique to complex systems monitoring demonstrate its superiority over conventional methods such as entropy, PCA, and random selection. 
41907	4190724	Fuzzy-Rough Sets Assisted Attribute Selection	Attribute selection is a problem in various fields, such as machine learning, pattern recognition, and signal processing, where the goal is to select the most predictive input attributes for a given outcome. Unlike other methods, attribute selectors preserve the original meaning of the attributes after reduction, making it useful for tasks with large datasets. Rough set theory has been successfully applied to this area, but is limited to discrete data. A new approach, fuzzy rough feature selection (FRFS), is proposed to address this limitation and retain dataset semantics. Experimental studies show that FRFS outperforms other dimensionality reducers and has better classification accuracy.
41908	4190841	Spatially varying registration using Gaussian processes.	The paper proposes a new technique for registering spatially-varying data using Gaussian process priors. This method involves modifying the spectrum of the Gaussian process based on a user-defined tempering function, resulting in a non-stationary process with varying levels of smoothness in different areas. Unlike other approaches, this method only affects the prior model and does not require any changes to the registration algorithm. This allows for the application of this method to various registration techniques, such as spline-based models and statistical shape or deformation models. The effectiveness of this method is demonstrated by registering cone beam CT images with high noise levels, resulting in improved accuracy and robustness. 
41908	4190862	Morphable Face Models - An Open Framework	This paper introduces a new open-source pipeline for registering faces using Gaussian processes, which can be applied to tasks like creating 3D face models. The Gaussian Process Morphable Models (GPMMs) used in this pipeline incorporate specific adaptations for different face types, such as symmetry and multi-scale details. The pipeline also includes a strategy for registering neutral faces and facial expressions. The researchers also release an open-source software framework for registration and model-building, along with a new version of the Basel Face Model (BFM-2017) with improved features. The framework and model are tested on publicly available databases, allowing for reproducibility, evaluation, and comparison by the community.
41909	41909129	How android app developers manage power consumption?: an empirical study by mining power management commits.	The popularity of the Android platform has led to a large number of Android applications being developed. Developers must consider power consumption management during the design and implementation process in order to maintain usability. With the availability of public repositories like GitHub, developers can access a wealth of information on common power management strategies used in Android applications. This paper presents an empirical study of power management commits in Android applications, with a focus on those from F-Droid. The study identifies 468 power management commits from 154 different applications belonging to 15 categories, and categorizes them into 6 groups based on open card sorting. The study also highlights the varying dominant power management activities across different categories of Android applications.
41909	41909213	Empirical Study of Usage and Performance of Java Collections.	Collection data structures are crucial for the performance of applications, especially in languages like Java, C#, or C++. This means that developers must carefully choose the appropriate collection from a wide range of options, including lists, maps, sets, and queues. While the default Java Collection Framework (JCF) provides basic implementations, there are many lesser-known third-party libraries that can greatly improve performance with minimal changes to code. A study was conducted on 10,986 Java projects to analyze the popularity and usage patterns of collection implementations. The results showed that there are alternative implementations for almost every JCF collection type that significantly reduce memory consumption and offer faster execution times. These results can serve as a guide for developers to identify scenarios where alternative implementations can greatly improve performance. Additionally, certain coding patterns can also have a significant impact on collection performance.
41910	41910237	Animating wrinkles on clothes	This paper presents a method for generating realistic wrinkles on clothes without using a fine mesh or requiring large computational resources. The method takes into account the area conservation property of cloth and uses a user-defined wrinkle pattern to modulate the deformation of individual triangles. This allows for the use of small in-plane deformation stiffnesses and a coarse mesh, resulting in a fast and robust cloth simulation. Additionally, the ability to design wrinkles on different types of deformable models makes this method versatile for creating synthetic images. The method can also be applied to other types of wrinkling phenomena, as it is based on geometric principles.
41910	4191024	Comparing Efficiency of Integration Methods for Cloth Simulation	This paper discusses the importance of efficient numerical methods for cloth simulation systems, as these methods are responsible for integrating the equations that describe the mechanical behavior of the cloth. The selection of an appropriate method should be based on a thorough understanding of the various techniques and their strengths and limitations. The paper provides a quantitative comparison of the efficiency of the most commonly used integration techniques for cloth simulation and highlights key considerations for optimal implementation depending on the specific simulation problem at hand. 
41911	4191173	Visually Dynamic Presentation of Proofs in Plane Geometry - Part 1. Basic Features and the Manual Input Method	We have developed a new way of presenting proofs in plane geometry using visually dynamic mediums like computer displays. This method involves creating a single diagram that is animated and updated with each step of the proof, making it easier to understand and follow. Our system, "Java Geometry Expert" (JGEX), offers two methods for creating these dynamic presentations: manual input and automatic. In this first part of our work, we introduce the main features of our method and explain the manual input method, which utilizes mouse clicks to create the dynamic diagram and proof text. This visually dynamic approach solves the issue of identifying geometry elements and makes the proof more visually appealing and intuitive.
41911	4191172	Automated Production of Traditional Proofs in Solid Geometry	This paper describes a method for creating clear and concise proofs for theorems in solid geometry. The method focuses on constructive geometry statements involving straight lines, planes, circles, and spheres. The main concept is to remove points from the conclusion of a geometric statement by utilizing fixed high-level propositions about the signed volumes of tetrahedrons and Pythagorean differences of triangles. The algorithm has been implemented and tested using over 80 examples from solid geometry. The program is efficient and produces short and easy-to-understand proofs.
41912	4191244	Improved algorithms for min cut and max flow in undirected planar graphs	This study focuses on the min st-cut and max st-flow problems in planar graphs, both in static and dynamic settings. The first algorithm presented can compute a min st-cut in O(n log log n) time for an undirected planar graph with two given vertices s and t. The same bound is achieved for computing a max st-flow in an undirected planar graph. These are the first algorithms to break the long-standing O(n log n) barrier for these problems. A fully dynamic algorithm is also presented, capable of maintaining min st-cut and max st-flow values in an undirected planar graph with edge insertions and deletions, with a time complexity of O(n(2/3) log(8/3) n) per operation. This algorithm is based on a new dynamic shortest path algorithm for planar graphs, which is also of independent interest. This is the first non-trivial dynamic algorithm for min st-cut and max st-flow.
41912	4191226	Min st-Cut Oracle for Planar Graphs with Near-Linear Preprocessing Time	This content discusses a type of query for a planar graph with nonnegative edge weights, which asks for the weight of a min cut between two given vertices. The authors present an algorithm that can answer these queries in constant time with O(n log4 n) preprocessing time and O(n log n) space, using a Gomory-Hu tree to represent all the min cuts implicitly. This is the first known subquadratic time algorithm for this problem. They also show how to obtain an implicit or explicit representation of a minimum-cycle basis in O(n log4 n) time and O(n log n) space, assuming unique shortest paths. This can be achieved either through randomization or deterministically with a small additional factor in the preprocessing time. 
41913	419131	Even more on advice on structuring compilers and proving them correct: changing an arrow	The author discusses their paper "On Representation of Data Types" and the concept of representation correctness in relation to compiler correctness. They note similarities between their work and previous works by Gaudel (1980), Broy and Wirsing (1980), McCarthy and Painter (1967), Burstall and Laudin (1969), Morris (1973), and ADJ (1979). However, they realize that an arrow has a different direction and ultimately conclude that their approach is correct. Their paper defines representations in terms of derivers and discusses the derived C-algebra ~A. The author also introduces the concept of a representation of data types T~ ,E by T~,E'' and defines it in relation to a homomorphism r.
41913	4191328	An Algebraic Framework for Higher-Order Modules	This paper introduces a new framework for handling higher-order parameterization by using arbitrary fitting morphisms for parameter passing. It defines a category of higher-order parameterized specifications and a typed λ-calculus to deal with these specifications. This approach is applicable to various types of basic specifications and operations, as long as certain conditions are met. The term "modules" is used to distinguish between the basic specification level and the parameterized units level. The presented calculus is similar to the simply typed λ-calculus, but does not allow for dependent types. The paper also presents a main result showing the adequacy of β-reduction with the given semantics.
41914	4191494	Handle and Hole Improvement by Using New Corner Cutting Subdivision Scheme with Tension	The Doubly Linked Face List (DLFL) is a new structure that allows for easy manipulation of 3D mesh models. It is topologically robust, meaning it always results in valid surfaces. This paper explores the relationship between DLFL and subdivision algorithms, with three main contributions. First, a new corner cutting scheme is introduced that allows for more control over the shape of the subdivided surface. Second, an efficient algorithm is developed for implementing this scheme on the DLFL structure. This ensures that the topological robustness is maintained. A comparison with the popular Catmull-Clark scheme shows that the new corner cutting scheme produces better results.
41914	4191471	A Prototype System for Robust, Interactive and User-Friendly Modeling of Orientable 2-Manifold Meshes	This paper presents a prototype system for creating orientable 2-manifold meshes that is robust, interactive, and user-friendly. The system introduces new topological entities and fundamental operators for manipulating 2-manifold mesh structures. These operators are implemented using extremely efficient algorithms. Additionally, the system features powerful and user-friendly operators at the user interface level, allowing for a wide range of homeomorphic and topological changes to be made. The system is topologically robust, preventing users from creating invalid mesh structures. It also allows for homeomorphic and topological surgery operations to be applied on 2-manifold meshes. Other features include the ability to blend surfaces, construct crusts, and open holes. The system can also manipulate shapes that appear solid, non-manifold, or 2-manifold with boundaries, and provides automatic texture mapping during topology changes.
41915	419156	A Maximum Entropy Tagger with Unsupervised Hidden Markov Models	This article presents a new tagging model that combines a hidden Markov model (HMM) with unsupervised learning and maximum entropy modeling. This approach reduces the cost of building taggers with limited resources, such as no dictionary and a small annotated corpus. The model was tested on English POS tagging and Japanese word segmentation tasks, and showed significant improvement in accuracy when trained with a small annotated corpus. In addition, the English POS tagger achieved better results (96.84%) compared to other state-of-the-art taggers when a large annotated corpus was available. This new approach has potential for improving tagging accuracy with limited resources.
41915	4191538	Ambiguous part-of-speech tagging for improving accuracy and domain portability of syntactic parsers	The goal of this research is to enhance the performance of a syntactic parser by utilizing a part-of-speech (POS) tagger as a preprocessor. Although pipelined parsers that combine POS tagging and syntactic parsing have advantages such as adaptability to different domains, their performance on raw text is often unsatisfactory due to errors in the automatic tagging process. To address this issue, the researchers propose a method where the tagger can output multiple possibilities when the tag cannot be determined accurately. This method is empirically validated using an HPSG parser trained on the Penn Treebank, and the results show that it improves parsing accuracy when weighted by probability values. The study also demonstrates the usefulness of this approach in adapting the parser to different corpora, specifically the GENIA corpus, while maintaining high accuracy.
41916	4191650	The Unsplittable Stable Marriage Problem	The Gale-Shapley algorithm is a well-known method for solving the stable marriage problem. This paper explores its use in the many-to-many stable marriage problem, also known as the stable allocation or ordinal transportation problem. A variant of the algorithm is presented that is suitable for "ordinal" assignment problems. It is similar to a bicriteria approximation algorithm by Shmoys and Tardos for scheduling on unrelated parallel machines with costs. This algorithm finds an unsplit (non-preemptive) stable assignment that ensures each job is assigned at least as well as it could be in any fractional stable assignment, and each machine is congested by no more than the processing time of the largest job.
41916	419168	Improved approximation algorithms for minimum-space advertisement scheduling	This article discusses a scheduling problem that involves placing advertisement images in a shared space over time. The goal is to minimize the makespan by scheduling each job on a specified number of parallel machines. This problem is a generalization of the classical scheduling problem P||Cmax. In 1969, Graham proposed an algorithm that yields a 4/3-approximation for P||Cmax by processing jobs in decreasing order of size and assigning them to the least-loaded machine. The article presents a proof that the natural generalization of Graham's algorithm also yields a 4/3-approximation for the minimum-space advertisement scheduling problem. The authors also introduce a new lower bound and linear programming relaxation for the problem, as well as a pseudo-polynomial approximation scheme.
41917	41917125	Normalized incremental subgradient algorithm and its application	The paper introduces a new incremental optimization algorithm, called NIS, for minimizing the sum of multiple component functions. This algorithm is specifically designed for problems where the component functions have common local minima. The NIS algorithm is implemented incrementally and can be used in distributed systems. It works by normalizing the subgradients of each component function, resulting in better convergence compared to other traditional methods. The convergence of the NIS algorithm is proven and its effectiveness is demonstrated through two applications: solving convex feasibility problems and distributed maximum likelihood estimation. The algorithm is also tested on numerical examples related to source and node localization in wireless sensor networks, showing its efficiency and effectiveness. 
41917	4191763	A simple iterative algorithm for range-based localization	The range-based localization problem is a common challenge in position estimation methods that use TOA or RSSI. This problem can be represented as a nonlinear least-squares (NLS) estimation problem. In this paper, the problem is reformulated as a constrained optimization problem, which is equivalent to the NLS problem. A simple iterative algorithm is then derived using a greedy optimization strategy, with closed-form expressions for the NLS localization. This algorithm can be easily implemented in a distributed manner. Simulation results demonstrate that the proposed algorithm performs very close to the Cramer-Rao lower bound for localization accuracy. 
41918	4191851	An analytical model for SMAC protocol in multi-hop wireless sensor networks.	The SMAC protocol in multi-hop wireless sensor networks (WSNs) uses an active/sleep dynamic and contention backoff scheme. To analyze its performance, we model each node as a single server queue with server shutdown and use a two-dimensional continuous-time Markov chain to represent each node's states. This allows us to calculate metrics such as average packet loss ratio, network throughput, average packet delay, and average power consumption. The accuracy of the analytical model is verified through experiment comparisons with simulation results. The model also allows us to study the tradeoff between energy efficiency and quality of service (QoS) requirements and determine optimal parameters like duty cycle, mean active period, and buffer size in multi-hop WSNs.
41918	4191838	Cross-Layer Optimization for Energy-Timeliness Tradeoff in TDMA Based Sensor Networks	Wireless sensor networks face the challenge of energy efficiency, as reducing transmission power to save energy also leads to increased channel errors and retransmissions, causing delays. To address this issue, a cross-layer approach is proposed that considers power allocation and routing path selection in TDMA-based networks. The problem is divided into two sub-problems, and simpler algorithms are proposed for each. Results show a tradeoff between energy efficiency and packet timeliness, with significant energy savings possible while meeting timeliness requirements.
41919	4191950	Lifting integer variables in minimal inequalities corresponding to lattice-free triangles	Two recent studies by Andersen et al. and Borozan and Cornuéjols have identified the minimal inequalities of a system with two rows, two free integer variables, and nonnegative continuous variables. These inequalities, known as split cuts or intersection cuts, are derived using maximal lattice-free convex sets. To use these inequalities for two rows of a general simplex tableau, the system must be extended to include integer variables and lifting functions must be developed to determine the coefficients of the integer variables in the corresponding inequalities. The authors of this paper analyze the lifting of minimal inequalities from lattice-free triangles, which can be classified into three categories. They prove that the lifting functions are unique for two of the categories, while for the third category, a fill-in inequality can yield minimal inequalities under certain conditions. The paper also presents conditions for the fill-in inequality to be extreme.
41919	4191931	Two row mixed-integer cuts via lifting	Andersen, Borozan, and Cornuéjols have recently studied the extreme valid inequalities of a mixed integer set with two equations, two free integer variables, and non-negative continuous variables. These inequalities can be split cuts or intersection cuts derived from maximal lattice-free convex sets. To use these inequalities in a general simplex tableau, one approach is to extend the system to include all possible non-negative integer variables and develop lifting functions to determine the coefficients of the integer variables in the inequalities. This paper focuses on the characteristics of these lifting functions. It is shown that there exists a unique lifting function for maximal lattice-free triangles with multiple integer points on one side or with integral vertices and one integer point on each side. However, for other cases such as maximal lattice-free triangles with one integer point on each side and non-integral vertices, or maximal lattice-free quadrilaterals, non-unique lifting functions may yield distinct extreme inequalities. Sufficient conditions are provided for this latter family of triangles to yield an extreme inequality for the two row mixed-integer infinite-group problem.
41920	4192022	Ramsey-type theorems for lines in 3-space.	The article discusses geometric Ramsey-type statements, which provide guarantees on the size of cliques or independent sets in graphs and hypergraphs induced by incidence relations between lines, points, and reguli in 3-space. The authors prove three main statements: one about the size of a clique or independent set in the intersection graph of n lines in R-3, one about subsets of lines that are stabbed by one line, and one about subsets of lines that lie on a regulus. These results are obtained using geometric incidence bounds and Turan-type results. The article also includes an intermediate result about points incident to a single algebraic curve. The proofs of these statements also yield polynomial-time algorithms.
41920	4192015	Making Triangles Colorful	This study demonstrates that given a finite set of points in a plane, a triangle, and a positive integer k, it is possible to color the points with k colors in such a way that any identical copy of the triangle containing at least 144k(8) points will have at least one point of each color. This is the first time a polynomial bound has been found for this type of problem, specifically in relation to homothetic polygons. Previous bounds only applied to the more complex case of octants in three-dimensional space, and were doubly exponential. 
41921	419216	Competitive Diffusion on Weighted Graphs.	This paper discusses the use of a weighted graph to model a social network, where individuals are represented as vertices and their relationships are represented as edges with varying levels of importance. In the competitive diffusion game, players choose a vertex as a starting point to spread their idea through the network, with the goal of maximizing the sum of weights of infected vertices. The paper focuses on the computational problem of determining whether a pure Nash equilibrium exists in a given graph, presenting both negative and positive results for different types of graphs. The problem is shown to be difficult for certain graph classes, but solvable in polynomial time for others.
41921	4192111	Polynomial-Time Algorithm for Sliding Tokens on Trees.	The sliding token problem is a mathematical problem where two sets of vertices in a graph, each with the same number of vertices, are given. The task is to determine if there exists a sequence of independent sets where each set in the sequence is obtained by moving one token along an edge in the graph from the previous set. This problem is known to be very difficult, even for simple graphs like planar ones or those with a bounded treewidth. However, this paper presents a solution for trees, where a sequence of independent sets can be found in quadratic time. It also mentions that there are certain instances where the length of the sequence is always quadratic.
41922	4192232	In-Core Computation of Geometric Centralities with HyperBall: A Hundred Billion Nodes and Beyond	The paper discusses the problem of determining the most central nodes in a social network. Many centrality measures have been proposed, but traditional methods for computing them are not efficient for very large graphs. The authors propose a method that leverages HyperLogLog counters, allowing for fast and accurate approximation of centrality measures while reducing the memory required. This approach is suitable for processing networks with billions of nodes and can be parallelized for even faster computation. 
41922	4192283	Large-scale Network Analytics: Diffusion-based Computation of Distances and Geometric Centralities	The question of which nodes are more central in a large complex network has been studied in various fields such as sociology, psychology, and computer science, leading to a variety of centrality measures. Classifying these measures in a mathematically sound and general way is challenging, as it requires simple properties to be identified as well as efficient algorithms to compute them on real networks. A new tool called HyperBall has been developed to address these challenges by using a semi-streaming approach to access the network and compute distance-based centralities. It has a small memory requirement and provides accurate results. 
41923	4192385	An Efficient Algorithm for Solving Pseudo Clique Enumeration Problem	The problem of identifying dense structures in a graph is important in data mining and engineering. Cliques are a common way to represent dense structures due to their simplicity. Pseudo cliques, which are subgraphs with a lower number of edges than a clique, are a natural extension of cliques. This paper focuses on a specific type of pseudo clique where the edge-to-vertex ratio is above a given threshold. The authors propose a polynomial time algorithm based on reverse search to enumerate all pseudo cliques in a graph. Computational experiments show the effectiveness of the algorithm on both randomly generated and practical graphs.
41923	4192365	Enumeration of the perfect sequences of a chordal graph	This paper discusses the problem of enumerating all perfect sequences in a chordal graph, which is a graph with no chordless cycle of length more than three. A chordal graph has a set of maximal cliques, which can be represented by clique trees. A perfect sequence is obtained by repeatedly removing the leaves of a clique tree in reverse order. However, developing an efficient algorithm for this problem is challenging due to the fact that a chordal graph may have multiple clique trees and a perfect sequence can be generated by different clique trees. The paper proposes a new method for enumerating perfect sequences without constructing clique trees, resulting in the first polynomial delay algorithm for this problem. 
41924	4192410	Dynamic Planar Convex Hull with Optimal Query Time	The dynamic maintenance of the convex hull of a set of points in the plane is a significant problem in computational geometry. A new data structure is introduced that allows for efficient point insertions and deletions in O(log n) time, and various convex hull queries in O(log n) time. The data structure uses O(n) space and has applications in solving the k-level problem and the redblue segment intersection problem with connected segments.
41924	4192437	Dynamic planar range maxima queries	The dynamic two-dimensional maxima query problem involves finding the maximal points in a set of n points in the plane, where a point is considered maximal if it is not dominated by any other point in the set. Two data structures have been developed to efficiently report the t maximal points that dominate a given query point, with the ability to insert and delete points in the set. In the pointer machine model, a data structure with linear space complexity and worst case query time of O(log n + t) and update time of O(log n) has been created, which is the first of its kind to achieve these bounds. This data structure also supports reporting maximal points in 3-sided orthogonal ranges without an upper bound. In the RAM model, a linear space data structure has been developed that supports 3-sided range maxima queries in O(log n/log log n + t) worst case time and updates in O(log n/log log n) worst case time, making it the first to achieve sublogarithmic worst case bounds for all operations in this model.
41925	4192531	On the decidability and finite controllability of query processing in databases with incomplete information	This paper focuses on studying how to answer queries over relational databases with integrity constraints (ICs) under the open-world assumption (OWA). The types of ICs considered are functional dependencies and inclusion dependencies, while the query languages considered are conjunctive queries (CQs), union of conjunctive queries (UCQs), and their variations with negation and inequality. The paper presents results on the decidability and finite controllability of OWA query answering, including identifying the decidability/undecidability boundaries and studying OWA query answering over finite and unrestricted databases. These results also have implications for problems such as implication of ICs and query containment under ICs. The paper closes two long-standing open problems in query containment. The findings have potential applications in areas such as view-based information access, ontology-based information systems, data integration, data exchange, and peer-to-peer information systems.
41925	4192548	On the finite controllability of conjunctive query answering in databases under open-world assumption	This paper examines the problem of answering queries over relational databases with integrity constraints (ICs) under the open-world assumption. It focuses on inclusion dependencies and functional dependencies, specifically key dependencies, and considers conjunctive and union queries. The authors present findings on the decidability of OWA query answering under ICs for both finite and unrestricted databases. They also discuss the concept of finite controllability, where OWA query answering over finite databases is equivalent to that over unrestricted databases. This paper also discusses the relationship between OWA query answering and classic problems in database theory, such as implication of ICs and query containment. The authors are able to solve long-standing open problems in query containment by proving finite controllability of containment of conjunctive queries under various types of dependencies. These results have implications for various research areas, such as data integration, data exchange, and peer data management systems.
41926	4192678	Partitioning similarity graphs: a framework for declustering problems	The paper presents a new similarity-based technique for declustering data in parallel computing environments. The proposed method takes into account factors such as query distribution, data types and sizes, and partition-size constraints. It uses a similarity graph and a max-cut partitioning approach to allocate frequently accessed data-items to different disks, improving overall performance. The method is flexible and can be applied to Grid Files at the data page level. Detailed experiments show that it is effective in adapting to different query and data distributions, and outperforms traditional mapping-function-based methods. 
41926	4192646	CCAM: a connectivity-clustered access method for networks and network computations	Current SDBMS are effective in handling point and range queries for spatial data such as points, line segments, and polygons. However, it is uncertain if these systems can efficiently support network computations that traverse line segments based on connectivity rather than proximity. To reduce I/O costs for network operations, the Weighted Connectivity Residue Ratio (WCRR) can be maximized, which allocates connected nodes to a common disk page. CCAM is an access method for general networks that uses connectivity clustering and supports insert, delete, create, find, get-A-successor, and get-successors operations. CCAM assigns nodes to disk pages using a graph partitioning approach and includes methods for static and dynamic clustering to maintain high WCRR. Experiments with the Minneapolis road map show that CCAM outperforms existing methods, and potential modifications can further improve WCRR for existing spatial access methods.
41927	4192775	Fuzzy MCDM approach for selecting the best environment-watershed plan	This paper discusses the use of a fuzzy decision support system in multi-criteria analysis for selecting the best plan alternatives in environment-watershed topics. The system uses the fuzzy analytic hierarchy process (FAHP) method to determine the weightings of criteria based on subjective perception. A questionnaire was used to gather input from 15 experts in the field. The system takes into account subjectivity and vagueness in the decision-making process and provides an overall performance value for each alternative. A case study from Taiwan is used to demonstrate the effectiveness of the approach in evaluating five alternatives for an environment-watershed plan. This system can be useful in destination planning and promoting the sustainability of watershed tourism resources.
41927	41927101	Hierarchical MADM with fuzzy integral for evaluating enterprise intranet web sites.	This research suggests that traditional evaluation methods may not be effective for criteria that are inter-dependent or interactive in the real world. Instead, a fuzzy integral model can better approximate the human subjective evaluation process. An algorithm is proposed to determine the overall evaluation using fuzzy densities and a λ-fuzzy measure. An example of evaluating enterprise intranet web sites is provided, demonstrating the hierarchical structure of the λ-fuzzy measure for a Choquet integral model. The results show that the fuzzy integral is more appropriate for human subjective evaluation or when criteria are not independent.
41928	4192829	Offline/online attribute-based encryption with verifiable outsourced decryption	In the era of big data, service providers often store data in third-party cloud systems, such as social networking websites, to protect its security and privacy. However, this presents new challenges in allowing authorized access to specific parts of the data without decryption. Attribute-based encryption (ABE) offers a solution by allowing fine-grained access control based on user attributes, but it can be computationally expensive, especially for mobile devices. To address this issue, a new ABE scheme is proposed that uses bilinear groups in prime order and combines offline/online key generation and encryption with verifiable outsourced decryption. This reduces the computational burden on devices and is secure against chosen-plaintext attacks. Simulation results show its effectiveness in reducing computational costs. 
41928	419281	Secure query processing with data interoperability in a cloud database environment	A cloud database system using the DBaaS model allows a data owner to export their data to a cloud database service provider for secure storage. To maintain data security, sensitive information is encrypted by the data owner before being uploaded to the service provider. However, current encryption schemes are limited in their ability to perform different types of computations on encrypted data, making it difficult to process practical queries. To address this issue, a secure query processing system (SDB) is proposed which allows for data interoperability and enables a wide range of SQL queries to be processed on encrypted data by the service provider. The encryption scheme used is proven to be secure and efficient.
41929	41929120	I/O cost minimization: reachability queries processing over massive graphs	This paper discusses the challenge of answering reachability queries on massive graphs that cannot fit entirely in main memory. Existing studies have used indexing techniques, but they are not suitable for handling such large graphs due to high I/O costs. To minimize these costs, the paper proposes a Yes-Label scheme and a heap-on-disk data structure for traversing the graph. Additionally, the paper suggests partitioning the heap-on-disk to ensure sequential I/Os. These approaches are shown to effectively answer multiple reachability queries and have been tested on both synthetic and real graphs, proving their efficiency.
41929	41929190	Incremental Graph Pattern Based Node Matching	Graph pattern based node matching (GPNM) is a technique used to find matches between nodes in a data graph (GD) and a given pattern graph (GP). This method has become increasingly important in applications such as group finding and expert recommendation. However, with frequent updates to both GD and GP, existing GPNM methods require starting the matching process from scratch, resulting in long processing times. To address this issue, a new method called INC-GPNM is proposed in this paper. It uses indices to maintain the shortest path length range between different label types in GD and identifies the affected parts of GD in the matching process. With its efficient indexing and search strategies, INC-GPNM can quickly deliver accurate node matching results for updated GD and GP, greatly reducing processing time and outperforming other methods in efficiency. Experiments on real-world social graphs further demonstrate the superior performance of INC-GPNM. 
41930	4193052	Variance Reduced Stochastic Gradient Descent with Neighbors	Stochastic Gradient Descent (SGD) is widely used in machine learning, but its slow convergence can be a problem. To address this, variance reduction methods like SAG, SVRG, and SAGA have been proposed, achieving linear convergence. However, these methods require either full gradient computations at pivot points or storing per data point corrections, which can slow down the process. To improve speed and convergence, this paper explores algorithms that utilize the structure of the training data to share and reuse information about past stochastic gradients across data points. This leads to faster optimization in the early stages. Additionally, the paper provides a unified convergence analysis for a group of variance reduction algorithms, referred to as memorization algorithms, and includes experimental results that support the theory.
41930	4193019	Active Content-Based Crowdsourcing Task Selection	Crowdsourcing has become a popular approach for tasks such as document relevance assessment, as it is a cost-effective alternative to using domain experts. However, this method relies on high levels of redundancy in order to account for potentially inaccurate worker submissions. This leads to a larger number of labels and higher costs. In this paper, the authors propose a new approach that utilizes document information to infer relevance labels for unjudged documents. They introduce an active learning scheme that maximizes accuracy while minimizing the required budget. Their experiments, based on TREC 2011 data, show that this method outperforms traditional crowdsourcing with a reduced budget of 17% - 25%. 
41931	41931120	Deterministic Annealing for Unsupervised Texture Segmentation	This paper presents a mathematical framework for deterministic annealing and mean-field approximation for partitioning, clustering, and segmentation problems. It outlines an efficient optimization approach and provides proof of convergence. The framework has potential applications in computer vision, pattern recognition, and data analysis. The paper also introduces a new method for unsupervised texture segmentation that uses statistical tests to measure homogeneity. This method formulates texture segmentation as a pairwise data clustering problem with a sparse neighborhood structure. The paper compares different clustering objective functions derived from invariance principles and evaluates the performance of the algorithms on a dataset of micro-texture mixtures and real-world images.
41931	4193193	A theory of proximity based clustering: structure detection by optimization	This paper presents a systematic approach for optimizing cluster analysis of similarity data. A set of axioms is proposed to distinguish different cluster criteria and the approach is extended to include hierarchical clustering. The optimization problems are solved using deterministic annealing and mean-field approximation, with efficient heuristics derived. The application of this approach to texture segmentation is evaluated on both synthetic and real-world images. Additionally, the performance of different clustering algorithms is compared on an information retrieval task. The results show the superiority of optimization algorithms for clustering. 
41932	4193214	Small stretch spanners in the streaming model: new algorithms and experiments	The article presents deterministic algorithms for computing small stretch spanners in the streaming model. A (α, β)-spanner of a graph is a subgraph that preserves distances within a certain factor. The algorithms can handle graphs with unknown numbers of vertices and edges and only one pass over the data is allowed. The authors demonstrate how to compute a (k, k-1)-spanner in O(1) processing time per edge/vertex, with O(n1+1/k) edges and memory space. The algorithms can also be adapted for limited internal memory. An experimental study supports the practical value of this approach.
41932	4193247	Greedy Algorithms For On-Line Set-Covering	The article discusses an on-line model for set-covering, where elements of a ground set arrive one-by-one and a greedy algorithm selects sets to cover the most uncovered elements. This algorithm has a competitive ratio of √ n and is asymptotically optimal. The model can also be used to solve minimum dominating set with a competitive ratio bounded by the square root of the input graph size. A more memory-consuming on-line model is also explored, where the sequence of set names is revealed with each element arrival. An algorithm with a competitive ratio based on the maximum frequency of the instance is proposed. The article also examines the maximum budget saving problem and shows that a natural greedy off-line algorithm is asymptotically optimal when the budget is at least equal to √ n times the size of the optimal solution.
41933	4193326	Splitting argumentation frameworks: an empirical evaluation	In a recent paper, Baumann demonstrated that splitting results, similar to those known for logic programs and default logic, can also be applied to Dung argumentation frameworks (AFs). This means that under certain conditions, an AF can be divided into two parts, allowing for more efficient computation of extensions. Specifically, this process involves computing an extension for one part, modifying the other part, and then combining the two extensions. The paper then presents an empirical evaluation of the effects of splitting on extension computation, showing that it can greatly improve algorithm performance.
41933	4193313	Preferred answer sets for extended logic programs	The paper discusses how the answer set semantics for extended logic programs, developed by Gelfond and Lifschitz, can be adapted to incorporate prioritization of rules. This involves using a defined ordering of rules to determine preferred answer sets, which leads to a larger set of program consequences. The authors propose both a strong and weak definition of preferred answer sets, with the former giving more weight to preferences. The modifications to the existing answer set semantics allow for the handling of prioritized programs and provide a more comprehensive understanding of their consequences.
41934	4193411	Cross-Referenced Dictionaries and the Limits of Write Optimization.	Dictionaries are a commonly studied data structure that allow for various operations such as insertions, deletions, and membership queries. In a RAM, these operations take O(log N) time on N elements. Dictionaries can also be cross-referenced, which involves creating multiple dictionaries on the same set of tuples. In external memory, using B-trees can achieve optimal insertions and deletions, but write optimization techniques have been developed to improve the insertion and deletion costs. However, for deletions in cross-referenced dictionaries, write optimization does not offer a significant advantage. In fact, a lower bound has been established that shows optimal cross-referenced dictionaries cannot match the write optimization bound for insert-only dictionaries. This highlights the limitations of write optimization techniques in certain situations.
41934	4193412	On the complexity of computing prime tables.	This article discusses the importance of prime tables in large arithmetic computations. It explains that the fastest algorithms for computing n! and ((n)(n/2)) both use prime tables, and that the time complexity for these computations is dependent on the time to compute a prime table up to n. The article presents two algorithms for computing prime tables and analyzes their complexity on a multitape Turing machine. These algorithms improve upon previous methods and can speed up the computation of n! and ((n)(n/2)). Finally, the article discusses the complexity of computing the factorial and shows that it takes O(M(n log(4/7-epsilon) n)) time assuming only multiplication is allowed.
41935	4193557	From Reference Frames to Reference Planes: Multi-View Parallax Geometry and Applications	The paper introduces a new method for analyzing the geometry of multiple 3D points in uncalibrated images. This method involves decomposing the projection of the points onto a reference plane and then re-projecting them onto the camera's image plane. This eliminates the need for internal camera calibration and orientation, simplifying the analysis. The use of a reference plane also allows for simpler tri-focal constraints and leads to useful applications in 3D scene analysis, such as New View Synthesis. The method also allows for partial reconstruction using partial calibration information. Overall, this new framework offers a simpler and more intuitive approach to analyzing 3D scene points from multiple images.
41935	4193512	A unified approach to moving object detection in 2D and 3D scenes.	The detection of moving objects is crucial for various tasks. Existing methods for this problem fall into two categories: 2D algorithms for flat surfaces and limited camera movements, and 3D algorithms for scenes with depth variations and camera translation. This paper proposes a unified approach that can handle both 2D and 3D scenarios by gradually increasing complexity. The method is based on stratification and employs techniques that progress from 2D to more complex 3D methods. Furthermore, the computations from one level can be used as a starting point for the next level. Examples from real-image sequences are provided to demonstrate the effectiveness of this approach.
41936	4193638	Complete arcs in PG(2,25): The spectrum of the sizes and the classification of the smallest complete arcs	This paper reports on a thorough computer search that determined the smallest size of a complete arc in PG(2,25) to be 12. It also found that complete 19-arcs and 20-arcs do not exist, thus fully determining the spectrum of complete arc sizes in this projective geometry. The authors also provide a classification of the smallest complete arcs, with 606 non-equivalent 12-arcs identified and their automorphism groups and geometrical properties studied. The success of the search was aided by using projective equivalence properties to limit the search space and avoid generating duplicate arcs.
41936	4193621	Covariance-based parameters adaptation in differential evolution	Differential Evolution (DE) is a widely used optimization method that mimics the process of natural selection to find the best solution in a real-valued space. Its unique feature is a differential mutation operator that balances exploration and exploitation. However, DE requires manual tuning of numerical parameters, which can be time-consuming and may not always yield optimal results. To address this issue, some adaptive schemes have been proposed, but they do not consider the interactions between multiple parameters. In this paper, a new self-adaptive DE scheme is introduced that takes into account parameter dependencies using a multivariate probabilistic technique. Experiments on benchmark problems demonstrate the competitiveness of this approach compared to other adaptive DE schemes. Furthermore, this scheme can potentially be applied to other Evolutionary Algorithms or meta-heuristic techniques with parameters that affect the search behavior.
41937	4193747	Assessing the Need for Referral in Automatic Diabetic Retinopathy Detection	This paper discusses the use of emerging technologies in healthcare to improve efficiency and reduce costs. Specifically, it focuses on the use of image recognition for the screening of diabetic retinopathy, a complication of diabetes that can lead to blindness. Previous research has focused on identifying specific lesions related to DR, but this may not be enough to determine if a patient should be referred for further treatment. The paper introduces a new algorithm that combines results from multiple lesion detectors to make a more accurate decision on patient referral. Results show that this approach achieved a high level of accuracy without the need for additional normalization techniques.
41937	4193746	3D facial tracking from corrupted movie sequences	This paper discusses a method for performing 3D face tracking on corrupted video sequences. The method uses a deformable model and a predictive filter to recover both rigid transformations and parameters describing facial expressions over time. A new method for measuring the correct distribution of observation of parameters is introduced, which is based on bounding confidence regions of 2D image displacements and propagating them into parameter space. This method avoids traditional assumptions about observation distributions and has been shown to be robust in experiments with compressed and poor-quality video sequences. Ground truth validation is also provided.
41938	4193836	Evolutionary regression modeling with active learning: an application to rainfall runoff modeling	Computer simulations have become a common way to study complex real-world phenomena when controlled experiments are not feasible. However, these simulations are computationally expensive, leading to the use of surrogate models such as neural networks and kernel methods. These models are cost-effective and useful for tasks like optimization and sensitivity analysis. To address the model calibration problem in rainfall runoff modeling, a new automated approach has been developed that incorporates advances in machine learning such as hyperparameter optimization, model type selection, and active learning. This makes the method more useful for domain experts.
41938	4193840	A Surrogate Modeling and Adaptive Sampling Toolbox for Computer Based Design	Many scientific and engineering fields require computer simulations to understand complex phenomena and solve design problems. However, these simulations can be computationally expensive. To address this, surrogate modeling techniques like neural networks and kernel methods have become essential. These models are efficient and accurate, making them useful for tasks like optimization, design exploration, and sensitivity analysis. As a result, there is a growing interest in tools and techniques that make it easier to create these models while minimizing computational costs. This paper introduces a machine learning toolkit that combines algorithms for data fitting, model selection, sample selection, hyperparameter optimization, and distributed computing to help experts efficiently build accurate models for their specific problem or data.
41939	4193944	Linear antenna array synthesis using fitness-adaptive differential evolution algorithm	This article discusses the use of an adaptive Differential Evolution (DE) algorithm to optimize the spacing between elements in non-uniform linear antenna arrays. The goal is to produce a radiation pattern with minimal side lobe level and controlled null placement. DE is a highly effective optimization method with few control parameters and easy implementation. Two simple adaptation schemes are used to regulate DE's control parameters, F and Cr. The algorithm successfully solves three difficult design problems and outperforms other metaheuristics like Genetic Algorithm, Particle Swarm Optimization, Memetic Algorithms, and Tabu Search. The results demonstrate the effectiveness of the proposed algorithm in achieving the optimization goal.
41939	41939147	Circular antenna array synthesis with a Differential Invasive Weed Optimization algorithm	This article presents a method for designing non-uniform, planar, and circular antenna arrays with the goal of achieving minimum side lobe levels and a minimum circumference. The method combines two popular optimization techniques, Invasive Weed Optimization (IWO) and Differential Evolution (DE), to create a hybrid approach called Differential IWO (DIWO). IWO mimics the behavior of colonizing weeds and has shown superior performance in engineering design problems. The modified IWO incorporates mutation schemes from DE. The results of the DIWO algorithm outperform other state-of-the-art metaheuristics like Particle Swarm Optimization (PSO) and DE in terms of design performance. The effectiveness of this approach is demonstrated through three difficult instances of the circular array design problem. 
41940	41940215	Families of Alpha- Beta- and Gamma- Divergences: Flexible and Robust Measures of Similarities	This paper presents an extension and overview of Alpha-, Beta- and Gamma- divergences, discussing their fundamental properties and showing that there are families of these divergences with consistent properties. The authors establish links and correspondences among the divergences through nonlinear transformations, such as generating Beta- divergences from Alpha- divergences and vice versa. Additionally, a new class of Gamma- divergences is shown to be generated from both Beta- and Alpha- divergences. The paper also explores the connections between these divergences and Tsallis and Renyi entropies, highlighting their information theoretic interpretations. Overall, this paper provides a comprehensive understanding of the properties and relationships between these divergences.
41940	4194098	Generalized Alpha-Beta Divergences and Their Application to Robust Nonnegative Matrix Factorization.	This article proposes a new class of multiplicative algorithms for Nonnegative Matrix Factorization (NMF) that are more robust against noise and outliers. This is achieved by introducing a new family of divergences called Alpha-Beta-divergences, which are controlled by two tuning parameters, alpha and beta. These divergences can smoothly connect existing divergences, including Alpha-, Beta-, and Gamma-divergences. By adjusting these parameters, a wide range of standard and new divergences can be obtained. The proposed algorithms for NMF are shown to be more general and integrate various existing ones, such as Lee-Seung, ISRA, EMML, Alpha-NMF, and Beta-NMF. The use of these tuning parameters also leads to improved robustness against noise and outliers. The relationship between AB-divergence and other divergences, particularly Gamma- and Itakura-Saito divergences, is also discussed. 
41941	4194113	CA4IOT: Context Awareness for Internet of Things	The Internet of Things (IoT) will connect billions of sensors, creating an opportunity for a sensing-as-a-service platform. With a large number of sensors collecting similar information and continuous advancements in sensor technology, selecting the most appropriate sensors for a given problem will be a challenge. This paper proposes the Context Awareness for Internet of Things (CA4IOT) architecture, which automates the task of selecting sensors based on the problem at hand. It focuses on configuring filtering, fusion, and reasoning mechanisms for sensor data streams collected by selected sensors. The goal is to provide more useful information than the raw data streams, based on the user's submitted problems.
41941	4194149	Dynamic configuration of sensors using mobile sensor hub in internet of things paradigm	IoT aims to connect a large number of sensors to the Internet through the use of smart devices like mobile phones. This can save energy by processing sensor data on the device itself, reducing the need for constant data communication. However, many current IoT solutions require manual and time-consuming steps to connect a mobile phone. To address this issue, the researchers developed MoSHub, a mobile app that connects different sensors to a phone and intelligently sends data to the cloud. They tested MoSHub with GSN middleware and found that it significantly reduces the need for manual setup by experts. Different methods were also explored to automate the configuration process.
41942	4194246	Validating the AMULET Microprocessors	The AMULET group at the University of Manchester has been working on designing asynchronous microprocessors for the past ten years. These designs have become increasingly advanced and complex. However, ensuring that these designs are correct on the first try is a difficult task. Therefore, the group has been exploring various verification and validation techniques to improve the quality of their designs and reduce the number and severity of bugs in the final product. 
41942	4194273	The Design and Evaluation of an Asynchronous Microprocessor	AMULET1 is an asynchronous version of the ARM microprocessor created by Manchester University from 1991 to 1993. It was functional upon its first silicon arrival in April 1994, proving that asynchronous design can be achieved with current CAD tools. This paper discusses the reasons for the project, the decisions made during the design process, challenges faced, and the features of the final product.
41943	4194323	Myopic Sparse Image Reconstruction With Application To Mrfm	We offer a solution for improving image clarity when the convolution operator or point spread function (PSF) is only partially known. By using small perturbations and finding principal components, we can better understand the uncertainty in a high dimensional space. Our method is based on the assumption that the image is sparse, which is commonly seen in magnetic resonance force microscopy (MRFM) images. Using a Bayesian Metropolis-within-Gibbs sampling framework, our algorithm outperforms other methods like the alternating minimization (AM) algorithm for sparse images. We demonstrate the effectiveness of our approach on real MRFM data of a tobacco virus. 
41943	419436	Bayesian linear unmixing of time-evolving gene expression data using a hidden Markov model	The paper discusses a new hierarchical temporal Bayesian model and Markov chain Monte Carlo (MCMC) algorithm for gene factor analysis. The algorithm utilizes a linear mixing model (LMM) to decompose data samples into characteristic gene signatures, also known as factors, with appropriate proportions. The LMM is combined with a hidden Markov model (HMM) to account for temporal dependencies between samples, with the HMM structure inspired by the behavior of host molecular response to an infectious agent. To handle the complexity of the resulting posterior distribution, a hybrid Gibbs sampler is used to generate samples, which are then used to estimate unknown parameters. The effectiveness of the method is demonstrated through simulations and analysis of a real dataset.
41944	4194422	The impact of data accuracy on system learning	This paper examines the relationship between database accuracy and system learning in information systems. The study focuses on a basic model featuring a database, rulebase, and embedded machine learning approach. The results show that the accuracy of the information system affects the quality of individual rules, and this impact can be determined by accounting for database inaccuracy. In some cases, this impact is monotonic. Additionally, the accuracy of the system can also impact the order of importance of rules within the rulebase. This highlights the importance of considering database accuracy in the design and behavior of learning systems.
41944	4194457	On the relationship between REA and SAP	The REA model is a popular theoretical database model for accounting, while SAP is the leading enterprise resource-planning system. This paper examines the connections between the data models of REA and SAP. The study uses Dunn and McCarthy's criteria to differentiate accounting systems, which include database orientation, semantic orientation, and structuring orientation. The paper summarizes the various relationships between REA and SAP based on these criteria.
41945	4194541	String hashing for linear probing	Linear probing is a popular method for implementing dynamic hash tables, where all keys are stored in a single array. When a key is received, it is hashed to a location and then consecutive locations are probed until the key or an empty location is found. In a study presented at STOC'07, it was found that with standard 2-universal hashing, the expected number of probes is Ω(log n). However, using 5-universal hashing can reduce this to a constant number of probes. This method is not available for complex domains such as variable length strings, so a solution is to first use collision-free hashing into a simpler intermediate domain and then apply the complicated hash function. This study found that an expected constant number of linear probes can be achieved as long as the second hash function is 5-universal and the first hash function has O(1) expected collisions with each key. This allows for a smaller intermediate domain, resulting in a simpler and faster overall hash function. The study also looked at how the overhead from linear probing decreases as the array size increases and the implications of storing strings directly as intervals in the array.
41945	41945112	Priority sampling for estimation of arbitrary subset sums	The purpose of this article is to explain a method for creating a sample of weighted items from a high-volume stream, which can then be used to estimate the total weight of arbitrary subsets. This method, called priority sampling, is particularly useful for analyzing Internet traffic, where the items are flow records and the subsets could be different time intervals of a worm attack. Priority sampling is weight-sensitive and avoids selecting heavy items multiple times, making it more accurate for heavy-tailed distributions. The article also discusses the benefits and implementation of priority sampling, as well as its unbiased estimators and near-optimality. 
41946	4194628	Exploring alternatives for representing and accessing design knowledge about enterprise integration	Enterprise integration involves finding ways for different legacy applications to communicate and work together effectively. This can be a challenging task due to the complexity and specificity of each integration effort. However, there are some tools that can help, such as Business Process Models (BPM) and Enterprise Integration Patterns (EIP). These tools provide designers with abstract descriptions of common integration tactics. However, they may have different perspectives and can be difficult to align. To address this issue, a new approach is proposed that uses the theory of speech acts to bridge the gap between BPM and EIP. This approach includes a re-representation of EIP as speech acts, and a mapping between speech acts and tasks in BPM. This approach has been successfully applied in real-world cases.
41946	4194645	Representing and Accessing Design Knowledge for Service Integration	The process of constructing new services from existing ones requires the use of appropriate design knowledge. This knowledge is available in the form of Enterprise Integration Patterns (EIP), which can be difficult to understand and use due to their different primitives from those used in process representation. To address this issue, a knowledge-base has been created that represents EIPs and adds semantics derived from speech acts, along with a set of heuristics to retrieve EIPs based on requirements. An example is provided to demonstrate how these tools can assist designers in the process of service construction.
41947	4194739	The 2009 Edsger W. Dijkstra prize in distributed computing	The ACM-EATCS Edsger W. Dijkstra Prize in Distributed Computing was established to recognize exceptional research papers in the field of distributed computing that have had a significant impact on the theory or practice of the subject for at least ten years. This award highlights the importance of understanding the principles of distributed computing and their lasting influence on the field. 
41947	419470	Topic 8 distributed systems and algorithms	Parallel computing is facing challenges as it becomes more integrated with distributed systems, including issues with asynchrony, long delays, network partition, failures, disconnected operation, and protocol standardization. This topic serves as a platform for both academic and industry research and practice related to distributed computing and distributed algorithms. Submissions were encouraged in various areas, such as design and practice of distributed algorithms, analysis of distributed systems and algorithms, fault-tolerance, operating systems and databases, scalability, performance, resource sharing and load balancing, telecommunications, mobile computing, security, and standards and middleware for parallel computations. Of the 27 papers submitted, eight were accepted, with a common focus on self-organisation and fault tolerance. Other themes addressed include mobile networks, mutual exclusion and consensus algorithms, publish-subscribe networks, data replication, checkpointing, garbage collection, and real-time processing. One paper, "Replication predicates for dependent-failure algorithms" by Flavio Junqueira and Keith Marzullo, was selected as a distinguished paper.
41948	4194840	Automating the analysis of design component contracts	Software patterns are a new way of designing software to solve problems within a specific context. These patterns describe the structure and collaboration among components in a software design. The promise of this approach is to simplify the construction of software systems and reduce costs by reusing previous experience. However, it also introduces challenges in ensuring the reliability and integrity of these complex systems. To address this, a formal model is needed to analyze pattern-based designs and their interactions. This paper presents a formal framework for analyzing object-oriented systems with pattern-based designs as building blocks. A case study is provided to demonstrate the approach in a distributed system. This framework can help ensure the integrity of software systems and reduce costs in their construction.
41948	4194823	Agents in object-oriented software engineering	This paper discusses the challenges faced by software engineers when developing multi-agent systems (MASs), which have different concerns such as autonomy, adaptation, interaction, collaboration, learning, and mobility compared to traditional object-oriented software engineering. Despite this, most MAS developers still use object-oriented design techniques and programming languages like Java, resulting in difficulties in managing and reusing MAS concerns. To address this, the paper presents a new approach that integrates agents into object-oriented software engineering from the early stages of design. This approach encourages the separate handling of MAS concerns and provides a structured way to combine them. It also utilizes aspect-oriented software development to incorporate agents into object-oriented systems, as demonstrated in the case of the Portalware MAS. This approach has the potential to improve the development of MASs and enhance their maintainability and reusability.
41949	4194923	Engineering multi-agent systems with aspects and patterns	Objects and agents are important concepts in software engineering, with agents being more complex as they have additional concerns such as beliefs, goals, and behavior properties. Multi-agent systems typically include different types of agents and objects, and a structured approach is needed for their composition. This paper compares an aspect-based approach and a pattern-based approach for building multi-agent software, both with the goals of aligning high-level agent models with object-oriented designs, separating agency concerns, and allowing for flexible and easy-to-maintain systems. The authors demonstrate the practicality of these approaches through a real-life example, the Portalware system.
41949	4194971	AgentZ: extending object-z for multi-agent systems specification	Agent-orientation has become increasingly important due to the rise of the World Wide Web. This has resulted in a growth of research focused on developing new software engineering techniques to support agent-oriented system specification, design, validation, and development. This paper introduces a formal notation called AgentZ, which combines the concepts and structure of TAO (Taming Agents and Objects) with the established formal representation languages Z and Object-Z. AgentZ aims to improve the quality of multi-agent systems by providing a formal notation for verifying design models, a crucial aspect in agent-oriented software engineering. 
41950	4195025	Analytic Tangent Irradiance Environment Maps for Anisotropic Surfaces	This paper discusses the use of spherical harmonic irradiance maps in environment-mapped rendering for anisotropic surfaces. While this technique is commonly used for Lambertian isotropic surfaces, it is not suitable for materials that are anisotropic and require shading based on local tangent direction. The authors propose an extension of spherical harmonic irradiance maps to anisotropic surfaces, using the diffuse term of the Kajiya-Kay model. They provide an analytic formula for the diffuse BRDF in terms of spherical harmonics, showing faster decay than Lambertian reflectance. This formula can be easily implemented in existing code for real-time rendering and has potential applications in offline rendering for fibers.
41950	4195041	A signal-processing framework for reflection	This paper introduces a signal-processing framework for studying the behavior of light reflected from convex curved surfaces under distant illumination. This framework has both theoretical and practical applications in computer graphics, such as determining lighting distributions and BRDFs, rendering with environment maps, and image-based rendering. The reflection operator can be thought of as a convolution, and the reflected light field can be expressed as a product of the spherical harmonic coefficients of the illumination and BRDF. This has implications for inverse rendering, where BRDF and lighting parameters can be estimated from real photographs, and for forward rendering, where objects can be efficiently rendered under complex lighting conditions. The analysis also has implications for computer vision tasks such as recognition, photometric stereo, and structure from motion.
41951	41951118	Hierarchical Iterated Local Search for the Quadratic Assignment Problem	Iterated local search (ILS) is a stochastic method that combines a perturbation step with a local search algorithm. This article proposes a new way of hybridizing ILS by using it as the embedded local search in another ILS. This nesting of local searches can be further iterated, creating a hierarchy of ILS. The paper explores this idea in the context of solving the quadratic assignment problem and shows that the hierarchical ILS outperforms a "flat" ILS in terms of efficiency and effectiveness. These results suggest that hierarchical ILS has potential for other applications and should be further investigated.
41951	41951119	Automatic algorithm configuration based on local search	Designing algorithms for hard problems involves determining appropriate values for free algorithm parameters, which can be a challenging and tedious task. These parameters include categorical choices and numerical values. Currently, manual tuning using rules of thumb and crude heuristics is the most common approach, with more principled methods being rarely used. However, a new local search approach for algorithm configuration has been developed and proven to converge to the globally optimal parameter configuration. It is versatile and can be used for minimizing run-time in decision problems or maximizing solution quality in optimization problems. The approach can be applied to any algorithm and has shown superior performance compared to other systems. The code, along with instructions, is available online. 
41952	4195215	Ruling analysis and classification of torn documents	This paper presents a new ruling classification method for document clustering. Unlike previous methods that focus on removing ruling lines, this method analyzes ruling lines for reassembling document snippets. It uses a novel Fourier feature and Support Vector Machines to classify lines into void, lined, or checked categories. The method also includes accurate line localization using projection profiles and robust line fitting. Evaluation on real-world document snippets shows an F-score of 0.987, and a synthetic dataset achieves an F-score of 0.931. The dataset used for evaluation is publicly available for benchmarking purposes. 
41952	4195237	A Gradient Extension of Center Symmetric Local Binary Patterns for Robust RGB-NIR Image Matching	RGB and Near Infra-Red (NIR) filters are commonly used for scene acquisition, but they can cause significant changes in intensity and texture between images of the same scene. This makes it difficult to perform interest point based image matching. To address this issue, a new method using Center Symmetric-Local Binary Patterns (CS-LBP) is proposed. This method extracts features from both intensity and gradient magnitude maps of image patches centered at interest points, and then uses these features in the SIFT algorithm for more robust descriptors. Experimental results show that this method improves descriptor matching and achieves better image matching results compared to other methods using CS-LBP and SIFT for interest point description.
41953	4195313	Enumerating Maximal Bicliques from a Large Graph Using MapReduce	In this study, the authors focus on finding the maximum number of bipartite cliques (bicliques) in a large graph, which is important for data mining in social networks and bioinformatics. They introduce new parallel algorithms for the MapReduce platform and test them using Hadoop MapReduce. Their approach involves breaking the graph into smaller subgraphs and processing them simultaneously. This is made possible by minimizing redundancy and balancing the load on different reducers. The results of their experiments show that the algorithms can handle large graphs with millions of edges and tens of millions of bicliques, which is a significant achievement in this field.
41953	4195312	A Streaming Approximation Algorithm for Klee's Measure Problem	The efficient estimation of frequency moments in one-pass is a fundamental problem in data stream processing. This includes finding the number of distinct elements in a data stream, known as the zeroth frequency moment. This paper focuses on computing the total number of distinct points covered by rectangles in a 2-dimensional space, also known as the Klee's measure problem. The authors propose a randomized streaming approximation algorithm that achieves sub-polynomial complexity bounds, with an amortized processing time per rectangle of $O(\frac{1}{\epsilon^4}\log^3 n\log\frac{1}{\delta})$, a space complexity of $O(\frac{1}{\epsilon^2}\log n \log\frac{1}{\delta})$ bits, and a query time of $O(\log\frac{1}{\delta})$. This is the first streaming approximation for the Klee's measure problem with sub-polynomial bounds.
41954	4195417	Fast and accurate processor models for efficient MPSoC design	The development of embedded software for upcoming MPSoC architectures is becoming increasingly challenging due to growing system complexity and greater software content. Traditional ISS-based validation methods are no longer feasible due to this complexity. To address this issue, the article introduces an approach of abstract processor modeling for multiprocessor architectures. This model combines computation modeling, an abstract RTOS, and accurate interrupt handling to create a versatile and multifaceted processor model with various levels of features. This model is then integrated into a system model using automatic generation and compilation, allowing for rapid design space exploration and validation. Experimental results on a multiprocessor mobile phone platform show the benefits of this approach, with a simulation speed of 300MCycles/s and less than 3&percnt; error. The results also demonstrate the trade-off between speed and accuracy at different levels of abstraction, providing guidance for future processor model designers.
41954	4195443	Quantitative analysis of the speed/accuracy trade-off in transaction level modeling	Embedded systems are becoming more complex, making it necessary to model them at higher levels of abstraction. Transaction level modeling (TLM) is a popular approach for simulating and exploring design options quickly, but it often sacrifices model accuracy. This article offers a systematic analysis of the speed/accuracy trade-off in TLM, including a classification of abstraction levels, metrics, and test setups for measuring and comparing performance and accuracy. The analysis is applied to three common bus architectures, demonstrating the potential for significant gains in simulation speed at the cost of lower accuracy. The article concludes that model granularity is crucial for effective TLM abstraction and provides guidelines for choosing the most suitable model for a given application.
41955	4195544	The PITA system: Tabling and answer subsumption for reasoning under uncertainty.	Probabilistic Logic Programming (PLP) is a field that combines probability and logic programs to represent uncertainty in real world domains. This has led to the development of languages such as Independent Choice Logic, LPADs, Problog, and PRISM. These languages share a similar distribution semantics, but computing the probability of queries in PLP programs is complex due to the need to combine non-exclusive explanations. PRISM reduces this complexity by restricting the form of programs it can evaluate, while Possibilistic Logic Programs use a simpler metric of uncertainty than probability. The PITA system, originally designed for LPADs, has been adapted to efficiently support both restricted PLP and Possibilistic Logic Programs. PITA uses tabling with answer subsumption and can be parameterized to support different variations of PLP, such as PITA(IND,EXC) for PRISM and PITA(COUNT) for counting different explanations. PITA has been shown to be competitive with PRISM and is available in XSB version 3.3.
41955	4195554	Well-definedness and efficient inference for probabilistic logic programming under the distribution semantics.	Distribution semantics is a popular approach for combining logic programming and probability theory, used in languages such as Independent Choice Logic, PRISM, pD, LPADs, and ProbLog. However, for programs with function symbols, the semantics is only well-defined if the set of explanations for a query is finite and each explanation is also finite. This limitation can be overcome by restricting the class of allowed programs or by explicitly imposing well-definedness. In this paper, the authors propose an efficient procedure, called PITA, for computing the probability of queries in a larger class of programs. PITA transforms a probabilistic program into a normal program and uses SLG resolution with answer subsumption. The algorithm was tested on six domains and outperformed other approaches in terms of execution time.
41956	41956232	Extraction of rules from discrete-time recurrent neural networks	This paper discusses the importance of extracting symbolic knowledge from trained neural networks and encoding it directly into networks before training. This allows for the exchange of information between symbolic and connectionist representations. The focus is on the extraction of rules from recurrent neural networks, which can be trained to classify strings of a regular language. These rules can be extracted in the form of deterministic finite-state automata using clustering algorithms. The study compares the generalization performances of different extracted models and introduces a heuristic for choosing the best model to approximate the learned regular grammar. 
41956	41956333	Pruning recurrent neural networks for improved generalization performance.	Determining the architecture of a neural network is crucial for effective learning, but for recurrent neural networks, there is no universal method for estimating the number of hidden layers, their size, or the number of weights. To address this issue, a pruning heuristic is proposed that improves the generalization performance of trained recurrent networks. This heuristic is demonstrated by training a fully recurrent network on positive and negative strings of a regular grammar. The results show that the pruned networks produce more consistent rules, and this performance improvement is achieved through pruning and retraining. Simulations using two regular grammars further demonstrate the effectiveness of this method, surpassing the results of training with weight decay.
41957	419574	Reflections on simultaneous impact	The problem of resolving simultaneous impacts in collision response modeling has not been adequately addressed by existing algorithms. These algorithms fail to fulfill five important physical requirements, leading to issues such as non-symmetry preservation, loss of kinetic energy, and inability to handle break-away. To overcome these limitations, a new generalized impact model has been proposed that combines the strengths of two popular approaches. This model satisfies all the identified requirements, including symmetry preservation, kinetic energy conservation, and break-away capabilities. Additionally, a complementary restitution model has been developed to address the problem of inelastic collapse. The proposed algorithm has been successfully applied to large-scale simulations of impacting rigid bodies, including frictional impacts. It has also been validated through physical experiments, accurately capturing the behavior of vertically oscillated granular materials.
41957	4195713	A multi-scale model for simulating liquid-hair interactions	The study of hair and its interactions with liquid is complex and important in understanding the appearance of humans and animals. To better understand this phenomenon, a new simulation framework was developed that combines a discrete rod model for hair and a particle-in-cell model for fluids. This framework also includes a representation of the thin layer of liquid on hair strands. The simulation considers the motion of the liquid along the hair, its cohesion effects between adjacent hairs, and its influence on the hair's dynamics. The framework also includes a drag model for coarse-scale interactions and strategies for transferring liquid between different representations. This simulation has been used to study various scenarios involving wet hair, such as hair flipping, animals shaking themselves dry, and hair coalescence effects.
41958	4195854	Occlusion-Aware Reconstruction And Manipulation Of 3d Articulated Objects	The article introduces a method for recovering complete 3D models of articulated objects using structure-from-motion techniques. By capturing 3D point cloud models of the object in two different configurations, the joint axes can be accurately determined and classified as revolute or prismatic. This allows a robotic system to manipulate the object along its joint axes and exercise its degrees of freedom. The models are occluded-aware, meaning the robot can plan paths to unseen parts of the object. The algorithm does not require prior knowledge of the object and can handle non-planar objects and scenes. Experiments with a PUMA 500 robotic arm demonstrate the effectiveness of the approach on various objects with different joint types.
41958	4195814	Rigid and non-rigid classification using interactive perception	Robotics research typically focuses on either non-contact sensing or machine manipulation, but not both. However, this paper suggests the benefits of combining the two to improve the classification of unknown objects, specifically in service robot applications. The proposed approach involves an object lying on a flat background, and the robot's goal is to interact with and classify each object based on color, shape, and flexibility. Experiments have shown that this method efficiently classifies and labels various objects through interaction. This approach has potential for further advancement in robotics research and could improve the capabilities of service robots.
41959	419591	Living up to Expectations: Computing Expert Responses'	This paper discusses the importance of truthful and informative responses in cooperative man-machine interaction. It emphasizes the need for a system to modify its response if it might mislead the user. The focus is on identifying and avoiding potentially misleading responses by considering the types of informing behavior typically expected of an expert. The paper proposes a formal account of different types of assertions that should be included in responses to questions about achieving a goal, in addition to a simple answer, in order to prevent the questioner from being misled.
41959	419594	Microplanning With Communicative Intentions: The Spud System	Microplanning in natural language generation (NLG) involves solving various problems that require bridging domain-specific representations and general linguistic representations. These include constructing linguistic expressions to identify domain objects, selecting appropriate words to convey domain concepts, and using complex linguistic constructions to succinctly communicate domain facts. The best approach to solving these problems is a uniform, comprehensive, and declarative process. This involves directly exploring a search space for utterances using a linguistic grammar, and using a model of interpretation to assess the progress in conveying domain-specific representations. Challenges for implementation and knowledge representation are addressed by using lexicalized tree-adjoining grammar (LTAG) and modal logic programming. A detailed methodology for designing grammatical and conceptual resources is also outlined. This approach involves a deliberative process of goal-directed activity, where interpretation represents the generator's communicative intent and provides a rich and uniform resource for NLG. By using representations of communicative intent, a generator can simultaneously augment the syntax, semantics, and pragmatics of a sentence and incrementally work towards solutions for microplanning problems.
41960	4196021	On the complexity of sparse elimination	Sparse elimination is a method used to solve polynomial systems that generate zero-dimensional ideals. It works by considering the Newton polytope, or geometric object, of the polynomial instead of its total degree. A monomial basis for the coordinate ring is defined using a mixed subdivision of the Newton polytopes, and a sparse resultant matrix is constructed to compute a multiplication map and find common zeros. The size of the monomial basis is equal to the mixed volume, a measure of complexity, and the computation of this basis is equivalent to computing the mixed volume. The algorithms used have a worst-case complexity proportional to the volume of the Minkowski sum, and new bounds are established in terms of sparsity parameters. A lower bound on mixed volume in terms of Euclidean volume is also proved, which has relevance beyond the context of sparse elimination. 
41960	4196075	A subdivision-based algorithm for the sparse resultant	Multivariate resultants are a generalization of the Sylvester resultant and are used to determine if a polynomial system has a solution. They also simplify the process of finding common roots by reducing it to a linear algebra problem. A determinantal formula has been proposed for the sparse resultant of a system of n + 1 polynomials in n variables. This resultant has a lower degree compared to the classical one and is more efficient for sparse polynomials. An algorithm using a mixed polyhedral subdivision of Newton polytopes has been developed to compute this resultant. It also implies a restricted version of an effective sparse Nullstellensatz. The algorithm can handle the general case with polynomial complexity in the resultant degree and exponential complexity in n. It is also believed that it can produce an exact rational expression for the sparse resultant.
41961	4196186	Average Whenever You Meet: Opportunistic Protocols for Community Detection.	The article discusses an asynchronous communication model, where one edge is randomly activated in each round and its endpoints can exchange messages and perform computations. A random process is studied, where the first time a vertex is an endpoint of an active edge, it chooses a random number and in subsequent rounds, the values of the endpoints are updated to their average. The authors show that if the graph has a two-community structure, the values held by the nodes will reflect this structure during a certain phase of the process. This analysis requires new concentration bounds on random matrices and is used to design opportunistic protocols for recovering community structure with low computational costs.
41961	4196111	Simple dynamics for plurality consensus	The article discusses the Plurality Consensus process, where anonymous agents in a communication network revise their initial opinions (represented by colors) based on a random sample of their neighbors. The goal is for all nodes to eventually support the most popular color. The initial color configuration has a bias towards the plurality color. The study focuses on a basic model with a clique network and a 3-majority update rule. A tight bound on the convergence time is proved, with a linear dependence on the number of colors and logarithmic dependence on the number of agents. It is shown that increasing the number of random neighbors does not significantly speed up the process.
41962	419623	Kernel-based machine learning for fast text mining in R	Kernel-based machine learning methods have made significant progress in processing text data using string kernels and suffix arrays. The kernlab package offers a wide range of already implemented algorithms and infrastructure for these methods. Additionally, by utilizing the text mining capabilities of the tm package, R users can efficiently process, visualize, and group large collections of text data. The focus of these packages is on the effectiveness of different types of string kernels in performing these tasks.
41962	4196236	Nonparametric Distribution Analysis for Text Mining.	Recently, new algorithms have been developed for nonparametric distribution analysis using Maximum Mean Discrepancy measures. These algorithms operate in Hilbert space and can be used for two-sample tests without assuming any specific distribution. With the advancement of string kernels, these methods have expanded the application of kernel-based methods in text mining. This article focuses on reviewing the use of these kernel-based two-sample tests in text mining and suggests new applications. The authors also present an efficient implementation of these methods in the kernlab package and introduce an integrated environment for applying modern machine learning techniques in complex text mining problems using the tm and kernlab R packages.
41963	419634	Multimodal authentication based on random projections and source coding	This paper presents an authentication framework for independent modalities using binary hypothesis testing and source coding with random projections. The source coding allows for the reconstruction of multimodal signals at the decoder using authentication data, while random projections are employed to address security, privacy, robustness, and complexity concerns. The performance of the authentication system is analyzed in both direct and random projection domains, with an asymptotic performance approximation compared to exact solutions. The effectiveness of modality fusion on the authentication system is also demonstrated. 
41963	4196332	Multimodal object authentication with random projections: a worst-case approach	This paper proposes a forensic authentication framework using binary hypothesis testing in random projections domain. The framework takes into account various counterfeiting strategies and evaluates authentication performance under the Neyman-Pearson framework and for average probability of error in direct and random projections domains. The paper also presents the worst-case attack/acquisition channel that results in the most significant performance loss in terms of Bhattacharyya distance reduction. Computer simulations confirm the theoretical findings.
41964	4196415	Balancing accountability and privacy using e-cash (extended abstract)	The concept of electronic cash (e-cash) allows users to withdraw coins from the bank and spend them anonymously and without being traced. However, for certain applications such as tax purposes, it is necessary to set limits on the amount of money that can be spent anonymously. This paper introduces a new e-cash system that allows for these limits to be enforced without the need for a trusted party. The system ensures anonymity as long as the user does not double-spend a coin or exceed the publicly-known spending limit with any merchant. This system is based on a compact e-cash system and is secure under the same assumptions in the random-oracle model. It is also efficient, with only O(ℓ+k) bits needed to store 2ℓ coins and the complexity of the withdrawal and spend protocols being O(ℓ+k). This allows for a balance between accountability and privacy, which is not possible with regular cash.
41964	4196435	Optimistic fair exchange with multiple arbiters	Fair exchange is a major issue in secure distributed computation, where two parties, Alice and Bob, want to exchange something without either of them being cheated. This requires a trusted third party, or arbiter, to ensure fairness. In optimistic fair exchange, the arbiter only intervenes if there is a problem, but must still be trusted. To reduce this trust, it is possible to use multiple autonomous arbiters, but it is difficult to achieve synchronization in a peer-to-peer setting. Avoine and Vaudenay proposed a protocol with multiple autonomous arbiters, but their approach has limitations. This paper introduces a new class of protocols called "distributed arbiter fair exchange" (DAFE) that addresses these limitations, but ultimately proves that no DAFE protocol can exist.
41965	4196533	Bayesian Generalized Kernel Mixed Models	The proposed methodology is a fully Bayesian approach for generalized kernel mixed models (GKMMs), which are extensions of generalized linear mixed models using a reproducing kernel. The regression vector of the model is assigned a mixture prior, allowing for sparsity and aiding in Bayesian computation. A Markov chain Monte Carlo (MCMC) algorithm is developed using data augmentation, reversible jump method for model selection, and Bayesian model averaging for posterior prediction. This approach can be related to the Karhunen-Loève expansion of a Gaussian process (GP), leading to a flexible approximation method for GPs. 
41965	4196525	A scalable community detection algorithm for large graphs using stochastic block models	Community detection in graphs is widely used in social and biological networks. The stochastic block model is a popular probabilistic tool for describing graphs with community structures. However, traditional inference algorithms for this model are not efficient for large datasets. In this paper, a multi-stage maximum likelihood approach is proposed to recover the latent parameters of the model in linear time. A parallel algorithm based on message passing is also introduced, allowing for faster processing on multiple processors. Experiments show that the algorithm produces high quality results on both benchmark and real-world graphs, and can identify more meaningful communities compared to other popular algorithms. For a graph with 1.3 million nodes and 10 million edges, the algorithm only takes 6 seconds to process on 64 cores. 
41966	4196645	Symbolic execution of concurrent objects in CLP	The concurrent objects model involves objects with dedicated processors in a distributed environment where communication is through asynchronous method calls. This poses a challenge in symbolic execution due to the combination of object-oriented features, concurrency, and backtracking. To address this, the paper proposes a CLP-based approach where the OO program is transformed into an equivalent CLP program with built-ins for handling concurrency. These built-ins include primitives for synchronization and scheduling. Symbolic execution of the transformed program then becomes a standard sequential execution of CLP. A prototype implementation in the PET system demonstrates the feasibility of this approach.
41966	4196649	Compositional CLP-based test data generation for imperative languages	Glass-box test data generation (TDG) is a method of automatically creating test input data for a program by examining its internal structure. This is done through symbolic execution, where variables are represented as expressions instead of actual values. CLP-based TDG translates imperative programs into CLP equivalents and uses the standard evaluation mechanism of CLP for symbolic execution. However, this process can become expensive for large programs due to the numerous paths that need to be explored. To address this issue, the authors propose a compositional reasoning approach where different parts of a program can be tested separately and their test cases can be combined to obtain information on the entire program. This approach also allows for handling native code, which may be written in a different programming language, by using test cases and compositional reasoning. 
41967	4196771	Conditional termination of loops over heap-allocated data.	Static analysis, a method used to analyze computer programs for potential errors, often does not take into account the values stored in the heap, making it less accurate. In this article, the authors propose a solution for accurately tracking heap-allocated data in Java programs by using ghost variables instead of directly tracking object fields and array contents. This allows for more efficient heap-insensitive analysis while still providing information on the original heap-allocated data. If certain conditions cannot be proven, the authors suggest using aliasing preconditions to ensure the program will terminate. Experimental results demonstrate that this method greatly improves accuracy with a reasonable overhead.
41967	4196758	Field-Sensitive Value Analysis by Field-Insensitive Analysis	Static analysis of shared and mutable data-structures is difficult, as most analyzers struggle to keep track of the values of numeric variables stored in the heap. This paper proposes a solution by identifying conditions in which heap-allocated numeric variables in object-oriented programs can be treated as non-heap-allocated variables. A static analysis is then presented to infer which numeric fields meet these conditions at the bytecode level, allowing for the use of ghost variables to make these fields observable to value analysis. Experimental results in termination analysis demonstrate that this approach expands the range of analyzable programs with minimal overhead.
41968	4196889	A Technique For Recursive Invariance Detection And Selective Program Specialization	This paper discusses a method for optimizing programs by reducing checks within cycles using Program Transformation and Abstract Interpretation. The technique involves using an abstract interpreter to detect simple invariants and then using program transformation to simplify predicates within the cycle. This allows for optimizations beyond what can be achieved with a single pass of the abstract interpreter. The paper presents a class of programs that can benefit from this technique and provides examples and evaluations in areas such as floundering detection and reducing run-time tests in automatic logic program parallelization. The technique can be implemented using existing tools, making it a valuable addition to the compiler design. 
41968	4196896	Abstraction-Carrying Code: a Model for Mobile Code Safety	Proof-Carrying Code (PCC) is a method for ensuring the safety of mobile code by attaching a certificate or proof to the program. This allows the program consumer to validate the certificate locally, making it simpler, more efficient, and automatic. The success of PCC relies on the availability of technologies that can prove programs correct and replace expensive verification processes with efficient checking procedures. Abstraction-Carrying Code (ACC) is a new approach that uses abstract interpretation as the enabling technology. It relies on abstract models of programs, computed by static analyzers, as certificates that can be checked by an abstract interpreter. This approach brings expressiveness, flexibility, and automation to mobile code safety. ACC has been implemented and benchmarked in the context of constraint logic programming, showing faster checking times and reasonable certificate sizes. It can also be used to generate certificates for complex program properties, including resource-related ones.
41969	4196980	Roadmap-Based level clearing of buildings	This paper presents a roadmap-based method for multi-agent search in buildings or multi-story environments. The method involves creating a graph representation of the environment, known as a roadmap, which allows for feasible paths to be encoded. The roadmap is divided into regions, such as levels, and specific strategies are designed for each region to efficiently cover and clear the environment. This approach offers guarantees on coverage and the minimum number of agents required. It is also capable of handling complex and realistic environments, unlike other methods that are limited to simple 2D environments. 
41969	4196912	Using Load Balancing to Scalably Parallelize Sampling-Based Motion Planning Algorithms	Motion planning involves finding feasible paths for a movable object in an environment and has various applications like robotics, CAD, and protein folding. The most effective methods for solving this complex problem are sampling-based planners. However, parallelization of these planners often leads to load imbalance due to varying characteristics of different regions and increasing heterogeneity as the number of processors increases. To address this issue, two techniques – adaptive work stealing and bulk-synchronous redistribution – have been introduced. These techniques have been applied to two major classes of parallel sampling-based planners, resulting in better scalability and load balancing on over 3,000 cores. 
41970	4197041	Small-space analogues of Valiant's classes	The uniform circuit model of computation uses the width of a boolean circuit to represent the space complexity of a function. Similarly, in Valiant's algebraic model, the width of an arithmetic circuit can be used as a measure of space. The class VL is introduced as an algebraic variant of deterministic log-space L. In the uniform model, the definition of VL is equivalent to that of VPSPACE at polynomial width. To define algebraic variants of non-deterministic space-bounded classes, "read-once" certificates for arithmetic circuits are introduced. It is shown that polynomial-size algebraic branching programs can be expressed as a read-once exponential sum over polynomials in VL, and that VBPs are stable under these sums. Additionally, it is shown that read-once exponential sums over a restricted class of constant-width arithmetic circuits fall within VQP, making it the largest known subclass of poly-log-width circuits with this property.
41970	419708	Arithmetic Circuits, Syntactic Multilinearity, and the Limitations of Skew Formulae	In arithmetic NC 1, functions have been shown to have equivalent constant width polynomial degree circuits, but it is not known if the converse is true. This article examines the question by showing that syntactic multilinear circuits of constant width and polynomial degree can be reduced in depth, but may not remain syntactic multilinear. The focus is then on studying polynomial-size syntactic multilinear circuits and the relationships between classes of functions with different resource restrictions (width, depth, degree). A characterisation of NC 1 and its arithmetic counterparts is also obtained through log width restricted planar branching programs. The power of skew formulae is also studied, and it is shown that even exponential sums of these may not be enough to express the determinant function.
41971	4197177	Robust fuzzy control for uncertain discrete-time nonlinear Markovian jump systems without mode observations	This paper focuses on the robust fuzzy control problem of uncertain discrete-time nonlinear Markovian jump systems without mode observations. It utilizes the Takagi and Sugeno (T-S) fuzzy model to represent the system with parameter uncertainties and Markovian jump parameters, resulting in an uncertain Markovian jump fuzzy system (MJFS). A stochastic fuzzy Lyapunov function (FLF) is used to analyze the robust stability of the uncertain MJFS, incorporating both system modes and membership functions. A mode-independent state-feedback control design is then developed using a non-parallel distributed compensation (non-PDC) scheme to ensure stochastically stability for all uncertainties. The conditions for robust stability and mode-independent stabilization are formulated as linear matrix inequalities (LMIs), which can be efficiently solved using existing optimization techniques. A simulation example is also presented to demonstrate the effectiveness of the proposed method.
41971	4197164	H2 guaranteed cost fuzzy control design for discrete-time nonlinear systems with parameter uncertainty	This paper discusses a method for designing H"2 guaranteed cost fuzzy controllers for discrete-time nonlinear systems with uncertain parameters. The Takagi and Sugeno fuzzy model is used to represent the uncertain system and a condition for the existence of these controllers is presented using linear matrix inequalities. These controllers not only ensure stability of the system, but also provide a guaranteed cost on the H"2 performance index. An optimal controller is obtained through an LMI optimization procedure and its effectiveness is demonstrated through simulations on a truck-trailer control system. Overall, this method offers a practical approach for designing robust fuzzy controllers for uncertain nonlinear systems.
41972	41972141	Adaptive document image binarization	The article introduces a new approach to document image binarization, where the page is divided into smaller components such as text, background, and pictures. It addresses issues caused by noise, illumination, and different types of degradation. Two new algorithms are used to determine a local threshold for each pixel. The performance of the method is evaluated using test images with ground-truth, evaluation metrics for textual and synthetic images, and a weight-based ranking system. The experimental results demonstrate that the proposed method performs well in different scenarios and outperforms other existing techniques.
41972	41972158	Outex - New Framework for Empirical Evaluation of Texture Analysis Algorithms	The paper discusses a new project that aims to create a framework and database for evaluating texture analysis algorithms. This Outex framework contains a variety of surface textures captured under different conditions, allowing for the creation of various texture analysis problems. The framework also includes test suites with baseline results from existing algorithms, showcasing its versatility in tasks such as classification, segmentation, and retrieval. The project has a website for public access to the database and comparative results from research groups globally. Overall, the Outex framework offers a comprehensive and accessible resource for testing and comparing texture analysis algorithms.
41973	4197331	Meta-Learning for Semi-Supervised Few-Shot Classification.	Few-shot classification is a type of learning algorithm that trains a classifier using only a few labeled examples. Recently, there has been progress in this field using meta-learning, where a parameterized model is trained on different classification problems, each with a small labeled training set and corresponding test set. This research aims to advance this paradigm by incorporating unlabeled examples within each episode. The study considers two scenarios: one where all unlabeled examples belong to the same classes as the labeled examples, and another more challenging situation with additional distractor classes. To address this, Prototypical Networks are extended to use unlabeled examples when creating prototypes and are trained in an end-to-end manner. Experiments on different benchmarks show that this approach can improve predictions due to the inclusion of unlabeled examples. A new split of ImageNet, with a hierarchical structure, is also proposed for further evaluation.
41973	4197326	BoltzRank: learning to maximize expected ranking gain	The process of ranking documents based on their relevance to a query is a common problem in information retrieval. However, learning ranking functions can be difficult as the metrics used to judge performance are not smooth. To address this issue, the authors propose a new approach that creates a probability distribution over document rankings for a given query. This allows for optimization of the expected value of a performance measure through gradient ascent. The distribution is based on a Boltzmann distribution and includes pairwise potentials, which allows for encoding of regularities in document scores. Experimental results show that this method out-performs existing approaches on a standard data set. 
41974	4197430	An innovative feature selection using fuzzy entropy	The paper presents a new approach for feature subset selection, which is divided into two phases. The first phase focuses on reducing the algorithm run time by finding the best number of clusters in the dataset using silhouette value and calculating entropy fuzzy measures for each feature. In the second phase, the goal is to select a feature subset that meets certain boundaries to achieve high accuracy. The method is tested on various datasets and results show that it is able to select the minimum number of features without significantly impacting the final classification accuracy, making it a promising approach for feature selection. 
41974	4197415	Optimizing Classification Ensembles via a Genetic Algorithm for a Web-Based Educational System	Classification fusion is a method that combines multiple classifications to create a more accurate solution. Feature extraction reduces the computational cost and improves accuracy by creating new features from existing ones. This paper proposes a method for predicting students' final grades using features extracted from a web-based educational system. By using a combination of classifiers and weighing the importance of features with a Genetic Algorithm, the classification performance is significantly improved. The paper also shows that feature weighting and transformation are more effective than feature subset selection when there are only a few features. This approach is adaptable to different courses, population sizes, and types of features.
41975	4197547	Lia: A Location-Independent Transformation For Asocs Adaptive Algorithm 2	Artificial Neural Networks (ANNs) with fixed topologies often face challenges during learning. However, ANNs that use dynamic topologies have shown promise in overcoming these issues. Adaptive Self-Organizing Concurrent Systems (ASOCS) is a type of learning model with dynamic topologies. This paper introduces Location-Independent Transformations (LITs) as a way to efficiently implement dynamic topologies in parallel hardware. LITs create location-independent nodes that can compute network outputs using local information, allowing for the addition and deletion of nodes during learning. The paper presents the Location-Independent ASOCS (LIA) model as an LIT for ASOCS Adaptive Algorithm 2, and provides formal definitions for LIA algorithms. This not only describes LIA, but also provides a formal description of basic ASOCS mechanisms in general. 
41975	4197543	A self-adjusting dynamic logic module	The ASOCS model is a way to process rule-based systems in areas like adaptive logic, robotics, and control using parallel processing. It consists of many simple computing elements that work asynchronously. This paper focuses on Adaptive Algorithm 2 (AA2) and its architecture and learning algorithm. AA2 has advantages in memory and knowledge maintenance compared to previous ASOCS models. The ASOCS can operate in a data processing mode or a learning mode. In learning mode, it receives a new rule and incorporates it in a short time. In data processing mode, it functions as a parallel hardware circuit.
41976	4197632	Principles for robust evaluation infrastructure	The "Cranfield" evaluation method has been used for almost 50 years and has played a crucial role in the development of large-scale information retrieval systems. Recent investigations have uncovered flaws in some experiments, leading to better understanding and prevention of these issues in future work. Based on their research and observations, the authors propose principles for designing new evaluation infrastructure. This statement highlights the importance of systematic evaluations in identifying strengths and limitations of retrieval systems and how this knowledge can inform future experiments and infrastructure design.
41976	4197651	Incorporating User Expectations and Behavior into the Measurement of Search Effectiveness.	Information retrieval systems are designed to help users find the information they need. However, in order to accurately evaluate the effectiveness of these systems, it is important to consider the user's goal and behavior. This includes the complexity of the search task and the rate at which the user's goal is being fulfilled. A new effectiveness metric called INST is proposed, which takes into account these factors and can adapt to changing user behavior. INST is compared to other metrics and found to be useful. Additionally, it is shown that there is significant variability in user queries, which has implications for experiment design and test collections. 
41977	4197721	On the Graph Traversal and Linear Binary-Chain Programs	Grahne et al. have introduced a graph algorithm for evaluating recursive queries, consisting of two phases. The first phase transforms a linear binary-chain program into equations with predicate symbols. The second phase involves constructing a graph and traversing relevant paths to produce answers. A new algorithm has been developed which is faster than Grahne's, by reducing the search space and generating answers directly from previously found answers and associated path information. This results in a linear time complexity for both acyclic and cyclic data. 
41977	419778	On the Optimal Top-down Evaluation of Recursive Queries	This paper introduces a top-down method for handling recursive queries that is set-oriented and uses RQA/FQI's mechanism for recording path information. The goal is to reduce the number of expansion operations typically required in other top-down strategies. By generating answers directly from intermediate results and path information, the evaluation process for cyclic data is sped up and achieves high efficiency. This approach results in a smaller search space and lower cost for generating answers compared to methods using algebraic operations. Overall, this method offers a more efficient solution for handling recursive queries.
41978	419786	CloudNet: dynamic pooling of cloud resources by live WAN migration of virtual machines	Virtualization technology has greatly expanded the possibilities for resource management, allowing for the easy migration of virtual machines (VMs) within a local area network (LAN). This has led to a shift from allocating resources on a single server to managing pools of resources in a data center. The next step in this evolution is expected to be the migration of VMs over wide area networks (WANs), which will allow for the provisioning of resources across multiple data centers. The CloudNet architecture, consisting of cloud computing platforms and a virtual private network (VPN)-based network infrastructure, aims to facilitate this transition. It offers optimized support for live WAN migration of VMs, with a set of optimizations that reduce memory migration time by 65% and decrease bandwidth consumption by 50%. This has been successfully evaluated on an operational cloud platform spanning the continental US.
41978	4197825	SMOG: a cloud platform for seamless wide area migration of online games	The popularity of highly interactive network applications, like online games, is increasing, but game providers struggle to support them due to their need for low latency. Cloud computing has been successful for other applications, but is not suitable for games due to their unpredictable workload and demands for latency and scale. To address this issue, a framework called SMOG is proposed, which uses dynamic server migration and network route control to optimize game server location and minimize observable effects of server migration. A prototype implementation on a Tier-1 ISP's backbone showed a decrease in end-user latency by up to 60% without disrupting gameplay. SMOG can also be applied to other latency-sensitive interactive applications.
41979	4197937	A Structure-preserving Clause Form Translation	Resolution theorem provers are commonly used to find proofs for theorems, but they often convert the theorem into a clause form first. However, this conventional translation can obscure the structure of the formula and make it much longer. To address this issue, a non-standard clause form translation has been developed that preserves more of the formula's structure and avoids the exponential increase in length. This approach can also be combined with replacing predicates by their definitions before converting to clause form. Additionally, a new method called lock resolution has been created specifically for this non-standard clause form translation, resulting in a significant reduction in search space and time for one example. These techniques make resolution theorem provers more appealing for program verification, as the theorems involved in this field are often simple but tedious for humans to prove.
41979	4197945	The space efficiency of OSHL	Ordered semantic hyper-linking (OSHL) is a first-order theorem prover that combines first-order and propositional techniques to increase speed. It converts first-order clauses to ground clauses and applies propositional methods to them. OSHL-U is an extension of OSHL that includes rules for unit clauses, making the instantiation strategy even faster. OSHL-U has been proven to generate many of the same proofs as the resolution prover Otter, without using true unification. This suggests that techniques used to speed up propositional provers can also be applied to first-order provers. OSHL-U also produces and stores less clauses than Otter on many TPTP problems, and has been shown to find more proofs on some TPTP groups, despite a slower inference rate.
41980	4198020	A Decidable Quantified Fragment Of Set Theory With Ordered Pairs And Some Undecidable Extensions	This paper discusses a fragment of set theory with restricted quantification, which includes pair related quantifiers and constructs. The goal is to explore its potential applications in knowledge representation. The decision problem for this language has a non-deterministic exponential time complexity, but for formulae with shorter quantifier prefixes, it becomes NP-complete. Despite this restriction, important set-theoretic constructs, particularly those related to maps, can still be expressed. The paper also presents some undecidable extensions of the language, such as those involving the operators domain, range, image, and map composition. Overall, this paper provides insights into the complexity and expressiveness of this fragment of set theory.
41980	4198015	A decidable two-sorted quantified fragment of set theory with ordered pairs and some undecidable extensions.	The paper discusses a two-sorted fragment of set theory with restricted quantification, which includes pair-related quantifiers and constructs. The decision problem for this language has a nondeterministic exponential-time complexity, but becomes NP-complete for formulae with limited quantifier prefixes. Despite this restriction, the language is still able to express useful set-theoretic constructs, particularly related to maps. The paper also explores the potential applications of this language in knowledge representation, specifically in metamodeling. Comparisons are made with similar languages in terms of expressivity, and some undecidable extensions are presented, involving operators like domain, range, image, and map composition.
41981	4198130	Dynamical Systems and Stochastic Programming: To Ordinary Differential Equations and Back	This paper examines the relationship between two types of models for biological systems: ordinary differential equations (ODE) and stochastic and concurrent constraint programming (sCCP). The authors define methods for converting between the two types of models and study their properties. They find that the mapping from sCCP to ODE preserves rate semantics for biochemical models and explore the invertibility of the mappings. They also investigate if the models produced by the mappings have the same dynamics, and provide examples where this property fails in the inverse direction. Overall, the paper highlights the importance of considering both types of models in understanding the behavior of biological systems.
41981	4198145	Hybrid dynamics of stochastic programs	Stochastic Concurrent Constraint Programming (sCCP) is a process algebra based on CCP that uses hybrid automata to provide a semantics. Each sCCP program is associated with both a stochastic and a non-deterministic hybrid automaton. These automata are compared to the standard stochastic semantics (Continuous Time Markov Chain) and a fluid-flow approximation technique based on ordinary differential equations. Two case studies, Repressilator and the Circadian Clock, are examined in detail to analyze the robustness and impact of discreteness on the dynamics of these systems.
41982	4198232	Optimal call admission and preemption control for public safety communications	Public safety communication systems use trunked mobile radio systems. The Engset model is commonly used to model these systems. This paper discusses the optimal call admission and preemption control for these systems using the Engset model. The system considers two classes of users, high and low priority. The optimal call admission policy for low priority users is a state-dependent threshold-based policy, and the optimal call preemption policy for low priority users is a threshold-based policy. This analysis provides a framework for efficient and effective management of public safety communication systems.
41982	4198233	Energy Efficient Relaying and Coalition-Forming in Relay Networks	This paper discusses the use of relaying in wireless networks to improve system performance by increasing spatial diversity. The focus is on the energy tradeoff made by relay nodes between transmitting their own data and forwarding others' information in fading channels. The authors propose a power control policy for minimizing energy consumption in a two-node relay network while meeting outage probability requirements. They also address the problem of forming optimal partial coalitions of relays in a larger system, taking into account selfish constraints. The paper presents a polynomial time approximation algorithm for this NP-hard problem, with a cost that is no more than twice that of the optimal solution.
41983	419832	Exclusion and Guard Zones in DS-CDMA Ad Hoc Networks	The main issue in direct-sequence code-division multiple-access (DS-CDMA) ad hoc networks is preventing a near-far problem. This can be addressed by using guard zones, such as an exclusion zone and a carrier-sense multiple-access (CSMA) guard zone. The exclusion zone represents the minimum physical separation between mobiles, while the CSMA guard zone deactivates potentially interfering mobiles beyond the exclusion zone. This paper analyzes DS-CSMA networks with these guard zones, considering a finite network with a uniform clustering spatial distribution. The analysis uses a closed-form expression for the outage probability in the presence of Nakagami fading, taking into account the network geometry. The tradeoffs between exclusion zones and CSMA guard zones are explored for DS-CDMA and unspread networks, with the spreading factor and guard-zone radius providing design flexibility for achieving specific levels of outage probability and transmission capacity. The advantage of an exclusion zone is that it allows for a constant number of active mobiles, leading to higher transmission capacities compared to a CSMA guard zone.
41983	4198341	Optimization of a finite frequency-hopping ad hoc network in Nakagami fading	This paper focuses on analyzing and optimizing a frequency-hopping ad hoc network with a limited number of mobile devices and a finite spatial range. The mobiles use coded continuous-phase frequency-shift keying (CPFSK) modulation to communicate. The system's performance is determined by the number of hopping channels, the error-correction code rate, and the CPFSK modulation index. To maximize the system's transmission capacity, which measures its spatial spectral efficiency, these parameters are jointly optimized. The transmission capacity is calculated using a recently developed expression for the spatially averaged outage probability in the presence of Nakagami fading. This expression can be solved analytically without shadowing and numerically with shadowing.
41984	419846	Integration of domain knowledge in the form of ancillary map data into supervised classification of remotely sensed data	In recent years, machine learning and data mining have become popular in remote sensing applications, particularly in land cover and vegetation mapping. This paper introduces a new method that incorporates additional information (domain knowledge) in supervised classification of land cover using high dimensional remote sensing data. The traditional classification schemes used for ecological or land use purposes often have classes that are difficult to distinguish in remote sensing data. This paper presents a technique that utilizes prior probabilities in a decision tree classification algorithm to effectively include ancillary data sources and improve classification accuracy. This method is based on recent developments in statistics and machine learning, allowing for robust estimates of class membership without penalizing rare classes. 
41984	4198445	Recursive Automatic Bias Selection for Classifier Construction	Empirical comparisons of learning algorithms have shown that each algorithm has different strengths and weaknesses, and it is often unclear which one will perform best on a given data set. To address this, a new approach using feedback from the learning process is proposed. This approach uses a hybrid classifier, combining different representation languages, and relies on heuristic knowledge to determine which one to use for a given data set. This approach, called Model Class Selection (MCS), has been evaluated and shown to achieve equal or higher accuracy than the best individual learning algorithms for each data set. This demonstrates the effectiveness of the heuristic rules used by MCS to select an appropriate learning bias.
41985	4198512	On mining complex sequential data by means of FCA and pattern structures.	Data sets in today's world are becoming increasingly complex and diverse, making data mining essential for real-world applications. This study focuses on analyzing "complex" sequential data using Formal Concept Analysis (FCA) and its extension, pattern structures. These structures, based on a subsumption operation, are used to mine complex data such as sequences or graphs. By utilizing projections, the study shows how meaningful patterns can be identified and computing efficiency can be improved. The method was applied to a French healthcare data set on cancer and the results, including annotations and analysis from a physician, are presented. The study highlights the importance of data mining and FCA in analyzing and understanding complex data sets.
41985	4198546	Mining gene expression data with pattern structures in formal concept analysis	This paper presents two methods for efficiently mining numerical data using formal concept analysis (FCA). Traditionally, FCA requires data to be binarized through a scaling process, leading to information loss or dense and difficult to process data. The authors propose two FCA-based approaches for gene expression data analysis: one using a specific scaling method and the other using pattern structures without prior transformation. They demonstrate that the two methods are equivalent, but the latter is more computationally efficient and produces more readable results. Real-world experiments with gene expression data are conducted to compare and evaluate the two methods, providing practical insights for their use.
41986	4198633	Privacy-preserving mobile crowd sensing in ad hoc networks.	Mobile Crowd Sensing (MCS) is a new paradigm that aims to outsource sensing data collection to the owners of mobile devices. This approach has the potential to revolutionize the traditional methods of collecting and processing sensing data. However, the widespread use of MCS raises concerns about privacy, especially when using third-party infrastructures. To address these concerns, this paper proposes three protocols for privacy preservation in ad hoc networks. The Privacy-Preserving Summation (PPS) protocol protects the privacy of Sensing Service Consumers (SSCs), while the Privacy-Preserving Difference Rank Computation (PPDRC) protocol ensures the privacy of Mobile Device Owners (MDOs). Additionally, the Approximate K-Nearest Neighbor with Privacy Preservation (AKN2P2) protocol is proposed to identify the k-nearest neighbors without compromising the privacy of either the MDOs or SSCs. The paper also includes performance evaluations of these protocols under different settings.
41986	4198629	Efficient Attribute-Based Comparable Data Access Control	The increasing use of mobile devices has raised concerns about secure data storage, computation, and access control in cloud computing for these devices. To address this, a new framework called Constant-size Ciphertext Policy Comparative Attribute-Based Encryption (CCP-CABE) has been proposed. This framework supports negative attributes and wildcards, and embeds attribute ranges and constraints into the user's key and ciphertext, allowing for flexible access control policies. CCP-CABE is efficient as it generates constant-size keys and ciphertext, and maintains the same computation cost on lightweight mobile devices regardless of the number of attributes involved. The framework can also be extended to fit scenarios with multiple attribute domains, prioritizing the decryption process to protect access policy privacy. Security analysis and performance evaluation have been conducted to demonstrate its efficiency. 
41987	4198758	Subspace-based linear multi-step predictors in type 1 diabetes mellitus.	Managing diabetes can be a challenge, as it requires balancing insulin dosage and food intake to maintain healthy blood glucose levels. This paper proposes a multi-step data-driven approach to predict blood glucose levels in the future, which can help patients make informed decisions about their treatment. The method uses physiological models and clinical data from 14 type 1 diabetic patients to develop predictors based on subspace identification methods. The results show that the predictors have a mean population prediction error standard deviation of 19.17 mg/dL, 37.99 mg/dL, 50.62 mg/dL, and 58.06 mg/dL for 30 min, 60 min, 90 min, and 120 min ahead predictions.
41987	419875	Direct Continuous Time System Identification Of Miso Transfer Function Models Applied To Type 1 Diabetes	This paper discusses the use of continuous time system identification techniques in Type 1 diabetes. A general MISO transfer function structure is assumed, and a parameter estimation process using an iterative prediction error method is proposed. The method is tested on a simulation example and then applied to real-life data from Type 1 diabetes patients to model blood glucose dynamics. The method is also adapted to account for the time-varying nature of the system. This application shows the potential for using system identification methods in understanding and managing Type 1 diabetes.
41988	4198866	Analysis and design of admission control in Web-server systems	Service control nodes, such as web sites and Mobile Switching Centers, can be represented as server systems with multiple servers handling incoming requests. To prevent overload, various admission control mechanisms are typically implemented. This paper focuses on the modeling of a service control-node from a control perspective. The queue is assumed to follow an M/G/l-system and is represented by a nonlinear flow model, with a simplified discrete-time model used for analysis and design. An admission control system using a PI-controller and anti-reset windup feature is developed and its stability is proven. Discrete-event simulations are used to verify the results from the simplified queue model. 
41988	419886	Control Theoretic Modelling and Design of Admission Control Mechanisms for Server Systems	The admission control mechanism is crucial for effective communication systems. This study focuses on load control mechanisms for server systems, specifically those modeled as queueing systems. By utilizing control theory, the researchers design a PI-controller for a G/G/1-system and compare its performance with a static controller in terms of steady-state and transient behavior. This study highlights the potential benefits of using control theory in designing controllers for server systems.
41989	4198930	Constructive Negation by Pruning and Optimizing Higher-Order Predicates for CLP and CC Languages	In this article, the authors discuss different forms of negation in constraint logic programming using the program's completion approach. They introduce a new method called constructive negation by pruning, which has a correct and complete operational semantics based on Kunen's three-valued logic. The authors place emphasis on a full abstraction result that allows for a deeper understanding of the operational behavior of CLP programs with negation. They also provide further insights derived from this approach.
41989	4198923	Closures and modules within linear logic concurrent constraint programming	Modules in programming languages can be viewed in two conflicting ways. On one hand, they are independent of the language's specifics. On the other hand, they may interfere with and duplicate other language features. To unify similar programming concepts and reduce arbitrary choices, this paper proposes a framework using linear logic concurrent constraint (LCC) programming languages. First, declarations and closures are represented as agents in a variant of LCC with precise semantics. Then, a complete module system is implemented in LCC with a code protection property. Finally, the approach is applied to constraint logic programs, demonstrating its generality for languages with logical variables.
41990	41990105	A field trial of privacy nudges for facebook	The article examines the issue of regretful online disclosures by Internet users and proposes two modifications to the Facebook interface to help users be more conscious of their content and audience. A 6-week field trial with 28 Facebook users was conducted to evaluate the effectiveness of these modifications. The results showed that reminders about audience can prevent unintended disclosures without causing inconvenience, but introducing a time delay before posting may be perceived as both helpful and annoying. Some participants found the modifications helpful, while others found them unnecessary or intrusive. The article discusses the implications and challenges of designing and evaluating systems to assist users with online disclosures.
41990	41990103	Power strips, prophylactics, and privacy, oh my!	In this study, the researchers investigate the impact of privacy policy information on Internet users' purchasing behavior. They used a "privacy-enhanced search engine" called Privacy Finder which displays privacy policy information for each website in its search results. Participants were asked to make purchases using this search engine and their behavior was compared to a control group using a regular search engine. The study found that the availability of privacy policy information had a greater influence on purchasing decisions for privacy-sensitive items compared to non-privacy-sensitive items. This suggests that individuals may be willing to pay more for privacy and seek out websites with better privacy practices when given access to comparison information.
41991	4199140	A compendium of formal techniques for software maintenance	This paper discusses the importance of software maintenance in practical software engineering and highlights how it has been neglected by theoretical computer scientists. It provides an overview of formal techniques that have been developed to assist in the software maintenance process, specifically in reverse engineering and re-engineering. The paper also suggests that maintaining specifications instead of programs could be more beneficial in the future. The described work serves as a foundation for a collaborative project that is exploring various other aspects of software maintenance. 
41991	4199117	FORTEST: Formal Methods and Testing	The use of formal methods in software development has traditionally been focused on specification and design. However, there is also potential for these methods to be applied in the testing stage. A panel session associated with this paper delves into the various ways in which formal methods can improve software testing. The contributors, all members of the UKFORTEST Network, discuss different possibilities and raise questions surrounding the use of formal methods. While the authors generally support the idea of formal methods aiding the testing process, this paper aims to stimulate discussion and welcomes dissenting views from the panel or individual authors.
41992	4199225	Developing and validating predictive decision tree models from mining chemical structural fingerprints and high-throughput screening data in PubChem.	Advances in high-throughput screening (HTS) techniques and compound libraries have enabled the rapid testing of millions of compounds for potential use in drug development. However, the large amount of data produced by HTS assays makes it difficult to identify promising compounds. In this study, Decision Trees (DT) models were developed to analyze HTS results based on chemical structure fingerprints. These models were effective in discriminating compound bioactivities in four assays, including those for 5HT1a agonists, antagonists, and HIV-1 RT-RNase H inhibitors. Further evaluation showed that the models could also be used for virtual screening and complement traditional methods for selecting potential drug candidates.
41992	4199226	Effects of multiple conformers per compound upon 3-D similarity search and bioassay data analysis.	PubChem3D is a project that adds three-dimensional descriptions to small-molecule records in the PubChem Compound database, in order to improve its usefulness. This study investigates the effects of using multiple conformers (different shapes and arrangements of the same molecule) on the 3-D similarity scores between biologically-tested compounds and non-inactive compounds in a given biological assay. The results show that using ten diverse conformers per compound does not significantly increase the separation between non-inactive and random compounds, although some assays did show noticeable differences. This study provides valuable information for understanding the impact of conformational diversity on 3-D molecular similarity and its application in analyzing biological activity data in PubChem. These findings can help improve search and analysis tools for molecular libraries in PubChem and other databases.
41993	4199367	Sequential Design of Experiments via Linear Programming	The multi-armed bandit problem is a well-known dilemma in decision theory that involves balancing the exploration and exploitation of a system. In this paper, the focus is on a variant of this problem where the exploration phase is costly and occurs before the exploitation phase. The goal is to find an inexpensive exploration strategy that optimizes the exploitation objective. This problem is NP-Hard, but the paper presents a polynomial time approximation algorithm for it. The approach involves a linear program rounding technique and can also be applied to other related problems in sensor networks. The resulting exploration policies are sequential and do not revisit any arm, making them desirable in scenarios where multiple explorations can happen simultaneously. 
41993	4199368	Iterated Allocations with Delayed Feedback	This paper introduces a new approach to optimizing bandit problems with delayed feedback, particularly in the context of allocation problems. The authors study the problem in a Bayesian setting and show that the delayed feedback can be structured to provide a O(1) approximation for a wide range of priors. This approach also has applications in scenarios where feedback is available immediately, and the authors improve upon previous results in this area. The insights developed in this paper are valuable for understanding the relationship between single arm policies and the optimal policy in these types of problems.
41994	41994101	Reasoning About Approximate Match Query Results	Approximate match predicates are essential for data cleaning operations and various types have been used within a declarative data cleaning framework. These techniques return pairs of tuples with a score indicating their level of similarity. The problem of estimating parameters for planning purposes in these algorithms is addressed in this paper. Precise knowledge of the result size and score distribution is crucial for decision making in identifying similar tuples, which is important for data cleaning. The proposed solution strategies are compliant with the declarative framework and are evaluated for performance. The experimental results validate the expectations and provide insights into the quality and performance of the estimation framework. These techniques can be easily implemented in data cleaning systems.
41994	41994122	Efficient Computation Of Spatial Joins With Intersection Predicates	This article introduces a new algorithm, called Size Separation Spatial Join (S-3 J), for computing the spatial join of two or more spatial data sets without the use of indexes. S-3 J utilizes a hierarchical decomposition of the data space and does not require duplication of entities. The execution time of S-3 J is dependent only on the sizes of the data sets being joined. The article also presents an analytical evaluation of S-3 J's input/output and processor requirements, and compares them with other proposed algorithms. The simplicity of S-3 J's cost estimation formulas allows for efficient implementation and integration with existing relational systems. Additionally, the article introduces Dynamic Spatial Bitmaps (DSB), a technique that enhances S-3 J's performance through the use of bitmap query processing. Experimental results with real and synthetic data sets show the effectiveness of S-3 J over other approaches.
41995	419950	Effective trust management through a hybrid logical and relational approach	This paper discusses the deployment of logical trust management systems in enterprise environments using a relational database management system (DBMS). The authors present a framework for managing trust management policies and describe a procedure for compiling credentials into dynamic views within the database. They also propose a hybrid algorithm for efficiently enumerating user capabilities. The paper includes an evaluation of a prototype implementation and suggests that their approach can be generalized to other trust management approaches. The authors argue that despite the lack of attention to deployment strategies, their framework offers a practical solution for managing trust in authorization.
41995	4199530	Denial of service attacks and defenses in decentralized trust management	Trust management is a method used in decentralized systems to control access in a scalable and flexible way. It involves a server evaluating a chain of credentials from a client, which can be resource-intensive. This paper examines low-bandwidth Denial-of-Service (DoS) attacks that target trust management systems, taking advantage of their delegation feature to overwhelm server resources. Using game theory, it is shown that unprotected trust management servers are vulnerable to clever attackers. An empirical study is conducted on existing trust management systems to demonstrate the severity of this threat. A proposed solution, using credential caching, is found to be effective against intelligent attackers. 
41996	4199617	Selecting the right objective measure for association analysis	The paper discusses the use of objective measures such as support, confidence, interest factor, correlation, and entropy in evaluating the interestingness of association patterns. However, these measures can sometimes conflict with each other, and data mining practitioners may not be aware of better alternatives. The paper suggests examining key properties in order to select the most suitable measure for a specific application. A comparative study of 21 measures from various fields is also presented, showing that each measure has its own strengths and weaknesses. The paper also highlights two scenarios where existing measures become consistent with each other. Finally, an algorithm is proposed to help domain experts select the best measure for their needs.
41996	4199645	Enhancing Predictive Modeling of Nested Spatial Data through Group-Level Feature Disaggregation.	Multilevel modeling and multi-task learning are commonly used methods for analyzing nested data, where observations are grouped based on shared characteristics. Despite their similar purpose, the relationship between these methods has not been thoroughly studied. In this paper, the authors compare the two approaches for analyzing two-level nested data and highlight their respective strengths and limitations. They demonstrate that the formulations of these methods are equivalent under certain conditions, but also show their limitations in predicting outcomes and identifying interactions between local and group-level features. To address these limitations, the authors propose a new method for disaggregating group-level features, which improves prediction accuracy and identifies cross-scale interactions more effectively. Experiments on synthetic and real-world data support the effectiveness of this approach. 
41997	4199748	Verifying Second-Level Security Protocols	A second-level security protocol relies on an underlying protocol to achieve its purpose. While the verification of traditional authentication protocols has become standard, second-level protocols pose new challenges. These include formalizing references to the underlying protocols, adjusting the threat model, and defining new goals. These issues have been addressed using Isabelle and the Inductive Approach. The effectiveness of this approach is exemplified by its successful application to a certified e-mail delivery protocol developed by Abadi et al.
41997	4199723	Soft constraint programming to analysing security protocols	Security protocols are guidelines for how different parts of a computer network should interact in order to achieve specific security objectives. The most important goals of these protocols are confidentiality and authentication, which can be achieved in different ways with varying levels of strength. A new approach to formalizing these goals uses soft constraints and includes a parameter for the security level. This allows for more flexibility in achieving different levels of confidentiality and authentication between different parts of the network. The framework for this approach can be mechanized using model checking, and has been applied to analyze the Needham-Schroeder protocol and discover a new attack. The same framework has also been used to analyze the widely used Kerberos protocol.
41998	4199843	Finding partitions of arguments with Dung's properties via SCSPs	The concept of forming coalition structures allows agents to combine their efforts in order to achieve a shared goal. The authors propose exploring homogeneous groups with distinct lines of thought, and extend the Dung Argumentation Framework to incorporate coalitions of arguments. This involves partitioning the initial set of arguments into subsets, where each subset represents a different line of thought with the same inherited property as Dung's framework. The use of (soft) constraints is suggested as a formal method to address NP-complete problems in weighted argumentation, with semiring algebraic structures used to model different optimization criteria for the coalitions. The authors demonstrate this approach using JaCoP, a Java constraint solver, on a small-world network.
41998	4199864	Solving weighted argumentation frameworks with soft constraints	The article discusses the use of soft constraints to represent and solve "weighted" Argumentation problems, where arguments have different levels of preference. This is achieved by selecting different semiring algebraic structures to represent factors such as "fuzziness", "cost", or probability levels of each argument. The proposed approach allows for a quantitative framework to compute classical Dung's extensions, with a score representing the quality of the set. The authors also propose a mapping from weighted Argumentation Frameworks to Soft Constraint Satisfaction Problems (SCSPs), which can be solved using JaCoP, a Java constraint solver. This allows for the computation of Dung's semantics, such as admissible and stable extensions.
41999	419993	Deconstructing datacenter packet transport	pFabric is a datacenter fabric design that focuses on providing high performance for high-priority flows while also maximizing overall network utilization. This is achieved through a minimalistic approach, using simple mechanisms at each switch and eliminating most buffering. The design relies on switches making locally and greedy decisions based on priority information in the packet header, without requiring any flow state or rate estimates. As a result, rate-control is rarely necessary and all flows start at line-rate, only slowing down in extreme cases of congestion. Simulation results with realistic workloads and topologies demonstrate that this straightforward design achieves near-optimal flow completion times and network utilization.
41999	4199950	pFabric: minimal near-optimal datacenter transport	pFabric is a new datacenter transport design that achieves almost perfect flow completion times for short flows and minimizes average completion time for long flows. This is achieved through a simple design that decouples flow scheduling from rate control. Packets are assigned a priority number by each flow and switches have small buffers with a priority-based scheduling and dropping mechanism. Rate control is also simplified, with flows starting at line rate and only throttling back under high and persistent packet loss. Extensive simulations demonstrate the effectiveness of this approach, making pFabric a highly efficient and easy-to-implement solution for datacenter transport.
411000	41100014	The design of bug fixes	Software engineers have multiple options when it comes to fixing bugs, and the choice they make can have various implications for both practitioners and researchers. These include the risk of introducing new bugs, the location of the bug fix in the code, and whether the fix addresses the root cause or just a symptom. A recent study conducted with 40 engineers, data from 6 bug triage meetings, and a survey of 326 engineers revealed that non-technical factors, such as the software's proximity to release, play a significant role in bug fixing decisions. The study also suggests potential improvements in bug prediction and localization methods. 
411000	41100018	The Design Space of Bug Fixes and How Developers Navigate It	In this paper, the authors investigate the decision-making process of software engineers when fixing bugs. They found that there are many factors that influence the choice of fix, such as the risk of introducing new bugs, the location of the bug in the code, and whether the fix addresses the root cause or just a symptom. The study was based on interviews with engineers from different companies and data from bug triage meetings. The results suggest that non-technical factors, like the proximity to release, also play a role in the decision-making process. The authors discuss the implications of their findings for both practitioners and researchers, including how to improve bug prediction and localization.
411001	41100136	Importance sampling based discriminative learning for large scale offline handwritten Chinese character recognition.	This paper presents a discriminative learning framework for large-scale classification tasks using importance sampling. The framework assigns weights to samples based on their importance and uses three methods to calculate these weights for learning a modified quadratic discriminant function (MQDF). These methods include rejection sampling, boosting algorithm, and minimum classification error (MCE) rule. The proposed framework focuses on cursive samples, which are typically more difficult to classify, and achieves higher accuracy with lower computational complexity compared to the traditional maximum likelihood estimation (MLE) rule. Experiments on Chinese handwritten character datasets show promising results, demonstrating the effectiveness of the proposed framework.
411001	41100124	Beyond weights adaptation: A new neuron model with trainable activation function and its supervised learning	This paper introduces a novel neuron model called Trainable Activation Function (TAF), which combines trainable weights with a primitive neuron activation function. A BP-like learning algorithm has been developed for Multi-layer Feedforward Neural Network (MFNN) built with TAF neurons. Two simulation examples demonstrate the superior network capacity and performance of the TAF MFNN compared to the traditional sigmoid MFNN. This new model offers a promising approach for improving the learning capability and effectiveness of neural networks.
411002	4110028	Synchronization Strings: List Decoding for Insertions and Deletions.	The study focuses on codes that can tolerate both insertions and deletions in a received codeword. These codes, called $L$-list-decodable, have an efficient algorithm that can report a list of $L$ codewords that contain the original one. The authors use the concept of synchronization strings to show that there exist efficient codes with a constant alphabet and sub-logarithmic list sizes, even when the fraction of insertions can be arbitrarily large. The results also reveal a significant asymmetry between the impact of insertions and deletions on error-correction, with deletions affecting the code's rate while insertions are borne by the adversary. The study also provides tight bounds on the parameters of these codes, showing that the alphabet size needs to be exponentially large in the gap to capacity. This is in contrast to the Hamming error model, where a polynomial alphabet size suffices for unique decoding.
411002	41100272	Decoding concatenated codes using soft information	We propose a decoding algorithm for concatenated codes with a Reed-Solomon outer code and an arbitrary inner code. This algorithm utilizes "soft" information from the inner decodings to improve the Reed-Solomon decoding process. This is the first analysis of such a soft algorithm for arbitrary inner codes, which were previously only analyzed for certain special inner codes. Our analysis relies on a combinatorial result on the coset weight distribution of codes based on their minimum distance. This enables us to decode up to the "Johnson radius" of a concatenated code, which is the maximum number of errors that can be corrected. Our algorithm also allows for the construction of efficient quary linear codes that can correct a large fraction of errors and have found applications in complexity theory. Previous constructions using algebraic-geometric codes were more complicated and had slower decoding processes.
411003	41100313	Large-scale factorization of type-constrained multi-relational data	Statistical modeling of large multi-relational datasets is becoming increasingly popular, with applications including knowledge bases like DBpedia, Freebase, YAGO, and Google Knowledge Graph. These datasets contain millions of entities, hundreds and thousands of relations, and billions of relational tuples. Collective factorization methods, particularly tensor approaches, have been successful in scaling up to these datasets using the alternating least squares (ALS) algorithm. This paper introduces an extension to the RESCAL tensor factorization method that considers relational type-constraints, which define the logic of relations by excluding certain entities from subject or object roles. The proposed approach is shown to be scalable and outperforms RESCAL without type-constraints in both runtime and prediction quality on large datasets.
411003	41100314	Improving Visual Relationship Detection Using Semantic Modeling of Scene Descriptions.	In this paper, the authors explore the use of structured scene descriptions in image databases and how combining a statistical semantic model with a visual model can enhance the task of mapping images to their associated scene descriptions. The scene descriptions are represented as sets of triples, each consisting of a pair of visual objects and their relationship. By using a combination of a visual model based on convolutional neural networks and a latent variable model for link prediction, the authors compare the performance of various link prediction methods in detecting visual relationships. The results on a challenging real-world dataset demonstrate that the integration of a statistical semantic model using link prediction can significantly improve visual relationship detection, outperforming the state-of-the-art method from the Stanford computer vision group.
411004	41100450	Representing Knowledge in Robotic Systems with KnowLang.	Building intelligent robotic systems is exciting but also incredibly difficult. Researchers have found that using a logical approach can help achieve robot intelligence, but there is still a struggle to connect this abstract logic with real-world meaning. This paper introduces KnowLang, a new formal language designed for knowledge representation in a specific type of intelligent robotic system called ASCENS. These systems, known as Autonomic Service-Component Ensembles, are made up of mobile, intelligent, and open-ended groups of service components that can reason locally and in a distributed manner. These service components contain rules, constraints, and self-adaptation mechanisms, allowing them to acquire and process knowledge about themselves, other components, and their environment. The paper also includes a case study showcasing how KnowLang can be used to represent knowledge in a robotic system.
411004	4110047	Knowledge Representation and Awareness in Autonomic Service-Component Ensembles - State of the Art.	This article discusses the importance of knowledge in creating intelligent systems and the role of knowledge representation and management in this process. The authors present their vision of using knowledge to create awareness in mobile swarm systems, which are made up of self-managing components. These components have rules, constraints, and mechanisms that allow them to acquire and process knowledge about themselves, other components, and their surroundings. The paper also includes a high-level model of structured knowledge and a formal model of awareness for these autonomic service components. Overall, the article highlights the crucial role of knowledge in creating intelligent systems and presents a specific approach for implementing it in mobile swarm systems.
411005	41100528	An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry	Software product lines have become increasingly popular in industry for creating related products, maximizing reuse, and utilizing variable and configurable options. However, as software becomes more dynamic and requires adaptable features, the need for systems to support runtime adaptation is growing. This is especially true for embedded systems, ecosystems, service-based applications, and self-adaptive systems. Traditional software product line architectures are not equipped to handle such dynamic conditions, leading to the development of Dynamic Software Product Lines (DSPLs). While these approaches attempt to address the challenges of runtime variability, they are still in an early stage of development. In this research, we provide an overview of the current techniques and state of the art in DSPLs, as well as the challenges and solutions needed to support runtime variability in these systems.
411005	4110054	A Self-Adaptive Architecture For Autonomic Systems Developed With Assl	The article discusses the implementation of a self-modifiable architecture for autonomic systems, using the ASSL framework. This framework utilizes a hierarchical multi-granular architecture composed of singleton classes, which is designed to support runtime evolving systems. The authors propose an extension to this architecture, allowing for both code generation and code management at runtime. They also provide an algorithm for customizing ASSL models to trigger changes in the structure of the generated autonomic systems. This approach aims to improve the flexibility and adaptability of autonomic systems, making them more efficient and effective in dynamic environments. 
411006	4110061	Extending the UMIOP specification for reliable multicast in CORBA	OMG has released a specification for unreliable multicast in CORBA applications called UMIOP, which is based on the best-effort IP Multicast protocol. However, fault-tolerant and groupware applications often require more strict guarantees for message delivery, such as reliable multicast with FIFO, causal, or total ordering. Currently, OMG does not have a specification for meeting these requirements. To address this gap, the ReMIOP protocol was proposed as an extension to UMIOP, providing a reliable multicast mechanism in CORBA middleware. Performance tests were conducted to compare ReMIOP, UMIOP, and UDP sockets for IP multicast communication, showing the additional costs of implementing reliable and unreliable multicast at the middleware level.
411006	41100622	Proactive Byzantine Quorum Systems	Byzantine Quorum Systems is a technique used for replicating data to ensure availability and consistency, even in the face of arbitrary faults. The paper introduces a protocol that ensures atomic semantics, even with Byzantine clients and servers. It is also integrated with a protocol for proactive recovery of servers, allowing the system to handle any number of failures during its lifetime. The proposed solution is applicable to asynchronous systems without any time assumptions. The paper also includes the implementation of read and write protocols for the quorum system, with demonstrated efficiency through experiments on the Emulab platform.
411007	41100743	Byzantine Fault-Tolerant MapReduce: Faults are Not Just Crashes	MapReduce is a commonly used system for running important tasks like scientific data analysis. However, research has shown that unexpected faults can occur during these jobs, potentially corrupting the results. While MapReduce runtimes like Hadoop can handle crashes, they are not equipped to handle arbitrary or Byzantine faults. To address this issue, a new MapReduce algorithm and prototype have been developed to tolerate these types of faults. An experiment showed that this approach uses twice the resources of Hadoop, which is still more efficient than other fault-tolerance methods. This added cost is deemed acceptable for critical applications that require a high level of fault tolerance.
411007	41100734	On the efficiency of durable state machine replication	State Machine Replication (SMR) is a crucial method for ensuring the reliability of critical services in large-scale internet infrastructures. However, SMR alone cannot prevent complete system crashes, so it is often combined with secondary storage to ensure data durability. This study demonstrates that traditional durability measures such as logging, checkpointing, and state transfer can significantly impact the performance of SMR-based services, even when using SSDs instead of disks. To address this issue, the researchers propose three techniques - parallel logging, sequential checkpointing, and collaborative state transfer - that can be implemented without changing the SMR programming model or requiring additional resources. These techniques were successfully tested on an open-source replication library and evaluated in the context of a key-value store and coordination service, showing promising results.
411008	41100835	RStore: A Distributed Multi-Version Document Store	This article discusses the problem of storing a large number of versions of keyed documents in a distributed environment while efficiently answering retrieval queries. The need for such a system is increasing in various application domains. The article explores the design space and trade-offs involved in building such a system, and proposes a novel system architecture with tuning knobs to adapt to different data and query workloads. The system acts as a layer on top of a distributed key-value store and uses algorithms for efficient data partitioning and handling new versions. Extensive experiments show that the system outperforms standard baselines by orders-of-magnitude.
411008	41100878	Managing large dynamic graphs efficiently	This paper discusses the need for graph databases to handle large volumes of graph-structured data in various applications. While there is plenty of research on single-site graph databases and efficient query execution, there is a lack of understanding about distributed graph databases. The paper proposes a design for an in-memory, distributed graph data management system to handle a large-scale, dynamic graph with low-latency querying. The main challenge in distributed graph databases is the large number of traversals needed to answer queries across partitions. The paper suggests aggressive replication of nodes and three techniques to minimize communication and storage requirements. These techniques include a hybrid replication policy, clustering-based approach, and a fairness criterion. The proposed system is implemented on top of the CouchDB key-value store and is evaluated on a social graph, showing efficient handling of large graphs and decreased network bandwidth consumption.
411009	41100929	Reconstruction of neural action potentials using signal dependent sparse representations	In this study, a method is presented to construct a sparse representation dictionary for neural action potentials using the K-SVD algorithm and Discrete Wavelets Transform. This dictionary is then used in a Compressive Sensing (CS) framework to recover the neural signal. The proposed approach outperforms non-signal dependent CS recovery algorithms by achieving the same quality of reconstruction with 2.5 times fewer measurements. It also increases the signal to noise and distortion ratio (SNDR) by 6 dB compared to non-signal dependent methods. The recovered signal is evaluated using spike sorting techniques, which show clear separation of spike clusters even at low compression ratios. This method has implications for hardware implementation of compressed sensing, as it can reduce power and chip area by the same order.
411009	41100989	An Unsupervised Dictionary Learning Algorithm For Neural Recordings	Our team has developed an effective solution for the increasing demand for wireless and power efficient neural recording systems. We have successfully implemented an unsupervised dictionary learning algorithm within the Compressed Sensing (CS) framework, making it suitable for VLSI systems. This algorithm does not require prior label information of neural spikes and creates a dictionary with discriminative structures for spike sorting. We have also proposed a joint prediction method to further improve the reconstruction and classification performance. Our approach has shown significant improvement in terms of reconstruction quality and classification accuracy when compared to state-of-the-art CS methods. With a compression ratio of 50, our approach has achieved an average gain of 2 dB and 15 percentage units in these metrics.
411010	41101025	Compact Multicast Routing	A compact multicast scheme is a routing method in a distributed network where any source can send messages to a chosen group of targets. The amount of space needed to store the routing table on each node is balanced with the stretch factor, which is the maximum ratio between the cost of the multicast route and the cost of a steiner tree for the same target nodes. Different versions of the problem were studied, including labeled, name-independent, and dynamic, where the goal is to minimize both the cost of the multicast tree and the cost of control messages for updating the tree as nodes join and leave the multicast service.
411010	4110107	On space-stretch trade-offs: lower bounds	Compact routing schemes face a trade-off between the amount of space needed to store routing tables on each node and the stretch factor, which measures the ratio between the cost of a route and the minimum cost path between the same nodes. Researchers have developed a lower bound for the name-independent model, which applies to single-source schemes and does not rely on a girth conjecture. This lower bound states that any routing scheme with a maximum stretch less than 2k + 1 will require at least Ω((n log n)1/k)-bit routing tables for networks with arbitrary weights and node names. This result is extended to show that any name-independent routing scheme with (n/(9k))1/k-bit routing tables will have an average-stretch of at least k/4 + 7/8, which is significantly higher than recent results for labeled routing schemes.
411011	41101122	Abstract Transformers for Thread Correlation Analysis	The authors propose a new method for accelerating the analysis of concurrent programs that use shared memory. The focus is on thread correlation, which involves identifying relationships between the local states of different threads and the global state. This is crucial for verifying properties of concurrent programs. However, tracking these correlations is expensive, especially due to the cost of applying abstract transformers. The proposed technique involves using footprints and memoization to compute abstract transformers more efficiently. The technique has been implemented in a concurrent shape analysis framework and has been used to prove several properties of complex concurrent programs. Empirical results show a significant reduction in analysis time.
411011	41101133	A Framework For Efficient Modular Heap Analysis	Modular heap analysis techniques use summaries to analyze the effects of procedures on an input heap. These analyses rely on pre-computed summaries for called procedures, and are efficient but lack formalization and correctness proofs. This article introduces a framework that generalizes four popular modular heap analyses, formalizing them as instances of the framework. This simplifies the process of extending or modifying these analyses. The framework is evaluated on real-world C# applications, showing its ability to produce a variety of analyses with different levels of precision and scalability. This approach allows for easier development and customization of heap analyses.
411012	41101218	Complexity of Computing with Extended Propositional Logic Programs	The paper introduces the concept of an F-program, where F is a collection of formulas. These programs are a generalization of standard logic programs, allowing for formulas other than atoms to be used in rules. Examples of F include the set of all atoms, literals, Horn clauses, and clauses with varying numbers of literals. The paper explores the complexity of computing with F-programs, including determining the existence and membership of stable and minimal answer sets. The complexity of reasoning involving these notions is also studied, and several open problems are listed.
411012	4110120	Answer Set Optimization	This article explores the integration of answer set programming and qualitative optimization techniques. Answer set optimization programs (ASO programs) consist of a generating program that produces potential solutions and a preference program that expresses user preferences. The preference program creates a preference relation based on the satisfaction of rules. The article discusses potential uses for ASO programming, provides complexity results, and suggests methods for implementation. Additionally, the relationship between ASO programs and CP-networks is examined.
411013	41101383	Stochastic Models Of Slc Hr Sar Images	The paper introduces two algorithms for extracting texture primitive features from Single Look Complex (SLC) and Polarimetric Synthetic Aperture Radar (PolSAR) SLC data. These algorithms use a Gauss-Markov Random Field (GMRF) model to capture the spatial correlation in the data. One algorithm uses a complex GMRF model to characterize the spatial relationship of SLC SAR data, while the other extends this model to capture inter-band correlation between polarimetric channels. The authors use a Bayesian approach to effectively handle model fitting and selection. The algorithms are tested on a polarimetric E-SAR L band scene of Mannheim, Germany and the results are presented.
411013	41101326	Scale Space Exploration For Mining Image Information Content.	Images are complex signals with a lot of information, making them difficult to analyze automatically. A hierarchical approach is useful in understanding image content. The paper describes the use of a scale-space clustering algorithm called "melting" to explore image information. This algorithm groups data by minimizing free energy and uses temperature as a scale parameter. To handle the high dimensionality of images, a fast cluster center estimation algorithm is used for initialization. The information extracted by melting is represented in a tree structure, allowing for efficient exploration of image content. This structure also enables the reconstruction and investigation of hierarchical sequences of image classifications. The effectiveness of the approach is demonstrated using examples of satellite images and radar data.
411014	41101444	An active camera system for acquiring multi-view video	The system discussed is designed to capture multi-view video of a person moving through an environment. It utilizes a real-time tracking algorithm to adjust the parameters of multiple active cameras, ensuring that the person remains centered in each view. The result is a set of synchronized and time-stamped video streams, providing multiple perspectives of the person's movements.
411014	41101450	Gait shape estimation for identification	The article discusses a new method for identifying individuals based on their shape, using silhouettes from video. The method uses a spectral partitioning framework to cluster similar poses and extract gait shapes. A variance-weighted similarity metric is used to create clusters that cover different stages of the gait cycle. This approach is applied to the HumanID Gait Challenge dataset to measure the effectiveness of the shape model and its use in human identification. The results show that this method can accurately identify individuals based on their shape, providing a potential new tool for identification purposes.
411015	41101599	A hierarchical characterization of a live streaming media workload	In this article, the authors present a detailed analysis of live streaming media content on the Internet. They studied over 3.5 million requests over a 28-day period, focusing on client, session, and transfer levels. They found that the interactions between users and objects are different for live and stored objects, with live objects being more driven by the object itself rather than the user. They also observed a Zipf-like pattern in user interest for a specific object, compared to the classic Zipf-like popularity of objects for a user. The variability in transfer lengths was found to be due to the stickiness of clients to a particular live object. The authors also suggest that the characteristics of live media access workloads are likely dependent on the type of live content being accessed. This is supported by the strong temporal correlations observed in the study, which can be attributed to the synchronizing impact of live content on access patterns. The authors present a model for live media workload generation based on their findings, which is implemented in Gismo. 
411015	4110151	SomeCast: A Paradigm for Real-Time Adaptive Reliable Multicast	SomeCast is a new approach to reliably multicast real-time data to a large number of receivers. It is scalable in terms of the number of receivers, the varying paths between senders and receivers, and changing conditions of those paths. This is achieved by allowing receivers to adjust the rate at which they receive multicast information, ensuring their quality of service requirements are met. This is done by joining multiple simultaneous multicast sessions, with each session delivering a portion of the data. This paradigm is a combination of previously proposed paradigms, AnyCast and ManyCast, and has been shown to have significant advantages through simulations. The paper provides an overview of SomeCast, describes a specific protocol, and presents simulation results to support its effectiveness.
411016	41101684	Multi-layer Virtual Transport Network Design.	Service overlay networks and network virtualization are used to overcome deficiencies of the Internet, such as resiliency, security, and quality of service guarantees. However, most overlay/virtual networks are only used for routing and tunneling, instead of providing scoped transport flows with mechanisms for error and flow control and resource allocation. This results in limited network resource allocation and utilization. To address these limitations, a multi-layer approach to virtual transport network (VTN) design is proposed. This allows for dynamic scope management, improving application and network management. The multi-layer VTN design partitions the network into smaller scopes, achieving better performance compared to traditional single-layer designs. Simulation and experimental results demonstrate the effectiveness of this approach in enhancing network performance.
411016	41101635	Slice embedding solutions for distributed service architectures	Network virtualization allows for the creation of multiple virtual networks on a single physical network, providing both research and market benefits. The ability to experiment with new Internet architectures and protocols is a research advantage, while infrastructure service providers can explore new business models such as leasing virtual slices of their network or hosting multiple network services. The slice embedding problem, which involves discovering, mapping, and allocating resources for virtual networks, is a key challenge in this paradigm. Different solutions exist, based on the types of constraints, dynamics, and allocation strategies used. Further research is needed to address this complex problem. 
411017	4110177	Entropy-preserving cuttings and space-efficient planar point location	Point location is the process of organizing a polygonal subdivision into a data structure to efficiently determine which cell contains a given query point. The structure should minimize the expected query time, taking into account the probabilities of the point being in each cell. The entropy of the probability distribution is a key factor in determining the lower bound on the expected search time. The number of edges in the subdivision is a lower bound on the required space, but there is currently no method that can achieve both a query time of H + &Ogr;(H) and space of &Ogr;(n). This paper introduces entropy-preserving cuttings to achieve a query time of H + &Ogr;(H) with only &Ogr;(n log* n) space.
411017	4110173	Optimal Expected-Case Planar Point Location	Point location is the process of organizing a planar polygonal subdivision into a data structure to efficiently determine the cell that contains a given query point. This is measured by the expected query time, which is affected by the probabilities of the query point being in each cell. The entropy of these probabilities is the main factor in the lower bound for expected query time. It is possible to achieve optimal query time of H+O(√H+1) with space efficiency of O(n) by using entropy-preserving cuttings, even in subdivisions with convex cells and a uniform query distribution.
411018	41101882	Distributed error confinement	The article discusses error confinement in distributed applications, where only nodes directly affected by a fault are allowed to deviate from their correct behavior. This is impossible if an adversary can cause arbitrary faults, but the article introduces a new measure called agility to quantify fault tolerance. Broadcast algorithms are proposed that guarantee error confinement with optimal agility, which can be used in more complex systems. Previous studies on fault locality were not error confined and allowed a wide range of behaviors to be considered correct. The article also presents a new technique for analyzing the "cow path" problem.
411018	4110180	Asynchronous recommendation systems	This paper discusses a recommendation system where players have unknown preferences for objects. The players can probe objects to discover their preferences, but this incurs a cost. To save on cost, players can share the results of their probes on a public billboard. In a synchronous system, players probe in rounds, while in an asynchronous system, players probe in an arbitrary order. The paper presents algorithms for both exact and approximate preference reconstructions in an asynchronous system with polylogarithmic cost overhead. This allows for players to discover their preferences even under adversarial scheduling. 
411019	41101940	On Dual-Rail Control Logic for Enhanced Circuit Robustness	Ultra low-power design and energy harvesting applications require digital systems to operate under extremely low voltages. To achieve this, sub-threshold operation mode is used, which balances dynamic and static power consumption. However, this mode results in large delay variations, making asynchronous circuits necessary. But even these circuits can be vulnerable due to timing assumptions that are no longer valid. To address this, a paper presents an automated approach for synthesizing robust controllers for sub-threshold digital systems using dual-rail implementation. This eliminates the need for inverters, which can cause timing issues. Dual-rail controllers have minimal overhead and can even be faster than single-rail solutions. The presented synthesis techniques are efficient and can be applied to large controllers, as shown in benchmarks.
411019	41101917	On-Chip Measurement of Deep Metastability in Synchronizers	A new method for measuring deep metastability has been implemented on a chip using digital circuits. This on-chip implementation has advantages over previous off-chip versions using analog circuits, as it allows for integration of both the synchronizer circuits and measurement method and eliminates inaccuracies caused by high-speed off-chip paths. The use of digital counters and delay lines also makes control at the picosecond level easier. Results show that the digital delay line can be controlled to an increment of 0.1 ps, with an input time distribution of 5.2 ps compared to 7.6 ps for the analog version. This method can accurately measure the input distribution to within 1 ps by controlling the ratio of high to low outputs. The metastability time constant has been measured down to 10-17 s, with potential for even lower levels by increasing the measurement time. A new synchronizer circuit designed for robustness to variations in voltage performed better than the previous Jamb Latch, and was faster when voltage was reduced by 25%. 
411020	41102015	A measurement-based analysis of multihoming	Multihoming is a networking strategy used by stub networks to enhance reliability and performance. However, there is limited understanding of the tangible benefits and how to fully utilize them. This paper aims to quantify the potential benefits of multihoming for both high-volume content-providers and enterprises. Results show that multihoming can significantly improve performance, with a potential 40% performance penalty for choosing the wrong providers. There is also evidence of diminishing returns when using more than four providers. The paper also provides guidelines for choosing ISPs and practical strategies for using multiple connections to achieve optimal performance. Additionally, an analysis of the reliability benefits of multihoming is provided.
411020	41102032	Using batteries to reduce the power costs of internet-scale distributed networks	This paper discusses the use of batteries in large distributed networks, specifically content delivery networks (CDNs), to reduce power supply and cost. These networks have hundreds of thousands of servers in various locations, and their operating expenses are largely driven by the cost of powering these servers. The authors propose a theoretical model and algorithm for provisioning batteries in CDNs to minimize power supply and cost. They use load traces from Akamai's CDN to demonstrate the potential benefits, showing that batteries can provide up to 14% power savings and 33.8% cost savings. These savings can be achieved with a small cycle rate that does not significantly affect battery lifetime. Overall, the paper suggests that batteries can be a valuable component in future distributed network architecture, not just for CDNs but also for other similar networks.
411021	4110212	Utilizing Design Information in Aspect-Oriented Programming	Traditionally, aspect-oriented languages use pointcut designators to select specific parts of a program based on explicit lexical information, which can limit adaptability and result in hard-coded and implementation-specific code. To address this issue, design intentions can be used instead, which are represented by annotated design information that describes the behavior and meaning of program elements. This paper analyzes four techniques used in object-oriented languages to associate design information with program elements and discusses how it can be used in the weaving process of aspect-oriented languages. The paper also proposes language abstractions to support the composition of aspects with design information, and shows how the aspect-oriented language Compose* can be extended to support design information. The application of design information to improve the reusability of aspects is demonstrated, and the paper concludes with related works, a discussion, and conclusions. 
411021	41102135	Impact of Evolution of Concerns in the Model-Driven Architecture Design Approach	Separation of concerns is an essential principle in creating effective software systems, and it is utilized in both Model-Driven Architecture (MDA) and Aspect-Oriented Software Development (AOSD). While AOSD focuses on modeling crosscutting concerns, MDA focuses on separating platform-independent concerns from platform-specific ones and generating models. To explore the potential benefits of AOSD for MDA, a systematic analysis is conducted on crosscutting concerns within the MDA framework. This involves defining an abstract model of MDA transformation, identifying potential concerns, and analyzing the impact of these concerns on the model transformations. The analysis reveals key challenges in integrating and evolving crosscutting concerns in MDA, leading to recommendations for the language and process used in MDA. 
411022	41102254	Context adaptation of mamdani fuzzy rule based systems	Context adaptation involves adjusting a fuzzy rule based system (FRBS) to a specific context by modifying the meanings of linguistic terms and the fuzzy sets within the system. This is achieved through the use of operators that are optimized to balance interpretability and accuracy. The operators adjust the universe of input and output variables and modify the core, support, and shape of the fuzzy sets in the system. The parameters for the operators are chosen through a genetic optimization process. This approach has been applied to Mamdani fuzzy systems in two different domains, regression and data modeling. 
411022	41102225	A multiobjective evolutionary approach to concurrently learn rule and data bases of linguistic fuzzy-rule-based systems	This paper proposes using a multiobjective evolutionary approach to generate different linguistic fuzzy-rule-based systems for regression problems. The approach considers both accuracy and interpretability, measured by approximation error and rule base complexity. The approach uses the linguistic two-tuple representation model and integrates it with the (2 + 2) Pareto Archived Evolution Strategy (PAES) to manage the search space. The approach was tested on nine real-world datasets of varying sizes and complexity. Comparisons were made with the NSGA-II and a single-objective evolutionary algorithm. A data-complexity measure was also introduced to evaluate the effectiveness of the approach, showing positive results for high-dimensional datasets with high complexity. 
411023	4110231	Pattern Based Integration of Internet of Things Systems.	The Internet of Things (IoT) is a network of physical devices that can exchange data through sensors, actuators, and connectivity. It has a significant impact on society as more systems are becoming based on IoT. However, one of the main challenges is integrating the heterogeneous systems within the same communication network. Various studies have addressed this issue at different levels, but the approaches are scattered and not cohesive. This chapter presents a comprehensive and systematic approach for identifying and addressing integration concerns in IoT system architecture. It uses a pattern-based approach to provide generic solutions for common integration issues. The approach is illustrated through the example of integrating IoT systems in smart city engineering.
411023	41102313	Modeling and Reasoning about Design Alternatives of Software as a Service Architectures	A standard framework for Software as a Service (SaaS) can be used as a basis for designing different applications. However, to ensure that each application meets its unique functional and nonfunctional requirements, it is necessary to consider multiple design options within this framework. This paper introduces a method and tool for effectively designing SaaS application architectures. It includes a reference architecture, a feature model, and a set of design rules. By creating an application feature model based on specific business needs, designers can make informed decisions and create a tailored SaaS application architecture.
411024	41102411	Cache-aware timing analysis of streaming applications	There is a growing interest in designing hardware and software specifically for streaming applications, which process continuous streams of data. These applications are found in various devices and the timing analysis problem arises when determining performance metrics and mapping them onto hardware architectures. Previous work has neglected micro-architectural features such as caches, but a new framework has been presented that accurately models the evolution of instruction caches and the impact of previous data items on execution time. This framework combines program analysis techniques with mathematical methods for analyzing streaming applications, resulting in more accurate estimations of timing and buffer size. Experiments with an MPEG-2 encoder show that this approach is efficient, scalable, and leads to better results.
411024	41102458	Cache-aware optimization of BAN applications	Body-area sensor networks (BANs) are becoming increasingly popular for health monitoring, replacing traditional wired bio-monitoring techniques. However, the continuous processing of large amounts of data in biomonitoring applications poses challenges for BAN platforms, such as power consumption and computation bandwidth limitations. This has led to a focus on designing methods and models specifically for BANs and their applications. In this paper, the authors propose an optimization method for the communication gateway of a wireless BAN, along with accurate modeling of the gateway's processor, to improve resource usage and save power. They also investigate the impact of different memory layouts on cache reuse and propose a heuristic for jointly optimizing code layout and sensor sampling order. Their experiments show that optimizing the sensor sampling order has a greater influence on cache reuse than code layout. A case study using a faint fall detection application shows promising results in terms of power consumption, and the method can also improve sampling frequency for better reliability and response time.
411025	411025233	Worst-case temperature analysis for different resource availabilities: a case study	Three-dimensional chip integration can lead to high on-chip temperatures, which can affect the reliability and performance of real-time systems. To prevent overheating, dynamic thermal management methods are used, but their impact on system temperature must be considered when designing real-time systems. This paper proposes a framework for calculating the worst-case temperature of a system with varying resource availabilities, using real-time and network calculus models. Case studies on an advanced multimedia system demonstrate the effectiveness of this framework in analyzing the effects of dynamic frequency scaling and thermal-aware scheduling techniques on system temperature. 
411025	411025270	Fast worst-case peak temperature evaluation for real-time applications on multi-core systems	Multi-core systems face reliability issues due to high chip temperatures, which can cause both long-term concerns and short-term functional errors. This is especially problematic in real-time systems, where high temperatures can lead to missed deadlines and incorrect functionality. To ensure correct functionality, real-time analysis must incorporate the worst-case peak temperature. However, this can be time-consuming when exploring multiple design options. To address this challenge, a fast analytic method is proposed in this paper to calculate an upper bound on the maximum temperature of a multi-core system with non-deterministic workload. This method takes into account various thermal effects, such as heat exchange between cores and temperature-dependent leakage power. The proposed method is validated on a multi-core ARM platform and has shown to be efficient and accurate.
411026	41102644	The design of a stream cipher LEX	This paper introduces the concept of leak extraction from a block cipher and applies it to the AES cipher. The result is a new stream cipher called LEX, which is based on AES and is significantly faster than AES in both software and hardware. This demonstrates the effectiveness and practicality of leak extraction in improving cipher performance.
411026	41102637	Analysis of the non-linear part of mugi	The paper discusses a preliminary analysis of the Mugi stream cipher and identifies potential weaknesses in its nonlinear component. While the cipher cannot be fully broken, it is shown that small variations can lead to the recovery of the full state and secret key. If the linear part is removed, the secret state can be recovered with just three output words. Even with a simplified linear component, the cipher can be broken faster than exhaustive search. The results highlight the sensitivity of Mugi to variations and suggest improvements to its design.
411027	41102794	Static Analysis and Software Assurance	Computer networks have experienced significant growth in the last ten years, which has highlighted the importance of security. However, a major challenge in computer security is the software assurance problem, which deals with the fact that even our most trusted software, including security software, can contain bugs. The speaker in this talk will discuss how static analysis, a technique for examining software without actually running it, can help address this problem. They will also share their recent experiences with using static analysis tools for detecting vulnerabilities and will identify some remaining challenges in the field, as well as potential areas for future research.
411027	4110272	Cryptographic voting protocols: a systems perspective	Cryptographic voting protocols aim to provide secure and verifiable voting systems without relying on the trustworthiness of software. However, these protocols must be evaluated in the context of the entire voting system, including machines, software, and election procedures. This paper examines the security of two proposed cryptographic protocols by Neff and Chaum, revealing potential weaknesses such as subliminal communication in encrypted ballots, human error in cryptographic protocols, and denial of service attacks. These vulnerabilities could compromise election integrity, violate voter privacy, and enable vote coercion. A well-designed implementation and deployment may mitigate these weaknesses, but a thorough analysis of the entire voting system is necessary before these protocols can be used in large-scale public elections.
411028	411028167	Principles and applications of continual computation	Automated problem solving involves using computational resources to solve given problems. This process involves applying effort in real time to generate a solution, which marks the end of problem solving. However, continual computation goes beyond this by proactively allocating computational resources to potential future challenges. This approach also considers idle time and its allocation in different settings. The article presents various applications that demonstrate the potential of continual computation in practical tasks. © 2001 Elsevier Science B.V. All rights reserved.
411028	411028165	Investigations of Continual Computation	Autonomous agents operating in real-world environments often face a continuous stream of problems to solve. In traditional approaches, agents only focus on solving current problems that have already been identified. However, continual computation methods allow for the allocation of time to both current and potential future problems, taking into account uncertainty. Previous research in this field is reviewed, and new directions and results are presented, including the consideration of shared subtasks and multiple tasks. The computational complexity of this problem is explored, and approximations are provided for different models of computational performance. Special formulations are also discussed for dealing with uncertainty, learning about performance, and considering costs associated with delayed use of results.
411029	41102981	10-bit programmable voltage-output digital-analog converter	The paper discusses the development of a compact and low-power 10-bit floating-gate digital-to-analog converter (FGDAC) using nonvolatile floating-gate voltage references. This new design eliminates the limitations of traditional charge amplifier voltage-output DACs, resulting in improved accuracy and reduced element spread. The FGDAC was fabricated using a 0.5 micrometre CMOS process with a total area of 0.0522 mm2. Experimental results demonstrate that it can achieve INL and DNL values of less than plus or minus 0.5 LSB (0.68 mV). This FGDAC allows for programmable linear or nonlinear conversion with high precision.
411029	41102963	A tunable floating gate CMOS resistor for low-power and low-voltage applications	This paper discusses a tunable CMOS resistor that can be integrated into low-power and low-voltage applications using standard CMOS technology. The resistor is created using the capacitive coupling and voltage storage capabilities of a floating-gate transistor, and a scaled-gate linearization technique is used to minimize nonlinearities. The resistance can be adjusted using Fowler-Nordheim tunnelling and hot-electron injection quantum mechanical phenomena. Experimental data from a 0.5 mu m CMOS chip show that the resistor has a total harmonic distortion (THD) of less than 7% and 1K Hz 1V(pp) sinusoidal input linearity. 
411030	41103041	A semantic environment model for crowd simulation in multilayered complex environment	This paper discusses the challenges and importance of modeling environments in crowd simulation, particularly in complex and multilayered environments. The authors propose a three-tier semantic model for representing the environment, which includes geometric, semantic, and application levels. This model allows for better interactions between individuals and the virtual environment. A modified continuum crowd method is then applied to this model to simulate realistic behaviors of large crowds in complex environments such as buildings and subway stations. The method is tested in two synthetic urban spaces and the results demonstrate the effectiveness of the semantic environment model in providing accurate information for crowd simulation.
411030	4110308	Optimization-based group performance deducing	The paper discusses the challenges of animating large-scale group performances and proposes an optimization method to address this issue. The method uses a group motion bigraph technique to transform the problem into a constrained optimization problem. This allows for the automatic generation of large group motions with velocity constraints. The paper also describes an interactive system for constructing the group motion bigraph, which allows for flexible editing and control. The results of the animation show that this method is successful in generating large-scale group performances from a small number of motion clips performed by smaller groups.
411031	41103165	Stabilizing Locally Maximizable Tasks in Unidirectional Networks Is Hard	A distributed algorithm is considered self-stabilizing if it can recover from faults and attacks without external intervention in a finite amount of time. This paper discusses the problem of constructing self-stabilizing solutions for locally maximizable tasks in uniform unidirectional networks. The authors show that deterministic self-stabilization is impossible in these networks, and that the "silence" property (where communication is fixed from a certain point in the execution) cannot be guaranteed. However, they present a series of generic protocols that can be used for all locally maximizable tasks. These include a deterministic protocol with polynomial time and space complexity, and two probabilistic protocols with expected polynomial time complexity. 
411031	41103161	Stabilizing Maximal Independent Set in Unidirectional Networks is Hard	A distributed algorithm is self-stabilizing if it can recover from faults and attacks without external intervention in a finite amount of time. This paper focuses on constructing a maximal independent set in uniform unidirectional networks of any shape. It is proven that deterministic self-stabilization of this problem is impossible in uniform networks, and the "silence" property (communication being fixed from a certain point in every execution) cannot be guaranteed by either deterministic or probabilistic protocols. However, a deterministic protocol with polynomial space and time complexity is presented for networks with unique identifiers. For the uniform case, two probabilistic protocols are also presented, with one having infinite memory but handling asynchronous scheduling and the other having polynomial space complexity but only handling synchronous scheduling. Both probabilistic protocols have expected polynomial time complexity.
411032	4110323	A Generalized Semantics for Concurrent Constraint Languages and their Abstract Interpretation	The authors propose a framework for abstract interpretation of concurrent constraint languages, using the concept of abstraction between constraint systems. They define an abstract program for a concrete program, with the execution of the abstract program performing the abstract interpretation. This framework is based on a denotational semantics of concurrent constraint languages, where each agent is seen as a closure operator. The authors demonstrate the practical implementation of this analysis through a reexecution algorithm, and also show how suspension analysis can be incorporated into the framework. This extends a previous framework for abstract interpretation of concurrent logic languages.
411032	4110325	A Constraint-Based Language for Virtual Agents	The authors have developed a high-level language called VRCC, which is integrated into VRML, for describing behaviors of autonomous agents in 3D virtual worlds. This language is based on the concept of constraints and uses the Timed Concurrent Constraint framework, which is suitable for animation systems like VRML. They demonstrate the effectiveness of this approach by creating virtual creatures with autonomous movement capabilities and behaviors inspired by biological navigation models. The integration of VRCC into VRML allows for a more comprehensive and efficient way of describing agent behaviors in virtual environments.
411033	4110334	Towards the use of cad models in VR applications	Large industrial companies have a goal of creating integrated information systems to manage their projects. These systems include 3D visualization of models that are realistic enough to be used for virtual prototyping, design review, and training. However, there are challenges in producing Virtual Reality (VR) models from Computer-Aided Design (CAD) models. To address this, an application called ENVIRON (ENvironment for VIRtual Objects Navigation) was created to allow real-time interaction and an immersive experience with large industrial engineering models. This application was developed to meet the growing need for using VR in the industry, particularly with CAD models.
411033	41103348	Environ: integrating VR and CAD in engineering projects.	Large industrial-engineering departments have a key goal of implementing integrated information systems to effectively manage the life cycles of their projects. Environ is an application designed to meet this demand by utilizing Virtual Reality in large engineering models created through CAD tools. Its primary objective is to provide 3D visualization capabilities that are realistic enough to serve as an integration tool for various engineering tasks in the oil and gas industry. This allows for improved coordination and efficiency in the overall project management process.
411034	41103436	Authenticated system calls	System call monitoring is a technique used to detect and control compromised applications by checking each system call at runtime. A new approach to this is introduced, using authenticated system calls which are augmented with extra arguments and a cryptographic message authentication code (MAC). This extra information is used by the kernel to verify the system call and its policy. The application is automatically generated with authenticated system calls through an installer program that uses static analysis to generate policies and rewrites the binary. The paper presents the approach, a prototype implementation, and experimental results showing its effectiveness in protecting against compromised applications at a reasonable cost.
411034	4110341	Facilitating Mixed Language Programming in Distrbuted Systems	The article proposes a method for making mixed language programming easier in distributed systems. This involves implementing a generic remote procedure call feature and using a type system to define the interfaces and data transfer between procedures. The type system also ensures a consistent representation of data across languages. Standard mappings are defined for each programming language to automate the data conversions needed for cross-language calls. A prototype of this approach has been built on machines running Berkeley UNIX®. This allows for seamless communication between program components written in different languages, with the option for manual control if needed. 
411035	41103519	On Constrained Spectral Clustering and Its Applications	Constrained clustering algorithms, such as -means and hierarchical clustering, have been extensively studied, but handling a large number of constraints in these algorithms is known to be difficult. One solution is to use spectral clustering, which is still an area of ongoing development. This paper proposes a new approach for constrained spectral clustering that explicitly incorporates Must-Link and Cannot-Link constraints into the optimization problem. This method has several practical advantages, including the ability to specify the level of confidence in the constraints and a guarantee of how well the constraints are satisfied. It can also be solved efficiently and inherits the benefits of spectral clustering. Empirical results demonstrate the effectiveness of this approach, including its potential for transfer learning using constraints. 
411035	41103556	Flexible constrained spectral clustering	The paper introduces a new framework for constrained spectral clustering, which allows for the explicit encoding of Must-Link and Cannot-Link constraints. This is in contrast to previous methods that modify the graph Laplacian or the eigenspace to indirectly incorporate the constraints. The proposed method has several advantages, including the ability to set weights for the constraints, guaranteeing a minimum satisfaction threshold, and being solvable in polynomial time. The framework inherits the objective function from spectral clustering and can be seen as an extension of it. Empirical results on real-world datasets show the effectiveness of the approach for image segmentation and clustering tasks with both binary and weighted constraints.
411036	41103626	Re-tiling polygonal surfaces	This paper introduces an automatic method for creating surface models at different levels of detail, which is important for interactive graphics and rendering complex scenes. The method involves connecting new vertices over the surface, using surface curvature to determine vertex distribution, and smoothly interpolating between models. The key concept is creating an intermediate model called the mutual tessellation, which combines original and new vertices. The original vertices are then removed and the surface is locally re-triangulated to match the original connectivity. This technique has been successfully applied to various types of surface models. The paper also discusses relevant computer graphics categories and subject descriptors.
411036	41103635	Fast viscoelastic behavior with thin features	The authors propose a new method for animating deformable materials that combines a high resolution surface mesh with a tetrahedral finite element simulator. This allows for fast and detailed simulations of complex elastic and plastic behavior without common artifacts such as volume-loss, smoothing, and popping. By coupling a high resolution surface with low-resolution physics and using an efficient approach to creating the tetrahedral mesh, they achieve an order-of-magnitude speedup in re-meshing time. This method can simulate a wide range of material behaviors and maintain high resolution surface features in a short time. 
411037	41103723	Correctness proof for database reconstruction algorithm.	The rise in the use of databases for storing important information has also led to an increase in computer crimes involving databases. Despite some research and practical applications in digital forensics for databases, this area has not received much attention and lacks a defined model. This paper introduces an algorithm for reconstructing database information for forensic purposes and provides proof of its correctness. The algorithm uses the current database and a log of modifications to determine the data in a relation at a specific time. The paper also discusses inverse functions for relational algebra operators and introduces the concept of relational algebra log and value blocks. The majority of the paper is dedicated to explaining the proof of correctness for the database reconstruction algorithm.
411037	41103727	The role of triggers in database forensics	Database triggers are a commonly used feature in databases that automatically perform actions based on changes in the data or its structure. While they have not been extensively studied in the academic research community, it is important for digital forensic investigators to be aware of their potential impact on forensic analysis methods. This paper examines the various types of database triggers and their implementations, and how they could potentially disrupt or manipulate forensic investigations. The study finds that some current forensic analysis methods may be affected by the presence of triggers, and recommends that the handling and analysis of triggers should be included in the forensic interpretation and attribution processes. 
411038	41103817	A Systematic Approach to the Development of Event Based Applications	LECAP is a new framework proposed for building event-based applications. It offers several advantages over existing approaches, such as supporting a while-parallel language, dynamic binding of programs to events, stepwise development, and composition of specifications. The event-based architectural style is recognized for its ability to develop large and complex systems by loosely coupling components, making it popular in various environments. However, current approaches to developing event-based applications lack support for reasoning about correctness. LECAP aims to address this issue by providing a compositional and stepwise approach to specifying and verifying event-based applications. 
411038	41103871	Constructing Deadlock Free Event-Based Applications: A Rely/Guarantee Approach	The paper proposes a formal semantics for a programming language that allows for the announcement of events. It highlights the similarities between event-based systems and parallel systems, particularly in how announcing an event leads to the parallel execution of subscribers. The paper also discusses how traditional concurrency concepts like synchronization and mutual exclusion can be applied to event-based applications using Jones's rely/guarantee method. Additionally, it addresses the development of deadlock-free event-based applications and extends Stolen's technique for handling auxiliary variables to support more complex applications.
411039	41103963	Starburst SSD: an efficient protocol for selective dissemination	Starburst is a routing-based protocol designed for efficient data dissemination in sensor networks. It uses a routing hierarchy to quickly and reliably deliver data to nodes that meet specific criteria. The protocol dynamically adapts its delivery policy based on the number of nodes requiring updates, using direct routing for a small number of nodes and more efficient algorithms for larger numbers. Dynamic beacon selection algorithms also improve scalability and fault tolerance. Starburst has been successfully implemented and evaluated on two routing protocols, showing a significant reduction in transmission cost and latency for small node subsets. Tests on a real-world testbed also validate its effectiveness.
411039	41103959	The firecracker protocol	The Firecracker protocol is proposed for efficient data dissemination in wireless sensor networks. It utilizes a combination of routing and broadcasts to quickly distribute data to all nodes in the network. The process begins with the data source sending data to distant points in the network, followed by a broadcast-based dissemination along the paths, similar to a string of firecrackers. By incorporating an initial routing phase, Firecracker can disseminate data at a faster rate than scalable broadcasts with fewer packets. The selection of routing points greatly impacts performance, highlighting the need for efficient any-to-any routing protocols in wireless sensor networks.
411040	411040110	On the communication and streaming complexity of maximum bipartite matching	In this work, the authors focus on understanding the communication and space complexity of the maximum bipartite matching problem. They introduce the concept of an ε-matching cover, which is a sparse subgraph of the original graph that preserves the size of maximum matching within an additive εn error. They show that a 2/3-approximation can be achieved with a message of size O(n) by constructing a 1/2-matching cover with additional properties. They also consider a restricted version of the problem and show that a 3/4-approximation can be achieved with linear space. Additionally, they design a deterministic one-pass streaming algorithm for a specific scenario, which is the setting of a well-known randomized algorithm for online bipartite matching. This is the first deterministic algorithm for this scenario with only O(n) space complexity.
411040	41104067	Streaming lower bounds for approximating MAX-CUT	The problem of estimating the max cut value in a graph using limited space in the streaming model of computation is considered. A simple 2-approximation solution using O(log n) space exists, while a near-optimal solution using Õ(n) space can be obtained by storing an Õ(n)-size sparsifier. It is questioned if a non-trivial approximation can be achieved using only poly-logarithmic space. Recent research has shown that estimating the size of a maximum matching can be done in poly-logarithmic space. The main result is that any algorithm breaking the 2-approximation barrier requires [EQUATION] space, even with random order of input edges. A distribution of graphs is presented to illustrate the necessity of [EQUATION] space for differentiating between bipartite and 1/2-far from bipartite graphs. It is also shown that obtaining a (1 + ε)-approximation to the max cut value in adversarial order requires n1−O(ε) space, proving that Ω(n) space is necessary for obtaining an arbitrarily good approximation. 
411041	41104146	Muse: Mapping Understanding and deSign by Example	Information integration involves designing relationships between two schemas, known as schema mappings. This is a complex task, and while automated tools can suggest potential mappings, there are few tools available to help designers understand and create alternative mappings. Muse is a mapping design wizard that uses data examples to assist designers in refining mappings towards the desired specification. It guides designers in two important components of mapping design: specifying grouping semantics for data sets and choosing among alternative interpretations for ambiguous mappings. Muse uses familiar databases and short sequences of small examples to infer the desired semantics and make the design process easier. Experiments with Muse on publicly available schemas have been successful.
411041	4110416	Designing and refining schema mappings via data examples	A schema mapping is a way to specify the relationship between a source and target schema. This is important for data integration and exchange. In the past, schema mappings were created manually or using mapping-design systems. Now, there is a new system that allows for interactive design using data examples. A sound and complete algorithm is used to determine if a GLAV schema mapping (with Global-and-Local-As-View constraints) can fit the data examples. This algorithm has been proven to be harder than NP-complete. Despite this, experimental results have shown that the system is effective and efficient in real-life scenarios. 
411042	41104252	A collaborative decision-making model for orientation detection.	Orientation detection is a crucial aspect of both biological vision and machine vision. The Hubel-Wiesel model, which suggests that orientation selectivity in simple cells is due to overlapping receptive field centers, has some limitations. To address these limitations, this paper proposes a collaborative decision-making approach using a double-layer neural network. The first layer estimates the relative position of a contrast edge, while the second layer uses a least square optimization to determine orientation. This approach is not only flexible but also applicable to image processing. Statistical and simulation experiments show that this model is efficient and effective, and can explain visual illusions. Additionally, it outperforms other image processing algorithms on natural images. The neural mechanism of this model is in line with neurobiological findings, making it suitable for higher level visual tasks.
411042	4110421	Neural encoding based on frequency states using multi-spike train data	Neural networks play a crucial role in both neuroscience and artificial intelligence, but understanding how they encode external stimuli and internal decision-making processes remains a challenge. While various assumptions and models have been proposed, there is little biological evidence to support them. This study aims to find an encoding method based on neuron firing frequency states and its transformation model under specific stimuli. It also introduces a new method to analyze spike train data, which has been proven effective. The results from analyzing simultaneous recordings from multiple microelectrodes in the temporal cortex and hippocampus of a mouse experiment support and validate this model.
411043	411043247	Robust wireless video streaming using hybrid spatial/temporal retransmission	The paper discusses two main challenges faced by wireless video streaming applications: bandwidth demands and timing constraints. To address these challenges, the authors propose a hybrid spatial/temporal retransmission protocol which uses overhearing nodes as relays to retransmit failed packets. This approach increases individual throughput and overall network capacity. The protocol also includes a Time-based Adaptive Retransmission strategy to meet timing constraints by dynamically determining whether to transmit or discard a packet based on its retransmission deadline. Results from evaluations on both a testbed and in real-world scenarios show that this hybrid approach improves streaming performance, particularly in busy networks, under fading conditions, and for mobile users.
411043	41104323	Classification-Based relay selection for video streaming over wireless multihop networks	Real-time streaming of audiovisual content over wireless networks is becoming increasingly important in multimedia communication. This paper introduces a new method for selecting relays to support time-aware opportunistic relaying for video streaming over wireless mesh networks. The approach involves using a support vector machine (SVM) to determine which candidate relays should forward packets for a source-destination pair. The SVM considers features related to video quality and is trained to maximize user experience. Testing showed that this classification-based approach outperforms the traditional method.
411044	4110449	Functional ASP with Intensional Sets: Application to Gelfond-Zhang Aggregates.	This paper introduces a variant of Answer Set Programming (ASP) with evaluable functions that allows for a fully logical treatment of aggregates. This is achieved by incorporating a new type of logical term, intensional sets, which can be used as predicate or function arguments and can be nested within other intensional sets. This approach has several advantages over other semantics for aggregates, including a compositional semantics and the ability to explicitly define aggregates within the logical language. The proposed semantics for aggregates is also shown to coincide with the one defined by Gelfond and Zhang for the Alog language, when restricted to that syntactic fragment.
411044	41104414	A logical characterisation of ordered disjunction	This paper discusses the logical treatment of the ordered disjunction operator ×, which is used to represent preferences in logic programming under the answer set semantics. The authors focus on the first step of the semantics, which involves translating the LPOD into a set of normal programs. They show how this can be done using a translation of the ordered disjunction into the logic of Here-and-There. This not only provides an alternative implementation for LPODs, but also allows for checking properties of the × operator, such as its distributivity with conjunction and regular disjunction. The authors also compare their approach to an extension proposed by Kärger et al. that combines × with regular disjunction.
411045	41104538	Idea Inheritance, Originality, and Collective Innovation.	This paper discusses the process of creating new products through the combination of previous ideas. While previous studies have focused on patent citations, this paper examines the impact of originality on popularity and practicality through the analysis of collaborations in an online virtual community. The authors propose a new method for measuring innovation based on the distance between 3D shapes. This research sheds light on the creative process and the role of technology in promoting creativity. It also highlights the potential for open innovation, where individuals can collaborate and evolve designs without belonging to the same organization or exchanging money.
411045	41104534	Introduction to Collective Intelligence and Crowds: Structure, Roles, and Identity Minitrack.	Internet technologies have opened up new possibilities for creating products and services by leveraging social networks and crowds. These collectives, which can be loosely connected, have the ability to produce a variety of outputs, from digital media to collaborative databases. Social networks grow organically over time, while crowds can be rapidly assembled. The three papers chosen for this year's minitrack explore how social media is used for status production, how crowd monitoring can improve transparency in the freight industry, and the potential side effects of incentives in online contests. These papers offer insight into the role of crowds and collective intelligence in today's digital age.
411046	41104666	Construction and analysis of ground models and their refinements as a foundation for validating computer-based systems	In order to make the verified software challenge proposed by Hoare and Misra have practical impact, it is necessary to include rigorous definitions and analysis before code development. This involves creating ground models, or blueprints, that describe the desired application of the programs. These models must be linked to the code in a traceable and checkable way, and the relevant properties must be refined in a stepwise manner. The Abstract State Machines (ASM) method is a reliable system development discipline that can bridge the gap between informal requirements and executable code by combining application-centric modeling with mathematically verifiable detailing. This allows for compile-time verification of the code.
411046	41104649	Modularizing Theorems for Software Product Lines: The Jbook Case Study	A software product line is a group of related programs that are assembled economically. This paper explores how theorems about program properties can be used in feature-based development of software product lines. The authors use a Java/JVM compilation correctness proof as a case study and demonstrate how features can be used to modularize program source, theorem statements, and proofs. They also discuss the similarities between the refinement concepts used in Abstract State Machines and Feature-Oriented Programming, suggesting that these two communities could benefit from collaboration.
411047	41104735	A logic programming approach to knowledge-state planning: Semantics and complexity	The article introduces a new planning language called K, which is based on logic programming principles. It allows for describing transitions between states of knowledge rather than fully described states of the world, making it suitable for planning under incomplete knowledge. K also supports default principles through negation as failure. It is flexible, as it can also represent transitions between states of complete knowledge. This allows for natural and compact problem representation. The article provides a thorough analysis of K's computational complexity, showing that it can handle various planning problems, including standard and secure planning. These findings serve as the basis for the DLVk system, which implements K using the DLV logic programming system. 
411047	41104748	A logic programming approach to knowledge-state planning, II: the DLVk system	In this series of papers, a new planning language called K is proposed for describing transitions between states of knowledge. It is well-suited for planning under incomplete knowledge and can also represent transitions between states of complete knowledge. Part II introduces the DLVK planning system, which utilizes K on top of the disjunctive logic programming system DLV. This system can solve difficult planning problems, including secure planning with incomplete initial states. A comparison with other logic-based planning systems shows that DLVK is competitive and often provides a more natural and simple representation of planning problems. 
411048	41104814	Compressed Sensing of Approximately-Sparse Signals: Phase Transitions and Optimal Reconstruction	Compressed sensing is a technique used to measure signals that are sparse, meaning they only have a small number of significant components. However, most signals are only approximately sparse, meaning that while they have a small number of relevant components, the others are not exactly zero but close to it. In this study, the researchers use a Gaussian distribution to model these approximately sparse signals and examine their compressed sensing with dense random matrices. They use replica calculations to determine the optimal reconstruction error for these signals and analyze the performance of the G-AMP algorithm. They also propose a special "seeding" design for measurement matrices to improve the algorithm's optimality.
411048	4110481	Bayesian signal reconstruction for 1-bit compressed sensing.	The 1-bit compressed sensing framework is a method for recovering a sparse vector by using only the sign information of its linear transformation. This approach is useful in practical applications as it reduces the amount of data needed. In this paper, a Bayesian approach to signal reconstruction for 1-bit compressed sensing is presented and analyzed using statistical mechanics. The setup assumes that the measuring matrix has independent and identically distributed entries and the measurements are noiseless. Using the replica method, it is shown that this approach outperforms the commonly used l(1)-norm minimization method, especially when the non-zero entry positions of the signal are known. Numerical experiments support these findings.
411049	41104941	Componentising a Scientific Application for the Grid	Grid applications are complex and managing this complexity can be a challenge. A promising solution is component-based development, which is gaining popularity in the grid community. To evaluate its effectiveness, a case study was conducted on reengineering a high performance numerical solver into a component-based grid application. The chosen component model is a specialized version of the Fractal model designed for grid environments. The results of the study show that componentization has improved the application's ability to be modified and reused without sacrificing performance. This provides evidence that component-based development can be a successful approach for building and evolving grid applications. 
411049	41104967	A Flexible Model and Implementation of Component Controllers	The GCM (Grid Component Model) is a component model created by the CoreGRID institute to improve existing component systems for Grid computing. It is based on the Fractal component model and includes features such as hierarchical composition, structured communications, and support for adaptivity and autonomicity. The GCM distinguishes controllers for functional and non-functional concerns and includes a membrane for organizing components. This article presents a refinement of the Fractal/GCM model and an API for component membranes, with a focus on adaptivity and autonomicity. The design also considers hierarchical composition and distribution of the membrane, allowing for coexisting "classical" controllers and dynamic, reconfigurable controllers.
411050	411050175	Traceback-Based Optimizations for Maximum a Posteriori Decoding Algorithms	MAP decoding is an important tool for turbo coding and other advanced feedback-based algorithms. However, in order to use these techniques in resource-constrained systems, it is necessary to limit the complexity of their implementation while maintaining their superior performance. One way to achieve this is by incorporating traceback information into the MAP algorithm, which can simplify the computational requirements. This approach also offers the potential for new architectural variants for the decoder, each with its own advantages depending on the memory hardware and number of trellis states. These enhancements can significantly reduce computational complexity without sacrificing performance. 
411050	411050111	Multicore Processing and Efficient On-Chip Caching for H.264 and Future Video Decoders	The demand for high performance video decoding is expected to increase in the future due to higher resolutions and faster frame rates. Multicore processing is an effective solution to handle the increased computation, while power-constrained devices can benefit from voltage scaling. This paper proposes multiple parallel decoder techniques and on-chip caching schemes to improve performance and reduce power consumption. These techniques can achieve up to a 2.91 times speedup with N parallel hardware decoders, or a 60% power savings with a single decoder. Additionally, the proposed on-chip caching methods can significantly reduce off-chip memory bandwidth and improve overall performance and energy efficiency. These techniques were validated and benchmarked using Verilog simulations and can be applied to various decoder architectures. The evaluation metrics include performance, power, area, memory and coding efficiency, and input latency.
411051	41105124	Bounds on Redundancy in Constrained Delay Arithmetic Coding	In this paper, the authors discuss the issue of a finite delay constraint in an arithmetic coding system. This constraint can lead to significant delays in encoding or decoding due to the nature of the coding process. To overcome this problem, the authors propose a method of inserting fictitious symbols, which results in a redundancy in the coding rate. They derive an upper bound for this redundancy and show that it decreases exponentially as the delay constraint increases. This makes their method more effective than existing block to variable methods. The authors also demonstrate the practical application of their results in compressing English text.
411051	4110514	Delay and Redundancy in Lossless Source Coding.	This study examines the impact of a finite delay constraint on lossless source coding for a memoryless source. While block-to-variable and variable-to-variable codes typically have a polynomial decrease in redundancy with increasing delay, this research shows that it is possible to achieve an exponential decrease in redundancy with the delay constraint. The minimum redundancy-delay exponent is found to be equal to the R\'enyi entropy of the source, and the maximum is determined by the probability and size of the source's alphabet.
411052	41105232	Using the mobile application EDDY for gathering user information in the requirement analysis	The success of product design relies heavily on understanding the knowledge and requirements of the target users. Many user-centered design studies have been conducted over the years and can be used as a resource by developers. This paper introduces a framework called EDDY, which aims to aid in the development of mobile applications for data collection. These applications can be useful for studies that involve users collecting information themselves, such as Cultural Probes and Experience Sampling Method. The paper evaluates the benefits of using EDDY for these studies and compares it to traditional methods.
411052	411052101	Measuring the impact of multimodal behavioural feedback loops on social interactions.	The paper examines the use of automatic behavioural feedback loops (BFL) during social interactions. These BFLs analyze the user's behavior in real time and provide instant feedback to improve the quality of their behavior. The researchers developed a software framework for implementing BFL on Android mobile devices and conducted a user study to understand the effects of BFL on face-to-face interactions. The study compared four types of BFL using tactile, auditory, and visual modalities, with the goal of improving users' perception of their speaking time in group discussions. The results provided insights into the effectiveness of BFL on conversations and how people respond to them. 
411053	41105376	Dynamic Bayesian network based interest estimation for visual attentive presentation agents	The paper discusses a user study that was conducted to compare two methods of estimating users' interest in a multimodal presentation based on their eye gaze. The study took place in a virtual showroom where 3D agents presented product items and adapted their performance according to users' attentiveness. The system analyzed eye movements in real-time to determine users' attention and visual interest towards interface objects. Previous research used algorithms based on the time spent looking at an object, but this was not effective for dynamic presentations. The paper proposes using dynamic Bayesian networks to consider the context of the object of interest, resulting in more timely and appropriate responses from the presentation agents. The benefits of this approach are demonstrated theoretically and empirically.
411053	411053130	Gaze-based infotainment agents	Our proposed infotainment presentation system utilizes eyegaze as an easy and non-intrusive way for users to interact with the interface. It tracks eye movements in real-time to determine the user's attention and interest in different objects on the screen. The system features a virtual showroom where two lifelike 3D agents showcase products in a captivating and enjoyable manner. The presentation adjusts according to the user's level of engagement and interest, creating a more personalized and engaging experience. This system aims to enhance the user's experience and make the presentation more user-centric.
411054	41105415	Performability assessment by model checking of Markov reward models	This paper introduces efficient methods for model checking Markov reward models, particularly for evaluating the performability of computer-communication systems. The authors propose the use of the logic CSRL for specifying performability measures, which offers flexibility and allows for numerical evaluation of various measures. The use of CSRL also helps reduce the size of Markov reward models that need to be analyzed. The paper presents background information on both Markov reward models and CSRL, including their syntax and semantics. It also discusses a duality result between reward and time. The paper then presents five numerical algorithms for verifying time- and reward-bounded until-properties, a key operator in CSRL. A case study is provided to demonstrate the versatility of this approach. 
411054	411054145	On the Logical Characterisation of Performability Properties	Markov-reward models are used to evaluate the performance and dependability of systems. However, the specification of reward-based measures has been done manually and informally. To address this issue, a new continuous-time, reward-based stochastic logic is introduced in this paper. This logic is suitable for expressing a wide range of performability measures. The paper presents two sub-logics, CSL and CRL, which complement each other. The main duality theorem shows that reward-based properties expressed in CRL can be interpreted as CSL properties over a derived continuous-time Markov chain. This allows for the use of existing model checking procedures for CSL in evaluating reward-based properties. 
411055	411055129	Efficient on-the-fly algorithms for the analysis of timed games	The paper introduces a new efficient on-the-fly algorithm for solving timed game automata with a focus on reachability and safety properties. It is a symbolic extension of a previous algorithm for model-checking finite-state systems. The algorithm can terminate before fully exploring the state-space and utilizes zones as a data structure for efficient steps. The paper also suggests optimizations and methods for obtaining time-optimal winning strategies for reachability games. The algorithm is evaluated through experiments, showing promising performance results.
411055	411055151	Efficient Guiding Towards Cost-Optimality in UPPAAL	This paper introduces an algorithm for efficiently finding the minimum cost to reach a goal state in the model of Uniformly Priced Timed Automata (UPTA), which is a submodel of linearly priced timed automata. The algorithm uses a symbolic semantics of UPTA and a difference bound matrix representation to optimize its search order. Similar to Dijkstra's shortest path algorithm, the search order can be chosen for optimal exploration of symbolic states. Techniques inspired by branch-and-bound algorithms are also included to limit the search space and quickly find near-optimal solutions. The algorithm has been implemented in the verification tool UPPAAL and has shown to reduce the explored state-space by up to 90%. 
411056	41105647	Learning to Map between Ontologies on the Semantic Web	Ontologies are essential for the success of the Semantic Web, as they allow for the publication of machine understandable data. However, with the distributed nature of the Semantic Web, data will come from various ontologies, making information processing across them challenging without knowing the semantic mappings between elements. Manually finding these mappings is not feasible, and thus, the development of tools to assist in the ontology mapping process is crucial. Glue is one such system that uses machine learning techniques to find mappings between two ontologies. It employs multiple learning strategies and incorporates commonsense knowledge and domain constraints to improve matching accuracy. Experiments have shown that glue proposes highly accurate semantic mappings in various real-world domains.
411056	41105630	Learning to Match the Schemas of Data Sources: A Multistrategy Approach	Data integration from multiple sources has been a major concern in the database and AI communities. The focus has been on creating systems that can provide a consistent interface for querying different sources. However, the manual process of creating semantic mappings between the query interface and source schemas has been a bottleneck. To address this, a multistrategy learning approach is proposed, where multiple learner modules are used to automatically find mappings by utilizing various techniques such as Naive Bayes, nearest-neighbor classification, entity recognition, and information retrieval. This approach is employed in the LSD system, which also incorporates domain integrity constraints, user feedback, and nested structures in XML data to improve matching accuracy. Experimental tests on real-world domains validate the effectiveness of this approach in proposing accurate semantic mappings.
411057	41105733	A-Ordered Tableaux	The use of A-orderings of literals in resolution proof procedures has been extensively studied, but in tableau proof procedures it has only recently been introduced by the authors of this paper. The paper presents a completeness proof for A-ordered ground clause tableaux, which is easier to follow than previous proofs. This technique is then extended to the non-clausal and non-ground cases, and an ordered version of Hintikka sets is introduced. It is shown that regular A-ordered tableaux are a proof confluent refinement, but when combined with connection refinements, they become an incomplete proof procedure. The paper also introduces regular A-ordered first-order NNF tableaux and discusses their completeness and implementation.
411057	41105768	Towards an Efficient Tableau Proof Procedure for Multiple-Valued Logics	Tableau-based theorem provers are useful for non-standard logics, but they are not very efficient in practical applications. They are, however, intuitive and flexible for proof theory. To improve efficiency for multiple-valued logics, a new concept called "signed formulas" is introduced. This approach leads to complete and sound tableau systems for any finite-valued propositional logic.
411058	41105861	Conditions for Resolving Observability Problems in Distributed Testing	This paper discusses the issues of controllability and observability that can arise when using a test architecture with multiple remote testers. These problems often require external coordination messages to be used during testing. The goal is to create a test or checking sequence that is free from these problems without relying on external coordination. The paper explores conditions that make it possible to construct such a sequence and provides procedures for creating subsequences that eliminate the need for external coordination messages. This allows for more efficient and effective testing of systems that meet these conditions.
411058	411058106	Checking sequences for distributed test architectures	Controllability and observability problems can arise when using a checking sequence in a test architecture with multiple remote testers. These issues often require coordination messages between testers, but this can be costly and difficult to implement. It can also introduce delays, which can be problematic for systems with timing constraints. To avoid these problems, it is desirable to create a checking sequence directly from the system's specification, without the need for coordination messages. This paper presents conditions and an algorithm for achieving this using multiple distinguishing sequences. By doing so, the testing process can be more efficient and reliable. 
411059	4110593	Cross-Modal Supervision For Learning Active Speaker Detection In Video	This paper proposes a method for using audio to supervise the learning of active speaker detection in videos. The Voice Activity Detection (VAD) technique is used to guide the learning of a vision-based classifier in a weakly supervised manner. The classifier utilizes spatio-temporal features to capture upper body motion related to speaking, such as facial expressions and hand gestures. The authors also introduce a person-specific model to improve the generic model for active speaker detection. Furthermore, the paper demonstrates the online adaptation of the generic model to previously unseen speakers using audio (VAD) for weak supervision. This approach overcomes the lack of clean training data by leveraging temporal continuity. Overall, this is the first system to learn and automatically adapt to new speakers in a different dataset using audio-visual data. The work showcases the potential of multi-modal data for unsupervised learning by transferring knowledge from one modality to another.
411059	41105947	Object Classification with Adaptable Regions	This paper proposes a new approach to classification of objects by focusing on the semantic representation of an image. Instead of just improving the low level representation, the authors aim to learn the important visual components and their interactions in order to correctly classify objects. They introduce a new latent SVM model that jointly learns the object class, spatial location, and appearance. This helps to regularize the complexity of the model and penalize unlikely configurations. Experimental results on the Pascal VOC dataset show improved performance in classification and weakly supervised detection. The authors also demonstrate how this semantic representation can be used for finding similar content. 
411060	41106027	The tradeoffs of fused memory hierarchies in heterogeneous computing architectures	The rise of general purpose computing on graphics processing units (GPGPU) has led to a convergence of consumer and high-performance computing markets. Many top-ranked HPC systems now include GPU accelerators, but previous connections between the CPU and GPU through the PCIe bus have limited scalability. A new trend towards integrating the CPU and GPU has removed this bottleneck and created a unified memory hierarchy. This trend is examined through AMD's Fusion Accelerated Processing Unit (APU) as a testbed, comparing its performance, power consumption, and programmability to discrete GPUs. The unified memory hierarchy offers potential tradeoffs for high-performance scientific computing.
411060	41106031	Memphis: Finding and fixing NUMA-related performance problems on multi-core platforms	This paper discusses the impact of Non-Uniform Memory Access (NUMA) on high-end scientific applications. While these applications were previously immune to NUMA-related performance issues, recent micro-processor design trends have made them more susceptible. The paper addresses this issue by first outlining the potential problems NUMA can cause for multi-threaded applications and proposing solutions. It then presents evidence that NUMA can significantly hinder the scalability of scientific applications. The authors also describe three methods of using hardware performance counters to identify NUMA-related problems. Finally, they introduce Memphis, a tool that uses Instruction Based Sampling to improve the performance of production-level codes such as HYCOM, XGC1, and CAM by up to 24%.
411061	41106172	Combining Multi-robot Exploration and Rendezvous	This study focuses on the problem of exploring an unknown environment using two mobile robots. The objective is to achieve a quick rendezvous between the robots while maximizing their speed in exploring the environment. The main challenge is to minimize the reliance on communication for the rendezvous. This involves identifying unique potential rendezvous locations, ranking them based on their uniqueness, and synchronizing with the other robot to meet at one of these locations at a specific time. These tasks must be performed simultaneously while exploring and mapping the environment. The proposed approach combines the exploration and rendezvous tasks by considering the cost and uniqueness of potential rendezvous locations. Results show that this approach improves the efficiency of the joint tasks compared to using uniqueness alone.
411061	4110618	Probabilistic Cooperative Localization And Mapping In Practice	This paper presents a probabilistic approach to reducing uncertainty in a moving robot's pose during exploration by utilizing a second robot. The technique involves using a Monte Carlo Simulation (Particle Filter) to model and decrease odometric error. The study focuses on determining the necessary conditions for obtaining an accurate and timely pose estimate. The method is applied to a team of two robots exploring an indoor environment, with potential for extension to larger groups. The approach involves one robot acting as a "robot tracker" to estimate the position of the other robot and using this information to guide the exploration process. Results from both real-world and simulated experiments are presented, demonstrating the effectiveness of the method in guaranteeing complete exploration without overlaps. 
411062	4110625	Effort Games and the Price of Myopia	Effort Games are a game-theoretic model used to study cooperation in open environments. It is based on the principal-agent problem from economic theory, where a central authority (the principal) tries to incentivize agents to exert effort towards completing a common project. The probability of completing a task is higher when effort is exerted, but there is a cost for the agent. This domain can be modeled as a normal form game, with payoffs based on probabilities of tasks and a boolean function defining successful completion. The Price of Myopia is proposed as a measure of the influence of rationality on the principal's minimal payments. It is computationally complex to test dominant strategies and find reward strategies, and these problems are at least as hard as calculating the Banzhaf power index. However, in certain restricted domains, these problems can be solved in polynomial time. The article also provides simulation results for specific types of effort games.
411062	41106253	Power and stability in connectivity games	The article discusses the use of game theory in analyzing network reliability. A network is modeled as a coalitional game, where agents control vertices and aim to connect a specific subset of vertices to ensure communication. Power indices are used to identify potential points of failure and improve network reliability. Calculating the Banzhaf power index is challenging in general graphs, but a polynomial algorithm is suggested for trees. The core, which allows for fair division of payments, can also be computed using a polynomial algorithm. These methods can help identify and address potential failures in communication networks.
411063	4110632	On worst-case allocations in the presence of indivisible goods	In this study, we examine a problem of fairly allocating a set of goods to a group of n agents. While it is known that proportional allocations are possible in the case of infinitely divisible goods, this is not always the case with indivisible goods. We focus on the algorithmic and mechanism design aspects of this problem, and identify a lower bound for the value that each agent can receive. We also develop a polynomial time algorithm for finding such allocations and explore the design of truthful mechanisms. Our findings show that while a deterministic mechanism cannot achieve a truthful $\frac{2}{3}$ -approximation, a simple algorithm can achieve a constant approximation when the number of goods is limited. We also provide a negative result for randomized mechanisms under certain conditions. 
411063	41106333	Social Networks with Competing Products	This article presents a new model for social networks where nodes can adopt multiple alternatives based on influence from their neighbors. The authors identify different types of social networks that can lead to unanimous adoption of a product or guarantee a unique outcome. They also propose polynomial time algorithms to determine these properties. The study also investigates networks without unique outcomes and shows that determining if all nodes have adopted a product is NP-complete. The complexity of determining if a specific node has adopted a product is also resolved. The authors also demonstrate the NP-hardness of approximating the minimum spread of a product in contrast to efficiently computing the maximum spread. The problems can be solved in polynomial time when there are only two products.
411064	41106425	A Novel Cryptographic Algorithm Based on Iris Feature	Biometric cryptography is a method that uses biometric characteristics to encrypt data, increasing its security and addressing the limitations of traditional cryptography. A new algorithm is proposed in this paper, using the iris as the biometric feature due to its high accuracy. The algorithm extracts a 256-dimensional textural feature vector from the iris image using 2-D Gabor filters. The data is then encrypted and decrypted using add/subtraction operations and the Reed-Solomon error-correcting algorithm. Experimental results prove the effectiveness of this system.
411064	41106411	A Novel Cryptosystem Based on Iris Key Generation	Biometric cryptography is a method of encrypting data by using biometric features, which can improve the security and address limitations of traditional cryptography. This paper introduces a new biometric cryptosystem that uses the highly accurate iris as the biometric feature. In the encryption process, a 256-dimension textural feature vector is extracted from the preprocessed iris image using 2-D Gabor filters. An error-correct-code (ECC) is also generated using Reed-Solomon algorithm. This feature vector is then converted into a cipher key using a Hash function, which is used to encrypt the secret information. In decryption, the feature vector is corrected using the ECC and translated back into the cipher key using the same Hash function. The encrypted information can then be decrypted using general decryption algorithms. The system has been tested and proven to be effective.
411065	411065177	Shared Linear Encoder-Based Multikernel Gaussian Process Latent Variable Model for Visual Classification	This paper introduces a new multiview learning method based on the Gaussian process latent variable model (GPLVM). Unlike previous GPLVM methods, this approach considers a back constraint and uses a linear projection to map multiple observations to a consistent subspace before projecting them onto the latent variable space with a Gaussian process prior. Additionally, a multikernel strategy is used to design the covariance matrix, making it more adaptive for data representation. To improve classification, a discriminative prior is also incorporated into the learned latent variables. Experimental results on three real-world databases demonstrate the effectiveness and superiority of this method over existing approaches.
411065	411065112	Data Classification on Multiple Manifolds	This article discusses a new approach to data classification that takes into account the possibility that data from different classes may reside on different manifolds of varying dimensions. Unlike previous algorithms that assume all data is on a single manifold, this method utilizes multiple manifolds, each corresponding to a specific class. The manifolds are first learned for each class separately, and a stochastic optimization algorithm is used to determine the optimal dimensionality of each manifold for classification purposes. The classification process involves a minimum reconstruction error based classifier. The method can be easily adapted to different manifold learning methods and search strategies. Experiments on synthetic data and facial expression image databases demonstrate the effectiveness of this approach. 
411066	41106661	Very deep feature extraction and fusion for arrhythmias detection.	The electrocardiogram (ECG) is a widely used tool for diagnosing diseases related to abnormal heart rhythm. However, detecting these abnormalities is challenging due to the complexity and noise of ECG signals. To address this, researchers propose a deep convolutional neural network (VDCNN) using small filters to reduce noise and improve performance. They also introduce multi-canonical correlation analysis (MCCA) and the Q-Gaussian multi-class support vector machine (QG-MSVM) for better feature learning and classification. Results show that this approach outperforms other methods in accurately differentiating between normal and abnormal heartbeats without any noise filtering or pre-processing.
411066	411066104	Orientation selection using modified FCM for competitive code-based palmprint recognition	Coding-based methods are being increasingly used for palmprint recognition due to their advantages of small feature size, fast matching speed, and high verification accuracy. One such method is competitive coding, which involves convolving the palmprint image with a set of Gabor filters and encoding the dominant orientation into a bitwise representation. However, there has been limited research on the impact of the number of Gabor filters and their orientations. In this paper, a modified fuzzy C-means cluster algorithm is proposed to determine the orientation of each Gabor filter based on statistical orientation distribution and orientation separation characteristics. Experimental results show that this approach outperforms the original competitive coding and other popular methods in terms of verification accuracy. Furthermore, it is found that using six orientations provides the best balance between computational complexity and verification accuracy for palmprint recognition.
411067	41106750	Super-recursive Algorithms and Modes of Computation	In contemporary computer science, computers and computer networks can function in two modes: functional recursive mode and functional super-recursive mode. Some researchers argue that interactive computation is more powerful than Turing machines, while others believe that the Church-Turing Thesis still holds. The disagreement is due to the fact that traditional computability theory does not consider real world factors such as time and space. However, it has been proven that even a finite system of interacting recursive automata or algorithms can achieve super-recursive power in real world conditions. This paper explores the importance of considering modes of information processing in the design of efficient distributed hardware and software systems. 
411067	4110677	Nonlinear Phenomena In Spaces Of Algorithms	Nonlinear phenomena play a crucial role in nature and society and can also be observed in the world of algorithms and computations. To better understand this world, formal computability spaces are introduced. Traditional approaches to algorithms, such as Turing machines, result in linear subspaces of the computability space. However, by using more powerful algorithms, such as inductive Turing machines, nonlinearity arises and allows for much higher computing power. This has implications for understanding chaos, emergent computations, and infinity from an algorithmic perspective. 
411068	41106817	Automatic protocol reverse-engineering: Message format extraction and field semantics inference	Understanding the command-and-control (C&C) protocol used by a botnet is crucial for identifying potential malicious activity. However, these protocols are often undocumented, making it difficult to defend against botnets. Automatic protocol reverse-engineering techniques allow for a better understanding of these protocols and are essential for security applications. This approach involves analyzing the program that implements the protocol and extracting accurate and complete information, even for encrypted protocols. The proposed approach, called Dispatcher, was used to analyze the previously undocumented C&C protocol of MegaD, a major spam botnet responsible for a significant portion of Internet spam. 
411068	4110680	Polyglot: automatic extraction of protocol message format using dynamic binary analysis	Protocol reverse engineering is a method used to extract the application-level protocol from an implementation without access to the protocol specification. This is crucial for network security applications. While previous approaches have used clustering on network traces, their limitations include a lack of semantic information. In this paper, a new approach called shadowing is proposed, which uses dynamic analysis on program binaries. This approach is based on the idea that the way an implementation processes received data can reveal information about the protocol message format. The effectiveness of this approach is evaluated on real-world implementations of five different protocols, and the results are compared to a state-of-the-art protocol analyzer. Small differences found between the two demonstrate the added benefit of this approach for tasks such as fingerprint generation, fuzzing, and error detection.
411069	41106929	Techniques for Edge Stratification of Complex Graph Drawings	The authors of this paper propose a method for exploring graph layouts in order to reduce visual complexity and improve clarity. This approach involves dividing the layout into layers with desired properties, using heuristics. These layers can then be combined and explored by the user to gradually reveal more details. A user study was conducted to test the effectiveness of this approach, and an experimental analysis was performed on popular graph drawing algorithms to evaluate the number of layers and their correlation to the number of crossings in a graph layout. The authors found that their approach was useful for exploring graph layouts, and suggest that the number of layers may be a reliable measure of visual complexity. However, the method may not be efficient for larger and more complex layouts, and further research is needed to extend and improve this approach. 
411069	41106952	Whatsonweb+ : An Enhanced Visual Search Clustering Engine	WhatsOnWeb+ is a search clustering engine that offers enhanced graph visualization techniques to help users browse and analyze search results. It incorporates various visual interfaces, animation, and interaction features, as well as different clustering technologies. The paper presents the results of an extensive experimental analysis to evaluate the effectiveness of these features. This system is an advanced version of a previous web clustering engine, showcasing significant improvements in its capabilities.
411070	41107031	Integer-grid maps for reliable quad meshing	Quadrilateral remeshing is a process used to improve the quality of a mesh, particularly in computer graphics, by transforming irregularly shaped polygons into regular quadrilaterals. This is achieved through global parametrization, which allows for explicit control over irregular vertices and smooth distribution of distortion. However, current techniques are not reliable when applied to real-world input data, often resulting in non-injectivities and therefore unusable quadrilateral meshes. In this paper, a new convex Mixed-Integer Quadratic Programming (MIQP) formulation is proposed to ensure the resulting map is within the class of Integer-Grid Maps, guaranteeing a quad mesh. To overcome the computational challenges, two additional optimizations are proposed: a complexity reduction algorithm and singularity separating conditions. These improvements result in a more reliable and accurate remeshing process, allowing for the global search of high-quality coarse quad layouts. 
411070	41107044	Progressive compression for lossless transmission of triangle meshes	The efficient transmission of 3D meshes is crucial for many applications, including collaborative design and engineering. To address this, a new progressive encoding approach is proposed in this paper for lossless transmission of triangle meshes. This approach utilizes a valence-driven decimation method, patch tiling, and strategic retriangulation to maintain regularity in valence. Additionally, the technique includes decorrelation of normal and tangential components of the surface, resulting in improved mesh quality and a good rate-distortion ratio. Compared to previous methods, this approach achieves a better compression rate and can reduce the size of a VRML 3D model to only 1.7% of its original size while maintaining a very progressive reconstruction.
411071	41107155	Animation of deformable models using implicit surfaces	This paper discusses a method for creating and animating complex deformable models using implicit surfaces. These surfaces act as an additional layer around moving and deforming structures, providing a smooth definition of the object's surface and efficient collision detection. The implicit layer deforms to create accurate contact surfaces between colliding objects, and a physically-based model is used to calculate collision responses. This approach also allows for easier control of an object's volume through the use of local controllers. Two applications are presented to demonstrate the effectiveness of this technique: animating characters with implicit flesh and modeling soft, inelastic substances with constant volume during animation. 
411071	41107111	Active Implicit Surface For Animation	The paper discusses a new model called active implicit surfaces for animating deformable surfaces. Instead of directly animating the surface, the model uses a potential field stored in a grid to create an iso-potential surface that can follow objects using a snake-like strategy. The model also allows for the addition of surface tension and other characteristics such as constant surface area or volume. The implicit formulation enables easy topology changes during simulation and an optimized implementation restricts computations to a small neighborhood around the surface. This model has various applications, including simulating deformable materials and generating metamorphosis between shapes with different topologies.
411072	41107257	Multivalued mappings, fixed-point theorems and disjunctive databases	This paper discusses the meaning of disjunctive programs and databases and how they lead to multivalued mappings and fixed points. Various fixed-point theorems for these mappings are explored, some of which are already familiar and some of which are new. The concept of a normal derivative of a disjunctive program is introduced, which is a type of logic program that can be determined by the disjunctive program. This makes it easier to find fixed points using established methods. The paper also explains how fixed points of multivalued mappings are related to fixed points of single-step operators from normal derivatives, which can simplify the construction of models for disjunctive databases. The paper also presents a collection of known results on fixed points of single-valued mappings for reference. The paper concludes by discussing potential challenges and future research topics related to this work.
411072	41107260	A Comparison of Disjunctive Well-founded Semantics	The Answer Set Programming, or stable model semantics, is widely used for disjunctive logic programs. However, there is still a need for a satisfactory extension of this approach for disjunctive programs. The current proposals for such an extension are diverse and difficult to compare due to their different mechanisms. To address this, a systematic comparison based on level mappings, a framework used for comparing normal logic program semantics, is introduced for disjunctive logic programs. This framework is extended to include the strong well-founded semantics, generalized disjunctive well-founded semantics, and disjunctive well-founded semantics. This allows for a better understanding of how each of these approaches handle negation.
411073	41107362	Continuous and Parallel LiDAR Point-Cloud Clustering	In the world of the Internet of Things, the need for analyzing large amounts of data generated by high-rate sensors at the edge of infrastructures is crucial. One example is LiDAR technology, which can detect objects with high precision in large areas. This data, known as point clouds, can support automated functions in distributed systems. The problem of clustering point clouds is key in extracting useful information from this data. A proposed solution is Lisco, which is a continuous clustering algorithm that can be parallelized for efficient processing. Its parallel version, P-Lisco, can be used on different computing architectures and has shown to be more efficient and scalable compared to traditional methods in experimental evaluations. 
411073	4110734	Performance Modeling of Stream Joins.	Streaming analysis, used in various environments such as cloud computing and network's edge, requires accurate modeling of streaming operators' performance for predicting application behavior. This is especially important for computationally-intensive operators like stream joins, where throughput and latency are sensitive to rate-changing data streams and deterministic processing is necessary. A modeling framework is presented for estimating the throughput and latency of stream join processing, starting from a non-deterministic centralized join and expanding to a deterministic parallel join. The model considers the number of input streams, parallelism, and determinism requirements in influencing throughput and latency. An experimental validation of the model is provided, showing its potential for understanding different system deployments and the impact of determinism and parallelization on stream join behavior.
411074	41107416	Accelerating Multiagent Reinforcement Learning by Equilibrium Transfer.	Equilibrium-based multiagent reinforcement learning (MARL) is an important approach that uses equilibrium solution concepts from game theory to guide agents' decision-making. However, existing algorithms for this approach have difficulty scaling due to the high computational cost of finding equilibria. This paper introduces the concept of "equilibrium transfer," where previously computed equilibria are reused when agents have a low incentive to deviate from them. This leads to a new framework called equilibrium transfer-based MARL, which is shown to significantly accelerate learning (up to 96.7% reduction in time) and achieve higher rewards compared to other algorithms. This framework also scales well as the state and action space grows and the number of agents increases. 
411074	41107476	Learning in Multi-agent Systems with Sparse Interactions by Knowledge Transfer and Game Abstraction	In multi-agent systems, interactions between agents are often sparse, and utilizing this sparseness can enhance learning performance in multi-agent reinforcement learning (MARL). Additionally, agents may already possess single-agent knowledge before the multi-agent learning process. This study explores how such knowledge can be used to improve policies in multi-agent systems with sparse interactions. The approach uses game theory-based MARL, which has better coordination abilities. The study introduces three knowledge transfer mechanisms - value function transfer, selective value function transfer, and model transfer-based game abstraction - to transfer agents' local value functions, improve learning in states with slightly different dynamics, and reduce equilibrium computation. Experimental results demonstrate significant improvements in MARL algorithms using these mechanisms compared to the state-of-the-art algorithm CQ-learning. 
411075	411075188	The effectiveness of task-level parallelism for high-level vision	 The paper discusses the issue of slow execution in large production systems, also known as rule-based systems, and how this limits their usefulness in both practical applications and research. 2. Previous attempts to speed up these systems have focused on match parallelism, but it has been found that this only provides limited speed-ups. 3. The paper suggests that task-level parallelism, which involves breaking down the system into high-level tasks, could provide much larger speed-ups when combined with match parallelism. 4. The authors use a mature research system called SPAM to investigate task-level parallelism and report promising speed-ups of over 12 fold using 14 processors. 5. They also discuss their methodology for selecting and applying task-level parallelism and the potential benefits of using shared virtual memory in this implementation. 6. Task-level parallelism has not been well-studied in the literature, but the authors believe it could be a valuable tool for improving the performance of large-scale production systems.
411075	41107596	The effectiveness of task-level parallelism for production systems	The slow execution of large production systems has hindered their practical and research application. Attempts to speed up these systems have primarily focused on match parallelism, but have not been sufficient. This paper explores task-level parallelism, achieved through high-level decomposition of the production system. By combining task-level parallelism with match parallelism, significant speed-ups can be achieved. The authors use the production system SPAM as a case study and report promising results, including a 12-fold speed-up with 14 processors and potential for 50-100-fold speed-ups. The paper also discusses the use of virtual shared memory and highlights the potential of task-level parallelism as a valuable tool for improving the performance of large-scale production systems.
411076	41107629	Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and Blind Unmixing of Hyperspectral Images.	The process of hyperspectral unmixing involves estimating the number of endmembers present in a scene, which is crucial for accurately extracting their spectral signatures and determining the abundance fractions of pixels. This is typically done in two separate steps, but a new approach presented in this paper combines the two tasks into one using a multiple constrained optimization framework. This is achieved through a low-rank and sparse non-negative matrix factorization method, where a l(1)/l(2) norm penalty term is used to promote low-rankness. The proposed approach is validated through experiments with simulated and real data, showing its effectiveness in accurately estimating the number of endmembers.
411076	41107640	Online Bayesian low-rank subspace learning from partial observations	This paper introduces a new method for efficiently learning low-dimensional subspaces from incomplete high-dimensional data in an online setting. The approach uses a hierarchical Bayesian model that incorporates sparsity-promoting priors to enforce low rank in the subspace matrix. This automated algorithm is shown to outperform existing methods in terms of accuracy and estimation of the true subspace rank, as demonstrated by numerical simulations. 
411077	41107738	Sparsity Pattern Recovery Using Fri Methods	The search for a sparse representation of signals has been a popular topic in recent years. Various methods have been developed to solve this problem, including relaxing a non-convex optimization problem and applying the finite rate of innovation (FRI) theory, which uses algebraic techniques based on Prony's algorithm. Recent advancements in this theory have shown the potential to recover sparse representations beyond the previously established uniqueness limits. This paper focuses on applying these methods to signals that are sparse in the union of Fourier and canonical bases, specifically in the case of the union of DCT and Haar basis. An extension is proposed that takes advantage of the even symmetry of cosine functions and operates on the observed vector in a dual domain. The performance of this new approach is compared to other state-of-the-art algorithms through simulations, showing its superiority in various scenarios.
411077	41107739	Robust image recapture detection using a K-SVD learning approach to train dictionaries of edge profiles	In this study, researchers aim to detect recaptured images from LCD monitors by analyzing the edge profiles in the images. They train two dictionaries using the K-SVD approach, one for single captured edges and one for recaptured edges. By observing which dictionary gives the smallest error in representing the edges of a query image, they can determine if the image has been recaptured. Experiments show that the method is highly accurate and effective for a variety of natural images. 
411078	4110781	Automatic synthesis of filters to discard buffer overflow attacks: a step towards realizing self-healing systems	Buffer overflows are a common target for network-based attacks and are often used by worms to spread. While some techniques, such as StackGuard, have been developed to protect servers from these attacks, they can cause the server to crash and make its service unavailable. However, a new approach has been developed that learns the characteristics of attack inputs and filters them out in the future without changing the server code. This allows for faster recovery from attacks without the need for server restarts. Testing has shown that this approach is effective in generating accurate filters for most buffer overflow attacks.
411078	41107831	Automatic generation of buffer overflow attack signatures: an approach based on program behavior models	.Buffer overflows are the most popular method for network-based attacks and are often used by worms and other automated attacks. While various techniques have been created to prevent server compromises, they can still result in server crashes and repeated restarts of the attacked application. To address this issue, a new approach has been developed that can learn and filter out specific attack patterns, increasing server availability. This method is automated, does not require source code, and has low runtime overheads. In experiments, it successfully defended against most attacks without any false positives. 
411079	41107914	Capti-speak: a speech-enabled web screen reader	 Capti-Speak is a speech-enabled screen reader designed to alleviate the frustrations of web browsing for people with vision impairments. It uses a custom dialog model to interpret speech utterances and translates them into browsing actions, providing audio feedback. In a user study with 20 blind subjects, Capti-Speak was found to be significantly more usable and efficient compared to regular screen readers, particularly for ad-hoc browsing, searching, and navigating to desired content. This shows the potential of augmenting screen readers with speech input interfaces to improve the browsing experience for those with limited keyboard shortcut vocabulary or unfamiliarity with webpage structures.
411079	41107982	Speed-Dial: A Surrogate Mouse for Non-Visual Web Browsing.	The article discusses the challenges faced by people with vision impairments in browsing the web due to the reliance on a mouse for navigation. This makes it difficult for them to access the visual elements of a web page, causing them to rely on a screen reader and keyboard shortcuts, which can be slow and overwhelming. To address this issue, the authors propose a system called Speed-Dial, which uses a physical Dial as a substitute for a mouse. This system allows for faster and more intuitive navigation through the semantic model of a web page, improving the browsing experience for blind individuals. A user study showed positive results, with participants able to quickly select content of interest, similar to using a mouse.
411080	41108052	"Press Space to Fire": Automatic Video Game Tutorial Generation.	The concept of tutorial generation for games is presented as an AI problem. This involves creating tutorials that can effectively teach players how to play games. Different approaches to tackling this problem include generating written explanations of game rules, designing helpful game levels, and producing demonstrations of gameplay using agents that mimic human behavior. The General Video Game AI framework is proposed as a valuable tool for addressing this issue.
411080	41108040	A Panorama of Artificial and Computational Intelligence in Games	This paper provides a comprehensive overview of the field of artificial and computational intelligence (AI/CI) in games. It identifies ten main research areas, including NPC behavior learning, search and planning, and player modeling. The paper examines these areas from three perspectives: the dominant AI methods used, the relation to the end user, and the placement within the human-computer interaction perspective. It also explores potential interactions between these areas and provides references to published studies. The paper aims to improve understanding of the game AI/CI research field and its interdependence, and suggests potential areas for future research.
411081	41108113	Feature analysis for modeling game content quality.	This paper explores the potential for automatic game content generation to enhance player entertainment by tailoring the player experience in real-time. The authors analyze the relationship between level design parameters in platform games and player experience, using a neuroevolutionary preference learning method to predict players' preferences and emotions based on game content. They conduct experiments on a modified version of Super Mario Bros, using statistical parameters and frequent sequences of level elements as features. The results suggest that a smaller feature window decreases prediction accuracy, and that models built on a combination of features outperform those using partial information. This research highlights the importance of understanding the relationship between game content and player experience for effective game personalization.
411081	41108137	Modeling Player Experience for Content Creation	This paper discusses the use of computational intelligence techniques to create quantitative models of player experience in a platform game. These models accurately predict the emotional states of players based on both their actions in the game and the parameters of the level being played. The experiments were conducted using a modified version of Super Mario Bros, where the levels could be adjusted and gameplay metrics were collected. The models were trained using player preference data gathered through questionnaires and neuroevolutionary learning. These models can be used to optimize design parameters for specific player experiences, enabling designers to automatically generate levels that elicit the desired emotions from players. 
411082	41108256	RORI-based countermeasure selection using the OrBAC formalism	As threats against information systems become more advanced, it is becoming increasingly difficult for security administrators to detect and respond to attacks. Implementing strong security policies is an effective way to protect systems, but it requires expertise and knowledge. While strong policies can serve as powerful countermeasures, inappropriate policies can have disastrous consequences for an organization. To combat complex attacks, current research suggests using multiple countermeasures, but the methodology is often unclear or difficult to implement. This paper presents a structured approach for evaluating and selecting optimal countermeasures based on the return on response investment (RORI) index. A real case study of a mobile money transfer service is used to demonstrate the applicability of the model, and the service, security policies, and countermeasures are expressed using the OrBAC formalism.
411082	41108233	Enabling automated threat response through the use of a dynamic security policy	The use of different techniques, such as authentication, encryption, and access control, is currently being employed to address information systems security issues. However, security monitoring is often disconnected from security policies, leading to a manual and reactive response to threats. To bridge this gap, an approach is proposed that connects monitoring techniques with security policy management. This allows for a dynamic and automatic deployment of a generic security policy, taking into account the threat level identified by intrusion detection systems. This approach aims to improve the protection of resources and services by addressing new requirements and developing intrusion prevention systems. 
411083	4110832	Maximum rate single-phase clocking of a closed pipeline including wave pipelining, stoppability, and startability	Aggressive design techniques, such as using level-sensitive latches and wave pipelining, are being utilized to improve the performance of digital systems. However, the optimal clocking problem for these designs is challenging to solve due to its nonconvex solution space. Current algorithms use linear programs to solve a simpler case, but a new algorithm called Gpipe has been developed to efficiently determine the maximum single-phase clocking rate for a closed pipeline with a specified degree of wave pipelining. Additionally, a method for introducing or increasing wave pipelining while maintaining the clock rate has been discovered, as well as techniques for stopping and restarting the pipeline under stall conditions without losing data or affecting testing capabilities. 
411083	4110831	An integrated approach to developing manufacturing control software	.This article discusses an integrated approach to developing control software for manufacturing systems to improve efficiency and dependability. The approach involves using a formal model of the manufacturing system to plan the sequence of operations for different types of jobs. To increase efficiency, the reservation table technique is used to create efficient schedules for processing batches of jobs. To ensure dependability, a plan-oriented fault detection and correction strategy is suggested. By combining these methods, the control software for manufacturing systems can be optimized for both efficiency and dependability.
411084	41108430	Specifying User Interfaces for Runtime Modal Independent Migration	The evolution of computing systems has shifted from a procedural approach to a problem-oriented one. In the future, computer usage will revolve around specific services rather than platforms or applications. These services should be accessible through a uniform interface, independent of technology. This paper proposes a framework for runtime migration of user interfaces by using general interface descriptions in XML and converting them with XSLT. This will allow for easy extension of services to different devices and modalities. A proof of concept for this approach is demonstrated through the conversion of a joystick in a 3D virtual environment to a 2D dialog-based user interface.
411084	41108410	Real-Time Hand-Painted Graphics for Mobile Games.	The aim of this paper is to develop a digital painting technique that can accurately recreate scenes from children's books or graphic novels for use in mobile games. The technique, called mesh-based strokes, combines the strengths of existing painting techniques while considering the limitations of mobile devices. Instead of using pixels, strokes are created as flat meshes following the brush stroke's contour. The authors worked closely with artists to create new digital brushes and tools to capture the style of children's books. The Bo children's book series was used as a case study. The system is user-friendly, performs well on mobile devices, and offers a unique perspective on stylized animation in mobile games. 
411085	41108590	Scalable multimedia content analysis on parallel platforms using python	In today's world, there is a high demand for web-scalable solutions for analyzing multimedia content due to the dominance of consumer-produced media. One approach to achieving scalability is through mapping application computation onto parallel platforms. However, this poses challenges such as increased code complexity, limited portability and required low-level knowledge of hardware. To address this, a Python-based framework called PyCASP has been developed, which automatically maps computation onto various parallel platforms, making it easier to develop and scale applications. PyCASP follows a systematic, pattern-oriented approach and allows for easy prototyping and efficient performance. Illustrations of this framework are provided through three different multimedia content analysis applications, showcasing its automatic portability and scalability while allowing for high-level language prototyping and efficient low-level code performance.
411085	41108546	Boda: A Holistic Approach for Implementing Neural Network Computations.	Neural networks (NNs) are becoming increasingly popular in both machine learning research and practical applications. GPUs are the most commonly used platform for NN research and are also gaining traction for applications like autonomous vehicles. However, the development of efficient GPU code for NN operations is hindered by complexity, low productivity, and lack of portability. To address this, a framework has been developed to enable high-efficiency GPU programming across different hardware and programming models. This framework includes support for metaprogramming and autotuning of operations over ND-Arrays. The effectiveness of this approach is demonstrated by implementing a selection of NN operations for image processing on different hardware platforms, including NVIDIA, AMD, and Qualcomm GPUs. Results show portability and competitive performance on NVIDIA GPUs, productive development of target-specific optimizations on Qualcomm GPUs, and promising performance on AMD GPUs with minimal effort.
411086	41108631	Toward a deeper understanding of the relationship between interaction constraints and visual isomorphs	Interaction and manual manipulation are important for problem solving according to cognitive science literature. Different types of interactions or constraints on interactions can make a problem seem easier or harder. However, the visual analytics community has not fully utilized this knowledge for analytical problem solving. This paper proposes that constraints on interactions and visual representations can affect the effectiveness of problem solving strategies. A user study was conducted using a mathematic game called number scrabble to test this hypothesis. The game has an optimal visual isomorph and the study aimed to see if participants could find it and analyze their strategies. Results showed that interaction constraints do impact problem solving and certain constraints can increase the chance of finding the optimal isomorph. 
411086	4110864	Finding Waldo: Learning About Users From Their Interactions	Visual analytics involves collaboration between humans and computers, but current systems have limited ways of understanding their users and their analysis processes. Research has shown that a user's interactions with a system can reveal their reasoning process, but there has been limited progress in developing real-time techniques to learn about the user. In this paper, the authors demonstrate that machine learning techniques can accurately predict a user's task performance and infer their personality traits by analyzing interaction data. They conducted an experiment in which participants performed a visual search task, and achieved between 62% and 83% accuracy in predicting task completion time. They also show that with limited observation time, the computer can accurately infer aspects of the user's personality. This highlights the potential for mixed-initiative visual analytics systems. 
411087	41108714	An Acoustic Identification Scheme for Location Systems	Pervasive computing applications often require location awareness to effectively integrate technology into daily life. This is achieved through a location system using sensors to determine a user's position and provide relevant information. In this paper, an acoustic-based location system is proposed, using a set of microphones connected to a central server. Mobile users can produce acoustic signals through standard speakers on their devices to locate themselves through the system. The design focuses on using multi-frequency symbols for robust and unique identification of users. Experiments were conducted to test the system's ability to recognize and decode signals from different users in the same area at various distances.
411087	4110875	Virtual cities in urban planning: the uppsala case study	The task of making cities more sustainable is challenging, as the traditional urban models from the 20th century need to be reevaluated. However, change is often met with resistance from economic and political models. To promote change, conceptualization and visualization are powerful tools. When considering new urban plans, it is important for city officials, planners, and the public to be able to visualize how they will interact with existing infrastructure. One example of an innovative urban plan is the solar-powered Personal Rapid Transit (PRT) system proposed in Uppsala, Sweden. The process of discussing this plan involved the use of a virtual reality simulation to help stakeholders understand its potential benefits. This paper discusses the role of virtual reality in the development of the PRT system and its impact on the decision-making process. 
411088	4110884	Investigating gesture and pressure interaction with a 3d display	This article explores the use of a mobile device as a multifunctional input and output for a stereoscopic 3D television display. The combination of gestural and haptic input (touch and pressure) is utilized to navigate multimedia and TV content, with visual feedback enhancing the user experience. A user evaluation was conducted to compare these prototypes with traditional devices for multimedia interaction, revealing the benefits and providing design guidelines. This approach allows for efficient and engaging interaction with complex information spaces, making use of both the mobile device and the 3D TV display.
411088	41108837	Investigating one-handed multi-digit pressure input for mobile devices	This paper discusses the use of one-handed squeezing as a method of interacting with a mobile phone. It examines the accuracy of using each individual digit and combinations of digits to apply force on the device. The study also considers how to adjust the interaction space to match the number of digits being used. The results show that not all digits are equally accurate and that some perform better when used in combination with others. Increasing the size of the interaction space improves user performance. This research can inform the design of one-handed, multi-channel input for mobile devices. 
411089	411089195	Text segmentation via topic modeling: an analytical study	This paper discusses a new approach to text segmentation using the latent Dirichlet allocation (LDA) topic model. This method not only identifies segment boundaries, but also provides information on the topic distribution within each segment. This can be useful for tasks such as segment retrieval and discourse analysis. The proposed approach performs better than a standard baseline method and outperforms most unsupervised methods on a benchmark dataset. This highlights the potential of using topic modeling for text segmentation.
411089	411089112	TV news story segmentation based on semantic coherence and content similarity	This paper presents two new methods for segmenting TV news into stories: one using video stream and the other using close-caption text stream. The video stream method detects anchor person shots while the text stream method utilizes a Latent Dirichlet Allocation (LDA) based approach. The LDA method not only segments the text into stories, but also provides the associated topic distribution for each segment. The methods were tested on the TRECVid 2003 benchmark database and a combination of both approaches showed a significant improvement in performance compared to using either method alone. 
411090	41109020	Mapping Simple Polygons: The Power of Telling Convex from Reflex	The exploration of a simple polygon by a robot is studied using a visibility graph. This graph has a vertex for each vertex of the polygon, and an edge between two vertices if they can see each other. The robot is capable of ordering the vertices it sees and determining whether the angle between them is convex or reflex. An upper bound on the number of vertices is known. The general result is that a robot can always determine the base graph of a locally oriented, arc-labeled graph. Combining this with other techniques, it can reconstruct the visibility graph of the polygon. Multiple identical robots can also solve the weak rendezvous problem by positioning themselves to mutually see each other. 
411090	41109048	Telling convex from reflex allows to map a polygon	The article discusses the exploration of a simple polygon by a robot using the visibility graph. The visibility graph consists of a vertex for each vertex of the polygon and an edge between two vertices if they can see each other. The robot can order the vertices it sees in a counter-clockwise manner and determine if the angle between two vertices is convex or reflex. The article assumes a known upper bound on the number of vertices and shows that the robot can reconstruct the visibility graph. It also demonstrates that multiple identical and deterministic robots can position themselves to form a clique in the visibility graph.
411091	4110916	Algorithms and data structures for flash memories	Flash memory is a type of memory that can store data even when the power is turned off. It is commonly used in handheld devices such as phones and cameras because it is compact and can hold a lot of information. However, it has some limitations - it can only be erased in large blocks and can only be erased a limited number of times. This means that special techniques and structures are needed to effectively use flash memory. These techniques help to update data efficiently, reduce the number of erasures, and evenly distribute wear on the memory blocks. This survey discusses these techniques, many of which have only been described in patents until now.
411091	41109153	Competitive analysis of flash memory algorithms	Flash memories are commonly used in various computer systems, from small embedded systems to larger workstations and servers. These memories have a limited number of write cycles, typically between 10,000 and 1,000,000, and data must be erased before new data can be stored. To prolong the life of the device, flash-based systems use wear-leveling algorithms that move data around to reduce the number of erasures and evenly distribute wear on the memory blocks. These algorithms have been used since 1993 but have not been mathematically analyzed until now. This article analyzes the two main wear-leveling problems and shows that a simple randomized algorithm is optimal in both the competitive and absolute sense. It also shows that deterministic algorithms are not as effective and that an offline algorithm can improve upon naive approaches for a more difficult problem, while online algorithms cannot.
411092	41109248	Enhancing interactive web applications in hybrid networks	There are multiple options for mobile internet users, including high bandwidth cellular data services and WiFi. However, WiFi can be intermittent, making it difficult for vehicles to support interactive applications like web search. The Thedu system was created to address this issue by using aggressive prefetching and m2m transfers to transform web search into a one-shot request/response process. A prototype was deployed on the DieselNet testbed in Amherst, MA and showed that Thedu can deliver four times as many relevant web pages with a mean delay of 2.3 minutes in areas with high AP density. However, in rural areas with sparsely deployed APs, m2m routing may improve the number of relevant responses delivered, but with a significantly higher mean delay of 6.7 minutes. 
411092	41109254	Replication routing in DTNs: a resource allocation approach	Routing protocols for disruption-tolerant networks (DTNs) have various mechanisms to improve the chances of finding a path with limited information, including meeting probability discovery, packet replication, and network coding. These approaches do not have a significant impact on traditional routing metrics such as delivery delay. This paper introduces RAPID, an intentional DTN routing protocol that can optimize specific metrics like worst-case delivery delay or the percentage of packets delivered within a deadline. By treating DTN routing as a resource allocation problem, RAPID allocates per-packet utilities to determine how packets should be replicated. The protocol is evaluated through a prototype deployed on a vehicular DTN testbed and simulations based on real traces. RAPID outperforms existing protocols in various metrics, and for low loads, it is comparable to optimal performance. This is the first paper to report on a routing protocol deployed on a real outdoor DTN.
411093	41109364	Global Optimization for Value Function Approximation	Value function approximation methods have been widely used in various applications, but they often lack error bounds. In this paper, a new approximate bilinear programming formulation is proposed for value function approximation, using global optimization. This formulation guarantees strong a priori guarantees on both robust and expected policy loss by minimizing specific norms of the Bellman residual. While solving a bilinear program optimally is NP-hard, the Bellman-residual minimization itself is also NP-hard, making it unavoidable. The paper presents optimal and approximate algorithms for solving bilinear programs, which offer a convergent generalization of approximate policy iteration. The behavior of these algorithms under incomplete samples is also briefly analyzed. The effectiveness of the proposed approach is demonstrated through consistent minimization of the Bellman residual on simple benchmark problems.
411093	411093107	Robust Value Function Approximation Using Bilinear Programming.	Value function approximation is a commonly used method in various applications, but it often lacks accurate error bounds. To address this issue, we introduce a new approach called approximate bilinear programming, which guarantees strong error bounds by minimizing the Bellman residual. Solving the bilinear program optimally is difficult, as the Bellman-residual minimization is NP hard. Therefore, we use a commonly used approximate algorithm for bilinear programs and analyze its convergence. Our analysis shows that this algorithm is a reliable generalization of approximate policy iteration. We also demonstrate the effectiveness of our approach by consistently minimizing the Bellman residual on a simple benchmark problem.
411094	41109491	Efficient Algorithm for Modified Local Polynomial Time Frequency Transform	This paper introduces new algorithms for analyzing non-stationary signals with multiple components. The algorithms use a modified local polynomial time frequency transform and divide the signal into segments to estimate parameters. This approach reduces the computational complexity by minimizing the overlap between segments. The use of adaptive window lengths also improves the time-frequency resolution for each component. These new algorithms are more efficient than previously reported methods and can accurately analyze non-stationary signals with multiple components. 
411094	411094114	On application of time-frequency analysis to communication signal detection and estimation	This paper discusses the importance of detecting and estimating frequency-modulated signals and extracting modulation parameters for intelligent telecommunication applications. It proposes a time-frequency analysis approach for robust detection, classification, and parameter estimation of linear frequency modulated signals and FSK signals in a blind environment. The local polynomial Fourier transform is used for this analysis and it has shown better energy concentration compared to the conventional short-time Fourier transform. The results of this approach have been promising, achieving accurate classification and parameter estimation with a signal-to-noise ratio of over 5 dB and short observation durations. 
411095	411095117	A New Courseware Diagram for Quantitative Measurement of Distance Learning Courses	Web-based distance learning offers flexibility for students, but lacks interaction between students and the instructor. To address this issue, a systematic assessment mechanism is proposed to enhance interaction and improve student learning performance. This mechanism utilizes courseware diagrams, which combine tools from conceptual mapping and influence diagrams. The courseware diagram has two main components: a course flow chart that can be systematically built to provide remedial courses for students based on their performance, and the ability to adjust course content based on students' learning performance. This user-friendly mechanism allows for prompt feedback from students and can be easily used by both instructors and students.
411095	41109578	Courseware development using influence diagram with SCORM compatibility.	Web-based distance learning has become increasingly popular and convenient, providing a flexible learning environment for students. However, it presents challenges for teachers in designing courses and assessing students' learning. To address this issue, a systematic assessment mechanism has been proposed to improve interaction between students and teachers. This mechanism combines the theory of concept and influence diagram to create a new courseware diagram with useful tools. It also features a user-friendly interface and transforms course content to the SCORM standard, making it easy for both instructors and students to use. This new mechanism aims to enhance the effectiveness of web-based distance learning.
411096	41109681	Audio-augmented paper for the therapy of low-functioning autism children	This paper introduces a prototype of audio-augmented paper for use in therapy for low-functioning autism children. The prototype allows therapists to record audio on regular paper using tangible tools, making it easy to share between therapist and child. The prototype is specifically designed to help therapists engage children in storytelling activities. An initial pilot study was conducted to test the effectiveness of the prototype.
411096	41109625	Three around a table: the facilitator role in a co-located interface for social competence training of children with autism spectrum disorder	This paper explains the development of a co-located interface on a tabletop device for social competence training in children with Autism Spectrum Disorder. The interface, designed for two children and a facilitator, takes advantage of the DiamondTouch table's ability to recognize multiple touches to enhance interaction flow. It discusses observations from a small field study where two therapists used the system with 4 pairs of children, and highlights the potential of collaborative games to teach social competence skills. The interface benefits both the children, by providing engaging games, and the facilitator, by providing tools to support and shape the training session. 
411097	41109753	Cooperative navigation in robotic swarms.	This study focuses on cooperative navigation for robotic swarms in an event-servicing scenario where robots need to service events at specific locations. The goal is to develop a system where robots can inform each other about events and guide each other to event locations using delay-tolerant wireless communications. This approach takes advantage of the swarm's redundancy, distribution, and mobility. The algorithm is tested in two scenarios, both in simulation and on real robots: a single searching robot finding a target while others perform their own tasks, and collective navigation where all robots navigate between two targets. The results show that this algorithm allows for efficient navigation and the emergence of a robust dynamic structure within the swarm.
411097	41109712	Haptic-Enabled Handheld Mobile Robots: Design and Analysis.	The Cellulo robots are small robots that can be used in learning activities as tangible interfaces and haptic devices. They are designed to represent virtual objects and can display autonomous motion. The article discusses the design and analysis of the haptic interaction module of these robots, including the low-cost and versatile hardware and controller design. The experimental procedure used to evaluate the robot's haptic abilities is also described. Results show that the robot is effective in most tasks and provide guidelines for designing haptic elements in future activities. Limitations and future work are also mentioned.
411098	41109855	Insider Attack Identification and Prevention Using a Declarative Approach	A process is a series of steps that use data to achieve a specific goal. Insiders who have access to data and annotations can carry out attacks on the privacy and security of the process. These attacks can be difficult to identify as they are hidden among non-malicious steps. Process and attack models are defined as directed graphs based on data flow. An attack is successful if it satisfies certain conditions and has a similarity match with the process model. A declarative approach is proposed for vulnerability analysis, using logic rules to define valid attacks. Possible ways for agents to carry out these attacks are generated, and improvement opportunities are identified and exploited to eliminate vulnerabilities. The improved process is then evaluated until all possible ways of carrying out the attack are thwarted. 
411098	4110989	Insider Threat Identification by Process Analysis	The insider threat poses a significant risk to computer security, and traditional methods of detection involve using decoys or intrusion detection mechanisms. These tactics aim to identify individuals who misuse their privileges, commonly known as "insiders". However, this type of attack requires the insider to have access to resources or data. The use of process modeling and subsequent analyses can provide insight into how the insider may compromise a system. By analyzing the tasks carried out by agents within a process, potential vulnerabilities can be identified and countermeasures can be implemented to strengthen the system's resistance to insider attacks.
411099	41109931	An approach to adapt service requests to actual service interfaces	The development of service oriented architectures has led to the creation of frameworks that allow for self-adaptive service compositions with dynamic binding. This means that a developer can specify an abstract service at design time and a concrete implementation will be selected at run time. However, in an open world setting, where services are provided by different organizations, there may be mismatches between the interfaces or protocols of services. To address this issue, researchers have identified possible mismatches and developed basic mapping functions that can be used to solve them. These mapping functions can be combined into scripts which can be executed by a mediator to adapt the operation request and ensure communication between services with different interfaces or protocols. The process of creating these scripts can also be partially automated.
411099	41109938	SCENE: a service composition execution environment supporting dynamic changes disciplined through rules	Service compositions are built using existing component services that are not under the control of the developer. However, it is becoming increasingly important for these compositions to be able to adapt to changes in the behavior of the component services and the execution environment. This requires a flexible runtime platform that can select alternative services, negotiate service level agreements, and replan the composition. Additionally, the composition language should support the designer in defining constraints and conditions for these runtime actions. The paper introduces the SCENE platform, which addresses these issues by extending the BPEL language with rules for binding and re-binding operations for self-reconfiguration.
411100	41110015	Load-balancing and caching for collection selection architectures	Modern Web search engines are facing the challenge of dealing with the rapid growth of the Internet. To handle this, they have adopted distributed organizations, where documents are distributed among servers and queries are answered in a parallel and distributed manner. One way to reduce the computing load in this setup is through the use of collection selection, which balances the quality of results with the cost of solving queries. This paper explores the relationship between collection selection, load balancing, and caching in a distributed search engine. It proposes a load-driven collection selection strategy and a novel caching policy that improves the effectiveness of results. The combination of these strategies results in a system that can retrieve two thirds of the top-ranked results with only one fifth of the computing workload compared to a baseline centralized index.
411100	4111003	Caching query-biased snippets for efficient retrieval	Web search engines provide a list of top-k relevant documents to a user's query, each with a title, snippet, and URL. Snippets, short sentences highlighting relevant portions of a document, aid users in selecting the most interesting results. However, the process of generating snippets is time-consuming and expensive, as it requires accessing multiple documents for each query. To improve performance, caching is proposed as a solution. By utilizing query logs, a concept called "supersnippets" is introduced, which contains sentences most likely to answer future queries. Experiments show that a supersnippet cache can answer up to 62% of requests, proving to be more effective than other caching methods.
411101	41110135	Using a Natural Language Understanding System to Generate Semantic Web Content	The article discusses the research and development of a system called OntoSem, which automatically creates detailed semantic annotations for text and makes it available on the Semantic Web. This system has been under development for over fifteen years and uses a special ontology and lexicon to translate English text into a custom knowledge representation language. The authors faced challenges in adapting OntoSem for the Semantic Web, but were able to develop a translation system, OntoSem2OWL, which converts the text meaning representations into the Semantic Web language OWL. They also used OntoSem and OntoSem2OWL to create a web service called SemNews, which processes news summaries and publishes structured representations of their meaning.
411101	41110177	Choices For Lexical Semantics	Computational lexical semantics has advanced to the point where it is beneficial to compare the goals and methods of the different approaches. This article suggests several options for discussing these goals and methods. It argues that key questions include the use of lexical rules for creating word meanings, the role of syntax and formal semantics in defining lexical meaning, the use of a world model or ontology as a framework for describing lexical semantics, the relationship between static and dynamic resources, the emphasis on descriptive coverage, the balance between generalization and uniqueness, and the preference for a method or task-oriented approach to research. This analysis is not limited to the generative lexicon and ontologic semantic approaches, but also considers other perspectives.
411102	41110227	Experimental Demonstration of a Hybrid Privacy-Preserving Recommender System	Recommender systems help merchants suggest products to customers based on their preferences. However, these systems often have privacy vulnerabilities. The ALAMBIC framework was created to protect customer privacy and merchant interests. It is a hybrid recommender system that uses different techniques to make recommendations. One unique aspect is that customer data is divided between the merchant and a third party, preventing either from accessing sensitive information. Experimental results show that this system can still perform well and be user-friendly. User testing also reveals positive reactions to the privacy measures in place.
411102	41110235	Privacy amplification by public discussion	This paper explores the use of a perfect authenticity channel to fix the flaws of an imperfect privacy channel. Alice and Bob need to agree on a secret random bit string and have access to both channels. The private channel may have transmission errors and leaks information to an eavesdropper, while the public channel is accurate but can be fully accessed by the eavesdropper. The paper proposes interactive protocols that enable Alice and Bob to assess the level of corruption on the private channel and repair it if it is not severe. These protocols are secure against unlimited computing power and result in a shorter string that only Alice and Bob have complete information about, while the eavesdropper has minimal or no information.
411103	41110346	Decision knowledge triggers in continuous software engineering.	Decision knowledge is an important aspect of software development, which includes information about the decisions made, the problems they address, and the rationale behind them. However, it is often not integrated into the development process due to the additional effort required and the lack of perceived short-term benefits. Continuous software engineering provides a solution to this problem by incorporating decision knowledge management into daily practices such as code commits and task management. This paper proposes ways to encourage developers to capture and utilize decision knowledge during these practices, specifically by packaging distributed knowledge, making tacit decisions explicit, and ensuring consistency between decisions.
411103	41110311	Experiences with Supporting the Distributed Responsibility for Requirements through Decision Documentation.	Agile development projects involve all developers in the process of requirements engineering, where they continuously elicit and shape requirements. This involves discussing and making decisions on how to implement requirements within the system's architecture. This decision-making process requires a shared understanding and language for requirements, as well as the ability to utilize past decisions. However, there is a lack of experience reports on how to manage these decision-making processes in agile projects. To address this, various approaches have been proposed for managing decision knowledge. The article presents a practical example of decision knowledge management in an agile project and reflects on it to analyze the effectiveness of decision documentation. It concludes that while decision documentation was already implemented in the project, it can be improved to better support the elicitation and shaping of requirements.
411104	41110450	Texture-based visualization of unsteady 3D flow by real-time advection and volumetric illumination.	 This paper presents a method for visualizing unsteady 3D flow using dense textures, with a focus on computational efficiency and visual perception. The technique utilizes a 3D graphics processing unit (GPU) to efficiently handle logical 3D grid structures through 2D textures. The final display is achieved through slice-based direct volume rendering, with two options for volumetric illumination (gradient-based and line-based). Different perception-guided rendering methods are also considered to address issues of clutter and occlusion. Performance measurements and results are discussed to showcase the effectiveness of this approach. 
411104	411104170	GPU-Based 3D Texture Advection for the Visualization of Unsteady Flow Fields	This article presents a new way to visualize unsteady 3D flow fields using an interactive approach. The first step involves a GPU-based 3D texture advection scheme that updates a section of the 3D representation in one rendering pass. The second step uses texture-based volume rendering to display the results of the advection process. Both steps are GPU-supported, allowing for interactive frame rates. The approach also adopts and generalizes the noise and dye injection scheme of Image Based Flow Visualization (IBFV) to allow for a flexible combination of advected and newly injected values. The methods can also be extended to transport and display different materials, providing a unified way to highlight or blend out specific regions of the flow.
411105	41110528	Dynamic logic for plan revision in intelligent agents	This paper introduces a dynamic logic specifically designed for a version of the agent programming language 3APL that operates on propositions. 3APL agents have both beliefs and plans, and executing a plan can alter an agent's beliefs. The unique feature of 3APL agents is their ability to revise plans during execution, making it difficult to analyze them using traditional methods like structural induction. Therefore, the authors propose a dynamic logic that takes into account the plan revision aspect of 3APL agents. They also provide a complete and sound set of axioms for this logic. 
411105	4111051	Agent logics as program logics: grounding KARO	This paper discusses different approaches to relate agent logics to computational agent systems. One approach is to find executable fragments of an agent logic or use model checking. However, the paper explores an alternative approach, treating an agent logic as a program logic. By constructing a denotational semantics, the paper shows how an agent logic can be used as a design tool for specifying and verifying agent programs. The paper focuses on the KARO agent logic and constructs an agent programming language that is formally related to it. This is achieved by mapping worlds in the modal semantics onto a state-based semantics, which can then define an operational semantics for KARO programs. This allows for a computationally grounded semantics for a significant part of the KARO logic, including operators for knowledge, beliefs, motivations, and belief revision actions of a rational KARO agent. 
411106	41110655	Interval-valued fuzzy strong S-subsethood measures, interval-entropy and P-interval-entropy.	The paper introduces a new concept called fuzzy interval-entropy, which provides a value within the closed subinterval of [0, 1]. This concept is used to define interval-valued fuzzy strong S-subsethood measures, which are then used to build fuzzy interval-entropies. To ensure that the results are intervals, the paper utilizes total orders for comparison. The concept of P-interval-entropy is also introduced to address the problem of finding equilibrium points for interval-valued negations with respect to total orders. An example is provided to illustrate the application of these concepts and the results are compared to those obtained using classical interval-valued entropy, which provides a real number instead of an interval.
411106	41110618	Another paraconsistent algebraic semantics for Lukasiewicz-Pavelka logic.	Turunen, Tsoukiàs and Öztürk have shown that given an evidence pair (a,b) on the real unit square, associated with a propositional statement α, we can construct evidence matrices using four values (t, f, k, u) representing true, false, contradiction, and unknown, respectively. These matrices are used as truth values in a Lukasiewicz-Pavelka fuzzy logic, which is paraconsistent. In this paper, the authors extend this result to the real unit triangle, where a similar structure can be obtained. However, since the real unit triangle does not have a natural MV-algebra structure, the authors introduce new mathematical techniques to overcome this limitation and obtain another injective MV-algebra structure in the set of evidence matrices. Moreover, the authors provide formulas to calculate evidence matrices for operations associated with common logical connectives. 
411107	41110722	Sonar Sensor-Based Efficient Exploration Method Using Sonar Salient Features and Several Gains	The paper presents a new method for exploring unknown environments using sonar sensors. It proposes a "sonar salient feature" (SS-feature) to accurately map the environment by extracting circle features from salient convex objects. The SS-feature is incorporated into an extended Kalman filter-based simultaneous localization and mapping (SLAM) framework. To efficiently explore the environment, a strategy is used that considers driving cost, expected information, and localization quality. This reduces unnecessary exploration and leads to more accurate pose estimation. The method was tested in various experiments and was found to be effective in building accurate maps autonomously with sonar sensors in different home environments.
411107	41110716	Thinning-Based Topological Exploration Using Position Probability Of Topological Nodes	Exploration is essential for a robot to autonomously map its environment using its sensors. Previous methods, such as frontier-based or topological exploration, had limitations in efficiently covering the entire environment or utilizing obstacle information. This paper introduces a new method called thinning-based topological exploration (TTE) that uses real-time position probability of topological map end nodes to guide the robot. The robot updates the probability based on range data and decides whether to visit the end node or not. Experiments show that TTE is more accurate than frontier-based and other topological exploration methods, as it can complete exploration without directly visiting every location.
411108	411108115	Crawling the infinite web	Web pages that are generated dynamically upon request and contain links to other dynamically generated pages pose a problem for search engine crawlers due to the limited resources available for indexing. To address this issue, probabilistic models for user browsing in "infinite" Web sites have been proposed and studied. These models aim to predict how deep users go while exploring Web sites, and can be used to estimate how deep a crawler must go to download a significant portion of the visited content. Real data on page views in several Web sites show that, in theory and practice, a crawler only needs to go a few levels, about 3 to 5 clicks away from the start page, to reach 90% of the visited pages.
411108	41110832	Crawling a country: better strategies than breadth-first for web page ordering	This article discusses different methods for ordering web pages during web crawling in order to download the most important pages first. It highlights the importance of indexing the most significant pages in order to improve search engine coverage, as it is impossible to index the entire web. The study uses data from real web pages and a crawler simulator to compare different strategies under the same conditions. The article proposes various page ordering strategies that are more effective than traditional methods such as breadth-first search and partial Pagerank calculations. This helps to improve the efficiency of web crawling and indexing.
411109	41110941	Topic-driven multi-type citation network analysis	In this paper, the authors discuss the use of automated citation analysis in ranking authors in scientific fields. While previous methods have used content-based or citation network link analyses, this paper presents a novel approach that combines both methods with topical link analyses. This integrated probabilistic model considers citations among papers, authors, affiliations, and publishing venues in a single model. The application of Topical PageRank is also introduced to account for researchers' expertise in different domains. The authors also explore the impact of weighting various factors in a heterogenous link analysis of the citation network. Experimental results show that the multi-type citation graph and the use of Topical PageRank can improve performance, and heterogenous link analysis with parameter tuning performs even better.
411109	41110937	Academic network analysis: A joint topic modeling approach	A new probabilistic topic model has been developed that integrates authors, documents, cited authors, and venues in one framework. This is an improvement from previous models that only include a few of these components. The model can be used for expert ranking, cited author prediction, and venue prediction in academic networks. Experiments on two real-world data sets have shown that the model is effective and outperforms other algorithms in all three applications.
411110	411110120	The effectiveness of automatic text summarization in mobile learning contexts	Mobile learning is a popular method of learning that utilizes the advantages of mobile devices and technology to allow learners to access information anytime and anywhere. However, it also comes with its own set of challenges, particularly in delivering and processing learning content. To address this, a study was conducted to explore the use of automatic text summarization as a tool for reducing the amount of textual content in mobile learning. While text summarization can condense the most important ideas, it may also affect the overall meaning of the content. The study focused on the effectiveness of automatic text summarization in mobile learning, and the results showed that it can effectively generate helpful summaries. This highlights the importance of properly summarizing learning content to accommodate the unique features of mobile devices. 
411110	41111020	I-DIGEST framework: towards authentic learning for indigenous learners	There is a lack of opportunities for Indigenous learners to engage with their culture and traditional knowledge in formal education. This is due to a lack of incorporation of Indigenous knowledge into the curriculum, which is often experientially based and tied to the land. Digital storytelling has emerged as a way to incorporate Indigenous knowledge into learning and a framework has been created to guide the process. To test the framework, prototype tools were used in a case study course and experts in education and Indigenous knowledge provided feedback. Strengths of the framework include involving knowledge holders and community members in the design process and providing a visual and oral alternative to written expression. Potential weaknesses include lack of support from educators and a need for protocol to determine valid knowledge. This approach also has the potential to connect remote Indigenous communities to the knowledgebase.
411111	41111175	Centralized Adaptive Routing for NoCs	As the number of applications and programmable units in CMPs and MPSoCs increases, the Network-on-Chip (NoC) has to handle diverse and time-dependent traffic loads. To address this, NoC load-balanced, adaptive routing mechanisms have been introduced, which are more efficient than traditional oblivious routing schemes for hardware implementations. However, current adaptive routing schemes rely on local or regional congestion signals rather than the global state of the system. This paper presents a novel paradigm of NoC centralized adaptive routing, specifically designed for mesh topology. This approach continuously monitors the global traffic load and adapts routing decisions to improve load balancing. The implementation is scalable and lightweight, outperforming distributed adaptive routing schemes in terms of load balancing and throughput.
411111	41111154	Distributed Adaptive Routing Convergence to Non-Blocking DCN Routing Assignments	The popularity of big-data applications has led to larger and longer traffic flows in Data Center Networks (DCN). This has caused issues with static routing, which cannot efficiently load-balance the traffic, resulting in network contention and reduced throughput. While adaptive routing can solve this problem, it can also cause out-of-order packet delivery which can negatively affect the performance of longer flows. However, a distributed adaptive routing algorithm has been shown to effectively handle this issue by throttling each flow's bandwidth to half of the network link capacity. This algorithm can converge to a non-blocking routing assignment within a few iterations, causing minimal out-of-order packet delivery. A Markov chain model has been developed to predict the convergence time and shows that with half-rate traffic, the algorithm has a weak dependency on network size. This means that it can provide scalable and non-blocking routing for long flows in a rearrangeably-non-blocking Clos network. The model has been evaluated and hardware implementation guidelines have been provided. Overall, this research shows that distributed adaptive routing can be successfully used in DCNs to provide fast convergence and improve network performance.
411112	41111256	WCET analysis with MRU cache: Challenging LRU for predictability	The article discusses cache analysis for calculating Worst-Case Execution Time (WCET) and how most previous work has focused on the LRU replacement policy. However, commercial processors often use non-LRU policies as they are more efficient and still perform well. The article specifically looks at the MRU policy used in Intel Nehalem processors and how it has been underestimated due to existing cache analysis techniques not matching well with MRU. The authors propose a new classification, k-Miss, to better capture MRU behavior and use formal conditions and efficient techniques to determine k-Miss memory accesses. Results show that the proposed MRU analysis is precise and efficient, making it a good candidate for cache replacement policies in real-time systems.
411112	41111242	Model-based validation of QoS properties of biomedical sensor networks	A Biomedical Sensor Network (BSN) is a small network of sensor nodes used for medical applications. The network is modeled using timed automata and communication is done using the Chipcon CC2420 transceiver. UPPAAL is used to validate and adjust network parameters to meet quality of service (QoS) requirements. The network allows for dynamic changes in topology due to sensor nodes switching to power-saving mode or moving. UPPAAL has a simulator and model checker for analyzing network behavior and can handle up to 50 nodes. A new text-based version of the simulator has been optimized for large data structures. The accuracy of the model is compared to traditional simulation tools and results are mostly similar, although there may be some differences due to a simplified wireless channel model in UPPAAL.
411113	41111332	Psychophysics Testing Of Bionic Vision Image Processing Algorithms Using An Fpga Hatpack	The Monash Vision Group is developing a bionic eye that will involve implanting stimulation tiles on the primary visual cortex of the brain. In preparation for the first human trial in 2014, the group is exploring different image processing techniques and user interfaces. A special device called the FPGA Hatpack has been created to simulate the visual experience of a bionic eye recipient for normally sighted individuals. This Hatpack has been used to test three different methods for selecting luminance thresholds, and the results have shown varying levels of performance and areas for improvement. These findings are important for improving the overall functionality of the bionic eye.
411113	41111342	Real Time Object Tracking using Reflectional Symmetry and Motion	This paper presents a real-time tracking system that uses reflectional symmetry to visually track moving objects found in domestic environments. The system does not require any prior object models and is robust to shadows, specular reflections, and transparent objects. Block motion detection and a Kalman filter are used for object tracking, with the Kalman filter improving the efficiency of the symmetry detector. The system provides real-time segmentation of the object and generates a rotated bounding box for other image processing operations. It is able to track single objects in 640x480 videos at over 40 frames per second using a standard notebook PC.
411114	41111452	A document retrieval system for man-machine interaction	The article discusses an automatic document retrieval system designed for the IBM 7094. The system is capable of processing English texts and search requests using statistical, syntactic, and semantic procedures. It is organized around a central supervisor that calls on different subroutines to alter processing sequences and matching criteria. This flexibility allows for a variable amount of information to be produced in response to a search request and enables user interaction by allowing them to change their search requirements. The system also allows for evaluation of different analytical procedures by comparing retrieval results under different processing conditions.
411114	41111418	A blueprint for automatic Boolean query processing	Conventional information retrieval systems rely on complex Boolean query formulations and inverted file technologies for search and retrieval. This often puts a burden on users who have to construct these queries themselves. To address this issue, a new Boolean retrieval environment is proposed where queries are automatically generated using natural language input from the users. The system also has the capability to improve existing Boolean queries by incorporating information from previously retrieved relevant documents. The automatic queries can be used in a standard Boolean system or in an extended system with relaxed interpretation of Boolean operators. This approach has been found to produce better retrieval results compared to manually prepared queries. 
411115	41111559	TimeFlip: Using Timestamp-Based TCAM Ranges to Accurately Schedule Network Updates	Network configuration and policy updates are necessary but can cause temporary disruptions if not performed carefully. Accurate time coordination has been proposed as a solution to reducing these disruptions, but implementing it presents challenges. A practical method, called TimeFlips, is introduced in this paper to address this issue. TimeFlips use a timestamp field in a TCAM entry to implement atomic bundle updates and coordinate network updates with high accuracy. It is shown that with enough flexibility in scheduling, a single TCAM entry and a single bit can be used to encode a TimeFlip while maintaining a high level of accuracy.
411115	41111541	Time-based updates in software defined networks	In this paper, the challenge of minimizing the impact of network configuration updates on Software Defined Networks (SDN) is discussed. Due to the frequent updates in SDNs, there is a tradeoff between maintaining consistency and update performance. To address this, the authors propose an approach that uses time to coordinate network configuration and reconfiguration. This approach, called TIMECONF, offers significant advantages over existing update approaches by briefly allowing for inconsistency. The paper also suggests using time as a tool to simplify existing update approaches without compromising consistency. Overall, this approach aims to improve the efficiency and effectiveness of network configuration updates in SDNs.
411116	41111646	A dynamic joint protocols selection method to perform collaborative tasks	This paper discusses the issue of static selection of interaction protocols in multi-agent systems, which limits openness, dynamic behavior, and integration of new protocols. To address this issue, the authors propose a method for agents to select protocols at runtime when interacting with each other. This allows for more flexibility and adaptability in multi-agent interactions. The paper outlines the concepts and mechanisms used for this dynamic selection process.
411116	41111621	Dynamic Protocol Selection in Open and Heterogeneous Systems	This paper discusses the issue of dynamic protocol selection in open and heterogeneous multi-agent systems (MAS) for collaborative tasks. Protocols are important for coordinating agent activities, but their selection by agent designers may lead to mismatches at run time. To address this problem, the paper proposes a solution where agents can dynamically select protocols based on the characteristics of the protocols and task descriptions. The method allows for multiple protocol formalisms and does not assume trust between agents. Local protocol selection is coordinated through message exchange, with mechanisms in place to detect and overcome errors. Empirical results show that this solution is practical and successful in coordinating protocol selection.
411117	41111729	The Development of Hopping Capabilities for Small Robots	This paper discusses the creation and evolution of small hopping robots that are specifically designed for traversing difficult terrain and exploring unknown environments, particularly in low gravity environments such as outer space. These robots use a discontinuous motion, where they jump, stop to recharge, and then continue on their journey. The development of these robots is described through the different prototypes that have been created, showcasing the importance of steering, jumping, and self-righting capabilities. The final prototype also includes wheels for precise movement after landing. The authors share their lessons learned during the development process and provide detailed descriptions and images of the various prototypes, which can be applied to the design of other jumping or hopping robots.
411117	41111758	A flexible sensor for soft-bodied robots based on electrical impedance tomography	Soft robotics is a growing research field that is having a significant impact on the robotic community. However, using soft materials in robotics also presents unique challenges, particularly in regards to providing sensing capabilities. In this paper, the authors introduce a flexible sensor made from the same material used in soft robots, using a technique called Electrical Impedance Tomography for sensing. This allows for the creation of a flexible sensor of any size and shape without the need for wires, enabling distributed and continuous sensing. The system was tested by analyzing its response to elongation and bending, showing promising results and potential for use in soft robots.
411118	41111899	Face transfer with multilinear models	Face Transfer is a technique that allows for the mapping of one individual's facial movements onto the facial animations of another person. It works by extracting speech-related mouth movements, expressions, and 3D pose from video footage and using this data to create a detailed 3D textured face for the target individual. The model accounts for the target's unique facial expressions and visemes, and can be easily edited to change these features or even the target's identity. This process is made possible by a multilinear model that separates the different attributes of the face, allowing for independent variation. Face Transfer also addresses common issues with creating this type of model, such as securing accurate data and minimizing errors.
411118	41111868	Pathfinder: Visual Analysis of Paths in Graphs.	The ability to analyze paths in graphs is important in many fields, but traditional graph layouts often struggle to handle large and complex networks. This is further complicated by the fact that many networks also have multiple attributes associated with their nodes and edges. To address this issue, the paper introduces Pathfinder, a visual analysis tool specifically designed for path-related tasks in large and multivariate graphs. Pathfinder allows for querying paths while considering various constraints and provides both a ranked list and a node-link diagram to visualize the results. It is able to scale to large graphs by using incremental query results. The effectiveness of Pathfinder is demonstrated through its use in analyzing a coauthor network and biological pathways. 
411119	41111982	System diagnosis with smallest risk of error	This article discusses fault diagnosis in multiprocessor systems. Each processor can test its neighbors, but faulty processors may give incorrect results. The goal is to accurately identify the status of all processors, with a constant probability of failure for each processor. The article presents efficient diagnosis algorithms for complete bipartite graphs and simple paths, which provide the highest probability of correctness. This is the first time such reliable fault diagnosis has been achieved for these systems without any assumptions about the behavior of faulty processors. The article was published in 1998 by Elsevier Science B.V. and all rights are reserved.
411119	41111979	Optimal Diagnosis of Heterogeneous Systems with Random Faults	The problem of fault diagnosis in multiprocessor systems is discussed, where processors perform tests on each other. Fault-free testers accurately identify faults, while faulty testers can give false results. Processors fail with independent probabilities. The goal is to correctly identify the status of all processors using the test results. An optimal diagnosis algorithm with the highest reliability is presented. A fast diagnosis algorithm that is always locally optimal is also introduced. If all processors have low failure probabilities, a locally optimal diagnosis is also optimal. However, if some processors have high failure probabilities, a locally optimal diagnosis may not have the highest reliability. Examples are provided to illustrate this. 
411120	41112023	Refining non-taxonomic relation labels with external structured data to support ontology learning	This paper introduces a method for enhancing ontology learning systems by incorporating external knowledge sources like DBpedia and OpenCyc. The method uses verb vectors extracted from large amounts of unstructured text to suggest labels for unknown relations in domain ontologies. This is achieved through the creation of a knowledge base that includes verb centroids, mappings between concept pairs, and ontological knowledge from external sources. By applying semantic inference and validation, the accuracy of relation labels is improved. The effectiveness of this hybrid method is compared to other methods that solely rely on corpus data or reasoning and external sources.
411120	41112056	Consolidating Heterogeneous Enterprise Data For Named Entity Linking And Web Intelligence	The paper discusses the use of named entity linking to connect entities to structured knowledge sources, allowing for more advanced Web intelligence applications. The authors introduce Recognyze, a component that utilizes linked data repositories and transforms siloed data into a linked enterprise data repository. They use real-world data from a Swiss business information provider to demonstrate the creation of a linked data repository with over nine million triples. Challenges and techniques for pre-processing and integrating this data are discussed, with a focus on disambiguation and ranking algorithms. The performance of Recognyze is evaluated using business news sources. 
411121	4111216	Generating implications for design through design research	Human-computer interaction (HCI) emphasizes the importance of designing technology around the needs and behaviors of users, based on social science research. However, a major challenge for designers is translating this research into practical design ideas. Despite efforts to bridge this gap, there is still a lack of understanding about the knowledge that informs design decisions. Through interviews with 12 experienced HCI design researchers, the roles and types of design implications, as well as the process of generating and evaluating them, were explored. A framework was developed to guide the generation of design implications, which revealed a wider range, additional sources, and evaluation criteria. These findings have implications for interaction design research.
411121	41112173	The design of interfaces for multi-robot path planning and control	The field of human-robot interaction has expanded to include group and team interactions, rather than just one person controlling one robot. Design research has focused on using biologically-inspired motion initiated by a human operator to control a subset of robots and affect the motion and path planning of another subset. An exploratory design study created a taxonomy for categorizing individual robot motions, and then combined these motions with time and velocity as design variables. This resulted in a prototype set of motions, which was tested with nine participants through an iPad interface. Challenges and design recommendations were identified through this effort.
411122	41112236	Surmounting BPM challenges: the YAWL story	Over the past decade, the field of Business Process Management (BPM) has undergone significant changes. Various proposals for business process modelling and execution have been introduced, but not all have endured. The Workflow Patterns Initiative was created to establish a more organized method for comparing and developing languages. This resulted in the creation of YAWL, a new workflow language based on distilled patterns. In this paper, the authors discuss the position of YAWL in relation to the evolution of BPM and the current challenges facing the field. This highlights the importance of structured approaches in BPM and the need for continued development in this area.
411122	41112240	Managing Process Model Complexity Via Abstract Syntax Modifications.	Business Process Management (BPM) technology has been increasingly adopted, leading to the need for stakeholders to understand and agree upon the process models used in configuring BPM systems. However, users struggle with the complexity of these models, making it challenging to comprehend them. To address this issue, there is a need to improve the understanding of process models. Existing literature provides some solutions, but more research is needed in this area. Overall, the adoption of BPM technology has highlighted the importance of effectively communicating and comprehending process models among stakeholders.
411123	411123223	A local Tchebichef moments-based robust image watermarking	This paper presents a new content-based watermarking scheme that uses Tchebichef moments to protect against geometric distortions and common image processing operations. The scheme combines feature extraction using Harris-Laplace detector with watermark embedding in non-overlapped disks that are invariant to scaling, translation, and rotation. The watermark is embedded in the magnitudes of Tchebichef moments via dither modulation, ensuring robustness against image processing operations and allowing for blind detection. Simulation results using Stirmark demonstrate the effectiveness of the proposed method in comparison to other image watermarking schemes. 
411123	411123267	Geometric Distortion Insensitive Image Watermarking in Affine Covariant Regions	Feature-based image watermarking schemes have gained significant attention due to their ability to withstand various geometric distortions. While existing schemes have shown robustness against rotation, scaling, and translation, they are not resistant to cropping, nonisotropic scaling, random bending attacks, and affine transformations. Seo and Yoo have proposed a geometrically invariant image watermarking scheme based on affine covariant regions (ACRs) to improve robustness. Building upon their work, the authors have developed a new watermarking scheme that is insensitive to geometric distortions and common image processing operations. The scheme consists of three components: feature selection using a graph theoretical clustering algorithm, local normalization and orientation alignment for improved robustness, and indirect inverse normalization to balance imperceptibility and robustness. Experimental results on a dataset of 100 images demonstrate the superiority of the proposed method over other watermarking approaches in terms of robustness. 
411124	41112436	Quantitative Model Refinement as a Solution to the Combinatorial Size Explosion of Biomodels	The practice of building a large system by progressively refining an initial abstract specification is common in software engineering, but not widely used in systems biology. This approach involves starting with a high-level model of a biological system and gradually adding more specific details about its components and reactions. In this study, the focus is on data refinement, where specific species in the model are substituted with multiple subtypes. The authors demonstrate how this refined model can be systematically obtained from the original one. As an example, they apply this methodology to a previously developed model for the eukaryotic heat shock response, refining it to include details about the acetylation of heat shock factors. Despite the increased complexity of the refined model, the authors show that their approach allows for minimal computational effort while preserving the experimental fit and validation of the model.
411124	4111248	Control Strategies for the Regulation of the Eukaryotic Heat Shock Response	Elevated temperatures can cause proteins in living cells to misfold, leading to the formation of larger aggregates and potentially causing cell death. The heat shock response is an evolutionary mechanism that helps control the level of misfolded proteins in cells. A new molecular model for this response in eukaryotic cells has been proposed, which involves temperature-induced activation, chaperoning of misfolded proteins, and self-regulation of chaperone synthesis. In this paper, the authors use a control-driven approach to study this regulatory network and identify its main functional modules. They also discuss the importance of feedback loops in this system, and demonstrate the need for complexity in order to achieve effective regulation. A new method for unbiased model comparison is also presented.
411125	41112584	Revocable Fingerprint Biotokens: Accuracy and Security Analysis	The paper discusses the challenges of using biometrics in security applications due to the inability to change biometric data if a database is compromised. To address this issue, the concept of revocable or cancelable biometric-based identity tokens (biotokens) is introduced. This approach separates biometric data into two fields, one of which is encoded and one that supports approximate matching, to enhance both privacy and security. The paper presents an adapted algorithm for fingerprint recognition that shows an average decrease in Equal Error Rate of over 30%, providing improved security and privacy. The approach also addresses the issue of protecting small fields in the biometric data.
411125	41112547	Appearance, Context And Co-Occurrence Ensembles For Identity Recognition In Personal Photo Collections	Modern research in face recognition has primarily focused on developing new feature representations and learning methods for combining them. However, this approach often ignores the issue of unmodeled correlations in the data, which can affect the accuracy of recognition tasks such as verification and identification. While the conventional solution is to use large amounts of training data and machine learning techniques to capture these correlations, this is not feasible for personal consumer photo collections. To address this problem, the authors propose an ensemble-based approach that combines different sources of information, such as facial appearance, visual context, and social co-occurrence, to improve the accuracy of face recognition in consumer photo collections. They demonstrate the effectiveness of this approach through experiments on two datasets, reporting better results than previous methods.
411126	41112673	Strong multidesignated verifiers signatures secure against rogue key attack	Designated verifier signatures (DVS) and multidesignated verifiers signatures (MDVS) are important tools in electronic voting and contract signing. DVS allows a signer to create a signature that can only be verified by a specific entity chosen by the signer, while MDVS extends this by allowing the signer to choose multiple designated verifiers. In this paper, the authors investigate the security of MDVS against rogue key attacks, where a designated verifier tries to forge a signature that passes verification by another designated verifier. They propose a new construction that does not rely on the knowledge of the secret key assumption, and a generic construction of strong MDVS. These contributions aim to strengthen the security of MDVS and make it a more reliable primitive in electronic applications. 
411126	41112681	Identity-based strong designated verifier signature revisited	Designated verifier signature (DVS) is a method where a signer can prove the validity of a statement to a verifier, without allowing the verifier to transfer this conviction. A stronger version, called strong designated verifier signature (SDVS), only allows the verifier to privately check the validity of the signer's signature. However, existing identity-based SDVS schemes have a weak unforgeability model that does not account for practical attacks. To address this, a new and stronger model is proposed, and an efficient SDVS scheme is constructed based on the Computational Diffie-Hellman problem. This scheme is perfectly non-transferable, meaning the signer and verifier can produce identical signatures on the same message. It is also the first identity-based SDVS scheme to be non-delegatable according to a proposed definition.
411127	41112759	Universal authentication protocols for anonymous wireless communications	This paper discusses a new approach to secure roaming protocols, which allow a user to access a foreign server while authenticating both the server and the user's home server. The traditional approach involves all three parties, but the proposed approach only requires the involvement of the user and the foreign server. Two protocols are proposed, one with better efficiency and user anonymity and the other with strong user anonymity. These protocols can be used universally, regardless of the user's home or foreign domain, making the system less complex. The paper also presents a practical solution for user revocation, which is a challenging issue in two-party roaming with strong user anonymity. These solutions can be applied in various types of roaming networks.
411127	41112713	Efficient anonymous roaming and its security analysis	The Canetti-Krawczyk model is a method for creating secure key exchange protocols by using reusable modular components. This approach allows for easier construction and proof of new protocols compared to other secure methods. The authors of this paper use the CK-model to create an efficient and secure key exchange protocol for roaming. This protocol only requires four message flows and utilizes standard cryptographic techniques. They also introduce a one-pass counter based MT-authenticator and prove its security under the assumption of a secure MAC against chosen message attacks.
411128	411128113	Security Analysis And Modification Of Id-Based Encryption With Equality Test From Acisp 2017	At the 2017 Australasian Conference on Information Security and Privacy, Wu et al. presented a method for identity-based encryption that aims to prevent insider attacks. However, our analysis shows that their approach is vulnerable to attacks and does not meet the claimed security standards. To address this issue, we propose a modified version of their method.
411128	411128119	Cryptanalysis of the convex hull click human identification protocol	The Sobrado and Birget protocol is a human identification method that utilizes a user's ability to mentally form a convex hull of secret icons and click randomly within it. While some security concerns have been raised, a thorough analysis has been lacking. This paper examines the security of the protocol and reveals two probabilistic attacks that can expose the user's secret after only a small number of authentication sessions. These attacks can be efficiently executed and one of them cannot be fully mitigated even with large system parameters. This highlights the vulnerability of the protocol and the need for further improvements.
411129	41112965	Packet classification using tuple space search	Routers need to quickly sort and forward packets in order to efficiently perform functions like firewalls and QoS routing. This involves matching packets against a database of filters and forwarding them based on priority. Current filter schemes are not suitable for large databases. A new algorithm called Tuple Space Search (TSS) maps filters to tuples and uses a hash table for faster searching. Techniques are also introduced to improve the search process. In testing, TSS showed significant speedup and is the only scheme that allows fast updates and search times. An optimal algorithm called Rectangle Search is also described for two-dimensional filters.
411129	41112968	Fast packet classification for two-dimensional conflict-free filters	Packet classification is a crucial feature in routers that enables advanced functions such as QoS routing, virtual private networks, and access control. Unlike traditional routers that only use destination addresses for forwarding packets, routers with packet classification capability can use multiple header fields, such as source address, protocol type, or application port numbers, for forwarding. However, multi-dimensional packet classification is more challenging than one-dimensional IP lookup. A lower bound of Ω(wk-1) has been proven for k-dimensional filter lookup, making a binary search scheme impossible. However, through research, it has been discovered that this lower bound depends on conflicts in the filter database. For conflict-free filters, a binary search scheme can be used with O(log2 w) hashes and O(nlog2 w) memory. This scheme has been tested and found to be as effective as existing ones, making it a practical solution for routers.
411130	41113074	Transforming examples into patterns for information extraction	Information Extraction (IE) systems are commonly used to extract specific information from text using pattern matching. However, adapting these systems to a new subject domain can be time-consuming and expensive as it requires building a new pattern base. To address this issue, a strategy for building patterns from examples is described. This approach allows for quicker adaptation to new domains by having the user provide examples and their corresponding logical form entries, which the system then transforms into patterns using meta-rules. This eliminates the need for manual construction of patterns, making the process more efficient. 
411130	4111307	Automatic acquisition of domain knowledge for Information Extraction	Developing an Information Extraction (IE) system for new events or relations requires identifying the various ways in which they are expressed in text. This has traditionally been done through manual analysis and annotation of large text samples. However, a new approach, called EXDISCO, uses an automatic discovery process to identify relevant documents and event patterns from unannotated text. Starting with a small set of "seed patterns," EXDISCO is able to generate patterns that perform similarly to manually constructed systems in actual extraction tasks. This alternative approach saves time and effort in the development of IE systems for new events or relations.
411131	41113146	What input errors do you experience? Typing and pointing errors of mobile Web users	Small devices, like PDAs, are popular for accessing the Web, but they face challenges due to limited interface bandwidth. This can lead to difficulties for users, such as small keyboards and limited pointing devices. While there is little research on the input difficulties caused by these limitations, anecdotal evidence suggests that there are similarities between able-bodied users of the mobile Web and users with motor impairments on desktop computers. This paper presents a study on the input errors of mobile Web users, identifying six types of typing errors and three types of pointing errors that are shared between both user domains. The study suggests that solutions used for motor impaired desktop users could also be beneficial for mobile Web users, as they face similar challenges despite using different input devices. 
411131	41113163	Mobile device impairment... similar problems, similar solutions?	Previous studies have identified a new type of impairment called situationally-induced impairment, which occurs when the characteristics of a device and the environment in which it is used affect the user's behavior. This is often seen in small devices used in a mobile or constrained setting. However, there is limited research on the extent and impact of this type of impairment. This study compares the problems faced by situationally-impaired users with those faced by motor-impaired users in the context of data input. The survey reveals 12 common problems in all forms of impaired input and suggests that solutions used for one type of impairment could also be applied to the other. This highlights the importance of identifying commonalities between different types of impairment to improve interventions and reduce the need for creating new solutions.
411132	41113234	Automatic sign translation	This paper discusses the challenge of automatically translating signs in natural environments, which contain a large amount of information. The authors present their current system for detecting and translating Chinese signs into English, using two data-driven machine translation methods: Example Based Machine Translation (EBMT) and Statistical Machine Translation (SMT). They compare the results of these methods, which were trained on a small bilingual sign corpus and bilingual glossary, and find that EBMT produces more accurate translations while SMT is better at recognizing new patterns. They are also working on a multi-engine system that can learn from data and combine the results of EBMT and SMT.
411132	41113249	PanDoRA - a large-scale two-way statistical machine translation system for hand-held devices.	The statistical machine translation (SMT) approach is a popular method in Machine Translation due to its high translation quality and cost-effectiveness. However, SMT systems typically require a lot of computing resources and cannot be run on hand-held devices. Existing hand-held translation systems either rely on complex grammar rules or require a server connection. This paper introduces PanDoRA, a two-way phrase-based SMT system designed specifically for stand-alone hand-held devices. It uses special techniques to enable real-time translation of dialogue speech on regular PDAs. PanDoRA has a large vocabulary and millions of phrase pairs for each translation direction, making it the first large-scale SMT system to run on hand-held devices. Experiments have shown that PanDoRA's translation quality is comparable to other state-of-the-art SMT systems. 
411133	41113314	SmartLabel: an object labeling tool using iterated harmonic energy minimization	Labeling objects in images is crucial for various visual learning and recognition applications, but manual labeling is time-consuming and prone to errors, making it difficult for large image databases. Semi-supervised learning algorithms, like Gaussian random field (GRF), can help by incorporating unlabeled data, but their one-shot nature limits their performance. This paper presents a new tool, SmartLabel, that uses four innovations to semi-automatically label objects in images: soft labeling, spatial constraints in graph construction, iterated harmonic energy minimization, and relevance feedback for human interaction. Results on six datasets show that SmartLabel outperforms GRF with minimal user input and significantly improves labeling accuracy.
411133	41113313	Semi-automatically labeling objects in images.	Labeling objects in images is important for various visual learning and recognition tasks, but it can be time-consuming to manually label a large dataset. To address this issue, a semi-automatic method called SmartLabel is proposed, which uses a graph-based semi-supervised learning algorithm to label images with reduced human input. This approach is improved upon with SmartLabel-2, which incorporates a novel scheme to automatically sample negative examples and uses image over-segmentation to extract smooth object contours. Evaluation on six object categories shows that SmartLabel-2 can achieve promising results with a small amount of labeled data and accurately extract object contours.
411134	41113442	Multimode Locomotion Via Superbot Robots	This paper introduces a versatile and customizable robot that can adapt to various modes of locomotion through reconfigurable modules. Each mode is optimized for specific characteristics such as environment, speed, turning ability, energy efficiency, and resilience to failures. The Superbot robot, inspired by MTRAN and CONRO, is used as an example to demonstrate this concept. Experimental results, including both real-life tests and simulations, have shown the effectiveness of this approach. The Superbot can move in various ways, such as forward, backward, turning, sidewinding, maneuvering, and traveling up to 500 meters on a flat surface with just its battery. In simulations, it can perform as different creatures and travel at a speed of 1.0 meter per second on flat terrain with less than 6W per module. It can also climb slopes of up to 40 degrees. 
411134	41113476	Implementing Configuration Dependent Gaits In A Self-Reconfigurable Robot	This paper examines locomotion in self-reconfigurable robots, which are made up of connected modules that can change shape and configuration. The focus is on understanding how different gaits can be represented and selected in these robots. A control system based on role based control was implemented in a seven-module physical robot, and experiments showed that the robot successfully changed gaits when manually reconfigured from a chain to a quadruped configuration. This demonstrates the potential of role based control as a method for controlling locomotion in self-reconfigurable robots.
411135	411135130	Building Association-Rule Based Sequential Classifiers for Web-Document Prediction	Web servers collect and store information about users' browsing behavior in web logs. This data can be used to create statistical models that predict the users' next requests based on their current behavior. However, analyzing these logs can be challenging due to their large size and sequential nature. While previous research has proposed various methods for building association-rule based prediction models using web logs, there has been no comprehensive study on the effectiveness of these methods. This paper compares different types of sequential association rules for web document prediction and identifies two important factors - the type of antecedents of rules and the criterion for selecting prediction rules. Based on this comparison, the authors propose a best overall method and test it on real web logs.
411135	411135116	MPIS: maximal-profit item selection with cross-selling considerations	Data mining literature has many algorithms for association rule mining, but there is limited research on how association rules can assist in specific targets. One such target is the maximal-profit item selection with cross-selling effect (MPIS) problem. This problem involves selecting a subset of items that will yield the maximum profit while considering cross-selling. It is proven to be NP-hard in a simple form. A new approach is proposed that incorporates the loss rule, a type of association rule that models the cross-selling effect. This transforms the problem into a quadratic programming one, with a heuristic approach offered as an alternative. Experiments demonstrate the effectiveness and efficiency of both methods.
411136	41113647	Privacy Preserving Record Linkage via grams Projections	Record linkage is a commonly used technique in data mining applications, but with the growing amount of available data, concerns about privacy have arisen. In this paper, the authors propose a new approach for private record linkage using secure data transformations and differential privacy. Their method involves embedding private frequent variable length grams from the original data and using personalized thresholds for matching individual records. This approach offers stronger privacy guarantees and better scalability compared to existing techniques, while still maintaining comparable utility. 
411136	4111364	Publishing Set-Valued Data Via Differential Privacy	This paper addresses the problem of publishing set-valued data for data mining tasks while ensuring privacy protection under the differential privacy model. Existing methods for set-valued data publishing are based on partition-based privacy models, which are susceptible to privacy attacks. Differential privacy, however, offers strong guarantees regardless of an adversary's background knowledge. The authors propose a novel approach using context-free taxonomy trees to efficiently release differentially private set-valued data with guaranteed utility. They also present a probabilistic top-down partitioning algorithm that can scale linearly with input data size. Extensive experiments on real-life datasets demonstrate the effectiveness and scalability of their approach for common data mining tasks. 
411137	411137103	Client-Centric Adaptive Scheduling of Service-Oriented Applications	This paper introduces a client-centric computing model that enables service-oriented applications to be executed in a more adaptive manner. This model allows for tasks to be distributed between the client and network side, making it more flexible and able to adjust to changes in the environment. It is also expected to improve scalability, performance, and privacy control. The paper presents scheduling algorithms and rescheduling strategies for the model and experiments demonstrate that it can significantly improve the execution of service-oriented applications. Overall, this model offers a more efficient and customizable approach to executing these types of applications. 
411137	41113792	A unified approach to optimal opportunistic spectrum access under collision probability constraint in cognitive radio systems	The article discusses a cognitive radio system with one primary channel and one secondary user. A channel-usage pattern model and fundamental access scheme are introduced and used to study the optimal opportunistic spectrum access problem. This is formulated as an optimization problem where the secondary user aims to maximize spectrum holes utilization while keeping the collision probability below a certain level. A unified approach is proposed to solve this problem, and optimal opportunistic spectrum access algorithms are presented for different distributions of idle periods. Theoretical analysis and simulation results show that these algorithms effectively utilize spectrum holes while maintaining a low collision probability. The impact of sensing error is also evaluated through simulations.
411138	41113814	Structured design with mathematical proofs	Structured design is a commonly used approach in software design, but it has limitations in ensuring design correctness. Formal methods, which involve using mathematical notation and reasoning, offer a way to address this issue. These methods can detect inconsistencies and ambiguities in a system and can be automated using computers. This paper introduces a formal structured design method with mathematical proofs and provides an example of its use in producing a provable design. The method integrates correctness verification into each step of the design process, making it different from traditional methods. This not only improves the quality of the design, but also allows for the verification of its correctness. 
411138	41113826	A formal method for proving programs correct	TUG (Tree Unified with Grammar) is a formal method of developing software. It involves writing a formal specification and systematically converting it into a structured design and program. This conversion is done using the Sequence, Selection, and Iteration patterns in the specification, along with a set of mapping rules. These patterns create a connection between the specification, design, code, and proofs. When there is a change in user requirements, TUG can locate and trace the impact and changes in the specification, design, and code through these patterns. This allows for a more efficient and organized approach to software development.
411139	4111397	Torben: A Practical Side-Channel Attack for Deanonymizing Tor Communication	The Tor network is widely used for anonymous communication online, but its security is being increasingly studied by researchers, including those from academia, industry, and even nation-states. While previous attacks on Tor have been proposed, they have had low accuracy and high false-positive rates, making them impractical for widespread use. This paper presents a new attack, called Torben, which is more reliable and less intrusive than previous attacks. It takes advantage of the fact that web pages can be manipulated to load content from untrusted sources, and that the size of request-response pairs in encrypted communication can reveal information about the web pages being visited. In testing, Torben was able to accurately detect web page markers with over 91% accuracy and no false positives. 
411139	41113933	Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection.	Machine learning has become a popular tool for detecting malware due to its ability to handle complex and evolving attacks. However, recent studies have shown that machine learning itself can be exploited by attackers to evade detection. This paper categorizes potential attack scenarios against machine learning-based malware detection tools and proposes a secure-learning approach to mitigate the impact of these attacks. The approach is tested on Drebin, an Android malware detector, and is shown to only slightly decrease the detection rate while significantly improving security. The authors argue that this approach can be applied to other malware detection tasks as well.
411140	41114024	Economics of a supercloud.	A Supercloud is a type of Infrastructure-as-a-Service (IaaS) known as a "CrossCloud" that allows users to have direct control over cloud deployments, even across multiple different cloud providers. This goes beyond the capabilities of federated or hybrid clouds. Users can perform privileged operations such as live migration of virtual machine instances across different cloud providers with varying virtual machine monitors, networking, and storage infrastructure. The Supercloud also allows for simultaneous deployments across various combinations of cloud providers and the ability to change placement at any time. It provides virtual machine, storage, and networking with a full set of management operations, allowing for optimized performance across different cloud providers.
411140	41114063	Experiences with the Amoeba distributed operating system	The Amoeba project is a research effort focused on creating a seamless and transparent distributed operating system for connecting multiple computers. The goal is to provide users with the illusion of a single powerful timesharing system, while in reality, the system is implemented on a collection of machines. This project has led to the development of the Amoeba distributed operating system, currently in its 4.0 version. The system is designed to handle wide-area networks and has various applications running on it. The article discusses the successes and failures of the project and its impact on future versions. Amoeba runs on a unique system architecture and is being jointly developed by two institutions in Amsterdam.
411141	411141140	Delayed reinforcement learning for adaptive image segmentation andfeature extraction	Object recognition is a complex process that involves multiple levels of algorithms. These systems typically lack feedback between levels, making it difficult to ensure their effectiveness. A new approach using "delayed" reinforcement learning has been introduced to address this challenge. This method involves learning the parameters of a multilevel system used for object recognition based on feedback from the highest level. Through experimental validation, this approach has shown improved results in recognizing 2D objects by controlling feedback in a systematic manner. This approach has the potential to solve a longstanding problem in the field of computer vision and pattern recognition.
411141	4111417	Adaptive target recognition	Target recognition involves using a series of algorithms at different levels to identify objects. These systems typically do not have feedback between levels, making it challenging to ensure correct identification and avoid false alarms. To address this, a closed-loop system using reinforcement learning is proposed. This system learns the parameters of the recognition system and adjusts them based on feedback from performance specifications, such as probability of correct identification and false alarm rate. The system has been tested on various types of SAR imagery and has successfully recognized different types of targets, including articulated targets and those with varying configurations and depression angles.
411142	41114238	On the performance of Dijkstra's third self-stabilizing algorithm for mutual exclusion and related algorithms	In 1974, Dijkstra introduced the concept of self-stabilizing algorithms and presented three algorithms for mutual exclusion on a ring of n processors. The third algorithm, while the most interesting, was non-intuitive and its worst case complexity (the upper bound on the number of moves until it stabilizes) was unknown. In 1986, a proof of its correctness was presented, but the complexity question remained open. In this paper, the authors solve this question and prove an upper bound of $${3\frac{13}{18} n^2 + O(n)}$$ and a lower bound of $${1\frac{5}{6} n^2 - O(n)}$$. They use potential functions and amortized analysis in their approach. The paper also presents a new three-state algorithm for mutual exclusion with a tight bound of $${\frac{5}{6} n^2 + O(n)}$$. In 1995, Beauquier and Debas presented a similar three-state algorithm with an upper bound of $${5\frac{3}{4}n^2+O(n)}$$ and a lower bound of $${\frac{1}{8}n^2-O(n)}$$. The authors improve upon this algorithm with an upper bound of $${1\frac{1}{2}n^2 + O(n)}$$ and a lower bound of $${n^2-O(n)}$$. Overall, their algorithm out
411142	41114251	A Self-stabilizing Algorithm with Tight Bounds for Mutual Exclusion on a Ring	In 1974, Dijkstra introduced the concept of self-stabilizing algorithms and developed a three-state algorithm for the mutual exclusion problem on a ring of processors. In this paper, a new three-state self-stabilizing algorithm for mutual exclusion is presented with a worst-case complexity of $\frac{5}{6} n^2 + O(n)$, which is an improvement over existing algorithms, including Dijkstra's. The authors also improve the upper bound analysis for Dijkstra's algorithm and demonstrate a bound of $3\frac{13}{18} n^2 + O(n)$. This work contributes to the understanding of self-stabilizing algorithms and their application to mutually exclusive systems.
411143	41114329	An approach to v&v of embedded adaptive systems	V&V techniques are crucial for ensuring the reliability of high assurance systems. However, the inclusion of embedded adaptive components in these systems has made conventional V&V methods less effective in handling uncertainties caused by environmental changes. This is particularly concerning in safety-critical applications like flight control systems, where these changes must be carefully observed and understood. To address this issue, a non-conventional V&V approach is proposed in this paper, specifically designed for online adaptive systems. It involves a novelty detection technique based on Support Vector Data Description and online stability monitoring tools based on Lyapunov's Stability Theory. Case studies using NASA's Intelligent Flight Control System demonstrate the success of this approach.
411143	4111439	Validating an online adaptive system using SVDD	Verification and validation (V&V) activities for online adaptive control systems aim to ensure that they can detect new system behaviors and provide safe control actions. Since these novel behaviors cannot be fully described in requirements, they must be observed during operation. This is where novelty detection methods, such as the support sector data description (SVDD), come into play. As a one-class classifier, SVDD can create a decision boundary around the learned data domain with little knowledge of outliers. In this study, the SVDD was used to validate an intelligent flight control system (IFCS) and successfully detected abnormal system behaviors, differentiating them from normal events. This demonstrates the effectiveness of SVDD as a tool for identifying safe regions in the learned domain during V&V processes.
411144	41114478	Language model-based sentence classification for opinion question answering systems	This paper proposes a language model-based approach with a Bayes classifier for classifying opinionative and factual sentences in an opinion question answering system. The proposed technique significantly outperforms current standard classification methods, with an improved accuracy of 93.35%. The motivation for this task comes from the desire to provide tools for analyzing attitudes and feelings in online resources. The system aims to present multiple answers to the user based on opinions derived from different sources. This can be achieved through a document retrieval component and a sentence retrieval component, but an accurate classification component is necessary to detect and classify opinionative and factual sentences. 
411144	4111447	A word clustering approach for language model-based sentence retrieval in question answering systems	This paper presents a term clustering approach to enhance sentence retrieval in Question Answering (QA) systems. Due to the smaller search scope in QA, data sparsity and exact matching pose significant challenges. To address these issues, the authors propose using Language Modeling (LM) techniques, specifically building class-based models through term clustering and incorporating higher-order n-grams. Experiments conducted on TREC 2007 questions demonstrate that this method significantly improves the mean average precision of sentence retrieval, increasing it from 23.62% to 29.91%. 
411145	41114510	IS-Label: an independent-set based labeling scheme for point-to-point distance querying	The article discusses the problem of finding the shortest path or distance between two vertices in a graph, which has many practical uses. Several indexes have been developed for this purpose, but they can only handle graphs with up to 1 million vertices. This is insufficient for the increasingly large real-world graphs, such as social networks and Web graphs. The authors propose a new labeling scheme based on the independent set of a graph, which is shown to be able to handle much larger graphs than existing indexes.
411145	41114545	Efficient algorithms for optimal location queries in road networks	This paper examines the optimal location query problem on road networks, which involves finding the best location to set up a new server in order to minimize the overall cost of serving clients. The cost for each client is determined by the distance to their closest server multiplied by their weight. Previous solutions to this problem have not been efficient enough, so the authors propose a new algorithm based on the concept of "nearest location component." The paper also discusses three extensions of the problem and presents results from experiments showing that their algorithm is significantly faster than the current state-of-the-art, with a speedup of over 200 times on large real datasets. 
411146	41114614	Distributed Maximal Clique Computation	Maximal cliques are important substructures in graph analysis. However, most existing algorithms for computing maximal cliques are either sequential and cannot scale, or are parallel but suffer from skewed workload. In this paper, a distributed algorithm is proposed for computing maximal cliques on a share-nothing architecture. The problem of skewed workload distribution is effectively addressed, resulting in reduced time complexity for computing maximal cliques in real-world graphs. Additionally, algorithms for efficiently maintaining the set of maximal cliques when the graph is updated are also devised. The effectiveness of these algorithms is verified through experiments on various real-world graphs from different application domains.
411146	41114641	Finding maximal cliques in massive networks	Maximal clique enumeration is a key problem in graph theory with various real-world applications. However, existing algorithms for this problem require significant amounts of memory, which is a concern due to the ever-increasing size of networks. To address this issue, a general framework is proposed for designing external-memory algorithms that can recursively compute maximal cliques in smaller subgraphs, eliminating the need for costly random disk access. It is proven that these recursive computations produce correct and complete sets of maximal cliques. The choice of base vertices used in the framework can be adjusted for different purposes, such as minimizing I/O cost in static graphs or maintaining updates in dynamic graphs. The framework is also applied to design an external-memory algorithm for maximum clique computation in large graphs.
411147	41114742	Context-awareness in software architectures	Context-awareness is becoming increasingly important in adaptable systems, and as such, there is a need for formal models and notations to incorporate this aspect into higher levels of modelling. This paper proposes a formal approach to designing context-aware systems that is well integrated with existing concepts and techniques for software architectures. This approach uses a set of primitives to model context as a first-class entity and explicitly address context-awareness as an additional dimension of architectural elements. An image search system is used as an example to illustrate this approach.
411147	4111475	Heterogeneous and Asynchronous Networks of Timed Systems	In this article, the authors introduce a component algebra and logic for heterogenous timed systems. The algebra consists of asynchronous networks of processes that represent the behavior of machines with different clock granularities, communicating asynchronously with machines at other nodes. The unique aspect of the theory is that not all network nodes need to have the same clock granularity. The authors determine conditions under which dynamic binding of machines with different clock granularities will result in a consistent orchestration of the entire system. They also examine which logics are suitable for specifying this component algebra.
411148	41114826	Biasing the overlapping and non-overlapping sub-windows of EEG recording	EEG recording requires subjects to sit still for a couple of hours and perform various mental, computational, motor imaginary or other tasks. This process can be tedious and complicated, as it is difficult for subjects to maintain concentration and there may be accidental muscle activity. This can make it challenging to extract useful information for classification, especially in longer task durations. To improve classification accuracy, this study examines the impact of different time segments on performance. The results suggest that the middle and end parts of the recording are more important for accurate classification, while the earlier segments may have lower performance. 
411148	41114818	Reducing training requirements through evolutionary based dimension reduction and subject transfer.	Training Brain Computer Interface (BCI) systems involves teaching the system to understand a person's intentions through analyzing their EEG data. Traditionally, this requires multiple training sessions, which can take several hours. However, this is not suitable for online BCI systems. To address this issue, researchers have explored using data from other subjects or previous sessions of the same subject for training the system. The goal of this study is to develop a method that can reduce the training and calibration time needed for both the subject and the classifier. The study compares different techniques, including evolutionary subject transfer and retraining with data from other subjects. The results show that using only 40% of the target subject's data is enough to train the classifier, and the approach with evolutionary subject transfer is the most effective. This study demonstrates the feasibility of adapting a BCI system trained on other subjects. 
411149	41114923	Curve fitting by fractal interpolation	Fractal interpolation is a method used to represent data with an irregular or self-similar structure. While most studies focus on using this technique for functions, it can also be applied to curves. A new method has been introduced that offers a more efficient way to represent curves compared to existing methods. Results show that this new method produces smaller errors or better compression ratios, making it a useful tool for curve fitting. 
411149	4111494	Feature fusion for facial landmark detection.	Facial landmark detection is a crucial step in facial analysis for various applications, but it is a challenging task due to the many sources of variation in 2D and 3D facial data. While there has been extensive study on using feature descriptors for landmark detection, the fusion of these descriptors is not well explored. This paper presents a new framework for combining facial feature descriptors and evaluates different fusion schemes. The proposed framework maps each feature to a similarity score and combines them to select the best solution for a queried landmark. Results show that a quadratic distance to similarity mapping with a root mean square rule for fusion achieves the best performance in accuracy, efficiency, robustness, and monotonicity.
411150	4111502	On the Security of Tandem-DM	We have provided the first proof of security for Tandem-DM, a well-known method for creating a cryptographic hash function from a block cipher. Our proof shows that when Tandem-DM is used with AES-256, it is resistant to collision attacks from adversaries making less than 2120.4 queries. We have also proven that Tandem-DM has a good level of preimage resistance. This is significant because Tandem-DM is one of only two known constructions that can create a hash function with provable resistance to collisions, making it a practical and desirable option for secure data encryption.
411150	41115028	Weimar-DM: a highly secure double-length compression function	Weimar-DM is a compression function that uses a block cipher with a double length key and half the block size to compress a 3n-bit string to a 2n-bit one. It has been proven that for n=128, no adversary can find a collision with probability greater than 1/2 with less than 2n−1.77=2126.23 queries, making it the most secure compression function of its kind. The security analysis for Weimar-DM is simpler than similar functions and a preimage security analysis also shows near-optimal bounds. These security bounds are asymptotically optimal.
411151	41115157	Training a sentence planner for spoken dialog: the impact of syntactic and planning features	Spoken dialog systems rely on the dialog manager to perform both domain specific tasks and general dialog functions. To separate knowledge about language from domain specific knowledge, natural language generation techniques can be used. However, the natural language generator must be customized for each application. This study introduces a new method for training the natural language generator automatically and explores the impact of domain specific and general features on performance. The results show that while general features have the greatest impact, incorporating domain specific features can improve performance without sacrificing the benefits of automatic domain customization through training.
411151	41115194	Knowledge requirements for the automatic generation of project management reports	Project management reports for software engineering projects require a specific type of knowledge representation in software engineering environments (SEEs). This includes a process model, a rich type hierarchy, and various entity relations. It is also important to maintain histories for certain types of information and have specific information about problems readily available. These types of knowledge are not only necessary for automatic project reporting, but also for a SEE to effectively provide comprehensive and useful services to its users. 
411152	41115236	Robust coordination in large convention spaces	In multi-agent systems (MAS), regulating the behavior of autonomous agents is important to solve coordination problems and minimize conflicts. Social conventions can be used by agents to coordinate, but agreeing on conventions without a central authority is challenging. A new spreading-based convention emergence mechanism is proposed in this paper to help agents agree on the best convention when there are multiple options. This mechanism is applied to a problem with a large convention space, specifically finding a common vocabulary for agents to communicate without ambiguity or inconsistencies. The approach is shown to be scalable and robust to unreliable communication in large and changing scenarios.
411152	41115287	Agent-based decision support for actual-world procurement scenarios	Multi-item, multi-unit negotiations in industrial procurement can be challenging for buying agents. These agents must consider a variety of constraints, including attributes of separate items and multiple items, when making a decision. To address this issue, a new agent-based service called Ibundler has been developed. This service helps buying agents determine the best bundle of offers based on their constraints and preferences, relieving them from the burden of solving a difficult problem. Ibundler is designed to be used as a negotiation service for buying agents and as a winner determination service for reverse combinatorial auctions with side constraints. It is the first service of its kind and aims to simplify complex negotiation scenarios for buying agents.
411153	41115346	Computing the communication costs of item allocation	Multiagent systems involve groups of agents working together to accomplish a task or allocate resources. One way to efficiently distribute these resources is through auctions, which allow agents to communicate their private values for the resource to a central decision maker. The communication requirements of different auction methods vary, and this study examines the use of entropy as a measure of communication bandwidth. A new method for measuring bandwidth usage is also presented, using dialogue trees to represent all possible communication between two agents. The study also provides new guidelines for choosing the most effective auction method, based on analysis of various auctions and their communication requirements. These guidelines differ from previous recommendations and take into account the number of bidders and the sample space of their valuations.
411153	41115323	Complexity, parsing, and factorization of tree-local multi-component tree-adjoining grammar	Tree-Local Multi-Component Tree-Adjoining Grammar (TL-MCTAG) is a useful approach for representing natural language because it can capture the appropriate domain of locality in its structures. Its multicomponent design allows for modeling of lexical items that may be far apart in a sentence, such as quantifiers and wh-words. When used in a synchronous grammar, it can express both close relationships and divergent structures needed to connect syntax and semantics in a single language or between two languages. TL-MCTAG's limited expressivity puts constraints on movement, but it has been mistakenly believed to have a simpler parsing complexity than it actually does. This article examines the recognition problem of TL-MCTAG and presents efficient algorithms for parsing and factorizing the grammar. Despite the difficulty, these algorithms can greatly improve processing efficiency. 
411154	41115428	Designing a Programming Language for Home Automation	The AutoHAN project at the Cambridge Computer Laboratory is developing programming languages suitable for use in home automation and networking. These languages will greatly impact the usability of electronic devices and expand the potential users who can benefit from psychology of programming research. The design process and criteria for these languages are still ongoing and this paper outlines the current progress and plans for further research. Home networking technologies are becoming more prevalent and standardization efforts are underway. This presents a significant challenge for the psychology of programming community. While there has been some previous research on programming home appliances, it is not typically considered relevant to programming language design. However, with the increasing complexity of home automation, these insights may become more valuable in the future.
411154	41115440	A user-centred approach to functions in Excel	This article discusses the integration of user-defined functions into Excel spreadsheets, with the goal of bringing the benefits of additional programming language features to the system. The main objective is to maintain compatibility with previous versions while also preserving the usability advantage of spreadsheets over traditional programming languages. The authors emphasize the importance of considering the cognitive requirements of the user in the design process, using analytical approaches such as the Cognitive Dimensions of Notations and the Attention Investment model. They argue that this approach can be applied to the design and extension of other programming languages and environments. Simply making Excel more like a programming language would not meet these objectives and constraints.
411155	41115519	Content-Based Cross Search for Human Motion Data using Time-Varying Mesh and Motion Capture Data	This paper introduces a cross search scheme for two types of 3D human motion data: time-varying mesh (TVM) and motion capture data. TVM is a sequence of 3D mesh models created from multiple-view images, allowing for recording of shape, color, and motion of real-world objects. Efficient retrieval systems are crucial for practical TVM archiving, but current systems require additional TVM generation for queries, which is time-consuming. Motion capture data, commonly used for capturing 3D human motion, has a different data structure and is incompatible with TVM. The proposed retrieval system utilizes a modified shape distribution algorithm to enable cross-referencing between TVM and motion capture data.
411155	41115540	Retrieval of Time-Varying Mesh and motion capture data using 2D video queries based on silhouette shape descriptors	This paper proposes a retrieval system for time-varying mesh (TVM) and motion capture data using 2D video queries. Unlike previous approaches, which used other TVM and motion capture data as queries and incurred high query generation costs, the proposed system uses 2D video queries that can be easily captured by a single camera. This enables end users to easily and interactively retrieve 3D motion sequences. The system introduces a new feature, the P-type Fourier descriptor, which is derived from 2D contour images. The feature vectors for TVM and motion capture data are generated using this descriptor. Experimental results showed an average retrieval accuracy of 88% for TVM and motion capture data.
411156	411156161	Sketch-on-Map: Spatial Queries for Retrieving Human Locomotion Patterns from Continuously Archived GPS Data	This article proposes a system for retrieving human movement patterns using tracking data collected from a GPS receiver over a large area and extended period of time. A clustering algorithm is used to segment the data based on the person's navigation behavior. Queries can be made using sketches on a map displayed on a computer screen, with five different types of queries available. Algorithms are implemented to analyze the sketch, identify the query, and retrieve results. A graphical user interface allows for hierarchical querying and visualization of intermediate results. The sketch-based user interaction strategy makes querying for movement patterns easy and clear. 
411156	411156175	Sketch-based spatial queries for the retrieval of human locomotion patterns in smart environments	The article discusses a system for retrieving video sequences of humans in a smart environment by using spatial queries. Sketches on the floor layout are used to create queries for different types of locomotion patterns. Directional search algorithms are used to find the best matches to the sketch, which are then ranked based on similarity and presented to the user. The system was developed in two stages, with modifications made after an initial user study to improve accuracy and usability. The system was found to have an overall accuracy of 93% in retrieving data from a real-life experiment, which was improved to 97% after modifications. Users found the system easy to learn and use and suggested potential applications for the user interaction strategy. 
411157	41115755	Autonomous visual model building based on image crawling through internet search engines	This paper presents a method for automatically creating visual semantic concept models from Internet search engine data without manual labeling. The process involves crawling images from search engine results and using a generalized Multiple-Instance Learning (MIL) framework to model the results as "Quasi-Positive Bags". An algorithm called "Bag K-Means" is used to find the maximum Diverse Density (DD) without negative bags, and a "Bag Fuzzy K-Means" algorithm is proposed to improve the model's accuracy. Experimental results show that this method can accurately learn models for specific concepts and outperforms the original Google Image Search.
411157	41115747	Speech-Based Visual Concept Learning Using Wordnet	The use of supervised and unsupervised machine learning methods for modeling visual concepts is gaining importance in video semantic indexing, retrieval, and filtering applications. Videos contain various types of data such as audio, speech, visual, and text, which are combined to infer semantic concepts. However, most research has been limited to a single domain. This paper proposes an unsupervised technique to create keyword lists for visual concept modeling using WordNet. Additionally, an Extended Speech-based Visual Concept (ESVC) model is introduced to enhance and reorganize the keyword lists through supervised learning based on multimodality annotation. Results show that the context-independent models perform similarly to supervised learning methods, while the ESVC model outperforms a state-of-the-art speech-based video concept detection algorithm. 
411158	41115862	The Time Index: An Access Structure for Temporal Data	The authors introduce a new indexing technique called the time index, designed to enhance the performance of certain types of temporal queries. This index allows for efficient retrieval of objects that are valid within a specific time period and supports the temporal WHEN operator and aggregate functions. The time index is also extended to improve the performance of the temporal SELECT operator, which retrieves objects that meet a specific condition during a given time period. The paper describes the indexing technique, search and insertion algorithms, and a method for processing a temporal JOIN operation. A simulation comparing the time index to other temporal access structures is also presented. 
411158	4111587	The Design and Evaluation of a Magnetic/Optical Access Structure for Temporal Databases	This paper introduces a magnetic/optical access structure called the MonotonicB+ -Tree (MBT) for append-only temporal databases. The MBT is designed for Write-Once Read Many (WORM) optical disks and has an insertion algorithm that does not require index node splitting. It also presents a storage architecture where magnetic disks are used for current and recent past versions, while optical disks are used for archiving older versions. The paper discusses techniques for migrating data between the two types of disks, including strategies for minimizing overhead and handling long time intervals. Simulation results demonstrate the efficiency of the migration process and identify key parameters that affect the performance of the access structure.
411159	4111598	A Framework For Dynamic Ebusiness Negotiation Processes	The business world is becoming increasingly customer-focused, leading to a rise in demand for personalized products and services. This has created a need for an open and dynamic environment for eBusiness processes in order to support new business models. While there has been previous research on negotiation, most of it has focused on simpler forms such as auctions and bilateral negotiations. However, more complex negotiations are necessary for the formation of supply networks and personalized requests in today's business landscape. Negotiation processes are often intertwined with other business processes, making it crucial to have a framework that can handle this complexity. This paper proposes a framework that incorporates existing research on negotiations and expands it to meet the demands of modern eBusiness applications. The framework consists of five components that work together to support dynamic negotiation processes: negotiation requirements, structure, process, protocol, and strategy.
411159	41115951	A web services-enabled marketplace architecture for negotiation process management	The increasing presence and complexity of eBusiness has led to a greater need for negotiation between companies. However, the adoption of negotiation systems has been slow in practice despite their potential value. This could be due to a lack of focus on process management aspects such as design, description, and deployment. Business negotiations should be viewed from a process management perspective as they are a part of corporate processes like procurement and sales. This paper proposes a Web Services-enabled marketplace architecture for managing B2B negotiations and further refines it with pattern-based process composition. The proposed architecture is implemented using BPEL4WS and evaluated from different angles to validate its effectiveness.
411160	41116041	CCS with Hennessy's merge has no finite-equational axiomatization	The paper confirms a conjecture made by Bergstra and Klop in 1984 by showing that the process algebra obtained by adding an auxiliary operator proposed by Hennessy in 1981 to Milner's Calculus of Communicating Systems is not finitely based modulo bisimulation equivalence. This means that Hennessy's merge cannot replace the left merge and communication merge operators proposed by Bergstra and Klop, if a finite axiomatization of parallel composition modulo bisimulation equivalence is desired. In other words, the additional operator does not provide a simpler or more efficient way of defining parallel composition.
411160	4111607	A complete equational axiomatization for MPA with string iteration	This study focuses on equational axiomatizations of bisimulation equivalence in a language that extends Milner's basic CCS with string iteration. String iteration is a modified version of the Kleene star operation where the first argument can only be a non-empty sequence of atomic actions. The study demonstrates that for a positive integer k, bisimulation equivalence for processes with loops of length at most k in this language can be finitely axiomatized as long as the set of actions is finite. Additionally, an infinite equational theory is presented that completely axiomatizes bisimulation equivalence for the entire language. It is also proven that no finite equational axiomatization for bisimulation equivalence in basic CCS with string iteration is possible unless the set of actions is empty.
411161	41116143	Axiomatizing the Least Fixed Point Operation and Binary Supremum	The least fixed point operation on continuous functions on complete partially ordered sets can be described by iteration algebras, which have a finite set of axioms. By combining this with the binary supremum operation on continuous functions on complete semilattices, a finite set of axioms can be found for these operations. This also leads to complete infinite equations and finite implications.
411161	4111616	Equational Properties of Iteration in Algebraically Complete Categories	The completeness theorem states that if a category's fixed point operation is defined by initiality, then the equations satisfied by the operation are the same as those in iteration theories. This means that in these categories, the axioms of iteration theories accurately and completely describe the equational properties of the fixed point operation.
411162	41116219	Equational theories of tropical semirings	This paper discusses the equational theories of exotic semirings, which are semirings with underlying carrier sets that are subsets of the set of real numbers and with operations of minimum or maximum as sum and addition as product. Examples include the (max,+) semiring and the tropical semiring. The paper shows that none of these exotic semirings has a finite basis for its equations, and this also applies to the underlying commutative idempotent weak semirings. The paper provides characterizations of the equations, decidability results, descriptions of free algebras, and relative axiomatization for these commutative idempotent weak semirings.
411162	41116228	Axiomatizing Tropical Semirings	This paper examines the equational theory of exotic semirings, which are semirings with underlying carrier sets that are subsets of the real numbers and use minimum or maximum as addition and addition as multiplication. Examples include the (max, +) semiring and the tropical semiring. The paper proves that none of these exotic semirings has a finite basis for its equations, and this also applies to the commutative idempotent weak semirings that they are based on. The paper provides characterizations of the equations, descriptions of the free algebras, and axiomatization results for each of these commutative idempotent weak semirings.
411163	41116327	Rules for Abstraction	The paper discusses the use of ion techniques for verifying reactive systems, which aim to combine automatic and interactive proof techniques. It focuses on homomorphic abstraction, which involves using proof rules in Lamport's Temporal Logic of Actions. The authors argue that a logical formalization of abstraction can lead to more refined techniques, and present two new techniques for verifying liveness properties over abstract models. The ultimate goal is to provide a theoretical basis for the integration of different proof techniques in the verification of reactive systems. 
411163	41116339	Transformation of B specifications into UML class diagrams and state machines	The article discusses a proposed method for converting B abstract machines into UML diagrams. This approach aims to provide a better understanding of the underlying structure of a B model, especially for non-experts. The focus is on creating class diagrams and state machines, and the method allows for flexibility in adapting the resulting UML models. The goal is to facilitate communication and explanation of B models to stakeholders.
411164	41116419	New routing techniques and their applications.	This paper discusses two new routing techniques that can be used to implement efficient routing schemes for different types of graphs. The first technique is for unweighted graphs with n nodes and m edges, and uses Õ(1/ε n2/3) space per vertex and Õ(1/ε)-bit headers to route messages between any pair of vertices on a (2+ε,1)-stretch path. This is an improvement compared to previous techniques that use Õ(n5/3) space. The second technique is for weighted graphs with a normalized diameter of D, and uses Õ(1/ε n1/3log D) space per vertex and Õ(1/ε log D)-bit headers to route messages on a (5+ε)-stretch path. This is an improvement over previous techniques that use Õ(n4/3) space. The paper also presents routing schemes for unweighted and weighted graphs with different stretch values, providing more efficient options compared to previous techniques. These new techniques have been proven to be almost optimal in terms of space usage.
411164	4111641	Preprocess, set, query!	Thorup and Zwick introduced the concept of distance oracles in 2001, showing that for any given integer k, a graph with n vertices and m edges can be preprocessed in Õ(mn1/k) time to create a compact data structure of size O(kn1+1/k). This structure allows for the retrieval of estimated distances between any pair of vertices with stretch 2k - 1 in O(k) time. Patrascu and Roditty made a breakthrough in 2010 by achieving a distance oracle for sparse unweighted graphs of size O(n5/3) and stretch 2 in constant time. This paper presents a new data structure that can achieve a stretch of 1 + ε in O(nm1-ε′) size and Õ(m1-ε′) query time. For sparse unweighted graphs, this translates to a data structure of size O(n1.86) that can produce estimated distances with multiplicative stretch 1.75 in O(n0.86) time. 
411165	41116544	Fast generation of multiple resolution instances of raster data sets	The paper discusses efficient algorithms for studying raster data sets at multiple resolutions in GIS applications. These algorithms involve generating coarser resolution rasters from a fine resolution raster. The paper describes an algorithm that can solve this problem in Θ(N) time when the data fits in the main memory of a computer. Two additional algorithms are presented for handling larger data sets in external memory. One algorithm requires O(sort(N)) data transfers, while the other requires O(scan(N)) transfers. A variant of the problem is also studied where a connected subregion is handled instead of the full input raster. An algorithm for this variant runs in Θ(U log N) time in internal memory and can be adapted for external memory using O(sort(U)) data transfers. The efficiency of two of the presented algorithms is demonstrated through implementation and practical use. 
411165	41116573	I/O-efficient batched union-find and its applications to terrain analysis	This paper presents an I/O-efficient algorithm for the batched (off-line) version of the union-find problem, which has been extensively studied for over four decades but no efficient algorithm has been found. The presented algorithm uses O(N log(M/B)N/B) I/Os, which is asymptotically optimal in the worst case. If there are union operations that join a set with itself, the algorithm uses O(N log(M/B)N/B + MST(N)) I/Os. The paper also describes a practical O(N log(N/M))-I/O algorithm for this problem, which has been implemented. The union-find problem has applications in terrain analysis, and the paper presents I/O-efficient algorithms for two related problems: computing topological persistence and constructing the contour tree. Preliminary experimental results show significant improvement over previous methods on large data sets that do not fit in memory.
411166	41116625	Independent Range Sampling, Revisited Again.	The range sampling problem involves storing a set of points with associated weights in a structure that allows for efficient extraction of random samples from a given query range. This problem was first studied in 2014 by Hu, Qiao, and Tao, and later by Afshani and Wei. The initial research focused on unweighted and dynamic versions of the problem in one dimension, while the latter considered weighted and unweighted versions in 3D for halfspace queries. The authors present three main results and insights, including the possibility of efficient data structures with expected query time, efficient worst-case query bounds by allowing approximate weight proportionality, and a conditional lower bound that shows one of these concessions is necessary. This leads to a significant gap between the expected and worst-case query time for 3D range sampling queries, with the former resulting in near-linear space and polylogarithmic query time, and the latter requiring near-linear space and query time close to $n^{2/3}$. This is the first known major gap between expected and worst-case query time for a range searching problem.
411166	41116631	Continuous matrix approximation on distributed data	The paper discusses the challenge of tracking and approximating data matrices in a streaming fashion, particularly when the data is coming from multiple distributed sites. This is known as the "tracking approximations to a matrix" problem and the paper proposes novel algorithms to address it. These algorithms maintain a smaller matrix as an approximation to a larger distributed streaming matrix, with the goal of tracking an ε-approximation to the matrix norm in any direction. The algorithms work in a streaming fashion and have low communication requirements, making them suitable for distributed computation. Extensive experiments with real datasets demonstrate the effectiveness of these algorithms.
411167	41116786	Simultaneous placement and scheduling of sensors	The article discusses the challenge of monitoring spatial phenomena using wireless sensors with limited battery life. The problem involves deciding where to place sensors to best predict unsensed locations while also considering when to activate them to maximize performance within power constraints. Traditionally, these two problems have been approached separately, but the authors propose a new algorithm, ESPASS, that simultaneously optimizes placement and scheduling. The algorithm has been proven to provide a constant-factor approximation to the optimal solution. It also allows for a smooth power-accuracy tradeoff and is effective in complex settings where sensing quality is measured. Empirical studies show significant improvements in performance compared to separate placement and scheduling methods. 
411167	41116734	Robust sensor placements at informative and communication-efficient locations	This article discusses the importance of selecting the best sensor placements when using wireless sensor networks to monitor spatial phenomena. It presents a data-driven approach that considers the predictive quality of sensor locations, communication cost, and optimization of the NP-hard trade-off. The approach uses nonparametric probabilistic models called Gaussian Processes to estimate predictive power and communication cost for unsensed locations. The algorithm, called PSPIEL, takes advantage of the submodularity and locality properties of the problem and has strong approximation guarantees. The article also demonstrates how PSPIEL can be used for mobile robot path planning and provides experimental validation on real-world problems. Overall, PSPIEL outperforms existing methods and has been implemented on 46 Tmote Sky motes.
411168	41116816	The Geometry of Differential Privacy: The Small Database and Approximate Cases.	This work focuses on the trade-offs between accuracy and privacy in linear queries over histograms, which includes contingency tables and range queries. The goal is to find a differentially private mechanism that minimizes the mean squared error compared to the true answer. For pure differential privacy, previous work has provided an approximation guarantee. This study extends this guarantee to (epsilon, delta)-approximate differential privacy and also considers the small database setting. By using statistical estimation techniques, they are able to achieve a polylogarithmic approximation to the optimal for both pure and (epsilon, delta)-privacy. This study also explores the accuracy gap between pure and approximate privacy notions. Finally, they establish a connection between hereditary discrepancy and private mechanisms, leading to the first polylogarithmic approximation to the hereditary discrepancy of a matrix.
411168	41116869	Unconditional differentially private mechanisms for linear queries	This article discusses the problem of designing differentially private mechanisms for a set of linear queries over a database with minimal added error. Previous work by Hardt and Talwar related this problem to geometric properties and proposed an approximation using the Slicing or Hyperplane conjecture. This study proposes a mechanism that works unconditionally and improves upon the previous approximation by using a result from Klartag and recent techniques from Dadush, Peikert, and Vempala. The article also presents a stronger lower bound on the optimum and introduces a symmetrization argument to show the existence of a near-optimal differentially private mechanism. This result has potential implications beyond just Differential Privacy.
411169	411169191	Cops and robbers on geometric graphs	Cops and robbers is a game where one robber is chased by a set of cops on a graph. The cop number, or minimum number of cops needed to catch the robber, is studied for geometric graphs. These graphs consist of points on a 2D plane with a certain distance between them. It is proven that the cop number for connected geometric graphs in a 2D plane is at most 9, with an example of a graph with a cop number of 3. The upper bound for random geometric graphs is improved for graphs that are dense enough. It is also shown that in the connectivity regime, if the distance between points is below a certain threshold, the cop number is 1 with high probability. On the other hand, if the distance is above a certain threshold, the cop number is at least 1 with high probability.
411169	41116936	On the Non-Planarity of a Random Subgraph.	In summary, by creating a random subgraph G(p) with probability p, we can prove that for any constant c, if p is equal to 1 plus a small value (epsilon) divided by the minimum degree of the original graph, then G(p) is likely to be non-planar as the minimum degree of the original graph increases. This extends previous findings on the planarity of binomial random graphs.
411170	41117064	A golden-block-based self-refining scheme for repetitive patterned wafer inspections	This paper introduces a new method for identifying defects in two-dimensional wafer images with repetitive patterns. The technique utilizes prior knowledge and has the ability to learn and create a database of golden blocks from the wafer images themselves. This database is refined and modified when used for further inspections. The use of golden blocks eliminates the need to recalculate periods and building blocks for new wafer images with the same pattern. The proposed algorithm results in a significant reduction in processing time and storage overhead. New building blocks can also be derived directly from existing golden blocks, improving the overall quality of defect detection.
411170	41117063	Intelligent product brokering for e-commerce: an incremental approach to unaccounted attribute detection	This research focuses on creating a versatile product-brokering agent that can understand user preferences and recommend products accordingly. It is able to detect both quantifiable and non-quantifiable attributes through a user feedback system, including attributes that are not within the system's predetermined ontology. Unlike previous methods, this approach does not require significant changes to the system when a new product attribute is introduced, only that it is included in the product's description. A genetic algorithm is used to verify candidate attributes and remove any excess ones. A prototype has been developed and has shown success in identifying previously unaccounted for attributes that affect a user's preference.
411171	411171102	Fast Algorithms for Testing Fault-Tolerance of Sequenced Jobs with Deadlines	Queue-based scheduling systems execute jobs in a predetermined sequential order, but faults can cause delays by forcing jobs to re-execute. Therefore, it is crucial to determine in real-time whether the scheduled jobs are fault-tolerant, meaning they will meet their deadlines regardless of faults. This allows for quick decisions, such as admitting urgent jobs without compromising the overall schedule's fault tolerance. The goal of this work is to create efficient algorithms for testing the fault tolerance of sequenced jobs in the presence of transient faults. The algorithms are exact and run in linear time, allowing for real-time decisions, and are based on different fault models that specify the allowed fault patterns and the time frame for restarting failed jobs. 
411171	41117182	Faster Information Gathering in Ad-Hoc Radio Tree Networks.	The study focuses on information gathering in ad-hoc radio networks, where the goal is to collect all information from each node in the network to a designated target node. The network has an unknown topology and aggregation of information is not allowed. The study focuses on tree topologies, where all edges are directed towards the root but the actual topology is unknown. Two deterministic algorithms are presented for this problem, with the first algorithm having a time complexity of O(n) and the second algorithm having a time complexity of O(log n) when acknowledgements are allowed. Overall, the study improves upon previous upper bounds for this problem.
411172	411172113	A linear time algorithm for consecutive permutation pattern matching.	Order-isomorphic sequences are those where the elements in one sequence are in the same order as the elements in another sequence. A linear time algorithm has been developed to check if a given sequence contains a subsequence that is order-isomorphic to a given pattern. This is called consecutive permutation pattern matching and is easier than the general permutation pattern matching problem which is known to be NP-complete. The algorithm has a time complexity of O(n+m) if the pattern can be sorted in O(m) time, otherwise it is O(n+mlogm). The algorithm is based on the Knuth-Morris-Pratt string matching algorithm.
411172	411172136	Compressed string-matching in standard Sturmian words	The algorithm presented is a simple and efficient way to find all occurrences of a given pattern in a standard Sturmian word. It works in O(|pat|+n) time and can also handle cases where the pattern contains "don't care" symbols. In these cases, the algorithm produces a linear number of arithmetic progressions instead of a single one. This allows for fast computations even when the input is given in a compressed form. In most cases, the length of the standard Sturmian word is exponential in comparison to the input size. 
411173	41117322	A Universal Integral	This content discusses the introduction of a general integral that can be defined on any measurable space, based on a minimal set of axioms. This integral operates on measures that are monotone set functions and measurable functions with a range within the unit interval. The concept of integral equivalence is introduced, which leads to a significant type of general integral known as the universal integral. Various types of this integral, including extremal ones, are described and characterized. 
411173	41117336	Integrals based on monotone set functions	This article provides an overview of various integrals that can be defined on arbitrary monotone set functions that disappear in the empty set (known as monotone measures). The well-known Choquet, Shilkret, and Sugeno integrals, along with their properties, are discussed. In addition, newer concepts such as universal integrals and decomposition integrals are also introduced and their properties, such as integral inequalities and convergence theorems, are explored. Overall, this survey presents a comprehensive understanding of integrals and their applications in different contexts.
411174	41117449	Splittable traffic partition in WDM/SONET rings to minimize SONET ADMs	SONET ADMs are a major cost factor in WDM/SONET rings. Various studies have proposed heuristics to minimize the number of ADMs needed by partitioning traffic and allowing it to be transferred between different wavelengths. This approach, known as splittable traffic streams, has been shown to potentially reduce the number of ADMs required. This paper discusses two variations of the minimum ADM problem with splittable traffic streams: one with prespecified routings and one without. Both variations are proven to be NP-hard, but the paper proposes heuristics with approximation ratios of 5/4 and 3/2, respectively.
411174	41117464	Selecting forwarding neighbors in wireless Ad Hoc networks	Broadcasting is a common operation in wireless ad hoc networks and the simplest method, known as flooding, involves each node retransmitting a message to all its 1-hop neighbors. However, this approach is inefficient and can cause problems such as redundancy, contention, and collision. To improve efficiency, one solution is to have nodes only forward the message to a smaller subset of neighbors that cover all of its 2-hop neighbors. This paper proposes two heuristics for selecting the minimum number of forwarding neighbors, with time complexities of &Ogr;(n log n) and &Ogr;(n2), and approximation ratios of 6 and 3 respectively. The previously known algorithm had a time complexity of &Ogr;(n3 log n) and &Ogr;(1) approximation. 
411175	4111752	Journal Self-Citation Viii: An Is Researcher In The Dual Worlds Of Author-Reader And Author-Institution	This paper examines the ethical implications of journal editors requesting authors to cite papers from the same journal in which they are submitting an article. The author argues that this issue must be considered within the context of two relationships: author-reader and author-institution/community. These relationships highlight that an author cannot exist in isolation and is constrained by the institutional system in which they operate. Therefore, the request for citations can be seen as a way to foster a positive author-reader relationship and contribute to the production of diverse knowledge. Ultimately, the paper concludes that this request is ethically important for maintaining good relationships and promoting valuable knowledge in the global community. 
411175	41117533	Knowledge acquisition via three learning processes in enterprise information portals: learning-by-investment, learning-by-doing, and learning-from-others	An enterprise information portal (EIP) is a knowledge community where members have specific tasks assigned through a division of labor. These members undergo three types of learning processes: learning-by-investment, learning-by-doing, and learning-from-others, which results in specialized knowledge related to their tasks. This paper presents a mathematical model for EIPs and defines the goal of members as maximizing net benefits from individual investment and effort. The results show that in high interest or high return conditions, it is better to delay learning, while intensive investment and effort are preferred when costs are discounted at a high rate, knowledge is durable and valuable, and there is sufficient transfer of knowledge among members. The size of the EIP also affects optimal decisions and learning processes. 
411176	41117650	Special issue on programming based on actors, agents and decentralized control.	The AGERE! workshop, held in conjunction with the ACM SPLASH conference since 2011, aims to bring together researchers in programming systems, languages, and applications based on decentralized control paradigms such as actors, active/concurrent objects, and agents. The goal is to promote the use of these paradigms in developing software for complex, real-world applications. The workshop is a continuation of previous workshops on Object-based Concurrent Programming, and has become increasingly relevant as concurrency and distribution have become commonplace in programming. This special issue presents extended and enhanced versions of selected papers from AGERE! 2011 and 2012, which have undergone additional review cycles to promote the development and adoption of high-level programming paradigms centered around concurrency.
411176	41117673	Dimensions in programming multi-agent systems.	Research in Multi-Agent Systems (MAS) has resulted in the creation of various models, languages, and technologies for programming not just individual agents, but also their interactions, the environment they operate in, and the organization they are a part of. This research has shifted from agent-oriented programming to multi-agent-oriented programming (MAOP). A MAS program is developed using a structured set of concepts and design and programming abstractions that go beyond traditional agent concepts. JaCaMo is a platform for MAOP that integrates three dimensions - BDI agents, their environment, and their organization. The goal is to support programmers in exploring the synergy between these dimensions and provide a comprehensive programming model and platform for MAS development. This paper presents an overview of MAOP using JaCaMo and discusses the different solutions that arise from emphasizing each dimension. 
411177	41117714	Multiscale morphological segmentation of gray-scale images.	The paper presents a new method for segmenting gray level images using multiscale morphology. The approach is similar to the watershed algorithm, but uses multiscale morphological closing and opening to gradually fill or clip dark and bright features on the image. The algorithm identifies valid segments at each scale based on three criteria and integrates them in the final result. It consists of two passes, with a preprocessing step to simplify small scale details, and detects potential features contributing to segment formation. The algorithm is tested on synthetic and real images and compared to other methods, with a quantitative measure of performance.
411177	41117723	Enhancing effective depth-of-field by image fusion using mathematical morphology	Reduced depth-of-field (DOF) can cause blurry images in light optical systems, as objects outside the focused zone appear unclear. To improve the DOF without sacrificing image quality, images from different focused regions can be fused together. This paper introduces a method for combining multiple focus images of a scene using morphological filters. The filters select sharply focused regions from each image and merge them to create a fully focused image. The results of this technique are compared to other fusion methods using an image gradient performance measure. This approach effectively enhances the DOF of the sensor and produces high-quality images.
411178	41117821	A Smart Home Agent For Plan Recognition Of Cognitively-Impaired Patients	Assisting individuals with cognitive deficiencies in a smart home presents complex challenges, including plan recognition. To address this issue, a formal framework based on lattice theory and action description logic has been proposed. This framework reduces uncertainty in predicting an agent's behavior by generating new implicit extra-plans. This approach provides an effective solution for plan recognition in a smart home, allowing for assistance to be provided to those with cognitive deficits. The framework has been implemented in a smart home laboratory and will be validated through experimentation with real-life scenarios. 
411178	41117853	A KEYHOLE PLAN RECOGNITION MODEL FOR ALZHEIMER'S PATIENTS: FIRST RESULTS	This article discusses the challenge of identifying the behavior of individuals with Alzheimer's disease in the early to intermediate stages. A keyhole plan recognition model, utilizing lattice theory and action description logic, is proposed as a solution. This model transforms the recognition problem into a classification task and allows for the formalization of the patient's incoherent intentions, caused by their cognitive impairment. The model was tested in a smart home laboratory using simulated real-life scenarios. This approach has the potential to improve the early detection and management of Alzheimer's disease.
411179	41117950	Large scale system evaluations using PHY abstraction for LTE with OpenAirInterface	This paper outlines the methodology for expected effective SINR mapping (EESM) and mutual information effective SINR mapping (MIESM) based PHY abstraction on the OpenAirInterface (OAI) LTE platform. The authors discuss the calibration process for these techniques using the OAI link level simulator, and then validate the calibrated scheme using the OAI system level simulator. The paper also covers the methodologies for different transmission modes of LTE, including transmission modes 1, 2, and 6 with various channel types. The results demonstrate that proper calibration is necessary for accurate results, and that the implemented PHY abstraction in the OAI system level simulator can significantly increase simulation speed while maintaining accuracy. The use of OAI, which implements the LTE release 8.6, also allows for portability to other LTE simulators.
411179	41117933	Low-complexity distributed MIMO receiver and its implementation on the OpenAirInterface platform	Future wireless communication systems will use a tight frequency reuse scheme to maximize system throughput. This results in high interference at the cell edges, making it necessary to have receivers that can take advantage of this interference rather than trying to eliminate it. The paper discusses the application of such a receiver structure in a distributed MIMO scenario, where two independent data streams are decoded from two synchronized base stations. The implementation of this receiver on the OpenAirInterface platform is also discussed, along with results from field trials and comparisons with simulation results. This distributed MIMO receiver has potential applications in single-frequency cellular and mesh networks. OpenAirInterface is a real-time hardware and software platform for wireless communication and signal processing experiments. Its current version includes a full software modem for cellular and mesh network topologies.
411180	41118011	Guard zone based D2D underlaid cellular networks with two-tier dependence	D2D communication is a promising feature in 5G networks due to its potential to enhance network efficiency. However, its implementation in current cellular networks can cause interference between D2D and macrocell tiers. To address this issue, a D2D underlaid cellular network is proposed, where D2D users can access the cellular spectrum with guard zones to avoid interference from macro base stations and other D2D transmitters. Using spatial models and analytical results, the D2D and cellular coverage probability as well as the D2D area spectral efficiency are studied. An optimal guard zone radius is derived to maximize the D2D network throughput. Simulation results confirm the accuracy of the proposed analytical expressions.
411180	41118045	Performance Analysis of Network-Assisted D2D Discovery in Random Spatial Networks.	Device-to-device (D2D) discovery is an important step for the direct exchange of data between nearby cellular users. This process can be initiated by D2D-enabled devices themselves or through core network functions. The latter is known as network-assisted D2D discovery. One major benefit of network-assisted D2D discovery is its ability to accurately estimate proximity between devices. This allows for more efficient and reliable communication between users. Overall, D2D discovery is crucial for enabling direct communication between devices in close proximity and has the potential to improve the overall user experience in cellular networks.
411181	41118136	Enhanced formulations and branch-and-cut for the two level network design problem with transition facilities.	This article introduces the Two Level Network Design Problem with Transition Facilities (TLNDF), which combines network design and facility location aspects. The objective is to find a minimum cost subtree connecting two types of customers while serving primary customers with a primary subtree that is embedded in a secondary subtree. This problem is modeled on an extended graph with additional arcs representing facility installation. A cut set based model is proposed and theoretical results are presented on relating cut set inequalities on the extended graph to the original graph. A branch-and-cut algorithm is developed and its efficiency is confirmed through a computational study. 
411181	4111814	The Generalized Regenerator Location Problem	The paper discusses the problem of finding the optimal placement of regenerators in an optical network, where a signal can only travel a certain distance before needing to be regenerated. This is important because regenerators are expensive, so it is desirable to use as few as possible. The authors introduce the generalized regenerator location problem (GRLP), which is a more general version of the previously studied regenerator location problem (RLP). They propose a heuristic framework and establish a mathematical correspondence between the GRLP and other problems, allowing for the development of integer programming models. The paper also presents an exact approach for solving the GRLP, which is effective for smaller networks, while the heuristic framework is better suited for larger networks.
411182	41118231	Exact solution of the robust knapsack problem.	The uncertain variant of the knapsack problem involves items with weight ranges and a limit on the number of items with unexpected weights. A dynamic programming algorithm is proposed for this problem, with techniques to decrease its complexity. The performance of this algorithm is compared to other exact algorithms for robust optimization problems through computational analysis.
411182	41118245	Competitive multi-agent scheduling with an iterative selection rule.	This work focuses on solving deterministic scheduling problems where multiple agents compete for the usage of a single machine. The agents have their own objectives and tasks are sequenced by selecting the shortest one in each step. The problem is examined in two different settings and with various cost functions. In a centralized perspective, the set of Pareto efficient solutions is characterized and the number of solutions and computational complexity are determined. From a single agent perspective, two heuristic algorithms, SPT and WSPT, are analyzed in terms of performance for different objectives. 
411183	41118328	Performance Evaluation of Reverse Engineering Relational Databases into Extended Entity-Relationship Models	This article discusses a method for automatically converting a relational database into an extended Entity-Relationship (EER) schema. The focus is on two key factors for evaluating the performance of this process: the extent to which it can be fully automated, and its efficiency. The latter is analyzed through time complexity analysis of the pseudo-code algorithms used in the reverse engineering process. By addressing these issues, the method aims to provide a high level of automation and efficiency in converting databases into EER schemas.
411183	4111836	Reverse engineering of relational databases: extraction of an EER model from a relational database	The article discusses a methodology for creating an extended Entity-Relationship (EER) model from a relational database. The process involves analyzing both the data schema and data instances to derive a more comprehensive and understandable EER model. The article also presents classification schemes for relations and attributes needed for the extraction, which have been shown to be usable in a knowledge-based system. A prototype system that implements this process is briefly discussed, and instances where human input is necessary are identified. The research demonstrates that the database reverse engineering can be automated to a high degree.
411184	41118468	A knowledge-based object modeling advisor for developing quality object models	Object models, also known as class diagrams, are commonly used in information system requirements to illustrate classes, attributes, operations, and relationships. However, developing high-quality object models can be difficult, particularly for inexperienced analysts in business environments. To address this challenge, a knowledge-based system has been developed as an extension to an open source CASE tool. This system uses an ontology of quality problems, based on a conceptual model quality framework, related empirical studies, and analysis patterns to provide recommendations for improving the quality of object models. An empirical evaluation of the prototype has shown that the system is effective in improving the semantic quality of object models, particularly in terms of model completeness. 
411184	41118457	An Ontology-Based Framework for Generating and Improving Database Design	The need to integrate domain knowledge into system development tools is widely recognized. While many existing case tools have been effective in supporting design based on syntax, they often lack the ability to incorporate information about the semantics of a specific application domain. This research proposes a framework for generating and analyzing conceptual database designs using ontologies. The framework has been implemented in a prototype database design assistant, demonstrating the effectiveness of the approach. This approach has the potential to greatly improve the accuracy and efficiency of database design by leveraging domain knowledge.
411185	41118527	Emotions and Multimodal Interface-Agents: A Sociological View	Creating user-friendly human-computer interfaces is crucial for the effective use of computer technology. As information systems become more complex, interface design has evolved to incorporate advanced concepts such as agent technology. Emotions play a significant role in human-computer interactions, making emotional agents increasingly important. The use of multimodal interfaces has also become more prevalent, allowing for more powerful communication between humans and computers. Combining the concepts of agents and multimodality can be beneficial, and sociological theories on emotion can provide valuable insights. The DFG-research project "Sozionik" uses sociological theory to enhance computational systems, specifically hybrid societies. Incorporating sociological perspectives can lead to a more accurate understanding of emotions in interface design, as emotions are also influenced by social norms and rules. Therefore, a sociologically-based user model and an agent's "social self" model are necessary for effective emotional agent design.
411185	41118539	Emotion: Theoretical investigations and implications for artificial social aggregates	The micro-macro link between individual action and social structure is a crucial topic in the social sciences and distributed artificial intelligence research. Emotion has been identified as a key component in understanding this link, but current sociological theories do not fully incorporate research from other disciplines. To address this, an integrated view on emotion is proposed, drawing from theories and models in cognitive science, psychology, neuroscience, and computer science. This approach aims to bridge the gap between specific aspects of emotion theory and general sociological theories of societal structuration. An exemplifying multi-agent architecture is used to illustrate this issue.
411186	41118639	Quantification of small cerebral ventricular volume changes in treated growth hormone patients using nonrigid registration.	Nonrigid registration is a technique used to measure small changes in the volume of anatomical structures over time. In this study, a nonrigid registration algorithm was used to measure changes in brain ventricle volume in a group of patients receiving growth hormone replacement therapy and a control group. The results showed a statistically significant difference in volume changes between the two groups. The accuracy of the measurements was validated by comparing them to previously published estimates and by determining the precision from three consecutive scans of volunteers. The results also showed a high level of shape correspondence between manually segmented ventricles and those obtained through segmentation propagation.
411186	4111864	Comparison of phantom and registration scaling corrections using the ADNI cohort.	Serial magnetic resonance (MR) studies can be used to measure the rate of brain atrophy in patients with Alzheimer's disease (AD), which can be helpful in assessing the effectiveness of therapies. However, changes in scanner voxel sizes can impact the accuracy of these measurements. To address this issue, the Alzheimer's Disease Neuroimaging Initiative (ADNI) included scanning a geometric phantom with every patient scan. This study compared the effectiveness of this method with a 9 degrees of freedom (9DOF) registration algorithm. The results showed that the registration algorithm was unbiased and had a similar effect on atrophy measurement as the geometric phantom correction. This suggests that the additional expense and effort of using a phantom can be avoided by using registration-based scaling correction. Additionally, using this correction method could potentially lower sample size requirements by about 10-12%.
411187	411187107	Mesh Connected Computers with Fixed and Reconfigurable Buses: Packet Routing and Sorting	Mesh connected computers have become popular due to their unique features. This paper discusses two variations of the mesh model - one with fixed buses and the other with reconfigurable buses. These models have been extensively researched and we provide solutions for important problems related to packet routing and sorting. We present lower and upper bounds for routing on a linear array and k驴k routing and sorting on a 2D mesh. Our improved algorithms for 1 驴 1 routing and matching sorting algorithm show better time bounds than existing ones. We also introduce greedy algorithms for various routing and sorting problems, which have better average performance and matching lower bounds. We also demonstrate that sorting can be achieved in logarithmic time on a mesh with fixed buses.
411187	41118735	Permutation Routing and Sorting on the Reconfigurable Mesh	This paper showcases the effectiveness of reconfiguration in improving the efficiency of randomized algorithms for packet routing and sorting on a reconfigurable mesh connected computer. The run times of these algorithms are shown to outperform conventional mesh time bounds. The authors introduce a new variation of the reconfigurable mesh called Mr, and also utilize the PARBUS model. This highlights the potential of reconfiguration in enhancing performance in computer systems.
411188	41118861	Studying compiler optimizations on superscalar processors through interval analysis	This paper explores the relationship between compiler optimizations and the performance of superscalar processors. Using interval analysis, the authors break down execution time into different components and study how compiler optimizations affect each component. This helps to understand the impact of optimizations on out-of-order processor performance. The analysis reveals interesting insights and provides suggestions for future research in this area. The authors also compare the effects of compiler optimizations on out-of-order and in-order processors. Overall, this paper highlights the complex interaction between compiler optimizations and microarchitecture, and the importance of considering this relationship in optimizing processor performance.
411188	41118828	A mechanistic performance model for superscalar out-of-order processors	The article discusses a mechanistic model for out-of-order superscalar processors and its application to the study of microarchitecture resource scaling. The model divides execution time into intervals separated by disruptive miss events, and uses the type and length of each interval to predict execution time. The model provides several advantages over prior approaches and has an average difference of 7% from detailed simulations. The model is then used to determine size relationships among microarchitecture structures and to study the scaling of pipeline depth and width. The results confirm previous findings and also show that overprovisioned processors can improve overall performance in certain situations. 
411189	41118976	INTERACTIVE FUZZY PROGRAMMING BASED ON FRACTILE CRITERION OPTIMIZATION MODEL FOR TWO-LEVEL STOCHASTIC LINEAR PROGRAMMING PROBLEMS	This article discusses two-level linear programming problems with random variable coefficients in objective functions and constraints. It proposes using the concept of chance constrained programming to transform these problems into deterministic ones, based on a fractile criterion optimization model. The article also introduces the use of fuzzy goals for objective functions and presents a fusion of stochastic and fuzzy approaches called interactive fuzzy programming. An illustrative numerical example is provided to demonstrate the effectiveness of this method.
411189	41118977	An interactive fuzzy satisficing method for multiobjective linear programming problems with random variable coefficients through a probability maximization model	This paper discusses two main methods for dealing with randomness and ambiguity in mathematical programming problems: stochastic programming and fuzzy programming. It specifically focuses on multiobjective linear programming problems with random variable coefficients. The paper proposes a probability maximization model to transform stochastic problems into deterministic ones and presents an interactive fuzzy satisficing method, which combines elements of both stochastic and fuzzy approaches, to find a satisfactory solution based on the decision maker's fuzzy goals. An example is included to demonstrate the effectiveness of this method.
411190	41119026	A robust analysis, detection and recognition of facial features in 2.5D images	The article discusses a method for recognizing 3D faces using 2.5D images. The method involves smoothing and converting 3D mesh face images to 2.5D range images. The nose-tip, as the most prominent feature on the face, is detected using corner points and curvedness values. Other facial landmarks are then located and a facial graph is generated. The feature space is described as a 3D function that maps depth values to pixel intensities. The extracted features are of dimensionality [1ź×ź21] and are used to classify face images using Multilayer Perceptron (MLP) and Support Vector Machines (SVM). The method has achieved maximum recognition rates of 75-87.5% on various databases.
411190	4111906	Illumination, Pose And Occlusion Invariant Face Recognition From Range Images Using Erfi Model	The authors of this paper focus on recognizing 3D face images from range images in an unconstrained environment, where factors like illumination, pose, and occlusion can vary. This is a challenging task in face recognition. The authors use the ERFI model to normalize the face images in terms of pose and occlusion. They also conduct both quantitative and subjective analyses, and use synthesized datasets to test the performance of recognition using SIFT and SURF features from Frav3D and Bosphorus databases. They also combine these feature sets using weighted fusion and classify them using K-NN and Sequence Matching Technique, achieving high recognition rates of 99.17% and 98.81%.
411191	41119160	Human Face Recognition using Gabor based Kernel Entropy Component Analysis.	This paper introduces a new method, called Gabor wavelet based Kernel Entropy Component Analysis (KECA), which combines Gabor wavelet transformation (GWT) and KECA for improved face recognition. The GWT is used to extract discriminative facial features that can handle variations in illumination and facial expressions. Then, KECA is extended to include cosine kernel function, and applied to the extracted features to obtain only the most relevant eigenvectors with positive entropy contribution. These features are then used for image classification using distance measures such as L1, L2, Mahalanobis, and cosine similarity. The method was tested on various face databases and showed promising results in recognizing both frontal and pose-angled faces.
411191	41119131	High Performance Human Face Recognition using Independent High Intensity Gabor Wavelet Responses: A Statistical Approach	In this paper, a new technique for improved face recognition is presented. The technique combines high-intensity feature vectors extracted from Gabor wavelet transformation with Independent Component Analysis (ICA). First, the high-intensity feature vectors are automatically extracted using the local characteristics of each face. Then ICA is applied to these vectors to obtain independent high-intensity feature (IHIF) vectors, which are used for image classification. This approach is advantageous because the Gabor wavelet transformation captures important spatial and orientation selectivity features, while ICA reduces redundancy and represents independent features. The effectiveness of this method is demonstrated through experiments on frontal facial images from various databases.
411192	411192134	Word-Level Script Identification Using Texture Based Features	Script identification is an important area of research in document image analysis. It is necessary for many post-processing tasks such as document sorting, machine translation, and text searching in multilingual environments. To accurately recognize scripts, a robust word-level handwritten script identification technique has been proposed in this paper. It uses texture-based features, including Histograms of Oriented Gradients (HOG) and Moment invariants, to identify words in seven popular scripts. The technique has been tested on 7000 handwritten words and achieved an overall accuracy of 94.7% using 5-fold cross-validation. The chosen classifier, Multi-Layer Perceptron (MLP), was also tested with different folds and epoch sizes. This paper is an extended version of a previous work and presents significant results in identifying complex and varied scripts.
411192	41119223	Word-Level Script Identification from Handwritten Multi-script Documents.	The paper proposes a robust technique for identifying handwritten scripts at the word level. It combines shape and texture features to distinguish between five scripts: Bangla, Devnagari, Malayalam, Telugu, and Roman. The technique is tested on 3000 words, with each script contributing 600 words. A feature set of 87 elements is used to evaluate the accuracy of the technique. After testing with multiple classifiers, the Multi Layer Perceptron (MLP) is chosen as the best classifier, achieving a recognition accuracy of 91.79% with 5-fold cross validation and an epoch size of 500. This is impressive considering the shape variations of the scripts. 
411193	41119334	Face Synthesis (FASY) System for Generation of a Face Image from Human Description	The paper discusses a new approach to generating a face based on a human-like description. The FASY System is a database retrieval and face generation system that is currently being developed. It has the capability to generate a requested face that is not found in the existing database, allowing for continuous expansion. This new concept has the potential to enhance the accuracy and diversity of face generation.
411193	41119317	FPGA Based Assembling of Facial Components for Human Face Construction	The paper discusses the development of a system called FASY (FAce SYnthesis) that aims to generate a new face from textual description. This system has a unique feature of being able to generate a requested face if it is not found in the existing database. The new face generation process involves three steps - searching, assembling, and tuning. The tuning phase is achieved using hardware description language and implemented on a Field Programmable Gate Array (FPGA) device. The system focuses on adjusting intensities and assembling components to generate a realistic face. The paper presents the details of the implementation and the use of VHDL for this purpose.
411194	41119429	Examples of Independence for Imprecise Probabilities	This paper discusses the concept of independence for imprecise probabilities. The authors argue that there are multiple definitions of independence that are relevant in different scenarios. They provide simple examples to illustrate the different definitions and their relationships. Overall, the paper aims to clarify the meaning of independence for imprecise probabilities.
411194	41119472	Determining dependence relations using a new score based on imprecise probabilities	This paper presents a new score for determining the independence of two categorical variables in data analysis. This interval-valued score is based on the Heckerman, Geiger, and Chickering's score and can be used in supervised classification tasks. The authors compare this score with other measures such as the Bayesian score metric, Bayesian information criterion, Chi-square test, and upper entropy score based on imprecise probabilities. The results show that the new score behaves similarly to statistical tests for small samples and Bayesian procedures for large samples, making it suitable for certain types of problems.
411195	41119513	Modelling and inference with Conditional Gaussian Probabilistic Decision Graphs	Probabilistic Decision Graphs (PDGs) are graphical models that represent a joint probability distribution using a decision graph structure. They can capture specific independence relations that other graphical models cannot, making them more efficient for certain operations. Previous PDGs were only defined for discrete variables, but we can now extend them to include continuous variables by assuming a Conditional Gaussian joint distribution. This allows for efficient inference to be carried out.
411195	41119532	Approximate probability propagation with mixtures of truncated exponentials	Truncated exponentials are a useful tool for hybrid Bayesian networks, providing an alternative to discretization. One advantage of the MTE model is that standard propagation algorithms can be used. However, due to high complexity, approximate methods are necessary. In this paper, a new approximate propagation algorithm for MTE networks is proposed, based on the Penniless method for discrete variables. The use of Markov Chain Monte Carlo for probability propagation is also explored. Experiments with random networks show the performance of these methods.
411196	41119644	Adaptive multi-task monitoring system based on overhead prediction	The growth and success of the Internet has made managing and monitoring ISP networks a complex process. To address this, a new monitoring system has been proposed that can adapt to network conditions and varying traffic. It utilizes an optimization method that predicts overhead and uses a weighted utility function to handle multiple monitoring tasks. To evaluate its performance, an experimental methodology is presented and the system is tested on different parameters. The results show that the proposed system effectively adjusts to changing network conditions and can handle multiple monitoring tasks efficiently. 
411196	41119617	Network-wide monitoring through self-configuring adaptive system.	The rapid growth of the Internet and its diverse applications pose challenges for ISPs in managing and monitoring their networks. This results in a mismatch between existing monitoring solutions and the increasing demands for network management. To address this, an adaptive centralized architecture is designed that uses a cognitive monitoring system to provide visibility across the entire network. This architecture adjusts the sampling rates on network routers to achieve maximum accuracy while adapting to changes in network traffic. An accounting application is used to demonstrate the effectiveness of this system in estimating aggregate flow volumes. The paper presents a comprehensive analysis of the system's performance and its behavior under different conditions, which is validated through experiments on a dedicated platform.
411197	41119710	A comprehensive solution to the XML-to-relational mapping problem	The use of relational database management systems (RDBMSs) for storing and querying XML data has gained attention due to their reliable data management services. However, there is a mismatch between the relational and XML data models, requiring the XML data to be shredded and loaded into relational tables before querying. Existing solutions for this problem are limited and often tied to a specific backend and use proprietary mapping languages. To address these limitations, ShreX has been developed as a comprehensive and end-to-end solution for XML-to-relational mapping. ShreX uses annotations to an XML Schema for mapping, making the process simpler and allowing for various mapping strategies to be combined. It also provides automatic document shredding and query translation capabilities and is portable across different database backends.
411197	41119729	FleXPath: flexible structure and full-text querying for XML	This paper explores the challenges of integrating database-style query languages, such as XPath and XQuery, with full-text search for querying XML documents. While keyword search is based on approximate matching, XPath requires exact matches, creating a mismatch between the two approaches. To address this, the authors propose a framework that considers queries on structure as a "template" and uses primitive operators to find the best match for both structure and full-text search. They also discuss principles for ranking schemes and propose efficient algorithms for answering top-K queries. Results from experiments demonstrate the effectiveness and scalability of their proposed framework. 
411198	41119842	Prioritizing risk pathways: a novel association approach to searching for disease pathways fusing SNPs and pathways.	Complex diseases are influenced by both mutated risk genes and environmental factors. Traditional methods for identifying susceptibility genes only consider a single-gene disease model, while pathway-based approaches take into account the joint effects of genetic factors and biological networks. With the availability of high-throughput SNP datasets and human biologic pathways, bioinformatics methods can be used to search for risk pathways associated with complex diseases. The proposed approach, called Prioritizing Risk Pathways fusing SNPs and pathways (PRP), uses a risk-scoring measurement to prioritize risk biologic pathways. This approach was applied to five complex diseases and revealed both shared and specific risk pathways, which was supported by literature research. Overall, this approach provides a new perspective for understanding the pathogenesis of complex diseases and generating new hypotheses.
411198	41119851	A New Sparse Rule-Based Fuzzy Reasoning Method	The paper introduces a new fuzzy reasoning method for sparse fuzzy rule bases, using similarity relations between fuzzy sets. This method allows for easy and accurate inference, producing normal and convex fuzzy sets without any restrictions. The authors believe this method has potential for use in real-world fuzzy applications.
411199	41119927	Reliability and route diversity in wireless networks	In this study, the issue of communication reliability in wireless networks is examined in the context of fading environments using the outage probability approach. The disconnect probability, which measures the likelihood of a transmission not being correctly received by any other node in the network, is calculated for one and two dimensional random networks. The end-to-end reliability of multi-hop transmissions is also evaluated using the outage probability metric, and algorithms are developed to determine the most reliable route while considering power constraints or minimum energy usage with a reliability constraint. Additionally, the relationship between outage probability and transmission power is explored, with and without route diversity.
411199	41119956	Achievable Rates and Scaling Laws of Power-Constrained Wireless Sensory Relay Networks	This paper discusses the achievable rates and scaling laws of power-constrained wireless relay networks in the wideband regime. The network consists of a source node, a destination node, and multiple intermediate relay nodes, with no prior knowledge of channel-state information. The study focuses on the joint asymptotic regime of the number of relay nodes, the channel coherence interval, and the bandwidth or signal-to-noise ratio per link. The authors first examine narrowband relay networks in the low SNR regime, specifically using the amplify-and-forward scheme with network training. They provide an equivalent channel model and characterize the achievable rate. The study is then generalized to power-constrained wideband relay networks, taking into account frequency-selective fading. The achievable rates for two power allocation policies are examined, and conditions for achieving the desired scaling law are identified. Surprisingly, the two power allocation policies result in achievable rates of the same scaling order, under the condition that the energy per fading block per subband is bounded below and the bandwidth is sublinear in the number of relay nodes.
411200	41120021	Completeness bounds and sequentialization for model checking of interacting firmware and hardware	The trend of implementing complex system management functions in firmware (FW) requires support for verifying FW within its hardware (HW) environment. A unified HW-FW model can help identify commonly-occurring interaction patterns between HW and FW, allowing for more efficient verification. This paper introduces a bounded model checking (BMC)-based methodology for FW verification, using static analysis techniques to determine the completeness bound. By exploiting commonly-occurring FW code patterns and sequentializing concurrent FW and HW code, software model checkers can be directly applied for verification. The authors have implemented a completeness bound analyzer and sequentializer to automate this process, and successfully evaluated the tool on three real FW benchmarks with multiple correctness properties. 
411200	41120010	Predicting serializability violations: SMT-Based search vs. DPOR-Based search	The authors of this paper have developed a method for detecting serializability violations in concurrent programs using predictive analysis. They use a graph-based approach to create a predictive model from a test execution and then explore this model using stateless model checking with dynamic partial order reduction (DPOR). However, this method is not effective on all benchmarks, so the authors propose an alternative using symbolic exploration with SMT solvers. They describe the details of both methods and compare their performance through experimental evaluation. This allows for predicting the preferred technique for a given instance based on its characteristics. This is the first work to use SMT-based encodings for detecting serializability violations with any number of threads and variables.
411201	41120134	On the Linear Convergence of the ADMM in Decentralized Consensus Optimization	Decentralized consensus optimization is a collaborative approach where agents in a connected network work together to minimize the sum of their individual objective functions over a shared decision variable. Information exchange between neighbors is limited in this process. The alternating direction method of multipliers (ADMM) is often used for this problem, involving iterative computation at each agent and information exchange between neighbors. This method has been found to be efficient and powerful. This paper establishes the linear convergence rate of ADMM for this problem when the local objective functions are strongly convex. The convergence rate is determined by the network topology, properties of local objective functions, and algorithm parameters. This result not only serves as a performance guarantee, but also provides guidance for speeding up the convergence of ADMM.
411201	41120112	A Proximal Gradient Algorithm For Decentralized Nondifferentiable Optimization	This paper focuses on solving the decentralized consensus optimization problem in a networked multi-agent system. Each agent has its own local objective and can only communicate with its neighbors. The proposed algorithm, PG-EXTRA, uses a fixed step size and takes into account the separable problem structure, making it suitable for applications where the local objective is a sum of differentiable and nondifferentiable parts. When the nondifferentiable part is zero, PG-EXTRA reduces to the existing EXTRA algorithm, and when the differentiable part is zero, it becomes P-EXTRA. Convergence and rate of convergence are proven for PG-EXTRA, and numerical experiments on a decentralized compressive sensing problem support the theoretical results.
411202	4112023	Constraint Handling In Multi-Objective Evolutionary Optimization	This paper presents a new method for handling constraints in multi-objective evolutionary algorithms. It uses adaptive penalty functions and distance measures to modify the objective space, allowing the algorithm to evolve optimal solutions from both feasible and infeasible spaces. The search in the infeasible space focuses on individuals with better objective values and low constraint violations. The number of feasible individuals in the population guides the search process towards finding more feasible or optimum solutions. The proposed method is easy to implement and does not require parameter tuning. It has been tested on various constrained multi-objective problems and has shown promising results. 
411202	4112020	An adaptive penalty formulation for constrained evolutionary optimization	The paper introduces an adaptive penalty function for using genetic algorithms to solve constrained optimization problems. The method focuses on utilizing infeasible individuals with low objective value and constraint violation to guide the search process towards finding feasible individuals or the optimal solution. It is easy to implement and does not require parameter tuning. The algorithm's performance is evaluated on 22 benchmark functions, demonstrating its ability to find high-quality solutions comparable to existing methods. Additionally, it consistently finds feasible solutions for all tested functions.
411203	41120324	Friends or foes? on planning as satisfiability and abstract CNF encodings	Planning as satisfiability is a method used to find parallel step-optimal plans, implemented in tools like SATPLAN. However, a limitation of this approach is proving the absence of plans of a certain length. To address this, the idea of using solution length preserving abstractions of the original planning task is explored. This is promising as the abstraction may have a smaller state space, similar to successful methods in model checking. However, when evaluated empirically, even handmade abstractions do not improve the performance of SATPLAN. This is due to the fact that in many cases, the shortest resolution refutation for the abstracted planning task is longer than that of the original task. This suggests a fundamental weakness in the approach and calls for further investigation into the relationship between transition-systems, abstractions, and SAT encodings.
411203	4112038	The metric-FF planning system: translating "Ignoring delete lists" to numeric state variables	Numeric state variables have been a challenge in planning for a long time, and were part of the 3rd International Planning Competition (IPC-3). One popular approach in STRIPS planning is using a heuristic function based on relaxing the planning task by ignoring delete lists. This approach has now been extended to handle numeric state variables, as long as the task is "monotonic". The subset of the IPC-3 competition language known as "linear tasks" can achieve monotonicity through pre-processing. The heuristic planning system FF has been extended to handle linear tasks, resulting in the highly efficient Metric-FF system, which was one of the top two performers in the IPC-3 competition.
411204	41120411	HMC: verifying functional programs using abstract interpreters	Hindley-Milner-Cousots (HMC) is an algorithm that simplifies the process of verifying safety properties of higher-order functional programs. It does this by breaking down the problem into interprocedural analysis for first-order imperative programs. This is achieved by using the type structure of the functional program to generate logical refinement constraints, which are then transformed into a first-order imperative program and an invariant. This invariant is used with an invariant generator for first-order imperative programs to verify the safety of the source program. HMC has been implemented and has shown promising results in verifying OCAML programs using two imperative checkers. By combining type-based and state-based reasoning, HMC offers a fully automatic way to verify programs written in modern programming languages. 
411204	41120437	Constraint solving for interpolation	Interpolation is a crucial part of modern methods used in program verification. It helps identify the difference between 'good' and 'bad' states. However, current interpolation algorithms rely on constructing proofs, which is a challenging process. This study presents a new algorithm for generating interpolants for the combined theory of linear arithmetic and uninterpreted function symbols. Unlike previous methods, this algorithm does not require pre-constructed proofs and instead utilizes constraint solving in linear arithmetic. This allows for the use of highly efficient Linear Programming solvers, making the process more practical. Experimental results demonstrate the effectiveness of this approach. 
411205	41120525	A Flexible Stochastic Automaton-Based Algorithm for Network Self-Partitioning	The article presents a flexible and distributed algorithm for network partitioning using stochastic automata. This algorithm is able to find the optimal k-way partition for various cost functions and constraints in directed and weighted graphs. The article discusses the motivation for the distributed partitioning problem, introduces the algorithm, and shows that it has a high probability of finding the optimal partition. It also explains why the algorithm is efficient and provides examples of its performance. The article concludes by exploring the potential applications of this algorithm in mobile/sensor classification, fault-isolation in power systems, and control of autonomous vehicles.
411205	41120520	A flexible algorithm for sensor network partitioning and self-partitioning problems	The article introduces a new algorithm for network partitioning that is suitable for sensor networking and autonomous vehicle control applications. The algorithm is flexible and distributed, allowing for optimal k-way partitioning with various cost functions and constraints in directed and weighted graphs. The need for such an algorithm is motivated and the details of the algorithm are reviewed. The algorithm can also be used for self-partitioning and has potential applications in mobile/sensor classification in ad hoc networks and other areas.
411206	41120630	Enhancing Product Detection With Multicue Optimization For Tv Shopping Applications	Smart TVs offer a more immersive viewing experience by allowing consumers to watch TV, use applications, and access the internet. However, the current technology still lacks the ability for seamless interaction with the content being streamed, particularly in TV-enabled shopping. This means that if a consumer sees a product they are interested in purchasing while watching a TV show, they must switch to a different screen or device to make the purchase. To address this issue, a multicue product detection framework is proposed for TV shopping. This framework utilizes appearance, topological, and spatio-temporal cues to detect complex products, improving the precision of the results. This approach allows for easier and more convenient purchasing for consumers through their smart TV.
411206	41120668	Automatic Video Event Detection For Imbalance Data Using Enhanced Ensemble Deep Learning	The rapid increase in multimedia data has made detecting events in videos a difficult task. Additionally, when the data is imbalanced, detecting interesting events becomes even more challenging. Deep learning has become a crucial aspect of many AI systems, but a single model cannot perform well for all applications. Therefore, a new ensemble deep learning framework is proposed in this paper to address these issues. This framework includes multiple deep learning feature extractors and an enhanced ensemble algorithm to handle over-fitting and imbalanced data. The Support Vector Machine (SVM) classifier is used as the final layer and weak learners in the ensemble module. The framework is tested on two large-scale and imbalanced video datasets, and the results show its effectiveness in comparison to other deep learning methods and conventional features with different classifiers.
411207	411207103	Correlation-Assisted Imbalance Multimedia Concept Mining And Retrieval	In recent years, there has been a significant increase in multimedia data due to the rise of social media and smart devices. This has made the task of mining and retrieving useful information from multimedia data, which includes texts, images, and videos, more important. However, the large amount of data and the gap between low-level features and high-level concepts make this task challenging. To address this, understanding the correlations among classes can help bridge this gap. Additionally, many real-world datasets have imbalanced class distributions, where minority instances represent important concepts such as frauds, intrusions, and unusual events. Despite efforts to address this issue, imbalanced concept retrieval remains a difficult problem. This paper proposes a new model that analyzes the correlation between retrieval scores and labels to improve imbalanced concept mining and retrieval, even with low scores from minority classes. Experiments on benchmark datasets show promising results for this framework.
411207	41120784	Negative Correlation Discovery for Big Multimedia Data Semantic Concept Mining and Retrieval	The increase in data production has made traditional processing methods insufficient. The effective management and retrieval of this big data is still being researched. One of the most challenging topics is multimedia high-level semantic concept mining and retrieval in big data, which requires collaboration between researchers in big data mining and multimedia. To overcome the semantic gap between high-level concepts and low-level visual features, correlation discovery is crucial. However, it is a computationally intensive task that requires analysis of large and growing repositories. This paper presents a novel system for discovering negative correlation in semantic concept mining and retrieval, using Hadoop MapReduce and Spark. Experimental results show the success of utilizing big data technologies in this process.
411208	4112081	Detection Of Partial-Band Noise Interference In Slow Fh/Qpsk Systems	This paper presents a detection algorithm for detecting unknown partial-band noise interference (PBNI) in slow frequency-hopped spread spectrum (SFH-SS) systems. The algorithm uses a simple hypothesis testing approach and a derived threshold level to detect the PBNI after dehopping. It utilizes the outputs of two correlators used for QPSK demodulation and sums them over one hop duration. The probability of detection (P-D) is formulated as a performance measure for the detector, and both analytical and simulation results demonstrate its effectiveness in detecting PBNI even at low interference power. Furthermore, the algorithm can also be applied to detect multitone interference (MTI) in SFH-SS systems. 
411208	4112085	Detection of Unknown Multitone Interference Using the AR Method in Slow FH/BFSK Systems over Rayleigh Fading Channels	The paper proposes a detection algorithm for unknown multitone interference (MTI) in frequency-hopped spread spectrum (FH-SS) systems over a Rayleigh fading channel and additive white Gaussian noise. The algorithm uses an autoregressive (AR) spectral estimation method and is performed in the AR coefficient domain. The incoming received data is divided into non-overlapping segments and the Yule-Walker (YW) AR method is applied to each segment to obtain the AR coefficient vector. These coefficients are then used to obtain the spectrum estimate of the MTI through an averaging process. The algorithm is compared to a fast Fourier transform (FFT) based technique through computer simulations, which show that the proposed method performs well even at low signal-to-noise power ratio (SNR). The results also suggest that the proposed AR detection technique is more accurate and reliable than the FFT-based technique, making it suitable for use in slow FH-SS systems for MTI detection.
411209	41120986	A Semidefinite Relaxation Approach for Beamforming in Cooperative Clustered Multicell Systems With Novel Limited Feedback Scheme	This paper discusses suboptimal cooperative beamforming strategies to maximize the user sum rate in a multicell system with limited feedback. The problem is decoupled into subproblems using the uplink-downlink duality theorem. A semidefinite relaxation technique is used to convert the nonconvex problem into a convex semidefinite programming problem, which can be efficiently solved in polynomial time. A novel limited feedback scheme based on compressive sensing is proposed to obtain high-quality channel state information (CSI). The paper also considers a scenario where base stations have different estimates of the same CSI and investigates the channel quantization criterion. Numerical results show the effectiveness of the proposed beamforming algorithm and limited feedback schemes.
411209	41120931	Secure communication over MISO cognitive radio channels	This paper discusses the issue of physical-layer security for a secondary user (SU) in a spectrum-sharing cognitive radio network (CRN) from an information-theoretic perspective. The focus is on a secure multiple-input single-output (MISO) cognitive radio channel, where a multi-antenna SU transmitter (SU-Tx) sends confidential information to a legitimate SU receiver (SU-Rx) while protecting it from an eavesdropper on the licensed band of a primary user (PU). The secrecy capacity of the channel is determined by finding the capacity-achieving transmit covariance matrix under power and interference constraints. Two approaches are proposed to solve this problem, and three suboptimal schemes are presented to reduce computational complexity. Simulation results show that these suboptimal schemes can approach the secrecy capacity under certain conditions.
411210	41121033	An abstract machine for efficiently computing queries to well-founded models	The well-founded semantics is a popular approach because it is a type of skeptical semantics that considers unknown atoms instead of assigning them a true or false value. This allows for more efficient computation and makes it suitable as a basis for non-monotonic reasoning. The Warren Abstract Machine (WAM)-based abstract machine, known as the SLG-WAM, uses tabling to efficiently compute the well-founded semantics and requires extensions to handle non-ground normal logic programs. These extensions include representing answers that are neither true nor false and implementing delay and simplification operations. The efficiency of this implementation is demonstrated through a theorem and performance results.
411210	41121029	Memory management for Prolog with tabling	Tabling, a feature in Prolog that allows for efficient reuse of computed answers, can be implemented using SLG-WAM in a WAM-based Prolog system. XSB, a Prolog system, currently uses SLG-WAM for tabling. However, the memory model in XSB is complex and attempts to build a precise garbage collector have been unsuccessful. CAT is a newer alternative to SLG-WAM, which suspends consumers by copying parts of the execution stacks. The memory model in CAT is simpler, making it easier to design a more precise garbage collector. This paper discusses the memory management of tabling in Prolog systems, whether using SLG-WAM or CAT. To address potential space issues with CAT, the paper suggests implementing a minor garbage collection upon creation of CAT areas. The effectiveness of this approach is also discussed.
411211	41121142	Self-regulating finite automata	Self-regulating finite automata are introduced and discussed in this paper. These automata use a sequence of rules from previous moves to regulate their rule usage. The concept of turns, where a new self-regulating sequence of moves begins, is given special attention. Two infinite hierarchies of language families are established based on the number of turns, and it is shown that these hierarchies align with those resulting from parallel right linear grammars and right linear simple matrix grammars. The paper also compares these hierarchies and suggests exploring self-regulating pushdown automata, noting that they do not result in infinite hierarchies like those achieved by self-regulating finite automata.
411211	4112115	An infinite hierarchy of language families resulting from stateless pushdown automata with limited pushdown alphabets	A stateless pushdown automaton is a type of computational model that does not have states. This means that its computational steps are determined solely by the currently scanned symbol and the top element of its pushdown store. This paper focuses on stateless pushdown automata with a restricted pushdown alphabet size. It presents an infinite hierarchy of language families that can be recognized by such automata. The paper also discusses similar results for stateless deterministic and real-time pushdown automata. It concludes with an open problem for further research.
411212	41121294	Learning convex combinations of continuously parameterized basic kernels	This study focuses on finding the optimal kernel for minimizing a regularization error functional, commonly used in regularization networks and support vector machines. The kernel is assumed to be a combination of basic kernels, such as Gaussian kernels, which are continuously parameterized by a compact set. The study proves that there is always an optimal kernel that can be expressed as a convex combination of at most m+1 basic kernels, where m is the sample size. It also provides a necessary and sufficient condition for a kernel to be optimal. The study's proof is constructive and leads to a greedy algorithm for learning the optimal kernel. Preliminary numerical simulations are presented to demonstrate the algorithm's properties.
411212	41121284	Combining flat and structured representations for fingerprint classification with recursive neural networks and support vector machines	New fingerprint classification algorithms have been developed using support vector machines (SVMs) and recursive neural networks (RNNs). RNNs are trained on a structured representation of the fingerprint image and can extract distributed features which are then integrated into the SVM. This approach has two main advantages: it can handle ambiguous fingerprint images in the training set and effectively identify difficult images in the test set, resulting in improved accuracy by rejecting these images. Experiments on the NIST-4 fingerprint database showed a 95.6% accuracy at a 20% rejection rate when SVMs were trained on both FingerCode and RNN-extracted features, suggesting the potential of SVMs for fingerprint classification. 
411213	41121339	Online space-variant background modeling with sparse coding.	The paper proposes a sparse coding method for background modeling in videos. The model is based on dictionaries that are constantly updated as new data is provided by a video camera. The background is considered as noisy data with local and global changes over time due to various factors. To capture these changes, a space-variant analysis is used where a dictionary of atoms is learned for each image patch. At run time, each patch is represented by a linear combination of these atoms and any changes are detected when the atoms are not sufficient to represent the patch. The proposed method shows good performance on different scenarios and can also detect periodic changes caused by natural illumination. It can be used as a component in change detection systems.
411213	41121314	Motion analysis from first-order properties of optical flow.	This paper discusses the use of first-order spatial properties of optical flow to analyze and describe moving images. It is shown that the spatial structure of rigid object motion can be approximated by a linear vector field over large regions of the image plane. A method for computing optical flow as a piecewise linear vector field is proposed, and its effectiveness in distinguishing between different types of motion, extracting shape information, and segmenting images is demonstrated through experimental results. The reliance on patchwise rather than pointwise flow estimates makes the methods robust to noise. Overall, the first-order properties of optical flow are found to be valuable in understanding visual motion.
411214	41121431	Automated 3D lymphoma lesion segmentation from PET/CT characteristics	PET using <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">18</sup> F-FDG is the most effective method for detecting lymphoma due to its high sensitivity and specificity. However, accurately delineating tumors on PET images remains a challenge. Most methods use intensity-based strategies, but recent approaches have incorporated anatomical information to improve accuracy. In this study, the authors propose a fully automated method for lesion detection and segmentation using a combination of machine learning and hierarchical image models. This approach achieved a detection rate of 92% and mean sensitivity and specificity of 0.73 and 0.99, respectively, without any user interaction. This method shows promise for improving the efficiency and accuracy of PET imaging for lymphoma patients.
411214	41121439	Grey-level hit-or-miss transforms-Part I: Unified theory	The hit-or-miss transform (HMT) is a commonly used operation on binary images for the past 40 years. However, its extension to grey-level images is not straightforward and has been explored by only a few authors. Despite its potential uses, there have been very few applications of the grey-level HMT. This paper focuses on providing a unified theory for the grey-level HMT, with Part I discussing the main definitions proposed by Ronse and Soille. This theory involves a fitting process to determine which grey-levels can be fitted to the image, followed by a valuation step to assign a final grey-level value. Three valuations are proposed: supremal, integral, and binary. 
411215	41121526	Multizone soundfield reproduction in reverberant rooms using compressed sensing techniques	We present a method for reproducing a multizone soundfield in a reverberant room by determining the acoustic transfer function (ATF) between a loudspeaker and the desired region using a limited number of microphones. This is achieved by assuming a sparse soundfield in the Helmholtz solution domain and using a compressed-sensing approach to find the ATF. This allows for the optimal characterization of the original sound using scarce sound pressure measurements. The resulting ATF is then used to derive the optimal loudspeaker filter that minimizes reproduction error over the entire region. Simulations show that this method greatly reduces the number of required microphones and enables accurate reproduction across a wide frequency range.
411215	4112150	Multizone soundfield reproduction using orthogonal basis expansion	We have developed a method for reproducing 2-D multizone soundfields by using orthogonal basis functions over the desired reproduction area. This method solves the Helmholtz equation to find the closest solution to the desired soundfield using weighted least squares. The basis functions are created using QR factorization and a set of solutions to the Helmholtz equation. The coefficients of the Helmholtz solution can then be calculated, making the sound reproduction problem simpler. This method is suitable for practical loudspeaker configurations and has shown success in accurately reproducing sound in bright zones while minimizing sound leakage in quiet zones.
411216	41121687	The Finite Capacity Dial-A-Ride Problem	The Capacitated Dial-a-Ride problem involves finding the shortest tour for a vehicle to deliver objects to their specified destinations while adhering to a maximum capacity of k objects at any given time. Previous approximation guarantees for this problem were limited to specific cases, but a new algorithm with an approximation ratio of O(sqrt{k}) has been developed for special instances on a class of tree metrics. For general metric spaces, the ratio is O(sqrt{k} log n log log n). A 2-approximation algorithm is also provided for the problem on a line metric. Additionally, a new framework is considered where the vehicle can leave and pick up objects at intermediate locations, and an O(1) approximation algorithm is designed for tree metrics. The ratio between the values of optimal solutions for the two versions of the problem is also studied, with a maximum possible value of \Omega(k^{2/3}). 
411216	4112162	Approximating min-sum k-clustering in metric spaces	The min-sum k-clustering problem involves dividing a metric space into k clusters in a way that minimizes the total distance between points in the same cluster. A polynomial time approximation algorithm has been developed for this problem, providing a ratio approximation and running in time $\runtime$. This is achieved by embedding metric spaces into hierarchically separated trees. Additionally, a bicriteria approximation result is presented, which yields a constant approximation factor with only a slight increase in the number of clusters. This is based on ideas from primal dual approximation algorithms for facility location.
411217	411217115	Construction of an annotated corpus to support biomedical information extraction.	Information Extraction (IE) is a text mining tool used to automatically identify important biomedical events from large collections of documents. Understanding the syntax and semantics of verbs and nominalized verbs is crucial for IE, and annotated corpora can be useful for training. A new annotation scheme has been developed for gene regulation events, which includes identifying all participants and assigning them semantic roles and biological concept types. The Gene Regulation Event Corpus (GREC) contains 240 MEDLINE abstracts annotated by biologists. It is unique in the biomedical field as it includes not only core relationships, but also other important details such as location, time, and environmental conditions. The corpus has already been used to develop a lexical resource for biomedical text mining and can also be used to train IE components. It is freely available for academic use.
411217	41121778	Enriching a biomedical event corpus with meta-knowledge annotation.	Biomedical papers are a valuable source of information on biological entities, facts, and events. To extract this information automatically, text mining techniques are used, which require annotated corpora for training. Event corpora, which are annotated with structured representations of important facts and findings, are used to extract protein-protein interactions, genotype-phenotype/gene-disease associations, and other relevant information. However, to accurately interpret these events, additional information such as the type of event and the confidence of the author is needed. Therefore, an annotation scheme has been developed to enrich event corpora with meta-knowledge, which is derived from the context of the event. This multi-dimensional scheme is detailed and allows for integration with different types of bio-event annotation. By augmenting event annotations with meta-knowledge, more sophisticated information extraction systems can be trained, enabling tasks such as database curation and textual inference. The scheme is unique in its comprehensive annotation of meta-knowledge aspects for each event. 
411218	41121825	Optimal resource allocation for cost and reliability of modular software systems in the testing phase	Reliability is a crucial aspect of commercial software and is measured by the number of failures that occur during development. To improve reliability, a comprehensive test plan must be in place to ensure all requirements are tested. However, software testing is often constrained by limited time and resources, making it important for project managers to allocate testing resources effectively among different modules. This paper presents an optimal resource allocation problem for modular software systems during the testing phase. The goal is to minimize development costs while achieving a desired level of reliability with a fixed amount of testing effort. The proposed optimization algorithm, based on the Lagrange multiplier method, is supported by numerical examples and sensitivity analysis. This allows for the identification of key parameters and their impact on achieving the desired reliability objective. Overall, the proposed approach aims to efficiently allocate limited testing resources and improve software reliability.
411218	41121858	Performance analysis of software reliability growth models with testing-effort and change-point	This paper proposes a scheme for constructing a software reliability growth model using the Non-Homogeneous Poisson Process. The model takes into account both testing-effort and change-point, which are important factors in software reliability. Most existing models assume a constant detection rate per fault, but in reality, this rate can vary depending on factors such as test team skill, program size, and testability. It may also be necessary to increase testing efforts, such as purchasing new tools or hiring more manpower, to achieve the desired reliability. To address these issues, the proposed model incorporates a generalized logistic testing-effort function and change-point parameter. Real application data is used to demonstrate the effectiveness of the model, showing accurate prediction capabilities.
411219	411219113	Minimum Edge Ranking Spanning Trees of Threshold Graphs	The minimum edge ranking spanning tree problem (MERST) is a challenging problem in graph theory, where the goal is to find a spanning tree with the lowest possible edge ranking in a given graph. This problem is known to be NP-hard for general graphs. However, a new study has found that for threshold graphs, which have practical applications, there exists a polynomial time algorithm to solve MERST. This is a significant result as it is the first non-trivial graph class in which MERST can be solved in polynomial time.
411219	41121925	Minimum edge ranking spanning trees of split graphs	The minimum edge ranking spanning tree problem (MERST) aims to find a spanning tree with the lowest possible edge ranking in a given graph G. While this problem is NP-hard for general graphs, a new study has found a polynomial time algorithm for split graphs, a widely applicable graph class. This is the first non-trivial graph class in which MERST has been shown to be polynomially solvable. Additionally, the problem can be solved in linear time for threshold graphs, which are a subset of split graphs. This research has important implications for practical applications of split and threshold graphs.
411220	41122041	One Graph to Rule Them All Software Measurement and Management	The software architecture of a system is its fundamental organization and includes the relationships between components and the system's environment. A holistic model is necessary for managing the architecture of a large software system and ensuring continuous integration and verification of all artifacts. A unified graph-based approach has been proposed as a solution to this problem, which also allows for convenient and efficient project measurement. This approach can translate existing software metrics into a model that is independent of programming language and introduces new metrics that can cross language boundaries. The model can be implemented using existing tools such as graph databases, query languages, and algorithms.
411220	41122011	Query Rewriting Based on Meta-Granular Aggregation	This article discusses the use of materialized partial aggregations in analytic database queries to improve performance and reduce resource consumption. The concept of metagranules, which represent grouping and aggregation information, is introduced and used to optimize queries. The article presents a query rewriting method and a cost model to select the optimal set of metagranules to materialize. The effectiveness of this approach is demonstrated through experimental evaluations on databases of various sizes. The use of metagranules is shown to significantly improve query performance, but also incurs a cost for updates. The article concludes by verifying the effectiveness of the approach on large databases.
411221	41122116	Wrapper maintenance: a machine learning approach	The rise of online information sources has led to a greater use of wrappers for extracting data from the web. However, there has been less focus on developing tools for maintaining these wrappers, which is important because web sources often change and cause the wrappers to malfunction. This article presents an algorithm that can learn structural information about data from positive examples, which can be used for wrapper maintenance tasks such as verification and reinduction. The algorithm was validated on 27 wrappers over a year and was able to detect 35 out of 37 changes, with a precision of 0.73 and recall of 0.95. The reinduction algorithm was also successful on ten web sources, achieving a precision of 0.90 and recall of 0.80 for data extraction. 
411221	41122153	Selective Sampling with Co-Testing: Preliminary Results	The authors introduce a new approach called co-testing for selective sampling in problems with redundant views, where there are multiple sets of features that can be used for learning. This approach selects queries among unlabeled examples based on disagreement among existing views. In contrast to standard active learning, which aims to split the version space in half, co-testing trains separate classifiers for each view and selects queries based on the degree of disagreement among them. This can lead to a faster reduction of the hypothesis space. The authors give an example of two views and show how co-testing can be more effective in this scenario.
411222	41122291	Tools for transparent synchronous collaborative environments	Synchronous collaborative environments allow people in different locations to work together in the same virtual space. There are two ways to create a shared workspace: the collaborative-aware approach and the collaborative-unaware approach. The latter is more popular because it allows for the use of single-user applications. The authors of this paper focus on the collaborative-unaware approach and discuss their implementation of various tools such as latecomer support for Java applications, client synchronization to reduce data transmission delays, and lightweight multi-session support for multiple collaboration groups. These tools improve existing synchronous collaboration systems and make them more realistic, comprehensive, and versatile. 
411222	41122267	JASMINE: A Java Tool for Multimedia Collaboration on the Internet	Collaboration tools have been around for a while, but recently, Internet-based multimedia collaboration has become popular due to the widespread use of the Internet. The Java platform and programming language have made it easier for ordinary users to access these tools. This has made it attractive to use Java for designing such systems. However, most systems require the shared Java application to be rewritten according to their API, which can be difficult or impossible. This paper presents a practical approach for transparent collaboration with Java, where the application can be shared without any modifications. The system works by capturing and distributing user events and reconstructing them, making it possible to collaborate on Java applications without changing their original code. The feasibility of this approach is proven through the implementation of the JASMINE1 prototype.
411223	41122366	Multi-level Hashing for Peer-to-Peer System in Wireless Ad Hoc Environment	Peer-to-peer (p2p) systems have become increasingly popular, especially with the rise of mobile technologies. However, the resource constraints and node mobility in these systems pose challenges for researchers. Efficient content distribution and discovery are crucial for p2p protocols, but routing in an ad hoc environment can be expensive. This paper proposes a solution using multi-level hashing for wireless ad-hoc networks, which helps reduce routing overhead. Extensive simulations demonstrate the effectiveness and necessity of this approach. Overall, this paper addresses important issues in p2p systems and offers a promising solution for improving their performance in mobile environments.
411223	41122387	Architectural Analysis of Multicast Routing Protocols for Wireless Ad Hoc Networks	Wireless ad hoc networks consist of cooperative nodes and are designed for dynamic connectivity. One type of communication in these networks is multicasting, which allows a sender to share information with multiple receivers in a group. However, ad hoc networks face challenges such as limited bandwidth, short battery life, and changing network topology due to node movement. To address these issues, efficient and robust multicast routing protocols are necessary. This paper examines the architecture of various multicast routing protocols and discusses their deployment challenges. It also provides guidance for researchers working in this area.
411224	41122421	Analysis of source sparsity and recoverability for SCA based blind source separation	Sparse component analysis (SCA) has an important application in underdetermined blind source separation (BSS). This paper uses a probability framework to address the recoverability problem of underdetermined BSS using a two-stage SCA approach. The study considers a scenario where both the sources and mixing matrix are randomly generated. A recoverability probability estimate is presented, with a focus on the nonzero entry number of source column vectors. The concept of sparsity degree for a signal is defined, and its relationship with the sources' sparsity degree is established in terms of recoverability probability. This relationship can be used to ensure the performance of BSS. Simulation results have shown the effectiveness of the proposed probability estimation approach.
411224	4112241	Analysis of sparse representation and blind source separation.	This letter discusses a two-stage approach for sparse representation of a data matrix, which is also useful for blind source separation. It analyzes the equivalence of l(1)-norm and l(0)-norm solutions, and shows that the l(1)-norm is a good sparsity measure due to its robustness to noise. The approach involves using a clustering algorithm to estimate a basis matrix, followed by normalization. It is capable of dealing with overlapping sources and unknown source numbers, and is robust to noise and estimation error. Four simulation examples and an EEG data analysis are provided to demonstrate the algorithm's effectiveness. 
411225	41122557	A Semi-Supervised Svm Learning Algorithm For Joint Feature Extraction And Classification In Brain Computer Interfaces	Machine learning based Brain Computer Interfaces (BCIs) face challenges in using a limited amount of labelled data to build a classifier for a specific subject. BCI Competition 2005 addressed this challenge. An effective BCI system should be adaptive to handle dynamic variations in brain signals. One solution is to adjust the parameters while the system is used online. A new semi-supervised support vector machine (SVM) learning algorithm is introduced in this paper, where feature extraction and classification are jointly performed in iterations. This allows for using a small training set while maintaining high performance and shortening the initial calibration process. The algorithm can be used online to make the BCI system robust to signal changes. The algorithm's robustness to noise and convergence are analyzed, and it is applied to data from BCI Competition 2005, demonstrating its validity.
411225	4112254	Joint feature re-extraction and classification using an iterative semi-supervised support vector machine algorithm	In this paper, the authors propose an iterative semi-supervised support vector machine (SVM) algorithm for joint feature re-extraction and classification in cases where the training data set is small. The algorithm is based on Rayleigh coefficient maximization and is effective in extracting features for discriminating two classes with different means or variances. The paper also discusses the effectiveness of the common spatial pattern (CSP) feature in Electroencephalogram (EEG) data analysis and brain computer interfaces (BCIs). The proposed algorithm is able to reliably extract features and perform classification simultaneously, even with a small training data set. The authors also propose a semi-supervised learning-based method for parameter setting and model selection using the Rayleigh coefficient. The results of data analysis demonstrate the validity of this approach.
411226	411226103	Verification of Cache Coherence Protocols by Aggregation of Distributed Transactions	 This paper introduces a method for verifying the correctness of protocols and distributed algorithms. The method involves comparing the state graph of an implementation with a specification representing the desired abstract behavior. The specification is composed of atomic transactions, which are not necessarily atomic in the implementation. To address this, the method uses an aggregation function that combines the steps of each transaction in the implementation into a single atomic transaction in the specification. This paper demonstrates the effectiveness of the method by applying it to a directory-based cache coherence protocol for the Stanford FLASH multiprocessor. The protocol, which has over a hundred different implementation steps, is reduced to a specification with only six atomic transactions. This allows for easy proof of important properties such as data consistency. The method is also used to verify that the reduced protocol meets a desired memory consistency model, marking the first time a theorem-prover has been used to verify the correctness of a cache coherence protocol of this complexity.
411226	41122695	Automatic Checking Of Aggregation Abstractions Through State Enumeration	Aggregation abstraction is a method used to compare the implementation of a complex transaction-oriented protocol to a simplified version. This allows for formal verification to ensure the correctness of the implementation. A technique for automatically checking this abstraction using a finite-state enumerator is presented. This approach verifies the abstraction relation on-the-fly and requires examining a minimal number of states, making it efficient for finite-state protocols. This technique can be used on its own or as a preliminary step before using a theorem-prover for a more comprehensive aggregation proof. The effectiveness of this technique is demonstrated through its application to the cache coherence protocol used in the FLASH multiprocessor system.
411227	41122721	cSculpt: a system for collaborative sculpting.	Collaborative systems are commonly used for sharing work among people, but they are not well established in computer graphics. In this paper, a system for collaborative 3D digital sculpting is presented. This system allows multiple artists to work together on the same polygonal mesh, sharing their edits automatically and seamlessly. A merge algorithm is proposed to handle concurrent edits, which is fast, respects users' edits, and works for both geometry and appearance modifications. The algorithm is based on a multiresolution edit representation, making it suitable for both fine adjustments and large scale modifications. Tests showed that this algorithm outperforms previous methods for collaborative mesh editing.
411227	41122745	3DFlow: continuous summarization of mesh editing workflows	3DFlow is a new algorithm that allows artists to easily share their entire mesh editing workflow in a detailed and visually appealing way. It takes a sequence of meshes as input and produces a visualization of the workflow at any level of detail. The output includes highlighted edited regions and optional visual annotations, such as brush strokes, to showcase the artist's work. 3DFlow has been tested with a variety of mesh editing techniques and has shown consistent performance. It is independent of the modeling software used and can be used to share work from different programs. The algorithm is open-source and the creators have released their datasets for others to improve upon their work. 
411228	41122858	Architecture of Scalable Embedded Device Management System with Configurable Plug-In Translator	This paper discusses the challenges of the increasing heterogeneity of nodes in ubiquitous computer networks and proposes a system architecture to address this issue. The architecture includes two sub-systems - a Configurable Plug-In Translator System and a Dynamic Management Script Resolver System. The Translator System provides generalized interfaces to applications by using a script execution engine to absorb the heterogeneous low-level interfaces. The behavior of remote nodes is described using a script language. The Resolver System utilizes DNS to create a global scale script repository, taking advantage of DNS's scalability, robustness, and authority management capabilities. The integration of these two sub-systems allows for application software to easily access any remote device on the Internet.
411228	41122817	Overlaying and slicing for IoT era based on internet's end-to-end discipline	This paper examines the deployment of IPv6 in Japan, specifically in regards to large scale multiple-stack layer 3 networks and the future development of networks. Due to the lack of compatibility between IPv6 and IPv4, the use of dual-stack operation is necessary. However, the integration of multiple single-stack networks using tunneling with a "locator" function has proven to be successful in both wired and wireless infrastructures. The paper also discusses the evolution of the Internet into a third wave with the rise of IoT and the need for systems to be designed and implemented based on the concept of "Internet-by-Design". Examples of this approach in practice include the integration of different IoT systems in smart buildings/campuses and the use of "locator" and "identifier" separation through "tunneling" in large scale multiple-stack layer 3 networks. 
411229	4112291	Multiple acoustic sources location based on blind source separation	This paper explores the use of blind source separation (BSS) with canonical correlation analysis (CCA) to locate multiple acoustic sources. The receiving array used is a sparse array with three subarrays. By using CCA on the receiving data, the separate components can be obtained. This allows for the calculation of time differences and the direction of arrival (DOA) for each acoustic source. The proposed method is able to reduce the impact of inter-sensor spacing and other factors, resulting in more accurate and stable results. Simulation results support the effectiveness of this approach. Overall, this new method offers a valuable contribution to the field of acoustic source location.
411229	41122940	Application of blind source separation to time delay estimation in interference environments	This paper focuses on the problem of time delay estimation in interference environments. The authors propose a new method using Blind Source Separation (BSS) to improve the accuracy and stability of time delay estimation. This approach reduces the complexity of estimation and minimizes the impact of interference on the estimation process. Simulations show that this method is effective and practical, resulting in more accurate and stable time delay estimates in interference environments. Overall, this research makes a valuable contribution to the field of signal parameter estimation, specifically in dealing with interference.
411230	4112303	Unsupervised Event Coreference Resolution.	Event coreference resolution is a crucial task in natural language processing, used in applications like information extraction and question answering. This article introduces new unsupervised, nonparametric Bayesian models for determining coreference clusters of event mentions from a group of unlabeled documents. By extracting various lexical, syntactic, and semantic features for each event mention, the models group together mentions with shared features (such as participating entities, location, and time). The main challenges in unsupervised event coreference include choosing a rich set of features and modeling events within and across documents. The first model presented combines the hierarchical Dirichlet process with the ability to capture uncertainty and incorporate a finite number of features. A new hybrid model also automatically infers the number of features and selects the most informative ones for the task. Evaluation shows significant improvement compared to baseline models for both within-document and cross-document event coreference.
411230	4112308	Using clustering methods for discovering event structures	Domain experts manually create event models to understand specific situations. These models are represented as sequences of events in document collections. Events in texts are interconnected and form event structures, such as in a crime scenario where a person is accused, arrested, interrogated, and tried. Extracting event structures from texts allows for various forms of inference, such as predicting future events or determining the probability of an event based on other events already mentioned. The goal is to automatically extract event structures from texts by identifying events that belong together and establishing their relationships. This work is motivated by the task of Topic Detection and Tracking, but instead of focusing on topical clusters, the goal is to build structured event representations and analyze event interactions.
411231	41123178	Delegation protocols for electronic commerce	The reliability of transactions in commercial and financial activities is essential in the electronic world, just as it is in real-life. However, many security mechanisms and protocols do not prioritize accountability, which is crucial in ensuring trust and integrity in these transactions. This is especially evident in delegation protocols. To address this issue, a new approach is proposed that explicitly includes accountability in delegation. This includes a new definition of semantics and a new protocol designed to eliminate the lack of accountability. By prioritizing accountability, electronic commerce can become more reliable and trustworthy in its transactions, similar to real-life commercial and financial activities.
411231	4112318	Support for Multi-Level Security Policies in DRM Architectures	Digital rights management (DRM) systems protect copyrighted content in digital format from piracy, ensuring that creators receive their rightful revenue. However, securing these systems is difficult as they must protect content while being accessed on potentially compromised devices. To address this, the paper proposes a shift in the current DRM architecture where all devices are trusted and have control over protected content, to a new model where devices are differentiated based on their tamper-resistance properties. This new model allows for improved intrusion-tolerance and more flexible business models for supplying content. The type of authentication protocol used in the system also plays a significant role in supporting these multi-level security policies.
411232	41123247	Real-Time Quadratic Sliding Mode Filter for Removing Noise.	This paper introduces a new sliding mode filter that effectively removes impulsive and high-frequency noise with a smaller phase lag compared to linear filters. It has improved performance in terms of overshoot and chattering, making it suitable for real-time applications. The filter uses a quadratic surface as its sliding surface and is based on a backward Euler discretization algorithm, which helps to prevent chattering. Experiments using an ultrasonic sensor and an optical encoder demonstrate the effectiveness of the filter. 
411232	41123236	Phase-Lead Stabilization of Force-Projecting Master-Slave Systems With a New Sliding Mode Filter	The force-projecting master-slave (MS) control scheme is the opposite of the traditional force-reflecting scheme. This paper introduces a technique for stabilizing force projecting MS systems using a linear phase-lead compensator and a modified nonlinear filter. The filter, based on Jin et al.'s parabolic sliding mode filter, has a small phase lag and is proven to be effective through numerical analysis. The filter is applied to an experimental MS system consisting of two industrial manipulators, achieving a force scaling factor of 25 while maintaining stability. This research is significant as it presents a viable method for implementing force-projecting MS control in real-world applications.
411233	4112338	The effectiveness of an optimized EPMcreate as a creativity enhancement technique for Web site requirements elicitation	This paper discusses the importance of creativity in requirements elicitation and the effectiveness of three creativity enhancement techniques (CET) in generating ideas for requirements. Two controlled experiments were conducted, with university students using either full EPMcreate, Power-Only EPMcreate, or traditional brainstorming to generate ideas for improving a high school's public website. The first experiment showed that Power-Only EPMcreate was the most effective in terms of both quantity and quality of ideas, followed by full EPMcreate and then brainstorming. The second experiment confirmed these results. In both experiments, a requirement idea was considered high quality if it was both new and useful.
411233	4112336	Creativity Techniques for Requirements Elicitation: Comparing Four-Step EPMcreate-Based Processes	The Elementary Pragmatic Model Creativity Technique, or EPMcreate, is a method for discovering creative requirements. It consists of 16 steps that incorporate different viewpoints from stakeholders. Experiments have confirmed its effectiveness, but there was a need to reduce the number of steps. To address this, a four-step technique called POEPMcreate was developed and tested. Recently, there has been a theoretical investigation into downsizing EPMcreate and an experiment was conducted to compare two four-step techniques: POEPMcreate and ROS-EPMcreate. The results suggest that ROS-EPMcreate is just as feasible and effective as POEPMcreate, and also offer new insights and considerations for future experiments. Additionally, the paper introduces Kano categories as a new way to evaluate the creativity of generated requirement ideas.
411234	41123447	Presentation Models By Example	The article discusses the limitations of traditional interface builders and multi-media authoring tools in creating dynamic displays. These tools only support the construction of static displays with known components, making it difficult to create more complex displays. High-level user interface management systems and automated designers offer more sophisticated options, but they are not user-friendly as they require dealing with abstract concepts. In response, the authors introduce HandsOn, a GUI development environment that allows for the creation of complex displays with changing data through direct manipulation. HandsOn incorporates principles of graphic design, supports constraint-based layout, and uses Programming By Example techniques to simplify the design process. It also relies on a model-based language for representing displays and providing information for the tool to reason about. 
411234	41123459	Interfaces for understanding multi-agent behavior	Synchronized punch-card displays are a powerful interface technique that can visually represent large amounts of data by using color-coded chips arranged in a rectangular grid. By linking multiple displays to a timeline of events, these displays can show animated visualizations of complex systems. This technique not only simplifies the understanding of overall system behavior, but also allows for a detailed focus on individual variables and specific time intervals. This paper discusses the benefits of synchronized punch-card displays and how they can be used to effectively analyze the behavior of complex multi-agent systems.
411235	41123519	Event Storage and Federation Using ODMG	The Cambridge Event Architecture (CEA) has enhanced its object-oriented, distributed programming environment by incorporating events through a language-independent interface definition language. This allows for the specification and publication of event classes. An extension to CEA has been developed using the ODMG standard to integrate the transmission and storage of events. This includes an ODL parser, event stub generator, metadata repository, and an event library for C++ and Java. The ODMG metadata interface enables clients to dynamically access interface specifications for event registration, allowing for new objects and independently developed components to interact seamlessly. Object database schemas can be used to define name services and interface traders, with type hierarchies allowing for matching between different domains. By utilizing metadata, contracts can be established between domains, facilitating event translation between heterogeneous systems.
411235	41123570	Extensible access control for a hierarchy of servers	A two-level hierarchy of storage servers with value-adding service layers requires a flexible and extensible access control mechanism. This scheme uses Access Control Lists (ACLs) to efficiently check access to objects based on specific policies. Capabilities are used to ensure that access is through the correct service hierarchy, such as a directory object being manipulated through a directory service. The implementation is stateless at the servers and allows for temporary delegation of rights to unprivileged servers. The short-lived nature of the capabilities eliminates the need for selective revocation and crash recovery. 
411236	41123619	A Discrete-Time Polynomial Model of Single Channel Long-Haul Fiber-Optic Communication Systems	The paper discusses the need for a two-dimensional discrete-time input-output model to address physical impairments in long-haul DWDM systems and maximize system capacity. The proposed model is based on the Volterra series transfer function method and can be extended to multichannel systems. It considers fiber losses, frequency chirp, and photodetection, which are often ignored in current literature. The model also takes into account the effects of intersymbol interference, self phase modulation, intrachannel cross phase modulation, and intrachannel four wave mixing on system performance. The model is validated through SSF simulation and is used to develop a constrained coding scheme to mitigate these impairments. 
411236	41123641	Model-centric nonlinear equalizer for coherent long-haul fiber-optic communication systems.	In long-haul fiber-optic communication systems, physical impairments such as fiber dispersion, nonlinearity, and noise from ASE can significantly affect system performance. To mitigate these effects and maximize system capacity, various signal processing techniques have been developed, but they are either limited to linear impairments or too complex to be practical. This paper proposes a new model-centric nonlinear equalizer based on a 2D discrete-time model of physical impairments. Unlike other equalizers, it can be implemented on basic signal processing devices and effectively suppress both linear and nonlinear impairments, especially at high power levels. This makes it a valuable tool for improving the performance of long-haul DWDM systems. 
411237	4112372	Removal of 3D facial expressions: A learning-based approach	This paper discusses a method for recovering the neutral 3D face of a person when given a 3D face model with facial expressions. The proposed approach involves using a learning-based expression removal framework, which models expression residue and uses it to recover the neutral face. This is achieved through a two-step non-rigid alignment method and the use of two spaces, normal space and expression residue space. The problem of expression removal is then formalized as the inference of expression residue from normal spaces. The neutral face model is generated using a Poisson-based framework. Experiments on the BU-3DFED database show the success of this approach. 
411237	41123724	3D face recognition using eigen-spectrum on the flattened facial surface	This paper introduces a new method for 3D face recognition. The facial surface is first transformed into a 2D planar triangulation to preserve its geometric properties. Then, instead of using the flattened surface image, the power spectrum image is used for eigenface analysis to ensure invariance in planar rotation. The proposed approach does not require 3D facial model registration during the recognition process. Experiments using 3D_RMA show that the method has comparable performance to existing methods.
411238	41123865	Cloud resource allocation schemes: review, taxonomy, and opportunities.	Cloud computing is a popular computing model that allows users to process data and run complex applications on a pay-as-you-go basis. As the demand for these cloud-based services increases, it becomes challenging to efficiently allocate resources while meeting the service-level agreement between providers and consumers. Factors such as resource heterogeneity, unpredictable workloads, and different objectives of cloud stakeholders make resource allocation even more complicated. As a result, there has been significant research to address these challenges. This paper provides a comprehensive review of existing cloud resource allocation schemes, highlighting their strengths and weaknesses. It also presents a thematic taxonomy to classify these schemes and analyzes them based on optimization objectives. Finally, it suggests opportunities for the development of optimal resource allocation schemes.
411238	411238170	A review on remote data auditing in single cloud server: Taxonomy and open issues.	Cloud computing is a popular computing paradigm that aims to provide reliable and high-quality services for users in both academic and business environments. However, the lack of physical control and possession over outsourced data in the cloud has raised security concerns for data owners. As a result, researchers have focused on remote data auditing (RDA) techniques to address this problem and enable public auditability. This paper presents a comprehensive survey on RDA in the single cloud server domain, discussing the challenges and issues in current protocols and presenting a taxonomy based on security requirements, metrics, level, auditing and update modes. It also examines lesser-known RDA approaches and identifies future research directions in this area.
411239	41123988	An Intelligent Fingerprint-Biometric Image Scrambling Scheme	The article discusses a solution to prevent attacks and disruptions in biometric image transmission through the use of a challenge/response-based system. The proposed system involves an intelligent biometrics sensor that can receive challenges from the authentication server and generate encrypted responses using the Facial Recognition Technology (FRT). Additional secret keys, such as scaling factors and random phase masks, are also used to enhance the encryption security. The random phase masks are generated chaotically to further improve security. Experimental and simulation results have demonstrated the effectiveness of this system in ensuring the security and reliability of biometric image transmission. 
411239	41123945	Securing biometric templates for reliable identity authentication	The implementation of biometric systems on a large scale brings attention to potential security vulnerabilities that could compromise the integrity and acceptance of the system. Just like passwords and PIN codes, biometric systems are also susceptible to security threats. To address this issue, a new chaotic encryption method is proposed to protect and secure biometric templates. This involves using two chaotic maps to generate a private key and encrypt the biometric data. Experimental results show that this method is effective in ensuring the security of biometric templates, while also being fast and easy to implement.
411240	41124030	Hybrid agent-based simulation for analyzing the National Airspace System	Hybrid agent-based simulation combines discrete-event and continuous-time models to accurately analyze large-scale complex systems like the National Airspace System (NAS). Its ability to incorporate different types of models makes it a reliable tool for evaluating system reliability and performance. However, there are important issues that must be addressed in order for it to be a valuable design and analysis tool. This paper discusses the development of hybrid agent-based simulation architectures for simulating the NAS, specifically using an object-oriented approach. It also explores methods for improving computational efficiency in updating the simulation, and compares their effectiveness.
411240	41124045	Hybrid-system simulation for National Airspace System safety analysis	This paper discusses the development of a simulation architecture for analyzing large and complex systems. The simulations involve a combination of continuous-time and discrete-event models, and must be efficient, flexible, and compatible with various component modules. The development process and challenges are explained, and the application of this architecture to safety analysis of the national airspace system is explored. The paper focuses on the use of object-oriented methods for hybrid-system simulation and compares different methods for updating the simulation in a computationally efficient manner. This simulation architecture has the potential to be a valuable research tool for analyzing complex systems.
411241	41124125	Displaced dynamic expression regression for real-time facial tracking and animation	The article presents an automatic method for real-time facial tracking and animation using a single video camera. This approach does not require calibration for each user and uses a generic regressor from public image datasets to accurately infer 2D facial landmarks and 3D facial shape from 2D video frames. The inferred landmarks are then used to adapt the camera matrix and user identity for better matching of facial expressions. This process is done in an alternating manner and quickly converges with increasing facial expressions observed in the video. The approach is shown to be robust and accurate, comparable to state-of-the-art techniques that require individual calibration, and runs at an average speed of 28 fps. The authors believe this approach is suitable for widespread use in consumer applications.
411241	41124144	Precomputed shadow fields for dynamic scenes	The soft shadow technique presented in this article is for dynamic scenes with moving objects. It uses a precomputed shadow field for each scene entity, which describes the shadowing effects at points surrounding it. This shadow field is created using cube maps and records radiance from both local light sources and dynamic environment maps. The difference between this technique and previous ones is that the shadow field can be precomputed independently of the scene configuration, making it ideal for dynamic scenes. This allows for real-time low-frequency shadowing effects and interactive all-frequency shadows, making it particularly useful for large dynamic scenes with multiple instances of the same entity.
411242	4112426	Video stabilization based on a 3D perspective camera model	The paper introduces a new method for stabilizing video sequences using a 3D perspective camera model. This approach is more advanced than previous methods, as it can handle depth variations and large camera movements. The stabilization problem is formulated as a quadratic cost function with smoothness and similarity constraints, allowing for precise control and efficient optimization. Instead of recovering dense depths, an approximate geometry representation is used and warping errors are analyzed. The results show that visually plausible stabilization can be achieved even with planar structures. The approach is robust and efficient, as demonstrated by various experiments. 
411242	41124260	Recovering consistent video depth maps via bundle optimization	This paper introduces a new technique for reconstructing high-quality video depth maps. The proposed method addresses issues such as image noise and occlusions by using a bundle optimization model. It incorporates both color constancy and geometric coherence constraints, which helps to maintain the temporal coherence of the recovered video depths without creating smoothing artifacts. The inference problem is solved using an iterative optimization scheme that initializes disparity maps using segmentation prior and then refines them using bundle optimization. Unlike previous methods, this approach models probabilistic visibility in a statistical manner. The effectiveness of this method is demonstrated through challenging video examples.
411243	41124381	Multiple-Access Channels With Confidential Messages	The article discusses a discrete memoryless multiple-access channel (MAC) with confidential messages, where two users try to transmit both common and private information to a destination. Each user also receives channel outputs, which may reveal the confidential information sent by the other user. However, both users view each other as potential eavesdroppers and want to keep their confidential information secret. The level of secrecy is measured by the equivocation rate, and the article proposes a performance measure called the rate-equivocation tuple. The capacity-equivocation region, which includes all achievable rate-equivocation tuples, is studied for both the case of two confidential messages and one confidential message. Bounds are derived and a tradeoff between secrecy levels is demonstrated. The article also explores a class of degraded MACs and provides results for two example channels: binary and Gaussian MACs.
411243	41124326	Secrecy Capacity Region of Binary and Gaussian Multiple Access Channels	The article discusses the study of a generalized multiple access channel (GMAC) with one confidential message set. This channel involves two users, where one user (user 1) wants to transmit both common and confidential information to the destination while keeping the confidential information secret from the other user (user 2). The capacity-equivocation region and secrecy capacity region are obtained for a deterministic GMAC. The two main classes of the GMAC, binary and Gaussian, are also studied and their corresponding capacity-equivocation and secrecy capacity regions are established. Overall, the article presents a comprehensive analysis of the GMAC with one confidential message set. 
411244	411244815	Energy-efficient resource allocation in wireless networks with quality-of-service constraints	The article proposes a game-theoretic model to address the problem of joint power and rate control with quality of service (QoS) constraints in multiple-access networks. In this game, each user aims to maximize its own utility by choosing transmit power and rate while meeting QoS requirements. The QoS constraints are based on average source rate and delay, including transmission and queuing delays. The utility function focuses on energy efficiency, making it suitable for wireless networks with energy limitations. The Nash equilibrium solution for this non-cooperative game is derived, and a closed-form expression for the utility achieved is obtained. The article also explores the tradeoffs among throughput, delay, network capacity, and energy efficiency using this framework. Analytical expressions are provided for users' delay profiles, and the delay performance at Nash equilibrium is evaluated.
411244	411244152	Energy-efficient power and rate control with QoS constraints: a game-theoretic approach	The proposed model uses game theory to address the problem of power and rate control in multiple-access networks while maintaining quality of service (QoS) constraints. Each user in the game chooses its transmit power and rate to maximize its own utility and meet its QoS requirements, which are defined in terms of average source rate and delay. The utility function measures energy efficiency and the delay includes both transmission and queueing delays. The Nash equilibrium solution is derived and the resulting utility is calculated. The QoS requirements of a user are represented as its "size", indicating the amount of network resources it consumes. This model allows for studying the tradeoffs between throughput, delay, network capacity, and energy efficiency. 
411245	41124535	Volumeshop: An Interactive System For Direct Volume Illustration	Illustrations have a significant impact on the learning process, whether they are used to teach medical procedures, anatomy, or technical devices. While there is a vast amount of volumetric data available, illustrations are typically created manually as static images, which can be time-consuming. The goal of this paper is to develop a dynamic three-dimensional illustration system that can directly operate on volume data. The system allows for interactive manipulation and exploration of single images, combining the traditional aesthetic appeal of illustrations with the flexibility of digital visualization techniques. This is achieved through a novel concept of direct multi-object volume visualization, which allows for control of the appearance of inter-penetrating objects. Additionally, the paper presents a unified approach for integrating various non-photorealistic rendering models and discusses several illustrative concepts, such as cutaways, ghosting, and selective deformation. A simple interface for specifying objects of interest through volumetric painting is also proposed. All of these methods are integrated into VolumeShop, an interactive application that utilizes hardware acceleration for direct volume illustration.
411245	41124514	Result-driven exploration of simulation parameter spaces for visual effects design.	In this paper, the authors propose a new method for visually exploring parameter spaces in physically-based simulations used by graphics artists. Current tools lack guidance and make it difficult and time-consuming to find the correct parameters for desired effects like smoke and explosions. The proposed approach uses sampling and spatio-temporal clustering techniques to generate an overview of possible variations and their evolution over time. This allows users to explore the simulation space in a goal-oriented manner and compose animations with desired characteristics. The system also utilizes interactive direct volume rendering for instant visual feedback. A user study was conducted to evaluate the system's usefulness in production.
411246	41124619	Combining 2D and 3D views for orientation and relative position tasks	This study compares displays that combine 2D and 3D views to displays with only 2D or 3D views. The combination displays include orientation icons, in-place methods, and a new method called ExoVis. The researchers specifically looked at performance differences in 3D orientation and relative position tasks. The results showed that 3D displays are good for general navigation and relative positioning, while 2D/3D combination displays are better for precise orientation and position tasks. The combination displays performed as well or better than 2D displays. However, clip planes were not effective for 3D orientation tasks, but could be useful for tasks requiring only one slice. 
411246	41124671	Visualization of Time-Varying MRI Data for MS Lesion Analysis	Traditional methods of diagnosing and treating Multiple Sclerosis (MS) require medical professionals to manually compare current and older images of a patient, which can be time-consuming and prone to error. While advancements have been made in 3D imaging, there is a lack of visual tools that specifically highlight changes over time. To address this, researchers have implemented different approaches such as displaying segmented lesions in different colors or as an animation, using ray casting to show voxel-wise differences between images, and rendering glyphs with lesion surface models to indicate localized changes. These methods aim to improve the efficiency and accuracy of diagnosing and monitoring MS.
411247	41124763	Evaluation of prostate segmentation algorithms for MRI: the PROMISE12 challenge.	Prostate MRI image segmentation is important for various tasks in the clinical workup of prostate cancer, but evaluating algorithms on multi-center, multi-vendor and multi-protocol data has been challenging due to differences in scanners and image appearance. To address this, the Prostate MR Image Segmentation (PROMISE12) challenge was created, where 100 cases from 4 centers were included and 11 teams participated using various methods. Results showed that active appearance model based approaches outperformed other methods in accuracy and computation time, with Imorphics and ScrAutoProstate teams being the top performers. However, combining algorithms may lead to even better results. The challenge results are available online for further research. 
411247	41124758	Extraction of Airways From CT (EXACT'09)	This paper presents a framework for evaluating airway tree extraction algorithms in a standardized way. The reference airway tree is constructed using results from all algorithms, which are visually scored by trained observers to determine correctness. Fifteen algorithms were evaluated on 20 chest CT scans, showing significant differences in performance. A fusion scheme was also presented, indicating room for improvement in airway segmentation algorithms.
411248	41124853	Conditional shape models for cardiac motion estimation.	The authors propose a statistical shape model that can predict cardiac motion from a 3D end-diastolic CTA scan. The model combines atlas based segmentation and 4D registration using data from 4D CTA sequences. This can be useful for dynamic alignment of pre-operative CTA data with intra-operative X-ray imaging. As 4D imaging data is not widely available due to the use of prospective electrocardiogram gating techniques, the ability to predict motion from shape information is valuable. The accuracy of the predicted motion was evaluated using CTA scans of 50 patients, showing an average accuracy of 1.1 mm.
411248	4112488	MACD: an imaging marker for cardiovascular disease	CVD is a leading cause of death in Europe and the United States, despite the knowledge that a healthy lifestyle and treatment of risk factors can prevent it. Abdominal aortic calcifications (AAC) have been found to be strongly correlated with coronary artery calcifications, making early detection important in predicting the risk of related coronary diseases. The Morphological Atherosclerotic Calcification Distribution (MACD) index was developed to screen for risk using low-cost imaging. Several potential severity scores related to the shape of calcified deposits in the lumbar aortic region were introduced and their combined predictive power was examined. A Cox regression analysis showed that MACD was the most efficient marker, with a larger individual predictive power than any other imaging markers. The MACD index was found to predict cardiovascular death with a hazard ratio of approximately four. 
411249	41124994	Linear dimensionality reduction via a heteroscedastic extension of LDA: the Chernoff criterion.	The article introduces a new technique, called eigenvector-based heteroscedastic linear dimension reduction (LDR), for handling multiclass data. This technique is an extension of a two-class method that uses the Chernoff criterion, and improves upon the limitations of the widely used linear discriminant analysis (LDA) technique. The new method addresses the issue of heteroscedasticity by generalizing the between-class scatter to capture differences in variances. The authors demonstrate the effectiveness of this approach through experiments and a comparison with other dimension reduction techniques. They also propose a multiclass extension of the Chernoff criterion, which combines information from both class means and covariance matrices. This article provides a comprehensive overview of the proposed technique and its advantages for analyzing multiclass data.
411249	41124982	Multi-Class Linear Feature Extraction by Nonlinear PCA	The traditional method for solving the feature extraction problem is to use Fisher mapping, which maximizes the difference between classes while minimizing the difference within classes. However, for multi-class problems, this method is not optimal due to class overlaps. A new method based on nonlinear PCA is proposed which is equally fast but may avoid class overlaps. This method is compared to Fisher mapping and a neural network based approach, and is found to outperform both, especially in cases of significant class overlaps. 
411250	411250138	Three-dimensional modeling for functional analysis of cardiac images: a review.	The field of three-dimensional (3-D) heart imaging is rapidly advancing and holds great potential in improving clinical diagnosis and research on cardiovascular diseases. Various techniques have been developed to analyze images and extract parameters of cardiac shape and function. This paper provides a comprehensive overview of two decades of research on cardiac modeling, aiming to serve as a guide for both clinicians and technologists. It critically reviews the performance and clinical evaluation of these approaches, concluding that while they have the potential to enhance the diagnostic value of cardiac images, there are still challenges such as robustness, 3-D interaction, computational complexity, and clinical validation that need to be addressed.
411250	411250169	Task-directed evaluation of image segmentation methods	This paper discusses the lack of validation in existing methods for segmenting 2D and 3D images in the field of image processing. The authors propose a validation methodology that focuses on developing a quality norm based on a task-specific cost analysis. This approach involves evaluating a segmentation method by comparing its cost reduction to the cost of a full-interactive (manual) segmentation, with a quality threshold constraint to allow for imperfect segmentations. This allows for a comparison of different segmentation methods designed for the same task, but with different characteristics. Overall, the paper highlights the importance of validating image segmentation methods and provides a useful methodology for doing so.
411251	41125131	Type-Safe Feature-Oriented Product Lines	A feature-oriented product line is a group of programs that share a common set of features, which represent design decisions and configuration options. These features can be added to a program, resulting in the introduction of new structures and refinement of existing ones. The generation of programs in a feature-oriented product line is based on a user's selection of features, and a key challenge in this process is ensuring the correctness of all member programs. This is accomplished through a type system that checks the entire code base of the feature-oriented product line. A formal model of a feature-oriented Java-like language has been developed to demonstrate the effectiveness and completeness of this type system. 
411251	41125125	Feature featherweight java: a calculus for feature-oriented programming and stepwise refinement	Feature-oriented programming (FOP) is a programming approach that combines programming language technology, program generation techniques, and stepwise refinement. Thaker et al. propose a type system for FOP in their GPCE'07 paper to ensure safe feature composition, meaning no type errors during feature combination. The type system is demonstrated through a simple feature-oriented, Java-like language called Feature Featherweight Java (FFJ). The article also examines four FFJ extensions and their impact on type soundness. This work highlights the importance of a strong type system in FOP to ensure safe and efficient feature composition.
411252	41125270	Topological segmentation in three-dimensional vector fields.	The article introduces a new method for segmenting three-dimensional vector fields, which involves replacing the original field with a segmented data set and using it to generate separating surfaces. The segmented data set is created by sampling the vector field with streamlines, and algorithms are developed to produce the separating surfaces. This method is particularly useful for generating local separatrices in the vector field, defined by a movable boundary region. The resulting partition can then be visualized using standard techniques, providing a higher level of abstraction for the vector field visualization.
411252	41125261	Visualizing Nonlinear Vector Field Topology	Our study focuses on the visualization of nonlinear vector field topology using Clifford algebra. We noticed that current methods for this task rely on linear or bilinear approximation, which can distort the local topology when dealing with nonlinear behavior. To address this issue, our algorithm identifies nonlinear areas and employs a polynomial approximation to accurately visualize the topology. This approach allows for faster processing as linear approximation is used outside of these specific areas. The paper discusses the algorithm in depth and provides a basic overview of Clifford algebra. 
411253	41125314	Multiplexed metropolis light transport	Global illumination algorithms using Markov chain Monte Carlo (MCMC) sampling are efficient for complex light transport scenes. These algorithms generate samples as a history of Markov chain states, distributed based on their contributions to the image. However, in light transport simulation, there is more information that can improve sampling efficiency. Multiple importance sampling (MIS) in bidirectional path tracing is an example of utilizing probability densities to construct paths with different estimators. Incorporating this information into MCMC sampling has been a challenge. This article introduces a novel MCMC sampling framework called primary space serial tempering, which combines MCMC sampling and MIS. A rendering algorithm, multiplexed Metropolis light transport, is also developed using this framework, resulting in comparable or better performance than more complex MCMC algorithms.
411253	41125312	Fusing state spaces for markov chain Monte Carlo rendering	MCMC rendering algorithms currently rely on two different state spaces - the path space and the primary sample space. These spaces are related by the sampling process, but existing algorithms only work within one of these spaces. A new framework has been proposed which connects the two spaces, allowing for the use of mutation strategies in both spaces. This framework was tested using a combination of manifold exploration and multiplexed Metropolis light transport, resulting in improved robustness and more uniform convergence compared to using just one space. This highlights the potential benefits of using both state spaces in MCMC rendering algorithms.
411254	41125478	A new framework for efficient password-based authenticated key exchange	Password-based authenticated key exchange (PAKE) protocols enable two users with a shared short password to establish a secure session key. These protocols need to be resistant to offline dictionary attacks, where an eavesdropper tries to match a password to observed transcripts. However, there are limited frameworks for constructing PAKE protocols in the standard model. A new methodology is proposed by abstracting and generalizing a protocol by Jiang and Gong, which allows for PAKE without the use of random oracles, in the common reference string model. This approach has several advantages over previous methods and can also be extended to be secure within the universal composability framework. This new protocol is more efficient than a previous one by Canetti et al. when using El Gamal encryption. 
411254	411254130	Modeling insider attacks on group key-exchange protocols	Authenticated key exchange (AKE) protocols are used in insecure networks to establish a shared session key for secure communication. While two-party AKE is well understood, group AKE is not as well studied, particularly in regards to insider attacks. This work addresses this gap by providing a formal model and definition of security for group AKE within the universal composability framework. This definition is shown to be stronger than previous ones and can be used concurrently with other protocols. Additionally, any AKE-secure protocol can be converted into one that is secure with respect to this definition. 
411255	4112557	Accelerating partial-order planners: some techniques for effective search control and pruning	The authors propose several techniques to improve the practicality of well-founded partial-order planners. The first two techniques focus on search control while keeping costs low. One technique involves adjusting the A* heuristic used by ucpop to select plans, while the other prioritizes "zero commitment" plan refinements and uses LIFO prioritization. A more radical technique is the use of operator parameter domains to prune search, which involves computing these domains from operator definitions and initial and goal conditions. Experiments showed significant speedups for a variety of problems, with the hardest problems showing the greatest improvements. The authors provide Lisp code for their techniques and test problems in online appendices. 
411255	4112555	Problems with parts	The article discusses two problems in representing and using relationships among parts of objects. The first problem is extracting information from overlapping partitioning hierarchies, which cannot be solved by simple label-propagation methods. However, a solution is proposed if the lowest-level parts are drawn from a common pool of disjoint "ultimate" parts. The second problem is property inheritance, where earlier solutions are criticized and a new solution based on function tables attached to concepts is proposed. Both problems have partial solutions that can help in deriving relationships among parts of objects.
411256	41125615	Finite-model theory—a personal perspective	Finite-model theory is a branch of mathematics that focuses on the logical properties of finite mathematical structures. This paper provides a personal perspective on the subject, with the author discussing his own history and interests, particularly stemming from his Ph.D. thesis. Some key topics covered include the differences between model theory for finite and infinite structures, the relationship between finite-model theory and complexity theory, 0-1 laws, and descriptive complexity theory. The paper aims to generate more interest in this field, as there has been a recent resurgence in its study.
411256	41125688	On an authorization mechanism	Griffiths and Wade introduced a dynamic authorization system that surpasses the traditional password method. This system allows database users to grant or revoke privileges on files they have created and authorize others to do the same. The system maintains a directed graph, starting from the creator, that tracks granted privileges. Nodes represent users and edges represent grants with timestamps. There are two types of edges, indicating whether the recipient can make further grants of the same privilege. In this modified approach, multiple edges of each type can exist between two nodes. The authors prove the correctness of this system and provide an example where the original method may restrict a user from exercising or granting a privilege they should have access to.
411257	41125729	Coimplication and its application to fuzzy expert systems	Two-valued logic, also known as classical logic, assigns extreme truth values of 0 or 1 to a set of propositions based on consistency rules. In contrast, fuzzy logic or multiple-valued logic allows for truth values in the range of [0,1] to represent the degree of truth in a formula. However, many expert systems still primarily use two-valued logic and probability, even though the knowledge base often contains imprecise and vague information. In order to handle these uncertainties, a new approach using the equivalence relation for modus ponens and the concept of coimplication is proposed. This allows for a more accurate approximation of reasoning in fuzzy expert systems.
411257	41125760	Complex fuzzy logic	This paper presents a new framework for logical reasoning called complex fuzzy logic. It is a generalization of traditional fuzzy logic, but uses complex fuzzy sets with complex-valued membership functions. This allows for more flexibility and a wider range of values compared to the traditional range of [0,1]. The paper discusses the mathematical properties of complex fuzzy sets, such as union and intersection, relations, and aggregation. Complex fuzzy logic maintains the benefits of traditional fuzzy logic while incorporating the properties of complex numbers. This framework has potential for further research and application due to its unique mathematical properties. 
411258	4112582	The visibility-Voronoi complex and its applications	The VV(c)-diagram is a new hybrid diagram that combines the visibility graph and the Voronoi diagram of polygons. It is useful for planning paths for a robot moving among polygonal obstacles in the plane, with a focus on creating natural-looking paths that are short, smooth, and maintain a certain distance from obstacles. This diagram evolves from the visibility graph to the Voronoi diagram as the clearance parameter increases. Additionally, an algorithm has been developed to preprocess a scene of configuration-space obstacles and construct a VV-complex data structure. This structure allows for efficient planning of motion paths for any start and goal configuration, without explicitly constructing the VV(c)-diagram for each clearance value. The preprocessing time is O(n^2 log n) and the data structure can be queried with a Dijkstra search. A Cgal-based software package has been implemented for computing the VV(c)-diagram and has been used in various applications.
411258	41125835	Lines through segments in 3d space	The LTS problem involves finding all the lines in three-dimensional space that intersect at least four given line segments. This is a common problem in various fields and can be solved using an output-sensitive algorithm, which is implemented and able to handle all degenerate cases. This is the first implementation of its kind, and it runs in O((n3+I)logn) time with O(nlogn+J) working space, where I and J are bounded by O(n4) and O(n2) respectively. The implementation uses Cgal arrangements and has been optimized with carefully chosen algebraic tools. Performance comparisons with other algorithms are also reported. The source code and input examples for the experiments can be found at http://acg.cs.tau.ac.il/projects/lts.
411259	41125978	Prediction and Predictability for Search Query Acceleration.	A commercial web search engine uses multiple servers to process search queries, and the response time is determined by the slowest server. Previous approaches focused on reducing the tail latency (high-percentile response time) of individual servers, but this is not effective for reducing extreme tail latency (99.99th percentile) when responses are aggregated from many servers. To address this, a new design space is proposed which considers when to predict query execution time and whether to override the prediction if predictability is low. By collecting dynamic features at runtime, the proposed framework is able to significantly reduce extreme tail latency and improve server throughput. This is evaluated in two scenarios: query parallelization and query scheduling on a heterogeneous processor, with results showing significant improvements compared to existing predictors.
411259	41125967	SocialSearch+: enriching social network with web evidences	This paper discusses the challenge of finding social media accounts, specifically Twitter accounts, using information available on the internet such as names, attributes, and relationships. Existing methods relying on textual matching often have low accuracy due to fake accounts and nicknames. To improve accuracy, the paper proposes using "relational" evidence from web-based entity relationship graphs and relational repositories. By combining textual and relational features, a ranking function is learned to accurately order potential matches. The paper also addresses the issue of confidence scoring and shows that using a separate classifier outperforms a baseline approach. The proposed system is evaluated on real-life internet-scale data.
411260	41126067	Uncovering hidden qualities - benefits of quality measures for automatically generated metadata	Digital libraries are increasingly using semantic techniques in their processes for metadata generation, search, and access. However, the quality of automatically generated metadata using these techniques is uncertain due to their statistical or collaborative nature. To ensure data quality in digital libraries, a user study was conducted to evaluate metrics for quality assessment and their usefulness for individuals during interaction. The study observed the interaction of domain experts in chemistry and presented them with three visualizations of sample semantic techniques, first without and then with quality information. The results showed that quality information is crucial not only for data curation in digital libraries but also for designing user-friendly interfaces for end-users. 
411260	41126076	Exposing the hidden web for chemical digital libraries	The rise of digital content has led to the creation of specialized digital libraries, including in the field of chemistry. However, the complexity of searching for chemical entities has hindered the usability of these libraries. This is due to the reliance on complex structures, making it difficult for search engines to index chemical documents. To address this, a framework has been developed to automatically generate metadata-enriched index pages for chemical collections, allowing for easier indexing and discovery of documents. Experiments show that this approach is more efficient than fulltext indexing and can potentially unlock a large portion of the hidden chemical web. This has implications for digital libraries and open access journals in providing easier access to chemical information.
411261	4112610	Automatic task based analysis and parallelization in the context of equation based languages	This paper discusses a method for automatically parallelizing complex task systems with dependencies. It includes techniques for analyzing and representing dependencies, as well as a library-based approach for clustering, profiling, and scheduling tasks. The goal is to simplify the process of parallelizing these systems, which can be challenging and time-consuming. The approach has been implemented in the OpenModelica simulation environment and is able to handle shared memory multi-core and multi-processor systems. The paper also highlights the use of parallelism in mathematical modeling languages and how it can be effectively utilized. 
411261	41126131	Semantic validation of physical models using role models	The article discusses the increasing complexity of models used to simulate physical systems, particularly in the design of wind turbines. To effectively validate these models, a role-based validation framework has been developed. This framework allows for the creation of validation rules for various design aspects, using role models to define restrictions and rules. These role models can be combined to cover all design features and can also define restrictions on model interactions. The framework has been tested on two modeling languages, Modelica and UML2, and has shown to be easily applicable and reusable. This approach is crucial in ensuring the structural correctness and behavior of wind turbine models.
411262	41126246	Depth reduction for circuits of unbounded fan-in	This content discusses the limitations of constant depth circuits of size nlogO(1)n when using the basis of AND, OR, and PARITY. It is proven that these circuits are not more powerful than circuits of the same size and depth four. This concept is applied to other depth reduction theorems, showing that any set in AC0 can be recognized by a family of depth three threshold circuits with a size of nlogO(1)n. It is also noted that this size bound is the best possible when considering depth reduction over AND, OR, and PARITY. Overall, the results support the idea that most of the limitations of constant depth circuits still hold true.
411262	41126219	Non-commutative arithmetic circuits: depth reduction and size lower bounds	This study examines the concept of depth-reduction in commutative and non-commutative arithmetic circuits. It is proven that in the commutative setting, logarithmic depth uniform semi-unbounded circuits have the same power as polynomial degree uniform circuits with unrestricted depth. This result unifies the circuit characterizations of the classes LOGCFL and #LOGCFL. Additionally, it is shown that AC1 is not more powerful than arithmetic circuits of polynomial size and degree nO(log log n), which improves upon the previous bound of nO(log n). Similar connections are drawn between TC1 and arithmetic circuits of polynomial size and degree. In the non-commutative setting, it is demonstrated that over the algebra (∑∗, max, concat), arithmetic circuits of polynomial size and degree can be reduced to depths of O(log2n) or even O(logn) if unbounded-fanin gates are allowed. This proves that OptLOGCFL is in AC1 and is the first result of depth reduction for arithmetic circuits over a non-commutative semiring. The study also introduces the notions of "short-left-paths" and "short-right-paths" and demonstrates their ability to characterize the classes of arithmetic circuits for which optimal depth reduction is possible. These classes can also be characterized using the AuxPDA model. Finally, the languages generated by efficient circuits over the semiring (2∑∗, union, concat) are characterized in terms of simple one-way
411263	41126319	Finding temporal order in discharge summaries.	This paper suggests a method for automatically analyzing time-oriented clinical narratives, which would be useful for medical decision making, data modeling, and biomedical research. The proposed approach involves using a robust corpus-based method to identify temporal segments and their ordering within medical discharge summaries. The method considers a temporal segment to be a part of the text that does not have sudden changes in temporal focus. It uses a supervised machine-learning framework that incorporates a variety of linguistic and contextual features to determine temporal order. The results show an 83% F-measure in temporal segmentation and 78.3% accuracy in determining pairwise temporal relations. 
411263	41126343	Automatic aggregation by joint modeling of aspects and values	The article introduces a model for aggregating product review snippets using both aspect identification and sentiment analysis. This allows for the discovery of highly-rated or inconsistent aspects of a product. The model uses a generative approach with an efficient variational mean-field inference algorithm and is easily adaptable. The model is tested on two tasks, joint aspect identification and sentiment analysis on Yelp reviews, and aspect identification alone on medical summaries. Results show that the model outperforms other methods, with up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis. 
411264	41126452	Distributed computing with rules of thumb	In their research presented at ICS 2011, the authors focus on dynamic environments where computational nodes follow simple and unsophisticated rules of behavior, such as "best replying" and minimizing "regret." They investigate the conditions for convergence to an equilibrium point in these environments, specifically when nodes' interactions are not synchronized. This has applications in various fields including routing, congestion control, game theory, social networks, and circuit design. The authors also explore the relationship between their results and nontermination in distributed computing, the impact of scheduling on convergence, and the effects of asynchrony on no-regret dynamics.
411264	41126425	Dynamics at the Boundary of Game Theory and Distributed Computing	This article explores the use of ideas from distributed computing and game theory to understand how computational nodes interact in dynamic and decentralized environments with limited information. The study showcases a non-convergence result for asynchronous settings and discusses its implications in various real-world applications such as game dynamics, circuit design, social networks, Internet routing, and congestion control. The article also delves into the complexity of testing the convergence of these dynamics and suggests further research in the intersection of distributed computing and game theory.
411265	4112654	Multifractality in TCP/IP traffic: the case against	The discovery of long-range dependence in LAN and WAN packet data led to further research on multifractal behavior in TCP/IP traffic in WANs. However, the physical mechanisms behind this behavior have not been convincingly proven, leaving the question of whether multifractal traffic models are just a "black box" or if there is something real behind them. This paper examines the evidence for multifractal behavior in aggregate TCP traffic and finds that it is weak. The study also highlights misunderstandings in the literature about the scales at which multifractality has been claimed and explains other pitfalls that have led to an overestimation of the multifractal case. The paper argues for an alternative point process model that accurately reproduces the higher order statistics of the data without being multifractal. Overall, the paper suggests that the empirical evidence for multifractal behavior may be a misinterpretation due to limitations in statistical methodology.
411265	4112650	Measuring Long-Range Dependence Under Changing Traffic Conditions	Recent measurements of network traffic have suggested the presence of long-range dependence (LRD) and self-similarity, but this could also be explained by non-stationarity. The commonly used Hurst parameter H for estimating LRD assumes stationarity and can be biased when this is not the case. The Abry-Veitch (AV) joint estimator, developed to address this issue, is more reliable for non-stationary time-series. This study investigates the performance of the AV estimator under non-stationarities such as a change in mean or variance. Results suggest that a gradual change or a small jump does not significantly affect the estimates, but a sharp and large jump can introduce bias. The study also defines a class of non-stationary LRD processes and tests the estimator on real data. 
411266	41126631	Evaluating combinations of ranked lists and visualizations of inter-document similarity	The article discusses the use of document clustering ideas in improving the accuracy of ranked lists in interactive systems. The study focuses on evaluating the effectiveness of these systems and constructing them in the best possible way. A user study is conducted to evaluate the combination of clustering and ranked lists in instance-oriented retrieval. Results show that although users prefer the combination, it does not necessarily improve effectiveness. In the second part of the study, a new approach that directly combines ranked lists with inter-document similarity information is developed and evaluated. This approach shows significant improvements in effectiveness and users are able to use the combined information effectively. The article also presents two prototype systems, AspInQuery and Lighthouse, which demonstrate the application of this approach.
411266	41126693	Evaluating a Visual Navigation System for a Digital Library	This paper explores a versatile interactive system for organizing information. Using a spring-embedding algorithm, documents are placed in 1-, 2-, or 3-dimensional space based on their similarity. The paper proposes a method to assess the effectiveness of this system by analyzing the clustering of relevant documents. It also introduces a way to measure the organization's structure and how it can be improved. The study finds that increasing the dimensionality of the organization leads to better results. Additionally, the paper presents two methods for adjusting the organization based on user feedback, which has been shown to enhance its effectiveness. All evaluations are performed without direct user involvement.
411267	41126750	ESCAPE: Efficiently Counting All 5-Vertex Subgraphs.	Subgraph counting is a crucial technique in analyzing networks, especially in fields like bioinformatics and social networks. While triangle counting has been extensively studied, counting 4-vertex or 5-vertex patterns is a challenging task with few practical solutions that can handle large graphs. In this study, a new algorithmic framework is introduced that can efficiently count any small pattern in a graph. By breaking down the pattern into smaller ones and utilizing the graph's degree orientations, the algorithm avoids the combinatorial explosion faced by traditional methods. Only four specific subgraphs (three with less than 5 vertices) need to be enumerated to compute exact counts for all 5-vertex patterns. Empirical experiments show that this algorithm can handle graphs with millions of edges in minutes, making it the first practical solution for 5-vertex pattern counting at this scale. Additionally, a faster method for counting 4-vertex patterns is also introduced, which outperforms current state-of-the-art methods by ten times.
411267	41126725	Why do simple algorithms for triangle enumeration work in the real world?	Triangle enumeration is a common graph operation, but there are no efficient algorithms for this problem. To overcome this, practitioners use simple heuristics that take advantage of the structure of real-world graphs with millions of vertices. One popular heuristic is to only consider paths of length 2 where the middle vertex has the lowest degree. This algorithm is easy to implement and has been shown to significantly improve running time. Researchers studied this algorithm on graphs with heavy-tailed degree distributions, which are common in real-world graphs. They found that the expected running time is determined by the l4/3-norm of the degree sequence, and for power law degree sequences with exponent α ≥ 7/3, the algorithm performs in linear time. This provides a major speedup compared to the trivial algorithm, which checks all paths of length 2.
411268	41126842	Non-parametric Jensen-Shannon Divergence.	The problem of quantifying the difference between two distributions is common in machine learning and data mining. However, in many cases, we only have empirical data and do not know the true distributions or their form. This means we must assume a distribution or perform estimation in order to measure their divergence. This can be unsatisfactory for exploratory purposes, as the focus is on exploring the data rather than our expectations. In this paper, the authors propose a non-parametric method for measuring divergence between two distributions using cumulative distribution functions. This eliminates the need for estimation and allows for efficient calculation of divergences directly from data. Empirical evaluation shows that this method outperforms the current state of the art in detecting differences between distributions.
411268	41126833	Telling cause from effect by local and global regression	The article discusses the problem of inferring the causal direction between two numeric random variables using observational data. This is challenging because the two possible directions are Markov equivalent, making it impossible to determine the correct direction using conditional independence tests. To address this issue, the authors propose an information theoretic approach based on the algorithmic Markov condition. They suggest that the true causal model can be identified by finding the most succinct description of the joint distribution in terms of Kolmogorov complexity. This approach also utilizes the Minimum Description Length principle and a score to determine the number of bits needed to transmit the data using a class of regression functions. The authors also introduce two analytical significance tests and two linear-time algorithms that outperform existing methods.
411269	41126948	Item Sets that Compress	Frequent item set mining faces a challenge of finding the most relevant frequent item sets due to the overwhelming number of results. This is caused by the similarity between large sets of frequent item sets. To tackle this issue, the MDL principle is used to determine the best set of frequent item sets that can compress the database the most. Four heuristic algorithms are proposed and tested, showing a significant reduction in the number of frequent item sets. Additionally, the approach is also effective in determining the optimal value for the min-sup threshold. 
411269	4112696	Reducing the Frequent Pattern Set	Frequent pattern mining can be challenging due to the large number of results, which makes it difficult to find the most relevant patterns. However, a recent study has found that using an MDL-based approach can significantly reduce the number of frequent item sets to be considered. This approach has also been successful in reducing the number of frequent patterns in other types of data, such as sequences and trees. The reductions can be as high as two to three orders of magnitude, particularly on data sets related to web-mining. This highlights the effectiveness of the MDL-based approach in handling the problem of explosion of results in frequent pattern mining. 
411270	411270116	Integrating Meta-modelling Aspects with Graph Transformation for Efficient Visual Language Definition and Model Manipulation	Visual languages (VLs) are important tools for modeling various aspects of systems. In addition to standard languages like UML, there are also domain-specific languages that are more commonly used when there is strong tool support available. Different types of generators have been developed to create visual modeling environments based on VL specifications. To define a VL, both declarative and constructive approaches are used. The meta modeling approach is declarative, using classes of symbols and relations to define a VL and its properties. Constraints can also be used to further specify the language. Alternatively, a graph grammar approach can be used where graphs describe the abstract syntax of models and graph rules define the language grammar. This paper introduces a new concept of node type inheritance in algebraic graph grammars, which allows for a combination of declarative and constructive techniques in VL definition and model manipulation. Two examples, GENGED and AToM(3), demonstrate how VLs can be defined and manipulated using these techniques.
411270	41127061	Consistent integration of models based on views of meta models	In software engineering, complex system models are now managed using different views. View-based modelling involves creating small models that describe different parts of the system. Existing techniques for this type of modelling use typed attributed graphs, where views are connected by graph morphisms. However, these techniques require a fixed meta model, data signature, and domain, which may not be suitable for view-oriented modelling where only certain parts of the meta model are known. This paper introduces generalized typed attributed graph morphisms, which allow for changes in the type graph, data signature, and domain. This enables the creation of type hierarchies and views between type graphs. The paper also discusses the integration and decomposition of consistent view models, as well as the use of constraints. A visual domain-specific modelling language is used as an example to illustrate these concepts.
411271	41127121	Visdok: Ein Ansatz zur interaktiven Nutzung von technischer Dokumentation	Visdok is a multimedia presentation system that combines 3D visualization, animation, and hypertext navigation with a knowledge-based discourse planner in a client-server architecture. It allows for efficient access to existing information through the generation of menus and automatically created indices that are sensitive to the media and context. The Visdok system is particularly useful for product presentations and training purposes. 
411271	41127117	Sprache zwischen Visualisierung und Benutzer	Die Frage, was Benutzer nach der Betrachtung von informierenden Visualisierungen in interaktiven Systemen unternehmen, ist entscheidend für die Gestaltung der Visualisierungen. In einigen Fällen führen die Visualisierungen zu einem Zwischenschritt in der Arbeit, der zu weiteren Visualisierungen führt. In anderen Fällen werden die Visualisierungen zu "mentalen Bildern", die nicht unbedingt verbalisiert werden müssen. Es gibt jedoch auch Visualisierungen, die besonders gut funktionieren, wenn sie zu geeigneten verbalen Ausdrücken seitens der Betrachter führen. Die Verbindung zwischen Visualisierungen und verbalen Ausdrücken ist daher ein wichtiger Aspekt bei der Gestaltung von Visualisierungen.
411272	411272174	Robust Iris Localization and Tracking based on Constrained Visual Fitting	The article discusses a computer vision algorithm for localizing and tracking the iris. The localization algorithm helps the tracking algorithm by providing multiple starting points in case of tracking failure. The tracking is done using a robust method for fitting ellipses with added search constraints, resulting in improved accuracy compared to traditional methods. Experimental results show that the algorithm is quick, precise, and robust, making it suitable for human-machine interaction applications, especially for individuals with severe motor disabilities. 
411272	411272258	Face Recognition by Super-Resolved 3D Models From Consumer Depth Cameras	The use of 3D scans for face recognition has gained attention in recent years, but the effect of scan resolution on the recognition process has not been studied. This is crucial for utilizing consumer depth cameras for biometric purposes. To address this issue, a super-resolution approach for 3D faces is proposed in this paper. This involves processing a sequence of low-resolution scans to create a higher resolution 3D face model. The technique uses a scaled iterative closest point procedure and 2D box-spline functions to align and approximate the high-resolution model. A dataset, Florence Superface, was created to evaluate the approach and results show its accuracy compared to other methods. This research has practical implications for improving face recognition with consumer depth cameras.
411273	41127334	Domain Model Based Hypertext for Collaborative Authoring	Domain information models are used to represent the structural and semantic characteristics of information in different domains. This chapter discusses the creation of a domain model-based hypertext system and the benefits it offers. Experiences show that using semantic nets for domain-based hypertext can improve organization and maintain consistency in collaborative environments. This type of system, with an active management document, can also aid in coordinating authoring teams. Additionally, the structured hypertext feature allows for the automatic construction of HTML, making it applicable to the World Wide Web. The paper suggests methods for structuring and managing collaborative hypertext that can be directly applied to the web. 
411273	41127341	Expertext for medical care and literature retrieval	An expert system called the expertext system has been developed to aid in medical literature research. It is built on the ROIS expert system shell and uses the EMBASE document database. The expert system uses a small but rich semantic net for domain knowledge, while EMBASE has a large hierarchical semantic net. The system creates Boolean queries based on patient records and medical knowledge and maps them onto the EMBASE semantic net. A reasoning strategy is then used to rank the retrieved documents. This allows for efficient and accurate literature retrieval for medical research. 
411274	4112746	A “Joint+Marginal” Approach to Parametric Polynomial Optimization	We consider polynomial optimization problems on $\mathbb{R}^n$ with a parameter set $\mathbf{Y}$, and assume that all moments of a probability measure $\varphi$ on $\mathbf{Y}$ can be computed. We then present a series of semidefinite relaxations that converge to a probability measure encoding information about all global optimal solutions as the parameter varies. This allows for the approximation of various functional properties of the optimal solutions, such as their mean and persistency. Additionally, using the dual semidefinite relaxations, we can provide polynomial or piecewise polynomial lower approximations with $L_1(\varphi)$ or $\varphi$-almost uniform convergence to the optimal value function.
411274	41127435	A new look at nonnegativity on closed sets and polynomial optimization	The article discusses the relationship between a continuous function f being nonnegative on a closed set K and the moment matrices of a signed measure d nu = f d mu with support mu = K being positive semi-definite. It presents a hierarchy of semidefinite approximations for polynomial optimization on simple closed sets, including the whole space R(n), the positive orthant, a box, a simplex, or the vertices of the hypercube. This hierarchy results in a nonincreasing sequence of upper bounds that converge to the global minimum by solving a generalized eigenvalue problem. This complements the convergent sequence of lower bounds obtained by solving a hierarchy of semidefinite relaxations.
411275	41127572	Towards shorter task completion time in datacenter networks.	Datacenters are crucial for modern commercial operations and scientific applications, but existing scheduling frameworks are limited by only considering task or flow level metrics. This leads to suboptimal performance for delay-sensitive applications. To address this, the paper proposes TAFA (Task-Aware and Flow-Aware), which combines both metrics while avoiding their individual problems. This approach has been shown to significantly improve performance, reducing task completion time by over 35% in existing data center systems.
411275	41127549	Trust-based on-demand multipath routing in mobile ad hoc networks.	A mobile ad hoc network (MANET) is a decentralized system of mobile wireless nodes, where each node functions as both a communicator and a router. This open environment is susceptible to attacks from malicious nodes, which can disrupt communication. To address this issue, the authors propose a trust-based multipath routing protocol, AOTDV, which evaluates the behavior of neighboring nodes in terms of both hop counts and trust values. This allows for the selection of the most reliable path for data packets. A detailed comparison and experiments show that AOTDV improves packet delivery and mitigates attacks from malicious nodes.
411276	41127629	Pegasus: coordinated scheduling for virtualized accelerator-based systems	Heterogeneous multi-core platforms, which consist of both general purpose and accelerator cores, are becoming more common. However, operating systems still view accelerators as specialized devices, making it difficult for applications to utilize all cores freely. The Pegasus system described in this paper offers a uniform resource usage model for all cores on such platforms by operating at the hypervisor level. Its scheduling methods efficiently share accelerators across multiple virtual machines, making them first class schedulable entities. An implementation of Pegasus using NVIDIA GPGPUs and x86-based host cores shows improved performance for applications by better managing resources. This results in performance improvements ranging from 18% to 140% over traditional GPU scheduling methods.
411276	41127657	Topology-aware resource allocation for data-intensive workloads	This paper suggests an improved architecture for allocating resources in IaaS-based cloud systems. Current systems often do not consider the specific needs of the hosted application, which can negatively impact performance for data-intensive applications. To address this issue, the proposed architecture uses a "what if" approach and includes a prediction engine, lightweight simulator, and genetic algorithm to determine an optimized resource allocation. A prototype, called TARA, was tested on an 80 server cluster with two MapReduce-based benchmarks and showed a significant reduction in job completion time compared to traditional allocation methods. This approach could greatly improve the efficiency of resource allocation in cloud systems.
411277	41127721	Multishift Variants of the QZ Algorithm with Aggressive Early Deflation	The article discusses new variants of the QZ algorithm for solving generalized eigenvalue problems. These variants include an extension of the small-bulge multishift QR algorithm, which allows for the use of level 3 BLAS operations and efficient utilization of high performance computing systems. The authors also propose an extension of the aggressive early deflation strategy, which can identify and deflate converged eigenvalues earlier than traditional methods. Additionally, a new deflation algorithm for infinite eigenvalues is presented. These developments result in a significantly improved QZ algorithm implementation, as demonstrated by numerical experiments with random matrix pairs and real-world applications.
411277	41127743	On aggressive early deflation in parallel variants of the QR algorithm	The QR algorithm is a popular method for solving dense nonsymmetric eigenvalue problems by computing the Schur form of a matrix. In the past decade, the use of multishift and aggressive early deflation (AED) techniques has significantly improved the efficiency of the algorithm in sequential implementations. These techniques have also been incorporated into a parallel QR algorithm for hybrid distributed memory HPC systems. However, as the number of processors increases, AED can become a computational bottleneck. To address this issue, a two-level approach has been proposed, combining AED with the pipelined QR algorithm in the ScaLAPACK routine PDLAHQR. Numerical experiments have shown that this implementation further enhances the performance of the parallel QR algorithm.
411278	41127847	Fcmac-Byy: Fuzzy Cmac Using Bayesian Ying-Yang Learning	The cerebellar model articulation controller (CMAC) is a neural network model known for its fast learning and simple computation. However, its rigid structure limits its ability to approximate certain functions. To address this issue, a new neural fuzzy CMAC called FCMAC-BYY is proposed. It uses Bayesian Ying-Yang (BYY) learning to optimize fuzzy sets and a truth-value restriction inference scheme to determine rule weights. Inspired by the Chinese philosophy of Ying-Yang, the FCMAC-BYY has better generalization ability, reduced memory requirements, and intuitive reasoning. Experiments on benchmark datasets show that it outperforms existing techniques.
411278	41127829	Self-Organizing Gaussian Fuzzy CMAC with Truth Value Restriction	The CMAC is a popular neural network model that uses auto-associate memory. Many researchers have combined it with fuzzy logic, resulting in the FCMAC, where input data is converted into fuzzy sets before being fed into CMAC. This paper introduces the self-organizing fuzzification (SOF) technique, which uses raw numerical values without preprocessing to form fuzzy sets and dynamically partition clusters. This results in a consistent fuzzy rule base and employs the truth value restriction (TVR) inference scheme in the defuzzification phase. Experiments on benchmark datasets show that this method outperforms existing models, particularly in handling uncertainty in the inference process.
411279	4112791	Web browser accessibility using open source software	A Web browser aims to provide a consistent user experience for accessing different types of information, but achieving universal accessibility and interactivity is still a long-term goal. Existing browsers only offer partial solutions for this, and integrating open source and free software components can be challenging due to their diverse implementation environments and incompatible standards. To address this, a middleware infrastructure called AMICO:WEB has been developed to enable access to a variety of components within a mainstream Web browser. This infrastructure allows for interoperability between different extension mechanisms and bridges the semantic differences between high-level Web APIs and low-level device-oriented APIs. Two scenarios are used to demonstrate the design decisions and benefits of AMICO:WEB for Web accessibility.
411279	41127936	Supporting subject matter annotation using heterogeneous thesauri: A user study in Web data reuse	The study conducted a user experiment with museum professionals using Web vocabularies to annotate museum objects. The paper focuses on identifying the requirements for the RDF dataset, search algorithms, and user interface design in a real-world setting. The advantages of reusing Web vocabularies are highlighted, along with discussions on overcoming potential disadvantages. The experiment took place at the Print Room of the Rijksmuseum Amsterdam, which is currently cataloguing a large collection of prints, photographs, and drawings. The paper reports on the analysis of cataloguers' current practices, the design process of an annotation tool, and a qualitative evaluation of the tool in a realistic annotation environment. The findings are discussed in terms of their impact on RDF data, semantic search functionality, and user interface.
411280	41128039	Visualization of high-dimensional model characteristics	Inductive learning techniques are effective for creating explanatory models for large and complex data sets. However, these models can be challenging for users to interpret. To address this issue, a set of visualization methods has been developed to help users evaluate the quality of these models, compare different models, and identify areas for improvement. These methods include techniques such as high-dimensional data space projection, variable/class correlation, instance mapping, and model sampling. The effectiveness of these techniques is demonstrated through their application to various models built from a census data set. These visualizations aid in understanding and utilizing these complex models for better data analysis.
411280	41128050	DD-PREF: a language for expressing preferences over sets	The article introduces a representation language called DD-PREF for expressing and reasoning about a user's preferences over sets of objects. The language allows users to specify their desired diversity and depth of objects in a set, with each object being represented as a vector of feature values. Diversity and depth preferences complement each other and are not necessarily opposites. The article also presents an objective function and a greedy algorithm for efficiently identifying the subset of objects that best satisfies the preferences stated in DD-PREF. The effectiveness of DD-PREF is demonstrated through experiments with a blocks world data set and a collection of Mars rover images, where it successfully captures individual preferences and retrieves high-quality image subsets.
411281	41128160	On the bandwidth of the plenoptic function.	The plenoptic function (POF) is a useful tool for understanding problems in image and video processing, vision, and graphics. It is particularly helpful for image-based rendering, where the POF is used for sampling and interpolation. To accurately characterize the POF, it is important to consider the bandwidth. In this study, a model is used where band-limited signals are applied to smooth surfaces. It is found that the POF is not band limited unless the surfaces are flat. Rules are then developed to estimate the essential bandwidth for this model, taking into account factors such as maximum and minimum depths, maximum frequency, and maximum surface slope. With a multidimensional signal processing approach, this study can verify other important findings in POF processing and determine necessary sampling rates.
411281	4112810	On the information rates of the plenoptic function	The plenoptic function is a concept that describes the visual information available to an observer at any point in space and time. This information can be seen in video and other visual content and represents a large amount of data. In this paper, a stochastic model is proposed to study the compression limits of a simplified version of the plenoptic function. This model isolates the two main sources of information in the POF: camera motion and the complexity of the visual reality being captured and transmitted. The model is first applied to static realities and camera motion, showing that current coding methods are optimal. It is then extended to dynamic realities and derived bounds for lossless and lossy information rates. Examples suggest that common coding methods may not be optimal within this model.
411282	41128257	Hardware-Accelerated Parallel-Split Shadow Maps	Shadow mapping is a commonly used technique for creating realistic shadows in real-time applications. However, it suffers from issues with aliasing, which can result in lower quality shadows. To address this, a new method called parallel-split shadow maps has been introduced. This method splits the view frustum into sections and generates a shadow map for each section, resulting in higher quality shadows in complex and large scenes. A fast and effective method for splitting the frustum is proposed, which reduces aliasing over the depth range. Additionally, hardware-accelerated processing is used to improve efficiency and eliminate extra rendering passes compared to standard shadow mapping.
411282	41128253	Parallel-split shadow maps for large-scale virtual environments	Shadowing effects play a crucial role in making virtual environments appear more realistic by providing visual cues. The widely used shadow mapping algorithm is efficient for real-time shadow rendering but often suffers from aliasing errors. To address this issue, the Parallel-Split Shadow Maps (PSSMs) scheme has been proposed. It splits the view frustum into smaller parts using parallel planes and generates multiple shadow maps for each part. A split strategy has been developed to minimize aliasing errors and improve the utilization of shadow map resolution. Hardware-acceleration is also used to reduce rendering passes, resulting in real-time performance for dynamic and large-scale virtual environments. This approach is easy to implement and does not require complex data structures. 
411283	41128370	Property-driven design for swarm robotics	The paper proposes a new method, called property-driven design, for developing collective behaviors in swarm robotics systems. This method aims to overcome the limitations of the traditional code-and-fix approach, which can be time consuming and relies heavily on the designer's expertise. Property-driven design involves formally specifying desired properties for the system and using an iterative process to develop a model that satisfies these properties. The model is then implemented in simulation and on real robots. This approach helps to minimize the risk of developing a system that does not meet the desired properties and promotes the reuse of models across different hardware. The paper also presents a specific application of this method using Discrete Time Markov Chains and Probabilistic Computation Tree Logic. The method is demonstrated in the design and development of a swarm robotics system for aggregation. 
411283	41128333	An Insect-Based Algorithm for the Dynamic Task Allocation Problem	This paper presents an insect-based algorithm for solving the Dynamic Task Allocation (DTA) problem in factory scheduling. While previous algorithms have focused on the homogeneous case, where all processing units are identical, this paper also considers the heterogeneous case. The proposed algorithm makes use of the specialization concept, where agents are assigned to specific tasks to increase efficiency. The effectiveness of this algorithm is demonstrated through four modifications made to a previous algorithm and a modified version of the dynamic threshold model proposed by Theraulaz. An experimental analysis shows that this algorithm outperforms previous ones, achieving a 10% improvement in performance in a real-life case study. 
411284	41128419	A sequential Bayesian approach to color constancy using non-uniform filters	This paper introduces a new method for improving Bayesian color constancy by incorporating non-uniform filter measurements into the traditional technique. The non-uniform filter, with varying spectral sensitivity, is placed on the camera lens and the sensor measurements are sequentially integrated into the Bayesian probabilistic formulation. The main goals of this paper are to present a framework for incorporating non-uniform filter measurements and to demonstrate that these additional measurements can reduce the impact of the prior in Bayesian color constancy. The proposed method is tested using a filter with two different spectral sensitivities and is shown to outperform previous approaches in experiments on real data.
411284	41128428	Detecting, localizing and classifying visual traits from arbitrary viewpoints using probabilistic local feature modeling	This article presents a framework for detecting, localizing, and classifying visual traits of object classes, such as gender and age, from different viewpoints. The framework uses local scale-invariant features and probabilistic quantification to identify features related to the desired visual traits. An appearance model is first learned for the object class, followed by training a Bayesian classifier to identify relevant features. This framework can be applied in realistic scenarios, unlike other techniques that require pre-aligned and cropped data. Experimentation on a standard database shows that this approach can accurately identify gender in face images from varying viewpoints, with lower error rates compared to other methods.
411285	41128546	Entropy-based gaze planning	The paper discusses an algorithm for identifying known objects in an unstructured environment through a monochrome television camera attached to a mobile observer. The algorithm utilizes an entropy map to guide the observer along an optimal trajectory that minimizes ambiguity and data collection. The recognition process is based on optical flow signatures generated by camera motion, which can be ambiguous. Gaze planning is used to improve discriminability, and a sequential Bayes approach is used to handle remaining ambiguity by accumulating evidence over time. Results from an experimental recognition system demonstrate the effectiveness of the algorithm on a variety of common objects.
411285	41128524	Informative Views and Sequential Recognition	This paper presents a method for differentiating between informative and uninformative viewpoints of an active observer trying to locate an object in a familiar environment. The approach utilizes a generalized inverse theory and a probabilistic framework, representing assertions through conditional probability density functions. This allows for the evaluation of beliefs associated with a set of assertions using data collected during the observation process. The proposed method provides a way to effectively identify informative viewpoints and improve the accuracy of object identification in known environments.
411286	41128653	Minimizing counterexample with unit core extraction and incremental SAT	K Ravi has proposed a two-stage algorithm for minimizing counterexamples by eliminating irrelevant variables. However, the second stage of the algorithm, called BFL, is time-consuming due to its reliance on a SAT solver for each candidate variable to be eliminated. To address this issue, a faster algorithm has been proposed that uses unit core extraction and incremental SAT techniques. This allows for the elimination of multiple variables at once and avoids repeating searches in similar instances. Theoretical analysis and experiments have shown that this new approach is significantly faster than K Ravi's algorithm while still effectively eliminating irrelevant variables.
411286	41128656	A faster counterexample minimization algorithm based on refutation analysis	The elimination of irrelevant variables from counterexamples is a hot research topic. The BFL algorithm is the most effective approach, but it has a large time overhead due to calling the SAT solver for each variable to be eliminated. To reduce this time, a new algorithm based on refutation analysis is proposed. This algorithm identifies the set of variables that lead to UNSAT instances in BFL and eliminates all other variables simultaneously. Experimental results show that this new algorithm is significantly faster than BFL, with minimal impact on its ability to minimize counterexamples. 
411287	41128776	Rate estimation for CABAC with low complexity	CABAC is an important technology in H.264/AVC for improving coding efficiency. However, its use in rate-distortion optimized encoding requires a lot of computations, particularly in the rate calculation for choosing the best coding mode. To address this issue, a fast rate estimation method for CABAC was proposed in this paper. The method utilizes two equations based on the probability state index and the match between the most probable symbol and the current binary to be coded. Simulation results showed that the proposed method can achieve significant run-time savings without any negative impact or additional memory requirements. Specifically, the method achieved an average run-time saving of 19.27% (up to 22.46%). 
411287	41128726	Early determination of mode decision for HEVC	This paper presents a new approach for reducing the complexity of video encoding in high efficiency video coding. The method involves detecting the SKIP mode early on in the coding process, based on the differential motion vector and coded block flag at the CU-level. Experimental results demonstrate that this technique can decrease encoding complexity by up to 34.55% in random access configuration and 36.48% in low delay configuration, with a minimal increase in rate compared to the reference software. This suggests that the proposed method can effectively reduce encoder complexity while maintaining video quality.
411288	41128816	Finite-State Models for Computer Assisted Translation	Current automatic translation methodologies are not capable of producing high quality translations. However, certain techniques based on these methodologies can improve the efficiency of human translators. One such technique is the use of finite-state transducers, which have been successful in tasks like speech and handwriting recognition, and domain-specific machine translation. These models have advantages such as being able to learn from bilingual data and having efficient algorithms for parsing translations. A new approach called interactive search has been proposed, where the user can input a prefix of the desired translation and the system will produce the full translation in real-time. This approach has been tested on a corpus of printer manuals and has shown promising results, with human translators only needing to type 25% of the characters in the final translation. 
411288	41128823	The EuTrans Spoken Language Translation System	The EuTransAll project is focused on creating automatic Machine Translation systems for specific domains using example-based methods. In the first phase of the project, a speech-translation system was developed using automatically learned subsequential transducers. This paper provides a detailed overview of the transducer-learning algorithms and system architecture, as well as a new approach for using categories in both input and output languages. Experiments were conducted on a hotel-reception communication task with a vocabulary of 683 Spanish words, resulting in a translation word-error rate of 1.97% in real-time on a Personal Computer. 
411289	4112894	Undecidability Results for Database-Inspired Reasoning Problems in Very Expressive Description Logics.	The field of knowledge representation is drawing inspiration from database theory, specifically in the areas of description logics and ontology languages. Interest has shifted from satisfiability checking to query answering, using query notions from databases such as conjunctive queries and path queries. The finite model semantics is becoming a viable alternative to the traditional unrestricted model semantics. This paper investigates database-inspired reasoning problems for highly expressive description logics, all featuring inverses, counting, and nominals. These problems involve role paths of unbounded length, resulting in non-locality. Undecidability has been established for all cases, including undecidability of finite entailment of unions of conjunctive queries for a fragment of SHOIQ and conjunctive queries for a fragment of SROIQ, both commonly used in ontology languages like OWL.
411289	41128936	Schema-Agnostic Query Rewriting in SPARQL 1.1.	SPARQL 1.1 is a language used to query RDF databases. It allows for the use of ontologies and OWL 2, a fragment of OWL, to enhance query results with logical entailments. Traditionally, OWL QL schemas were used to rewrite queries into equivalent sets, but with the adoption of SPARQL 1.1, databases are now able to answer more complex queries directly. This has led to the development of a new method of query rewriting that does not depend on the schema. This means that OWL QL queries can be rewritten into SPARQL 1.1 queries without needing to extract or preprocess OWL axioms. This allows for querying RDF data under OWL QL entailment with just one query. 
411290	41129026	Complex Pipelined Executions in OpenMP Parallel Applications	This paper introduces the idea of using directives in the OpenMP programming model to handle complex pipelined computations. These directives specify the order in which tasks should be executed, which is based on a name space that identifies the work being distributed by work-sharing constructs. This approach eliminates the need for programmers to manually create synchronization data structures and insert explicit synchronization actions in the code, which can make it difficult to understand and maintain. The compiler and OpenMP runtime library handle this process automatically. The proposal is demonstrated and evaluated with a multi-block example and is supported by the NanosCompiler for OpenMP.
411290	411290130	Nanoscompiler: Supporting Flexible Multilevel Parallelism Exploitation In Openmp	The NanosCompiler is a parallelizing compiler that supports nested parallelism in OpenMP. It uses a hierarchical internal program representation to capture parallelism expressed by the user and automatically discovered through analysis of data and control dependences. The compiler assigns work to threads, establishes execution order, and selects mechanisms for parallel execution. It also allows for experimentation with different work allocation strategies for nested parallel constructs and proposes OpenMP extensions for specifying thread groups and their precedence relations. This paper discusses the features and capabilities of the NanosCompiler and its potential for improving parallel program performance. Copyright (C) 2000 John Wiley & Sons, Ltd.
411291	41129171	Opinion graphs for polarity and discourse classification	The article discusses the construction of opinion graphs that incorporate both opinions and discourse relations. This allows for a more comprehensive interpretation of text, as the graphs consider the interplay between opinions and discourse. By using this approach, polarity classification and discourse-link classification can be enhanced and improved. This inter-dependent framework has the potential to improve the performance of local polarity and discourse-link classifiers.
411291	41129132	Recognizing stances in online debates	This paper introduces an unsupervised method for analyzing opinions in online debates, specifically determining which side of the debate a person is taking. To account for the complexities of this genre, the researchers use web mining to identify associations that indicate opinion stances in debates. They also incorporate discourse information and frame the task as an Integer Linear Programming problem. The results demonstrate that this method outperforms other baseline methods.
411292	41129281	Modified genetic algorithm-based feature selection combined with pre-trained deep neural network for demand forecasting in outpatient department.	The study proposes a novel approach for forecasting demand for key resources in a hospital's outpatient department (OPD) using a combination of a modified genetic algorithm (MGA) for feature selection and a pre-trained deep neural network (DNN). The approach is applied to real data from a hospital in China and is found to outperform other feature selection methods and demand forecasting models. The use of MGA and pre-trained DNN can provide valuable insights for staff scheduling and resource allocation in the OPD. This approach can be a useful tool for demand forecasting in hospitals and can improve the efficiency and accuracy of feature selection and forecasting compared to traditional methods.
411292	41129211	Secure Routing Protocol based on Multi-objective Ant-colony-optimization for wireless sensor networks.	This paper proposes a Secure Routing Protocol based on Multi-objective Ant-colony-optimization (SRPMA) for wireless sensor networks to achieve maximum network security while reducing energy consumption. The ant colony algorithm is improved to consider residual node energy and trust value as optimization objectives. The node trust is evaluated using an improved D-S evidence theory and the multi-objective routing results are obtained using the Pareto optimal solution mechanism. Simulation results using NS2 demonstrate the effectiveness of the proposed algorithm in combating the black hole attack in WSN routing.
411293	41129312	Methodology for Performance Evaluation of the Input/Output System on Computer Clusters	The growing use of high performance computing and its complex scientific applications requires more efficient Input/Output (I/O) systems. To ensure efficient use of I/O, this paper proposes a methodology for evaluating I/O performance on computer clusters with different configurations. This evaluation is crucial in understanding how different I/O subsystem setups affect application performance. The approach involves analyzing the I/O system at three levels: application, I/O system, and I/O devices. By varying system configurations and I/O operation parameters, the impact on performance is evaluated considering both the application and I/O architecture. This analysis also identifies factors that can be configured to improve I/O system performance and helps select the best configuration for a specific application.
411293	41129338	Analyzing the Parallel I/O Severity of MPI Applications.	In High Performance Computing (HPC), evaluating the performance of parallel applications is crucial. This also applies to parallel I/O performance, which requires an understanding of the application's I/O pattern and the capabilities of the HPC I/O system. This paper presents a methodology for evaluating I/O performance based on the concept of I/O severity, which considers the application's I/O requirements, I/O process mapping, and I/O subsystem configuration. These requirements are measured in I/O phases, defined by the temporal and spatial patterns of different files in the application. The methodology was tested on scientific applications on a supercomputer, and results showed its effectiveness in identifying I/O limitations and potential causes of I/O problems. 
411294	41129468	A decision support system for hospital emergency departments designed using agent-based techniques	This paper discusses an ongoing project that aims to create a model and simulation to assist hospital emergency department (ED) heads in making informed decisions. The model is an Agent-Based Model consisting of active agents representing people and passive agents representing services and systems. These agents interact using Moore state machines and the model includes the communication system and environment. The simulation, developed using NetLogo, evaluates the benefits of diverting non-urgent patients from the ED to primary care services. This project has the potential to aid ED decision-making and improve patient flow.
411294	411294104	An Agent-Based Decision Support System For Hospitals Emergency Departments	Modeling and simulation have been proven to be valuable tools in the field of Healthcare operational management. In particular, they have been found to be useful in hospital emergency departments (ED), which are known to be dynamic and complex. This paper discusses an ongoing project led by the Research Group in Individual Oriented Modeling (IoM) at the University Autonoma of Barcelona (UAB) and the ED Staff Team at the Hospital of Sabadell. The project aims to create a simulator that can assist ED heads in making well-informed decisions. The proposed model is an Agent-Based Model, consisting of active agents representing human actors and passive agents representing services and other reactive systems. The model also includes the environment in which agents interact, and it utilizes Moore state machines with probabilistic transitions to represent agent actions and communication. To validate the model, an initial simulation was developed using NetLogo, a suitable agent-based simulation environment for complex systems. 
411295	4112955	A Branching-Process-Based Method to Check Soundness of Workflow Systems.	Workflow nets (WF-nets) are commonly used to model and analyze workflow systems. One important property of WF-nets is soundness, which ensures that the system is free of deadlocks and livelocks and that each task can be completed. Van der Aalst has proven that the soundness problem is decidable for WF-nets and that it is PSPACE-complete for bounded ones. This means that soundness can be determined by analyzing the reachability graph of the net. However, this approach is hindered by the state explosion problem. To address this issue, this paper proposes an algorithm to generate a finite prefix of the unfolding of a WF-net, known as basic unfolding. This technique can effectively save storage space and a necessary and sufficient condition is also proposed to decide soundness based on basic unfolding.
411295	41129577	Modeling and monitoring of E-commerce workflows	E-commerce is a significant aspect of the Internet that is reshaping commercial transactions. However, the complexity of E-commerce systems and the absence of well-designed formal models make it challenging to model and monitor them effectively. This paper introduces labeled workflow nets (LWN) and their semantics, based on labeled Petri nets, as a suitable method for modeling E-commerce systems. It also presents an inter-organizational labeled workflow net (ILWN) that can graphically represent the dynamic behavior of these systems. The nonblocking property of ILWNs is defined to ensure a common goal among E-commerce participants. The proposed framework, E-commerce workflows (ECW), records the history of interactive events and monitors the execution of interactive activities to achieve a common goal. The main contribution of this paper is the introduction of ILWN as a formal mechanism that efficiently supports the modeling and monitoring of ECWs. A Customer-Producer-Supplier example is used to illustrate the application of this method.
411296	411296232	A Supervised Learning and Control Method to Improve Particle Swarm Optimization Algorithms.	The paper introduces a new optimization method called adaptive particle swarm optimization with supervised learning and control (APSO-SLC) to improve the performance of traditional particle swarm optimization (PSO) by addressing issues such as difficult parameter setting and premature convergence. APSO-SLC uses strategies from machine learning and control fields to adaptively choose parameters and maintain diversity in the swarm. It treats PSO as a system to be controlled and models it as a dynamic quadratic programming model with box constraints. Recursive least squares with a dynamic forgetting factor is used to estimate the parameters and back diffusion and new attractor learning are implemented to prevent premature convergence. Experiments conducted on benchmark functions show that APSO-SLC outperforms traditional PSO in terms of simplicity, consistency, and overall performance. 
411296	411296307	Scheduling Dual-Objective Stochastic Hybrid Flow Shop With Deteriorating Jobs via Bi-Population Evolutionary Algorithm	Hybrid flow shop scheduling problems have become increasingly important in real-world production systems. Previous studies have assumed that job processing times are deterministic and constant, but in reality they are often uncertain and variable due to factors such as machine wear and job characteristics. This paper presents a dual-objective stochastic hybrid flow shop scheduling problem, aiming to minimize both makespan and total tardiness. A hybrid multiobjective optimization algorithm is developed, incorporating global and local search methods, an information sharing mechanism, and resource allocation. Simulation experiments show that this algorithm outperforms other classical methods in solving the problem.
411297	41129750	The importance of trust in software engineers' assessment and choice of information sources	Engineers, specifically systems developers, primarily rely on their colleagues and internal reports for information. This is often explained by the principle of least effort, where engineers choose easily accessible sources rather than high-quality ones. However, this study argues that engineers also prioritize sources with a known or easily determinable trustworthiness. Trust is crucial in assessing the quality of information, as it is a perceived property. In a field study of a software design project, engineers placed more emphasis on quality-related factors when discussing and selecting information sources. They also tend to provide context for unfamiliar sources in conversation. Systems for managing knowledge and sharing expertise should consider these factors in order to accurately assess the credibility of information. 
411297	41129717	Visual overview, oral detail: The use of an emergency-department whiteboard	This study explores how whiteboards are used in an emergency department to facilitate coordination among healthcare professionals. The whiteboard serves as a visual overview of information, while oral communication allows for more detailed explanations and decision-making. The coordinating nurse, who is responsible for updating the whiteboard, plays a crucial role in this process. The study finds that the whiteboard and coordinating nurse are perceived as a unit, with clinicians regularly updating the whiteboard through communication with the nurse. This smooth integration of technology and communication is essential for effective coordination. The study suggests implications for the design of whiteboards in healthcare settings.
411298	41129813	Are Digraphs Good for Free-Text Keystroke Dynamics?	Keystroke dynamics research has primarily focused on patterns found in fixed text, such as usernames and passwords. Digraphs and trigraphs have been found to be effective features in this context. However, there is growing interest in free-text keystroke dynamics, where the user can type anything they want. This raises the question of whether digraphs and trigraphs are still effective in this scenario. This paper aims to answer that question and finds that general digraphs and trigraphs are not as effective, and instead, word-specific ones are needed. The study also shows that the typing patterns for some words can vary depending on their position in a larger word. This is the first study to investigate these issues and could inform future research on features for free-text keystroke dynamics.
411298	41129857	Using Targeted Statistics For Face Regeneration	Face occlusion can be a problem in applications that analyze images for faces, affecting detection, tracking, and recognition algorithms. This paper proposes a solution by considering occlusion as a damaged part that needs to be regenerated. The technique learns the statistical correlation between different regions of the face and uses this to predict the appearance of the occluded region from the most correlated unoccluded region of the same face. The effectiveness of the technique is shown to be influenced by factors such as the filtering of the dataset, the presence/absence of the target face during learning, and the location and size of the occlusion. This technique can be used to improve face processing algorithms or enhance face images for human viewing.
411299	41129935	Cross-language plagiarism detection	Cross-language plagiarism detection is the process of automatically identifying and extracting plagiarized sections in a multilingual setting. This involves analyzing a suspicious document and retrieving sections from a large, multilingual document collection that are found to be similar. The authors of this paper introduce a comprehensive retrieval process for cross-language plagiarism detection and review state-of-the-art solutions for two important subtasks. They also survey different retrieval models and compare the performance of three specific models: CL-CNG, CL-ESA, and CL-ASA. The evaluation is based on a large test dataset of 120,000 documents in six languages, and the results show that CL-CNG is the most effective model for ranking and comparing syntactically related texts across languages. CL-ESA performs well on arbitrary language pairs, while CL-ASA is best suited for "exact" translations but does not generalize well.
411299	41129946	Overview of the PAN/CLEF 2015 Evaluation Lab	The PAN/CLEF evaluation lab is a prominent research forum for text mining, specifically focusing on identifying personal traits of authors through their written texts. PAN 2015 featured three tasks: plagiarism detection, author identification, and author profiling. A new approach to developing evaluation resources was introduced through community-driven corpus construction, providing diversity in the data. The author identification task included cross-topic and cross-genre author verification, where texts of known and unknown authorship did not match in topic or genre. A new corpus was created for this challenging task, covering four languages. Author profiling also introduced five personality traits and a new corpus of Twitter messages in four languages. 53 teams participated in all three tasks, with submissions evaluated through the TIRA experimentation framework.
411300	41130017	Why Attackers Win: On the Learnability of XOR Arbiter PUFs.	Physically Unclonable Functions (PUFs) are being developed as a solution for secure storage and hardware authentication. However, vulnerabilities to Machine Learning (ML) attacks have been observed in one type of PUF, called arbiter PUFs. To counter these attacks, PUF manufacturers have shifted their focus to nonlinear architectures, specifically the XOR arbiter PUF with a large number of chains. This paper presents a learning framework for XOR arbiter PUFs, establishing a theoretical limit on the number of chains that can be learned in polynomial time with a certain level of accuracy and confidence. The paper also addresses the issue of noisy responses and concludes that no secure XOR arbiter PUF can currently be manufactured using existing IC technologies.
411300	41130012	Physical Characterization of Arbiter PUFs	Physically Unclonable Functions (PUFs) are a type of security solution used to combat insecure storage and hardware counterfeiting. However, successful attacks on certain digital PUFs have revealed vulnerabilities. While some types of PUFs, like SRAM PUFs, can be physically cloned, timing-based PUFs have only been vulnerable to modeling attacks. In this paper, the authors prove that all controlled and XOR-enhanced PUFs can be characterized and solved using photonic emission analysis, with a resolution of 6 picoseconds. This method requires the minimum number of linear equations and does not require knowledge of the PUF challenges or responses, making it a powerful tool for physical delay extraction. The authors demonstrate their results on a real arbiter PUF implementation on a Complex Programmable Logic Device (CPLD).
411301	411301150	Exploring presentation methods for tomographic medical image viewing	This paper discusses the challenges of presenting tomographic medical images on a computer screen due to the limitations of displaying multiple images at once. The focus is on magnetic resonance images and proposing filmless computer presentation methods. The study observes the traditional light screen environment and proposes solutions for meeting requirements in the computer environment. User feedback is obtained to determine feasibility and preferences, revealing three requirement categories: user control of film management, navigation of images and image series, and simultaneous availability of detail and context. The paper also introduces a framework for viewing tomographic medical images and presents solution directions for radiologist feedback. Results support the feasibility of proposed approaches and highlight the importance of presentation in medical imaging systems. 
411301	411301132	A framework for unifying presentation space	In user interface design, effectively utilizing display space has been a major concern. Despite advances in CPU power and memory, the sizes of our screens have not significantly increased, and the amount of information we want to display has grown. This has led to difficulties in navigation, interpretation, and recognition. Various presentation approaches have been proposed to address these issues, resulting in visually and algorithmically diverse displays. A unified framework has been developed to connect these methods and allow for multiple presentation options within a single interface. This framework also supports combining different presentation methods and offers a geometric presentation library for easy exploration and experimentation. 
411302	41130279	Exploring emotions and multimodality in digitally augmented puppeteering	The use of multimodal and affective technologies has raised new research questions in the development of expressive and engaging interactions. Two key challenges in this area are creating seamless multimodal systems for customized performance and content generation, and effectively utilizing emotional cues to create interactive loops. PuppetWall is a multi-user, multimodal system designed for digitally augmented puppeteering. It allows for natural interaction through hand movement tracking, multi-touch display, and emotion speech recognition. The system was evaluated with professional actors to explore expressive speech categories and identify challenges in tracking emotional cues from acoustic features. These findings have implications for the design of affective interactive systems.
411302	41130233	Active construction of experience through mobile media: a field study with implications for recording and sharing	This study examines the impact of interactive and ubiquitous multimedia on human memory and experience at a large-scale event. Researchers observed two groups of spectators at a FIA World Rally Championship in Finland, both using multimedia mobile phones. The study found that technology-mediated memories played an active role in constructing experiences, with elements like continuity, reflexivity, and group identity reflected in how multimedia was used. The study also highlights the importance of temporality and spatiality in creating experiences. The authors suggest that multimedia applications should not just record or store experiences, but actively participate in their construction. This has valuable design implications for creating engaging and shared experiences through multimedia.
411303	41130316	FixMyStreet Brussels: Socio-Demographic Inequality in Crowdsourced Civic Participation.	FixMyStreet (FMS) is a website that allows people to report environmental issues to the government, such as potholes and damaged pavements. This study examines the use of FMS in Brussels, Belgium, by analyzing 30,041 reports since it was established in 2013. The researchers found that there were significant differences in FMS use between the ethnically diverse districts in Brussels. They compared FMS use to sociodemographic indicators and social media data from Twitter. Their analysis showed that crowdsourced civic participation platforms may exclude low-income and diverse communities. These findings highlight the need for more inclusive design of such platforms in the future.
411303	41130350	OpenWindow: citizen-controlled content on public displays	This paper explores the potential of public displays by giving control over content creation to citizens rather than a central authority. The researchers conducted a three-week field study in which households were invited to provide their own content for small public displays behind their windows. They found that citizen-controlled public displays can have a positive impact on both participants and their community, promoting social cohesion and providing an opportunity for discourse. However, the effectiveness of these displays can be influenced by pre-existing social, cultural, or linguistic issues. Overall, the study highlights the potential for public displays to serve as a democratic medium of public communication in urban neighborhoods.
411304	41130468	Textile Interfaces: Embroidered Jog-Wheel, Beaded Tilt Sensor, Twisted Pair Ribbon, and Sound Sequins	Electronic textiles, or e-textiles, aim to merge electronics and computing with fabric. To further this goal, a team has developed a multi-use jog wheel, sound sequins made from PVDF film, and a tilt sensor using a hanging bead, embroidery, and capacitive sensing. In order to enable capacitive sensing over longer distances on the body, they have also created a twisted pair ribbon and tested its effectiveness compared to traditional sensing methods. The team shares their construction techniques and insights gained from this exploration of new e-textile interfaces and construction techniques in their Electronic Textile Interface Swatch Book.
411304	411304103	Mobile music touch: mobile tactile stimulation for passive learning	Mobile Music Touch (MMT) is a haptic music instruction system that helps users learn to play piano melodies while performing other tasks. It consists of fingerless gloves and a mobile Bluetooth-enabled device, like a phone, where passages to be learned are loaded. As the music plays, vibrators on each finger activate to indicate which finger should be used. Two studies were conducted to test the effectiveness of MMT. The first showed that using MMT for 30 minutes while performing a reading comprehension test was more effective than simply listening to the passage without finger vibrations. The second study compared passive and active training, and found that participants with no piano experience were able to repeat short passages after passive training, while those with piano experience often could not.
411305	41130564	A near-linear-time algorithm for computing replacement paths in planar directed graphs	The replacement paths problem involves finding the shortest path from a starting point s to an ending point t in a directed graph, while avoiding a specific edge e in the graph. The fastest known algorithm for this problem is to remove each edge in the shortest path and compute the distance from s to t in the modified graph, with a running time of O(mn + n2 log n). This problem has applications in finding k simple shortest paths and in computing Vickrey pricing in distributed networks. A recent article presents a near-linear time algorithm for solving this problem in weighted planar directed graphs, improving the running time of related applications by almost a linear factor. The algorithm uses new ideas and a data structure that supports multisource shortest paths queries in planar directed graphs in logarithmic time. It can also be adapted to handle different variations of the problem, such as finding the actual replacement path or avoiding specific vertices instead of edges. 
411305	411305128	Fault-tolerant spanners for general graphs	The paper discusses graph spanners that are resistant to failures of vertices or edges. A spanner is a subgraph of a graph that preserves distances between vertices within a certain stretch factor. These spanners have been extensively studied for over two decades and it is known how to efficiently construct them with a size-stretch tradeoff. The paper introduces the concept of fault tolerant spanners, which can tolerate failures of a certain number of vertices or edges. The paper presents an algorithm for constructing fault tolerant geometric spanners and also addresses the question of whether it is possible to construct a fault tolerant spanner for any undirected weighted graph. The paper provides a positive answer to this question and presents a fault tolerant spanner with a slightly larger size but the same stretch factor. The paper also discusses fault tolerant spanners for edge failures and presents an efficient algorithm for constructing them.
411306	41130611	A parallel algorithm for finding a separator in planar graphs	Our algorithm presents a randomized parallel approach to finding a simple cycle separator in a planar graph. This separator has a size of O(√n) and can separate the graph so that the largest part contains at most 2/8 ċ n vertices. The algorithm has a time complexity of T = O(log2(n)) and can be run on P = O(n + f1+ε) processors, with n being the number of vertices, f being the number of faces, and ε being any positive constant. It is based on a sequential algorithm by Lipton and Tarjan, which has a time complexity of O(n). When combined with another algorithm, it can find a BFS of a planar graph in O(log3(n)) time using n1.5/log(n) processors. A variation of the algorithm can also construct a simple cycle separator of size O(d ċ √f), where d is the maximum face size.
411306	41130651	Planar separators and the Euclidean norm	This paper presents a new algorithm for finding simple cycle separators in 2-connected embedded planar graphs with faces of sizes d! to df. The algorithm has a time complexity of O(m~(n. n)), making it almost linear. This is a significant improvement over previous algorithms, as it eliminates the need to construct or work with the face-incidence graph. The size of the separator is shown to be 1.58~,/dl 2 +...+d}, which is no larger than previous upper bounds expressed in terms of ~S. v, where d is the maximum face size and v is the number of vertices. The algorithm is also simpler to implement, as it works directly with the planar graph and its dual.
411307	4113079	On the locality of distributed sparse spanner construction	The paper presents a deterministic distributed algorithm that can construct a (2k-1,0)-spanner, with an optimal stretch-size trade-off, for an unweighted graph with n nodes in k rounds. If n is not known to the nodes, the algorithm can still construct the spanner in 3k-2 rounds. This algorithm is then used to propose another algorithm that can construct a (1+ε,2)-spanner with O(ε-1 n3/2) edges in O(ε-1) rounds, without prior knowledge of the graph. Lower bounds are also presented, showing that any (randomized) distributed algorithm must take at least k rounds in expectation to construct a (2k-1,0)-spanner of o(n1+1/(k-1)) edges. Additionally, it is shown that any (randomized) distributed algorithm that constructs a spanner with fewer than n1+1/k + ε edges in at most nε expected rounds must stretch some distances by a factor of nΩ(ε). This means that while spanners with O(n1+1/k) edges may exist, they cannot be constructed distributively in a sub-polynomial number of rounds in expectation.
411307	41130767	Local computation of nearly additive spanners	An (α, β)-spanner is a subgraph of a graph G that approximates distances within a given multiplicative factor α and an additive error β. This paper discusses algorithms for constructing a sparse (α, β)-spanner in a distributed and deterministic manner. The first algorithm can construct an (α, β)-spanner with O(βn1+1/k) edges in a constant number of rounds, matching the performance of the best known algorithm. It can also produce a (1 + Ɛ, 2 - Ɛ)-spanner with O(n3/2) edges in constant time for k = 2 and constant Ɛ. Additionally, it can construct a (1 + Ɛ, O(1/Ɛ)k-2)-spanner with O(Ɛ-k+1n1+1/k) edges in constant time. The second algorithm is based on computing small dominating sets and maximal independent sets and can construct a (1 + Ɛ, β)-spanner with O(βn1+1/k) edges, where β = klog(log k/Ɛ)+O(1). Overall, these algorithms provide the best known trade-off between k and Ɛ.
411308	41130828	Evolution of Spiking Neural Controllers for Autonomous Vision-Based Robots	This article presents a series of experiments on evolving spiking neural controllers for a mobile robot using vision-based inputs. The experiments were conducted on physical robots without human intervention, and the results showed that functional controllers were quickly evolved using a simple genetic encoding and fitness function. The study also included a neuroethological analysis of the network activity to understand the functioning of the controllers and the relative importance of individual neurons. Additionally, the controllers were found to be robust to synaptic strength decay, which is commonly seen in hardware implementations of spiking circuits. Overall, the research demonstrates the potential for using evolutionary algorithms to develop efficient and robust controllers for autonomous robots.
411308	41130818	Evolution of Altruistic Robots	The document discusses the various evolutionary methods that can result in the emergence of altruistic cooperation in robot collectives. Four algorithms based on biological theories are compared in two scenarios where cooperation can improve performance. The authors analyze the strengths and weaknesses of each method and provide recommendations for choosing the most suitable one for evolving altruistic robots. They also examine the concept of altruistic cooperation in nature, distinguishing between situations where there is no cost to the cooperator and situations where there is a fitness cost. Cooperation with immediate and direct benefits is common in nature, while cooperation with indirect benefits requires initial tendencies to cooperate, repeated interactions, and the ability to recognize and remember past interactions. 
411309	41130976	Posi-modular Systems with Modulotone Requirements under Permutation Constraints	The transversal problem involves finding a minimum set of elements in a finite set V, denoted as R, such that a given function f is less than or equal to a modulotone function r for all subsets of V not containing R. This problem is a generalization of the source location problem and external network problem in undirected graphs and hypergraphs. A greedy algorithm can be used to solve the transversal problem if the modulotone function r is 驴-monotone, meaning it satisfies a certain condition. The minimal deficient sets for the transversal problem can also be characterized by a basic tree structure. A fractional version of the problem can also be solved using a similar algorithm.
411309	4113091	Dual-bounded generating problems: Efficient and inefficient points for discrete probability distributions and sparse boxes for multidimensional data	This study proves that for two finite sets to be separated by a linear function, the absolute value of the sets must hold. This leads to an incremental quasi-polynomial time algorithm for generating all maximal integer feasible solutions, p-inefficient points of a discrete probability distribution, and maximal hyper-rectangles containing a specified fraction of points in a given set. This is a significant improvement over previous exponential time algorithms for similar problems in Integer and Stochastic Programming and Data Mining. Additionally, the study presents an incremental polynomial time algorithm for generating monotone systems with a fixed number of separable inequalities, allowing for separate generation of p-efficient and p-inefficient points in discrete probability distributions with independent coordinates.
411310	4113108	Outfix-Free Regular Languages and Prime Outfix-Free Decomposition	An outfix is a string that can be inserted between two parts of another string to create a longer string. A set of strings is outfix-free if none of the strings in the set can be an outfix of any other string. We have developed an efficient algorithm to determine if a regular language is outfix-free, which means it only contains a finite number of strings. This algorithm works for both finite sets of strings and languages represented by finite-state automata. We also explore the concept of prime outfix-free decomposition for these languages and have created a fast algorithm to compute this decomposition, which is unique for every outfix-free regular language.
411310	41131062	Pseudo-inversion: closure properties and decidability	The concept of pseudo-inversion, inspired by biological events like DNA transformations, involves reversing only parts of a string. This operation is studied from a formal language perspective and it is shown that regular languages can be transformed through pseudo-inversion while context-free languages cannot. The iterated pseudo-inversion operation is also explored, showing that it can be recognized by a specific type of machine. The idea of pseudo-inversion-freeness is introduced and its effects on closure and decidability properties for regular and context-free languages are examined. It is determined that determining pseudo-inversion-freeness is polynomial-time decidable for regular languages but undecidable for context-free languages.
411311	41131142	Integrating conflicting data: the role of source dependence	Data management applications often require integrating data from multiple sources, which can lead to conflicting values. To ensure high-quality data for users, it is important for data integration systems to resolve conflicts and find true values. This can be challenging, as false values can be spread through copying. In this paper, the authors propose a novel approach that takes into account dependence between data sources to improve truth discovery. They use Bayesian analysis to detect dependence and consider factors such as accuracy and similarity to further improve accuracy. Results from experiments on both synthetic and real-world data show the effectiveness and scalability of their algorithm.
411311	41131134	Data Fusion: Resolving Conflicts from Multiple Sources.	Data management applications often require integrating data from multiple sources. However, these sources may provide conflicting values, making it crucial to resolve these conflicts and discover accurate values. This process is known as data fusion. A new approach is proposed in this paper to find true values from conflicting information in cases where there are numerous sources, some of which may copy from others. A case study is presented to demonstrate the effectiveness and scalability of this algorithm in improving the accuracy of truth discovery. 
411312	41131261	Joint Segmentation and Recognition of Categorized Objects from Noisy Web Image Collection.	The problem of segmenting categorized objects across a collection of images is addressed in this article. Existing methods assume that all images contain the target object, which may not be the case in noisy image collections gathered from image search engines. To overcome this, a co-training algorithm is proposed which combines automatic object segmentation and category recognition. The object segmentation algorithm is trained on a subset of images with high confidence, while the category recognition model is guided by its results. This approach is shown to be effective on four datasets, including a new dataset with hand-annotated category and segmentation labels. The method outperforms previous techniques and is capable of handling noisy image collections.
411312	41131279	Automatic salient object extraction with contextual cue	The authors propose a new method for automatically extracting a significant object from an image using an energy minimization technique. Unlike previous methods that only use appearance information, this method also incorporates an auto-context cue. A saliency model is used to bootstrap the process, and the salient object is segmented and the auto-context model is learned iteratively without any user intervention. The resulting separation of the salient object is clear, and the learned auto-context classifier can be used to recognize similar objects in other images. Experiments on four benchmarks show that the method outperforms state-of-the-art methods, some of which require user interactions. 
411313	41131333	A mathematical model for human flesh search engine	The Human Flesh Search Engine (HFSE) is a phenomenon where individuals use online platforms, such as blogs and forums, to expose personal information of people who have committed perceived wrongdoings. With the rise of the internet, the HFSE has become a powerful tool in uncovering information that would otherwise be difficult to obtain. While previous research has focused on the legal and privacy implications of this tool, this study aims to create a mathematical model to better understand the search process and evaluate the effectiveness of this collaborative intelligence. By viewing the initiator and target of a search as nodes in a social network, the HFSE is modeled as a probabilistic flooding routing algorithm. The model is validated through case studies and simulations, providing new insights into the workings of the HFSE. 
411313	4113138	Efficient indexing for large scale visual search	Many large-scale image retrieval systems use text indexing techniques to represent images as a "bag of visual terms". However, there is a fundamental difference between image and text queries, leading to misleading results with some techniques. To address this issue, a novel indexing technique is proposed that decomposes the document-like representation of an image into two components: one for dimension reduction and one for preserving residual information. This allows for efficient indexing and retrieval, and has better generalization abilities compared to other dimension reduction algorithms. The technique can be achieved through a graphical model or matrix factorization approach, and has been shown to be scalable and accurate in experiments with a 2.3 million image database. 
411314	41131418	An image rectification scheme and its applications in RST invariant digital image watermarking	This research paper proposes a method for image rectification that can be used with any image watermarking algorithm to improve its robustness against rotation, scaling, translation, and flipping transformations. The method involves using log-polar mapping (LPM) to detect these transformations and compute the cross-correlation between a template and the LPM of the transformed image to determine the parameters of the transformation. This method is also efficient and cost-effective, with templates that can be compressed. The paper also presents experimental results and comparisons with other filtering methods, demonstrating the effectiveness of the proposed method. Additionally, the paper discusses three potential applications for this rectification scheme.
411314	41131421	Contourlet-based image and video watermarking robust to geometric attacks and compressions.	The paper presents a new blind image watermarking scheme that is robust against geometric attacks and compressions. The scheme uses contourlet transform (CT) and principal component analysis (PCA) to embed the watermark, and noise visibility function (NVF) to adjust the watermarking strength to preserve image quality. The watermark can be accurately detected after various distortions, with normalized correlation (NC) used as the evaluation criterion. The simulation results demonstrate the scheme's performance against rotation, scaling, and image compressions. The scheme is then extended to blind video watermarking and evaluated against video attacks. The use of CT provides robustness against compressions, while PCA offers resistance to geometric attacks. The scheme shows good performance in terms of both quality and robustness.
411315	41131511	Semantics-based software techniques for maintainable multimodal input processing in real-time interactive systems	Maintainability is an important quality requirement for software frameworks, especially in the complex system designs of Real-time Interactive Systems (RISs) used in Augmented, Mixed, and Virtual Reality and computer games. This becomes even more challenging when incorporating AI methods for multimodal interfaces and smart environments. To address this issue, existing approaches focus on establishing technical solutions to support the close temporal and semantic coupling needed for multimodal processing while maintaining a general decoupling principle between software modules. The article presents two key solutions for addressing the semantic coupling issue: a semantics-based access scheme and the specification of effects through semantic function descriptions, both modeled in an OWL ontology. The concepts are demonstrated through a prototypical implementation and illustrated with an interaction example in two application areas.
411315	41131535	Short paper: engineering realtime interactive systems: coupling & cohesion of architecture mechanisms	This paper discusses the use of coupling and cohesion as quality criteria for developing Realtime Interactive Systems (RIS). It examines the applicability of these criteria in evaluating RIS architecture mechanisms and discusses the use of existing software metrics. The paper evaluates three commonly found mechanisms - scene graphs, event systems, and entity models - with the goal of minimizing coupling and maximizing cohesion. It also suggests an analytical approach to evaluating software techniques and highlights the importance of considering software technology in the field of interactive simulations, given current challenges such as diversification, parallelization, and interconnection.
411316	411316111	Eliminating High-Degree Biased Character Bigrams for Dimensionality Reduction in Chinese Text Categorization	The main challenge in Text Categorization (TC) is the high dimensionality of the feature space. The use of Chinese character bigrams as features often leads to the inclusion of biased bigrams that are not useful for categorizing documents. This paper proposes a criterion for identifying these high-degree biased bigrams and presents two schemes, sigma-BR1 and sigma-BR2, for dealing with them. The first scheme eliminates the biased bigrams from the feature set, while the second replaces them with significant characters. Experimental results show that eliminating these biased bigrams and using the sigma-BR1 scheme is effective in reducing dimensionality and improving TC performance. This approach should be used after performing feature selection using a Chi-CIG score function.
411316	41131638	Reduce Meaningless Words for Joint Chinese Word Segmentation and Part-of-speech Tagging	The article discusses conventional statistics-based methods for joint Chinese word segmentation and part-of-speech tagging (S&T), which have the ability to recognize new words but also create a significant number of meaningless words. To address this issue, the authors propose a framework that incorporates features from a general lexicon, Wikipedia, and a large-scale raw corpus to improve word segmentation accuracy. The framework includes both character-based and word-based models, resulting in a significant reduction in meaningless word generation and an increase in the F1 measure for segmentation. Experiments on the Penn Chinese treebank 5 dataset show promising results, with a 62.9% reduction in meaningless word generation and an overall F1 measure of 0.984.
411317	41131759	Efficient Dissection of Bicomposite Problems with Cryptanalytic Applications	This paper introduces a new algorithm called dissection, which can solve a wide range of problems with a bicomposite structure. This algorithm has better time and memory tradeoffs compared to previous algorithms. An example of its use is in breaking multiple encryption schemes with independent keys, where it can find all possible keys in a shorter time and with less memory. The improvement ratio of this algorithm increases as the number of keys increases. It can also be combined with parallel collision search for even better tradeoffs. The dissection technique can also be used to improve attacks on hash functions and solve difficult combinatorial search problems.
411317	41131712	Efficient Dissection of Composite Problems, with Applications to Cryptanalysis, Knapsacks, and Combinatorial Search Problems.	The paper discusses a new algorithm called dissection that can solve a variety of problems with a bicomposite structure more efficiently than previous algorithms. For example, it can find the key to multiple encryption schemes with a smaller time and memory requirement. The algorithm has been shown to work for problems with a large number of keys, with the improvement ratio increasing as the number of keys increases. Additionally, by combining dissection with parallel collision search, even better tradeoffs can be achieved. The algorithm also has applications in attacking hash functions, solving hard knapsack problems, and finding the shortest solution to a generalized Rubik's cube.
411318	41131814	Simultaneous recognition and segmentation of cells: application in C.elegans.	The ability to automatically recognize cell identities is crucial for studying gene expression and regulation, cell lineages, and cell fates at a single-cell level in model animals. However, existing methods often rely on segmented cells and may encounter difficulties in new experimental settings. To address this issue, a new method called simultaneous recognition and segmentation (SRS) of cells has been developed and applied to 3D image stacks of the model organism Caenorhabditis elegans. SRS uses an atlas-guided voxel classification process to accurately recognize cells without relying on additional image features. The method achieved a 97.7% recognition accuracy for a key class of cells, and can be applied to other cell types as well.
411318	41131829	Identifying Satellites and Periodic Repetitions in Biological Sequences.	The paper introduces an algorithm for identifying satellites, which are repeats of DNA sequences that vary in length and contain mutations. The algorithm focuses on short to moderately long repeats with up to 15-20% variation from a consensus model. It is composed of two parts: a filter to eliminate unlikely regions and an exhaustive exploration of all possible repeating units. The first phase is efficient and takes O(n) time, while the second phase is more sensitive and takes time O(n . N(e, k)) in the worst case. However, experiments show that it performs better than the worst-case complexity suggests. The algorithm can also be applied to protein sequences and mixed direct-inverse tandem repeats.
411319	41131945	Rapid Construction Of Software Comprehension Tools	This paper discusses the concept of a software comprehension tool that utilizes an abstract software model, various views of the model, and analyses to create a mapping between the model and views. Instead of manually coding these components, the paper suggests using online configuration to set up the analyses, models, views, and their mappings. The paper introduces an architecture for implementing this approach and presents a framework as a proof of concept. Several examples are provided to illustrate the effectiveness and adaptability of this method.
411319	41131931	Automated Architecture Consistency Checking for Model Driven Software Development	Software projects often face the issue of the actual implementation and intended architecture drifting apart, causing problems for future maintenance. To address this, it is important to regularly check the implementation against the architectural description for consistency. The process of Model Driven Software Development (MDSD) allows for automated consistency checks by using information from the implementation, design documents, and model transformations. By applying this approach to a Java project, researchers have identified several inconsistencies that point to design issues. This method helps to detect these inconsistencies early on, ensuring the consistency of the MDSD artifacts and ultimately improving the maintainability and comprehensibility of the software.
411320	41132050	Application access control at network level	This paper presents a network-level access control mechanism that uses pre-computed encrypted counters called tickets to enforce access control decisions made at the application level. The mechanism works by verifying the presence of a valid ticket in each packet, and killing unauthorized packets. The tickets are not based on user data, which allows for efficient verification in shared media LANs. The mechanism is specifically designed for Internet protocols over Ethernet, and its properties for internetworking and multicasting are also discussed. 
411320	41132063	Multiple layer encryption for multicast groups	The proposed framework is designed for large, frequently changing groups and uses a counter-based block cipher mode of operation for access control. This involves intermediary elements in the network that contribute to the encryption, ensuring confidentiality, backward and forward secrecy, and containment of compromised access keys.
411321	41132152	Empirical evaluation of clone detection using syntax suffix trees	Despite the known negative impact of reusing software through copying and pasting, it is still a common practice in software development. To address this issue, various techniques have been proposed to detect duplicated code, also known as software clones. A recent study compared these techniques and found that token-based clone detection using suffix trees is fast but often yields clone candidates that are not syntactic units. On the other hand, techniques based on abstract syntax trees are more accurate but less efficient. This paper presents a new approach that combines the use of suffix trees and abstract syntax trees to efficiently and accurately detect syntactic clones. The results of a large case study on eight software systems in different application domains support the effectiveness of this approach. The paper also includes additional analysis on Java programs and explores an alternative path using parse trees instead of abstract syntax trees. It also investigates the impact of consistent parameter renaming on recall and precision in clone analyses.
411321	4113211	Studying clone evolution using incremental clone detection.	Software clones, or passages of duplicated source code, are a popular topic in both research and practice. Detecting, understanding, and managing these clones can add value to applications. However, current techniques for clone detection are limited to a single version of a program, which can lead to runtime overhead and incorrect mappings. In this paper, the authors present an incremental clone detection algorithm that builds on previous versions' analysis and creates a mapping between clones of consecutive versions. This approach offers advantages in runtime and can be useful for studying clone evolution. Copyright (c) 2010 John Wiley & Sons, Ltd.
411322	41132247	On random hyper-class random forest for visual classification	Random forest is a popular method for classification tasks due to its effectiveness and generalization capabilities. However, for visual classification problems with high dimensions and a large number of classes, the traditional impurity measurement used in training tree nodes may not be sufficient to capture the strong conditional dependencies among visual attributes. To address this issue, a new method called random hyper-class random forest (RHC-RF) is proposed. This method partitions the entire class space into two hyper-classes and trains a weak learner to discriminate between them. This approach maintains the randomness of traditional random forest while also directly targeting the discrimination of classes. Experimental results on various visual classification tasks demonstrate the superior accuracy, compactness, and robustness of RHC-RF compared to traditional random forest.
411322	41132223	Image Classification by Selective Regularized Subspace Learning	Feature learning is an important aspect of image classification, but most methods neglect the small sample size problem, where the number of training samples is smaller than the feature dimensionality. Subspace learning is a solution to this problem, but traditional methods struggle with multi-class classification. To address this, a new approach called selective regularized subspace learning (SRSL) is proposed, which learns a local subspace for each sample instead of a global one. This is achieved through a coarse-to-fine strategy that first identifies classes with similar visual appearance to the testing sample and then conducts subspace learning within those classes. Experimental results on four datasets show the effectiveness of SRSL for multi-class image classification. 
411323	41132368	iCoseg: Interactive co-segmentation with intelligent scribble guidance	This paper presents an algorithm for Interactive Co-segmentation, a process of separating a foreground object from a group of related images. Unlike previous approaches which rely on unsupervised methods, this algorithm uses ideas from interactive object-cutout techniques. It allows users to guide the output by identifying the foreground object through scribbles. This leads to simpler and more parallelizable functions, allowing for more images to be processed. To address the issue of user fatigue in examining multiple cutouts, the algorithm includes an automatic recommendation system called iCoseg. This system intelligently suggests where the user should scribble next, resulting in faster and more efficient cutout generation. The paper also introduces a new co-segmentation dataset, the CMU-Cornell iCoseg Dataset, which includes a large number of groups, images, and hand-annotated groundtruth. Experimental results show that iCoseg achieves good quality cutouts with less time and effort compared to exhaustive examination of all cutouts. 
411323	411323170	Seed image selection in interactive cosegmentation	Interactive image segmentation is a useful tool that allows users to guide the segmentation process towards their desired output. However, it can be time-consuming to mark scribbles on multiple images. Recent research has shown that user input from one image can be shared among related images to perform interactive cosegmentation. However, most approaches randomly select an image for user input. This paper addresses the issue of selecting the most appropriate image for user input, also known as Seed Image Selection. The authors propose a classification-based approach that outperforms the random selection heuristic used in previous works. This highlights the importance of selecting the right image for user input in interactive image segmentation.
411324	41132412	Semantic interpretation of temporal information by abductive inference	The interpretation of temporal information in a text not only relies on explicit clues found in verbs and adjuncts, but also on general knowledge and assumptions about the world. This theory explains the relationship between verbs, their tenses and adjuncts, and the events and time periods they represent and their relative temporal positions. It is presented in a logical format and is based on concepts from Ness Schelkens et al. The theory can be applied practically using an abductive resolution procedure to extract temporal information from texts.
411324	41132433	CHICA, an Abductive Planning System Based on Event Calculus	The article introduces CHICA, an artificial intelligence planner that utilizes techniques from computational logic and event calculus to generate plans for achieving a given goal. CHICA's representation language is Horn clause logic and it uses an abductive extension of SLDNF resolution to generate assumptions for proving the goal. The planner also incorporates domain constraints, inequality, and temporal relations through constraint logic programming. CHICA's generic search algorithm allows for customizable search strategies and domain-specific heuristics. The planner has successfully solved various planning problems, including multiple robot block world problems, assembly tasks, and room decoration. It can also be extended to handle plan execution and replanning.
411325	4113259	Web Document Classification Based on Rough Set	The Vector Space Model is a traditional way of representing Web documents, but it often leads to zero-valued similarity between vectors, reducing the quality of classification. To address this issue, a new approach based on rough set theory is proposed in this paper. The approach utilizes TF*IDF weighting scheme to assign weight values for Web documents, with missing information being supplemented by rough set for incomplete information. By generating tolerance classes in both term space and Web document space, the missing information is complemented and the essential information is extended, resulting in improved classification performance. Experimental results demonstrate the effectiveness of this approach.
411325	41132592	A reasonable rough approximation for clustering web users	The uncertainty in accessing Web pages presents challenges for the analysis of Web logs. Several rough k-means cluster algorithms have been successfully used for Web usage mining, but their introduction of rough approximations has not been fully explained. This paper examines the data in the boundary areas of clusters and introduces a new rough k-means cluster algorithm, RKMrra, which is based on a reasonable rough approximation. The algorithm is then applied to Web access logs and compared to other algorithms in terms of five characteristics. The results demonstrate that RKMrra is effective in discovering meaningful clusters of Web users and its rough approximation is more reasonable.
411326	41132636	Semi-Supervised Learning for Relation Extraction.	This paper presents a semi-supervised learning approach for relation extraction, which combines SVM bootstrapping and label propagation. It uses a co-training procedure with random feature projection to generate a set of weighted support vectors, and then applies a label propagation algorithm using these support vectors. The method was evaluated on the ACE RDC 2003 corpus and was found to outperform the normal LP algorithm, while also reducing computational burden. This suggests that the proposed method combines the strengths of both SVM bootstrapping and label propagation for more effective relation extraction.
411326	411326112	Label propagation via bootstrapped support vectors for semantic relation extraction between named entities	The paper proposes a method for extracting semantic relations between named entities using a combination of supervised and unsupervised learning techniques. By using a small amount of labeled data and a larger amount of unlabeled data, the method is able to bootstrap support vectors and then use a label propagation algorithm to classify unseen instances. Evaluation on the ACE RDC corpora shows that this method outperforms traditional label propagation methods and reduces computational burden. Overall, the method combines the advantages of both supervised and unsupervised learning to improve semantic relation extraction.
411327	41132769	Instrumenting Network Simulators for Evaluating Energy Consumption in Power-Aware Ad-Hoc Network Protocols	This paper discusses the development of an energy consumption model for ad hoc network protocols in network simulators. By explicitly accounting for low-power radio modes and considering the energy costs associated with different radio states, the model allows for accurate measurement of energy consumption. It can be applied automatically to any layer of the protocol stack. The model is validated through simulations and analytical results, as well as comparison to testbed experiments. The paper also demonstrates the model's usefulness by comparing different protocols in terms of energy consumption. Overall, the model provides a valuable tool for evaluating energy consumption in ad hoc networks.
411327	41132777	Energy-efficient, collision-free medium access control for wireless sensor networks	TRAMA is a medium access protocol designed for wireless sensor networks to reduce energy consumption and prevent collisions. It uses a distributed election scheme based on traffic information to determine which node can transmit at a specific time slot. By avoiding assigning time slots to nodes with no traffic and allowing nodes to switch to idle mode, TRAMA achieves fair and correct channel access. An analytical model and simulations show that TRAMA outperforms other protocols in terms of energy efficiency and performance. The protocol is evaluated using both synthetic and sensor-network scenarios, demonstrating significant energy savings compared to other protocols.
411328	41132861	Accessible protein interaction data for network modeling. structure of the information and available repositories	In recent years, there has been a significant increase in computational studies focused on understanding molecular biology systems. This has been made possible by the availability of new data derived from experimental proteomics, such as the yeast two-hybrid system and affinity purification methods. These studies have provided insights into the structure and organization of molecular networks and have helped in identifying protein interactions. Other fields like structural biology and computational structural biology have also contributed to this by providing detailed descriptions of protein complexes. However, the storage, manipulation, and visualization of the large amounts of data on protein interactions and networks pose challenges. As a result, several systems and standards have emerged to facilitate the analysis of this information. This review provides an overview of the experimental and computational methods used for studying protein interactions, as well as the databases and standards being developed to aid in the analysis of this data.
411328	41132856	MetaRouter: bioinformatics for bioremediation.	Bioremediation is the use of microorganisms to remove pollutants from the environment. This process involves a large amount of data from various sources. To address this, MetaRouter has been developed as a system to manage and analyze this data. This system is designed for laboratories working in biodegradation and bioremediation and allows for the organization and retrieval of both public and private data, as well as the extraction of new knowledge. One of its features is a program that can identify biodegradative pathways for specific chemical compounds. By integrating biodegradation information with protein and genome data, MetaRouter provides a comprehensive framework for studying the global properties of bioremediation networks. The system can be accessed and managed through a web interface and is available for free at http://pdg.cnb.uam.es/MetaRouter.
411329	41132934	A scalable sparse matrix-vector multiplication kernel for energy-efficient sparse-blas on FPGAs	Sparse Matrix-Vector Multiplication (SpMxV) is a commonly used operation in many scientific and engineering applications. However, traditional computing architectures struggle to efficiently handle the compression formats required for storing sparse matrices, resulting in lower computational throughput compared to dense matrices. To address this issue, a FPGA-based SpMxV kernel is proposed that is scalable and utilizes available memory bandwidth and computing resources efficiently. Benchmarking on a Virtex-5 SX95T FPGA shows an average computational efficiency of 91.85% and a peak efficiency of 99.8%, which is more than 50 times better than Intel Core i7 processors and over 300 times better than NVIDIA GPUs. Additionally, the FPGA kernel achieves higher performance while using only 64 processing elements, resulting in a 38-50 times improvement in energy efficiency compared to CPU and GPU counterparts.
411329	4113292	A Single-Precision Compressive Sensing Signal Reconstruction Engine On Fpgas	Compressive sensing (CS) is a promising technology for low-power and cost-effective data acquisition in wireless healthcare systems. However, its efficient real-time signal reconstruction remains a challenge, and there is a need for hardware acceleration. In this paper, a single-precision floating-point CS reconstruction engine is presented, implemented on a Kintex-7 FPGA using the orthogonal matching pursuit (OMP) algorithm. A highly parallel architecture is proposed, utilizing 128 processing elements (PEs) to share computing resources among different OMP tasks. This implementation operates at 53.7 MHz and can support larger problem sizes and more sparse coefficients, resulting in higher reconstruction accuracy. Tests on ECG reconstruction show similar accuracy to a double-precision C program, with an average speed-up of 41x compared to a 2.27 GHz CPU.
411330	41133034	Improving AR using shadows arising from natural illumination distribution in video sequences	The paper proposes a method for generating realistic shadows of virtual objects in a real video sequence, building upon previous work by Sato, Sato, and Ikeuchi (1999) using a static camera. The method involves calibrating both the video and graphic cameras, addressing false shadows caused by the limitations of the static camera approach, and using camera self-calibration and a "match move" technique for flexible graphic world coordinate system embedding. The method also takes advantage of information from the video sequence to overcome previous limitations. The paper concludes with experimental results from a real video sequence.
411330	41133022	Real-time camera calibration for virtual studio	In this paper, the authors present an algorithm for extracting real-time camera parameters, which is essential for creating a virtual studio. They also introduce a new method for calculating lens distortion in real-time, as it is important for ensuring realistic video production. The algorithm utilizes a special calibration pattern and cross-ratio concept to easily identify feature points and calculate camera parameters in real-time. The authors also address the issue of lens distortion in zoom lenses by proposing a new linear method that can be computed quickly for real-time applications. The algorithm was implemented and tested on a Pentium PC and Matrox Genesis boards, achieving a processing rate of 30 frames per second. The results show that this system can be practically used for virtual studio applications.
411331	411331132	A large-alphabet three-party quantum key distribution protocol based on orbital and spin angular momenta hybrid entanglement.	The orbital angular momentum (OAM) of a single photon can carry multiple bits of information due to its orthogonality. This allows for a high-dimensional Hilbert space to be spanned, increasing information capacity and security. By using a hybrid entangled state of spin and OAM, the Shannon dimensionality can be increased even further. A three-party quantum key distribution (QKD) protocol is proposed, using spatial light modulators (SLMs) and specific phase holograms to modulate the OAM state of photons. This allows for the shared key to be generated among the parties without classical message exchanges. This protocol can also be extended to multiple-party QKD by repeating the same operation for each party.
411331	411331143	Cryptanalysis of RC4(n, m) stream cipher	RC4(n, m) is a stream cipher created by G. Gong et al. based on the popular RC4 cipher designed by Ron Rivest. The authors claim that it is resistant to all known attacks on RC4. However, a paper reveals weaknesses in RC4(n, m) and presents two attacks. The first attack exploits non-randomness in the internal state of the cipher, allowing an algorithm to distinguish it from a truly random cipher with access to 24·n bits of the keystream. The second attack takes advantage of low diffusion of bits in the cipher's algorithms and can recover the secret key if the initial value can be manipulated. The cipher also has fixed inputs, but if these can be controlled by the attacker, a distinguisher and secret key recovery attack can be used to retrieve the key in a short amount of time. The attack has been successfully implemented on a standard PC, recovering the secret key of RC(8, 32) in less than a second.
411332	41133246	Sharing encountered information: digital libraries get a social life	The study examined the sharing of information encountered in everyday reading, along with the traditional focus on sharing intentionally retrieved materials in digital libraries. Through 20 interviews in home and work settings, the researchers found that sharing encountered materials, in the form of paper or electronic clippings, is a significant use for these materials. This practice is widespread among participants and serves a social role beyond simply informing the recipient. The study also discusses the broader spectrum of clipping practices, the function and value of shared information, and the social impact of sharing encountered materials. The researchers suggest moving beyond an email model for sharing and considering the social ties that are strengthened through this practice.
411332	41133230	Turning the page on navigation	This paper presents the findings of a study on reading and navigation behaviors in both print and digital materials. The researchers focused on periodicals and observed that readers use various navigation strategies while reading. They also discovered two phenomena that occur in print but not in digital materials: lightweight navigation and approximate navigation. The researchers then analyze the importance of these findings for digital library interfaces. They also highlight the significance of page-turning and its role in navigation. Overall, this study sheds light on how readers navigate through different types of materials and the implications for designing digital interfaces for reading and navigation.
411333	411333121	Process-Centered Software Engineering Environments, A Brief History and Future Challenges	The history of software engineering environments spans about two decades, with early environments focusing on small parts of the software process and later evolving to support the entire process. The concept of process-centered software engineering environments emerged ten years ago, introducing the idea of using a model of the software process as a guide for the environment's behavior. While some aspects of this vision have been realized, others have proven impractical. This article discusses the evolution of software engineering environments, with a focus on process-centered environments. It also explores the trend of distributed software processes and the need for a software process middleware to manage processes across different sites. Other current trends in software process research are also discussed.
411333	41133328	From An E-Business Revenue Model To Its Software Reference Architecture	Revenue models are a crucial aspect of a company's business model, as they dictate how the company generates its revenue. However, there is a lack of a concept that directly links revenue models to the software architecture of the underlying software system in e-Business companies. This paper introduces a concept that uses a 'classification cycle' to identify and classify various revenue models, with a focus on the subscription of services model. The resulting architecture can serve as a reference for software development in businesses using this revenue model, making it easier and more efficient for architects to design the overall software architecture. 
411334	4113347	Detecting and resolving unsound workflow views for correct provenance analysis	Workflow views are used to group tasks in a workflow into higher level tasks for easier reuse and provenance analysis. However, if these views are not carefully designed, they may not accurately preserve the dataflow between tasks, causing unsound views. This paper addresses the problem of identifying and correcting unsound views efficiently, with minimal changes. The authors prove that this is a difficult problem and propose two local optimality conditions, along with polynomial time algorithms, to correct the views. Experiments show that the proposed algorithms are effective and efficient, with the strong local optimality algorithm producing better solutions with minimal processing overhead.
411334	41133427	WOLVES: achieving correct provenance analysis by detecting and resolving unsound workflow views	Workflow views are used to group tasks in a workflow into composite tasks, making it easier to analyze, share, and reuse workflows. However, if a view is unsound, it disrupts the dataflow between tasks and can lead to incorrect provenance analysis. To address this issue, a system called WOLVES has been developed. It efficiently identifies and corrects unsound views with minimal changes. Since this correction problem is difficult to solve, WOLVES offers two options for local optimization: strong and weak. The system also implements efficient time algorithms to achieve these optimalities. Overall, WOLVES provides a solution for correcting unsound workflow views to improve the accuracy of provenance analysis.
411335	41133596	Querying big graphs within bounded resources	This paper discusses the issue of querying large graphs when resources are limited. The goal is to find a way to answer queries with a fraction of the graph, denoted as GQ, which has a size smaller than a given ratio α of the original graph G. The proposed solution is a dynamic scheme that reduces G to GQ. The paper investigates the accuracy of exact and approximate answers obtained from GQ, and develops resource-bounded algorithms for two types of queries: pattern queries with data locality and reachability queries without data locality. The experiments using real and synthetic data show that the algorithms perform well and provide accurate answers, even for small values of α. However, it is shown that obtaining 100% accuracy is not always possible, as it is NP-hard for pattern queries and impossible for reachability queries when α is not equal to 1.
411335	4113352	Answering Pattern Queries Using Views	This paper explores the use of views for querying relational and semistructured data, specifically for graph pattern queries using graph simulation. The authors propose a concept of pattern containment to determine if a query can be answered using a set of views. They also develop efficient algorithms for answering these queries and examine the complexity and efficiency of determining the containment of pattern queries. Additionally, the paper discusses the use of maximally contained rewriting for finding approximate answers when a query is not contained in the views. Experimental results demonstrate the effectiveness of these methods for querying large real-world graphs.
411336	41133650	Experimental Analysis of Guess-and-Determine Attacks on Clock-Controlled Stream Ciphers	This paper discusses Guess-and-Determine (GD) attacks on clock-controlled stream ciphers, specifically focusing on the analysis of irregular clocking. The proposed GD attacks are applied to a typical clock-controlled stream cipher (AA5) and the process complexity is calculated. The attacks assume random clocking of linear feedback shift registers (LFSRs), but the practicality of these assumptions is questioned as clocking is typically determined by internal states. The effectiveness of the GD attacks is evaluated through the implementation of miniature ciphers and comparison with other clock-controlled stream ciphers. The results of this research can be used to inform the design of clock-controlled stream ciphers.
411336	4113362	K2: A Stream Cipher Algorithm using Dynamic Feedback Control	This paper discusses the design and analysis of a new type of word-oriented stream cipher with an irregular clocking mechanism. This dynamic feedback control system, called K2 v2.0, is shown to be secure and high-performing and can be used in various applications. The authors believe that this new mechanism has the potential to be effective against both existing and novel attacks on stream ciphers. While there have been previous studies on clock-controlled stream ciphers and their vulnerabilities, there has been limited research on word-oriented ciphers with irregular clocking. The proposed design aims to fill this gap and provide a more secure option for stream cipher applications. 
411337	411337125	On the Power of Multidoubling in Speeding Up Elliptic Scalar Multiplication	We discuss methods for efficient computation of elliptic scalar multiplication, specifically focusing on multidoubling techniques. These methods allow for the direct computation of 2kP from a randomly selected point P on an elliptic curve, without needing to calculate intermediate points. Our algorithms are designed for elliptic curves with Montgomery and Weierstrass forms over finite fields with a characteristic greater than 3, using affine coordinates. These techniques are faster than repeated doublings and have been applied to scalar multiplication on elliptic curves, with a 28% and 31% reduction in running time achieved for Montgomery and Weierstrass forms, respectively, for a 160-bit curve over fields with characteristic greater than 3.
411337	4113378	Studying on economic-inspired mechanisms for routing and forwarding in wireless ad hoc network	This paper explores the challenges of information asymmetry and moral hazard in autonomous Ad hoc networks. It suggests that economic-based mechanisms can serve as both a signaling and sanctioning tool to reveal true costs in routing and incentivize nodes to act in the best interest of the network. The paper examines the concept of truth-telling as the dominant strategy in a VCG-like routing mechanism with mutually dependent link costs. It also introduces the concepts of Individual Rationality and Incentive Compatibility, which should be met in any game theoretical routing and forwarding scheme. Different solution concepts are evaluated to understand the economic implications of two types of forwarding approaches, with and without per-hop monitoring. 
411338	41133835	Desiring to be in touch in a changing communications landscape: attitudes of older adults	This paper explores the attitudes of older adults towards keeping in touch with important people. The findings, gathered from three focus groups with participants aged 55 to 81, reveal that older adults value keeping in touch but also see it as something that requires careful management in their daily lives. Communication is viewed as a way to showcase skills and express personality, and is seen differently than the casual interactions facilitated by new technologies. The paper suggests design implications and proposes design concepts for new communication devices based on the themes that emerged. 
411338	41133810	Embodiment in brain-computer interaction	There is growing interest in using Brain-Computer Interaction (BCI) in gaming, but there is a need to understand the opportunities and limitations of this interaction method. To supplement previous laboratory studies, there is a call for real-world research. This paper presents a study of BCI gaming in a home setting, focusing on the role of the body in BCI. Drawing on the concept of embodied interaction, the study shows how bodily actions are key to controlling brain activity and making intentions visible to others. This public display of bodily actions allows for social organization and coordination, and has implications for the use of BCI in gaming.
411339	41133974	On the Rate of Structural Change in Scale Spaces	The article examines the impact of different regularization parameters on the suppression of image details. It compares the effectiveness of first order Tikhonov regularization, Linear Gaussian Scale Space, and Total Variation image decomposition. The study looks at the squared L 2-norm of the regularized solution and the residual as the parameters vary. It finds that for first order Tikhonov regularization, the regularized solution norm is convex, while the residual norm is not concave. A similar result is seen for Gaussian Scale Space, but may not hold for different parameter values. This suggests that the regularized solution norm is not sufficient for scale selection. Empirical studies confirm that the squared residual norms contain vital scale information.
411339	41133931	What Do Features Tell about Images?	The Marr paradigm suggests that visual processing involves detecting low-level features and then processing them at a higher level according to the task. This means that any two images with the same features will produce the same result in visual processing. These images with identical features form a metameric class, from which the simplest image is chosen as a representative. The complexity of this image can be used to analyze the information content of features. The paper demonstrates that a small number of basic differential features can effectively reconstruct images similar to what a human observer sees. It also presents methods for reconstructing images with minimal variance and maximum entropy or maximum a posteriori based on natural image priors. The information content of blobs and edges is also discussed.
411340	41134041	Analysis of Two-Dimensional Non-Rigid Shapes	The analysis of deformable two-dimensional shapes is an important issue in various fields such as pattern recognition, computer vision, and computer graphics. This paper focuses on three main problems related to non-rigid shapes: similarity, partial similarity, and correspondence. The authors propose an approach to determine deformation-invariant similarity criteria for shape comparison, using intrinsic geometric properties and the Gromov-Hausdorff distance. They also introduce a method to compute similarity for shapes with similar parts but different overall structures, based on Pareto optimality. Additionally, the paper discusses how the problem of correspondence between non-rigid shapes can be solved as a byproduct of the similarity problem. The authors use generalized multidimensional scaling as a numerical framework for these problems. 
411340	41134030	Functional Maps Representation on Product Manifolds.	In this article, the authors discuss the representation, analysis, and manipulation of maps between shapes. They propose a model that treats maps as densities over the product manifold of the input shapes, allowing for signal processing techniques to be used. The product space itself has its own geometry, which is utilized to define map operations in the spectral domain. The authors also introduce the concept of localized spectral analysis of the product manifold for map processing. This framework can be applied to maps between 2D and 3D shapes without adjustments and can be implemented efficiently using sparse matrices.
411341	41134160	Self-stabilizing leader election in optimal space under an arbitrary scheduler	SSLE is a self-stabilizing algorithm designed for leader election in a connected unoriented network with unique IDs. It also creates a BFS tree with the leader as the root. The algorithm uses O(logn) space per process and stabilizes in O(n) rounds, even in the presence of an unfair daemon. This makes it efficient and reliable for networks with a large number of processes. 
411341	41134135	Slf-stabiliezing leader election in dynamic networks	Three algorithms are presented for electing a leader in a dynamic network with unique IDs, using the composite model of computation. Each connected component of the network elects a leader, and a BFS tree is constructed with the leader as the root. The election process takes O(Diam) rounds, where Diam is the maximum diameter of any component. The algorithms can handle changes in topology and corrupted data. The first algorithm chooses an arbitrary process as the leader, while the second algorithm selects the process with the highest priority. The third algorithm has the strictest leadership stability, ensuring that no process changes its leader more than once and each component elects a former leader if possible.
411342	41134227	Piecewise quadratic reconstruction of non-rigid surfaces from monocular sequences	This paper presents a new method for reconstructing 3D surfaces that are highly deforming, such as a flag waving in the wind, using only a single orthographic camera. The approach is based on tracking a set of feature points on the surface throughout an image sequence. Unlike previous methods that assume a global deformation model with small deviations from a rigid mean shape, this method uses a quadratic deformation model and divides the surface into overlapping patches. These patches are individually reconstructed and then registered together, ensuring that shared points correspond to the same 3D points in space. The results on challenging sequences with strong deformations show that this method outperforms global approaches.
411342	41134241	Structure from motion and photometric stereo for dense 3D shape recovery	This paper describes a method for creating a detailed 3D reconstruction from videos taken with a single camera. The approach combines photometric stereo and structure from motion techniques, without requiring any prior calibration of the camera or lighting. By using 3D information from a set of 2D landmarks, the algorithm is able to resolve the inherent ambiguity in photometric stereo and accurately estimate the surface of the object being captured. The effectiveness of the method is demonstrated through testing on the CMU Multi-Pie database, which contains images of 337 subjects under different lighting conditions and facial expressions. 
411343	411343116	Vibrotactile haptic feedback for intuitive control of robotic extra fingers	Wearable robots, mainly in the form of exoskeletons, have been designed to augment human capabilities or aid in rehabilitation. However, advancements in technology have led to the development of wearable extra robotic limbs. These extra limbs have been lacking in effective haptic feedback systems, until the development of a robotic extra finger coupled with a vibrotactile ring interface. Through perceptual experiments and a pick-and-place task with ten subjects, it was found that haptic feedback significantly improved performance and was preferred by all subjects. This highlights the importance of haptic feedback in wearable robots for enhanced control and task execution.
411343	41134324	Two finger grasping simulation with cutaneous and kinesthetic force feedback	This paper discusses an experiment involving two finger grasping, specifically in the task of peg-in-hole. The experiment compares two types of simulated force feedback - cutaneous and kinesthetic. The kinesthetic feedback is provided by a commercial haptic device, while the cutaneous feedback is provided by a new haptic display developed for this study. The display has a mobile surface that interacts with the fingertip through three wires connected to motors. The paper describes the design and kinematics of the display, and the results showed that cutaneous feedback was more effective than visual feedback alone.
411344	41134452	An Authoring Tool for an Emergent Narrative Storytelling System	This paper presents the initial conceptual design of an authoring tool for the FAtiMA architecture, which powers the virtual bullying drama FearNot!. The process of authoring emergent narrative involves designing a planning domain for a virtual character planner, which can be challenging for non-technical authors. After reviewing existing authoring tools, the authors propose a new approach where the author plays through example story lines to gradually increase the knowledge and intelligence of a virtual character. This approach also includes a mixed initiative feature, allowing the author to cooperate with the character planners. The authors intend to implement this design within the FearNot! framework and believe it may provide useful ideas for other interactive storytelling systems. 
411344	41134419	Keynote Speeches - Abstracts	This paper discusses the role of story in digital technology enhanced learning and the challenges of creating digital narrative learning environments. While story has been used in education for a long time, it has recently become a prominent topic of research in various fields, including knowledge management. The paper examines the existing use of story in educational applications and questions its effectiveness in creating a narrative learning environment. The relationship between story and computer games is also explored, highlighting the challenges posed by the interactivity of digital media. The paper concludes by discussing the current and future direction of technology and the available support for creating digital narrative learning environments. 
411345	41134522	Scalable Distributed Processing of K Nearest Neighbor Queries over Moving Objects	Many applications involving moving objects rely on processing k-nearest neighbor (k-NN) queries, but existing approaches are designed for a centralized setting and struggle to handle the large volume of data and concurrent queries in a distributed setting. To address this, the authors propose a suite of solutions including a new index structure called Dynamic Strip Index (DSI) and a distributed k-NN search (DKNN) algorithm based on DSI. These solutions are implemented on Apache S4 and are shown to outperform three baseline methods in extensive experiments. DKNN is particularly efficient and predictable due to its avoidance of potentially expensive iterations.
411345	41134529	Continuous KNN Join Processing for Real-Time Recommendation	The increasing use of user-generated content on social media platforms has created a need for real-time recommendations to suggest relevant content to users. This is important because people tend to prefer fresh content. To achieve this, users and content are represented as feature vectors in a high-dimensional space, and the problem of real-time recommendations is essentially a kNN join problem. The challenge lies in updating the kNN join results as new content is added. Existing methods have limitations due to the large volume of data and high search costs. This paper proposes a solution that identifies affected users and updates their kNNs using a new index structure called HDR-tree, which utilizes dimensionality reduction techniques. A variant of this, called HDR-tree, provides faster but approximate solutions. Experimental results show that these methods outperform baseline methods. 
411346	4113460	GOLD: a parallel real-time stereo vision system for generic obstacle and lane detection	This paper discusses a system called GOLD that uses stereo vision technology to detect obstacles and lane position while driving, improving road safety. It is based on custom hardware and can detect obstacles of any shape and lane markings in structured environments at a rate of 10 Hz. The system removes perspective distortion from images and uses morphological filters to detect lane markings. The output is displayed on a monitor and control panel for the driver. The system was tested on a mobile laboratory vehicle and proved to be robust in various driving conditions. 
411346	4113463	A stereo vision system for real-time automotive obstacle detection	This study presents a system for detecting obstacles in a pair of images captured by a stereo vision device on a moving vehicle. The system consists of two computational engines, one for low-level image processing and the other for medium-level tasks. A geometric transformation is applied to remove the perspective effect from the images, allowing for the detection of free space in front of the vehicle. This eliminates the need for high computational tasks typically required in stereo vision approaches. The system was tested on a land vehicle and proved to be robust against various conditions such as shadows, changing lighting, road textures, and vehicle movement. 
411347	41134749	Energy efficient resource allocation in mobile ad hoc computational grids	This paper discusses the issue of energy efficient resource allocation for interdependent tasks in mobile ad hoc computational grids. These tasks require a lot of data exchange, which consumes energy. Inefficient allocation can lead to increased energy consumption and communication costs, limiting the node's lifespan and possibly causing power failures. The proposal is a hybrid power-based resource allocation approach that considers task dependencies and types, and allocates them to nodes that can be accessed with minimum transmission power. The paper also presents a power-based algorithm to find the closest nodes for task allocation. This approach is less complex than traditional algorithms as it depends on the number of transmission power levels rather than the number of nodes in the grid.
411347	41134736	An Adaptive And Distance-Based Resource Allocation Scheme For Interdependent Tasks In Mobile Ad Hoc Computational Grids	Task completion time is influenced by two factors: execution cost and communication cost. In mobile ad hoc Grids, communication cost is high and unreliable, making it crucial for application performance. To reduce this cost, related tasks are assigned to nearby nodes. However, as nodes can move within the Grid, this may result in multi-hop communication, requiring an effective resource allocation scheme. Designing such a scheme for mobile ad hoc computational Grids is challenging due to the limited communication, node mobility, and lack of infrastructure. This paper presents an adaptive and distance-based resource allocation scheme that considers application and node characteristics and uses migration heuristics to address local node mobility. It is tested in a simulated environment with various workloads and parameters.
411348	41134881	Agent-Based Approach to Dynamic Meeting Scheduling Problems	Multi-Agent systems are increasingly being utilized to solve complex real-world problems, such as meeting scheduling (MS). MS is challenging due to its distributed and dynamic nature, as well as conflicting user preferences. Existing methods for MS are limited in that they treat it as a static problem, often relaxing constraints and disregarding consistency. To address these shortcomings, a new distributed approach based on the DRAC model is proposed. This approach allows for the relaxation of user preferences while maintaining arc-consistency, and uses localized asynchronous communications to efficiently reach an optimal solution. This approach is scalable and effective in handling strong constraints, making it a valuable solution for dynamic MS problems. 
411348	41134831	Meetings scheduling solver enhancement with local consistency reinforcement	Meeting scheduling is an important real-world problem that involves scheduling meetings while considering the preferences and availability of all participants. However, conflicting preferences often make it difficult to reach a satisfactory solution. Existing research often sacrifices some preferences to reach a compromise, but this is not always possible. They also do not address the issue of efficient message passing in distributed systems. To address these challenges, a new approach is proposed that uses a distributed reinforcement of arc consistency model. This approach focuses on satisfying the preferences of the meeting host while considering the availability of all participants. It minimizes the number of exchanged messages and respects the privacy of the users. Experimental analysis shows that this approach is scalable and effective for strong constraints. 
411349	41134970	Intelligent presentation skills trainer analyses body movement	Public speaking is a challenging task as it is influenced by nonverbal behaviors that are often expressed subconsciously. This paper discusses a study on the nonverbal behaviors of presenters, which was used to develop an intelligent tutoring system. The system uses a depth camera to capture the presenter's bodily characteristics, analyzes this information to evaluate the presentation's quality, and provides immediate feedback through a virtual conference room. The system also allows for control of simulated avatars' reactions based on the presenter's performance. This technology aims to help individuals practice and improve their public speaking skills by receiving real-time feedback.
411349	41134949	Physical and Virtual Tools: Activity Theory Applied to the Design of Groupware	Activity theory is a framework that focuses on how individuals interact with their environment and how this interaction leads to the creation of tools. These tools, referred to as objectification, can then be used for social interaction. The paper discusses how activity theory has influenced the design of groupware, specifically the BUILD-IT system. This system uses Augmented Reality to enhance group work and allows for cooperative planning through physical and virtual tools. Task analysis was used to determine the specific tools needed for different planning tasks. The paper also reflects on the effectiveness of activity theory in guiding the design process and presents a set of design guidelines.
411350	41135032	Data sparsity issues in the collaborative filtering framework	The need for efficient information filtering on the rapidly growing Web has led to the development of popular techniques in user profiling and Web personalization. This chapter focuses on one such technique - collaborative filtering - and presents an overview of its approaches. The k-Nearest Neighbor (kNN) algorithm, commonly used for collaborative filtering, and the Support Vector Machine (SVM), a state-of-the-art classification algorithm, are compared using different datasets. Results show that the quality of collaborative filtering recommendations is affected by the availability of data. The kNN algorithm is more effective for less sparse data, while SVM may perform better for highly sparse data. Experiments were conducted on standard and real-life datasets, demonstrating the applicability of supervised learning algorithms in collaborative filtering.
411350	4113505	Mapping Documents onto Web Page Ontology	The paper discusses a method for automatically mapping Web pages onto an ontology using document classification based on the Yahoo! ontology of Web pages. This involves using techniques for learning on text data and reducing the number of features by considering the hierarchical structure and using feature subset selection. The documents are represented as word-vectors and the problem is divided into subproblems based on the categories in the Yahoo! hierarchy. The resulting model consists of independent classifiers for each category. The approach is evaluated on real-world data and shows promising results. Experimental comparison of feature scoring measures also demonstrates the effectiveness of considering data and algorithm characteristics.
411351	411351105	Changes in the brain intrinsic organization in both on-task state and post-task resting state.	Intrinsic functional connectivity is crucial for maintaining the stability and flexibility of the brain. Studies have shown that this connectivity can be influenced by task performance, leading to changes in local spatial patterns and global organization. However, it is still unclear how this connectivity changes during and after a task. To better understand this, a functional MRI experiment was conducted with an active semantic-matching task and two rest periods before and after the task. Results showed that the brain's small-world topology remains robust, but with higher local efficiency and lower global efficiency during the task. The default mode network (DMN) is also found to be engaged during both task and post-task processes, with changes in spatial patterns and nodal graph properties. This study contributes to our understanding of the brain's intrinsic organization and its role in memory and learning.
411351	4113516	Two Perspectives To Investigate The Intrinsic Organization Of The Dynamic And Ongoing Spontaneous Brain Activity In Humans	Studies on spontaneous brain activity have shown that functional networks exhibit a 'small-world' property, where there are strong correlations between different brain regions. However, these studies have not explored the presence of negatively correlated functional networks or the dynamic changes in the brain's intrinsic organization after performing a task. In this study, researchers used functional MRI to examine changes in the brain's resting state before and after a task. They found that the positively-correlated brain functional network (PCBFN) had a small-world architecture, while the negatively-correlated brain functional network (NCBFN) did not. Furthermore, the PCBFN showed a stronger small-world effect after the task. Both networks followed an exponentially truncated power law degree distribution. This study provides a framework for further exploration of the brain's intrinsic organization.
411352	41135223	Modeling and Mining Domain Shared Knowledge for Sentiment Analysis.	Sentiment classification involves predicting the sentiment polarity (positive or negative) of user-generated content such as reviews and blogs. However, labeling training data for all domains is difficult since there are so many. In this article, we focus on sentiment classification adaptation, where a system is trained on one domain but used on another. One challenge is dealing with the significant differences in data distribution between the source and target domains. To address this, we propose a method that learns and mines domain shared knowledge from different sentiment review domains using a joint non-negative matrix factorization framework. This helps bridge the distribution gap between domains and our experiments show that it outperforms other methods for sentiment classification adaptation.
411352	41135216	A robust approach to optimizing multi-source information for enhancing genomics retrieval performance.	The authors of this paper propose a new approach to improve the performance of information retrieval systems in the field of genomics. This approach involves using multiple sources and three different fusion methods to re-rank retrieved information. The authors conducted experiments on two genomics data sets and found that the reciprocal method outperforms the other two methods, and that the combination of sources DFR, BM25, and language model yields the best results. They conclude that this new approach can be useful in directing fusion work in biomedical information retrieval.
411353	41135374	A search algorithm for motion planning with six degrees of freedom	The motion planning problem is crucial in the fields of robotics, spatial planning, and automated design. It involves finding a continuous, collision-free path for a moving object in an environment with obstacles. This paper presents an algorithm for solving the three-dimensional Movers' problem, where a rigid polyhedral object with six degrees of freedom needs to move from an initial to a desired configuration. The algorithm transforms the problem into a six-dimensional configuration space and uses three operators to find collision-free paths. The paper discusses the theoretical properties and implementation of the algorithm, including heuristic strategies for evaluating local geometric information. This research was conducted at the Massachusetts Institute of Technology, with support from various organizations. 
411353	41135385	Automatic sensor configuration for task-directed planning	This paper discusses the problem of planning the configuration of a sensor for a robot in a geometrically specified task in a 2D environment. The focus is on computing the regions from which a point-and-shoot sensor can detect a polygonal robot. The main algorithm allows for partially obstructed sensor configurations and computes the regions from which the robot can be detected as it moves through a goal at a known orientation. This algorithm has a time complexity of $O(kmn^{3}(n+m))$, where $n$ is the complexity of the environment, $m$ is the complexity of the robot, and $k$ is the complexity of the goal. The constructed regions can be used by an active sensing system to configure sensors that will observe the robot as it enters the goal.
411354	41135487	Generalised characteristic polynomials	Multipolynomial resultants are efficient tools for solving systems of polynomial equations or eliminating variables. They are defined as polynomials in m-n+1 variables and can determine if a system has a solution in the algebraic closure of a field. However, they are only exact for homogeneous systems, and can be zero even if there is no affine solution. To address this issue, a projection operator is introduced that is guaranteed to eliminate all components of the system with dimension m-n, including those that are not affine. This operator is based on a generalization of the characteristic polynomial for linear systems. As a result, a method is presented for finding all isolated solution points of a polynomial system in a single-exponential time, even when there are infinitely many solutions.
411354	41135477	Multipolynomial resultant algorithms	Computational methods for manipulating sets of polynomial equations are becoming increasingly important in various applications. Some cases require eliminating variables to obtain a smaller system, while others involve finding numerical solutions. Gröbner bases and polynomial continuation are popular techniques for these tasks, but can be slow and ineffective. This paper presents efficient techniques for computing multipolynomial resultant algorithms, which can be used for manipulating polynomial equations, interpolating polynomials, and finding solutions to non-linear equations. The algorithm has tight bounds on running time and storage requirements, and has shown promising results in applications. Additionally, a symbolic elimination algorithm is discussed for finding real or complex solutions, reducing the problem to finding roots of univariate polynomials. Implementation and performance on various applications are also discussed.
411355	41135528	Behavior-based search of human by an autonomous indoor mobile robot in simulation	The paper discusses the creation of a behavior-based strategy for an indoor autonomous mobile robot to locate an elderly person living alone in a cluttered home environment. The robot uses a Markov decision process to estimate the possible locations of the person based on a perception of their presence in different rooms at different times of the day. The strategy considers the distance to the destination and the probability of finding the person at that location, as well as the last known position of the person. The robot autonomously navigates to the desired location, avoiding obstacles along the way. A simulated environment with an animated human character has been created to test the effectiveness of the strategy, and the results are promising.
411355	41135542	Probabilistic search of human by autonomous mobile robot	This paper discusses a method for an indoor mobile robot to search for an elderly person living alone in a dynamic household environment. The robot needs to navigate autonomously and estimate the possible locations of the person, using a behavior-based Markov Decision Process. The criteria for searching the person are distance to the destination and probability of finding them there. The robot builds a grid map of the environment and plans a path to the target, using a Haar cascade classifier to search for the person's face. The method is validated through a 3D simulated environment with a small robot and human simulation. The results show the effectiveness of the approach.
411356	41135633	Vertex-arboricity of planar graphs without intersecting triangles	The vertex-arboricity of a graph is the smallest number of subsets that can be created from its vertices, where each subset forms an acyclic graph. A conjecture by Raspaud and Wang states that for planar graphs without intersecting triangles, the vertex-arboricity is always 2. This paper provides a proof for this conjecture, confirming that the minimum number of subsets needed to partition the vertex set of such graphs is indeed 2. This result has implications for understanding the structure and complexity of planar graphs.
411356	4113564	A note on the acyclic 3-choosability of some planar graphs	An acyclic coloring of a graph ensures that no adjacent vertices have the same color and there are no bicolored cycles. List assignments assign a list of available colors to each vertex. A graph is acyclically L-list colorable if there is an acyclic coloring where each vertex's color is in its assigned list. If a graph is acyclically k-choosable for any list assignment with at least k available colors for each vertex, it is said to be acyclically k-choosable. Borodin et al. proved that planar graphs with girth 7 or more are acyclically 3-choosable. Borodin and Ivanova then showed that planar graphs without cycles of length 4 to 11 are also acyclically 3-choosable. This note connects these two results by proving that planar graphs without cycles of length 4 to 7 and triangles at a minimum distance of 7 are also acyclically 3-choosable.
411357	41135736	The “mad cow disease”, Usenet Newsgroups and bibliometric laws	The paper explores the reactions of Usenet News users to "mad cow disease" and examines how the crisis was discussed on the internet. The study collected data from relevant news items for 100 days after the outbreak of the disease. The findings reveal similarities between the characteristics of news items on the internet and those found in scientific literature. This is one of the first attempts to use bibliometric methods to analyze information on the internet. With the vast number of newsgroups available, individuals are able to freely express their thoughts on various subjects, making the internet a valuable platform for studying societal reactions to events like "mad cow disease."
411357	41135761	Web links and search engine ranking: The case of Google and the query “jew”	The article discusses the importance of search engines in accessing information on the World Wide Web. It highlights the role of links in determining the ranking of a webpage on search engine result pages. The study focuses on the links to the top two pages retrieved by Google for the query "jew" in 2004: the "jew" entry on Wikipedia and the homepage of "Jew Watch," a highly anti-Semitic site. The study found that most of the links to these pages were from blogs and discussion links, and only a small number were based on the quality of their content. This raises concerns about the effectiveness of ranking algorithms based on link counts and highlights the difference between web links and academic citations.
411358	41135834	DC Proximal Newton for Nonconvex Optimization Problems.	Our novel algorithm addresses learning problems with nonconvex loss and regularizer functions by using a proximal Newton approach. It obtains a descent direction from an approximation of the loss function and performs a line search for sufficient descent. Theoretical analysis shows that the algorithm's iterates converge to stationary points of the difference of convex objective function. Numerical experiments demonstrate its efficiency compared to current methods for problems with a convex loss function and nonconvex regularizer. It also performs well in high-dimensional transductive learning problems.
411358	4113580	Direct Optimization of the Dictionary Learning Problem	The paper proposes a new method for solving the dictionary learning problem, called direct optimization. This approach avoids the traditional technique of optimizing sparse coefficients and dictionary atoms separately. Instead, it uses a joint proximal gradient descent step for both the atoms and coefficient matrix. The paper also discusses how the step sizes for the gradient descent are chosen and establishes a connection between this direct approach and the alternating optimization method. The algorithm is referred to as a one-step block-coordinate proximal gradient descent and has been shown to be more efficient than traditional alternating optimization algorithms based on a simulation study. 
411359	41135991	Relational reinforcement learning with guided demonstrations	Model-based reinforcement learning is a popular approach for teaching robots new tasks, but it often requires extensive exploration and prior knowledge of actions. To address this issue, a new algorithm has been proposed that incorporates teacher demonstrations to learn new domains with minimal exploration and no previous knowledge. These demonstrations help the robot learn new actions and reduce the amount of exploration needed. However, the algorithm is designed to only request demonstrations when they are expected to significantly improve the learning process, as the teacher's time is considered more valuable. Additionally, the algorithm uses rule analysis to identify incomplete parts of the state and provide guidance to the teacher on which actions to demonstrate, minimizing the number of demonstrations needed. Through experiments, it has been shown that this approach can reduce exploration by up to 60% and improve success rates by 35% in various domains.
411359	41135920	Learning Relational Dynamics of Stochastic Domains for Planning.	Probabilistic planners are effective but require a costly model of the domain. To address this, a new learning approach is proposed that only needs a set of state transitions. It can handle uncertain effects, uses a relational representation, and can learn both action and exogenous effects. It combines inductive logic programming and an optimization method to generate and select the best planning operators for a problem. Experimental results show that this approach outperforms previous methods.
411360	41136016	The structure of the 3-separations of 3-connected matroids	In short, Tutte defined a k-separation of a matroid as a partition of its ground set with certain conditions. If a matroid has no m-separations for all m less than n, it is considered n-connected. Whitney's work showed that a 1-separation is equivalent to a union of 2-connected components. Cunningham and Edmonds provided a tree decomposition for 2-connected matroids that displays all 2-separations. This paper presents a tree decomposition for 3-connected matroids that displays all non-trivial 3-separations.
411360	41136046	Tangles, tree-decompositions and grids in matroids	A tangle in a matroid is a hindrance to having a low branch-width. The highest possible size of a tangle is equal to the branch-width. It is shown that (i) there exists a tree-decomposition that shows all the largest tangles in a matroid, and (ii) if the matroid can be represented over a finite field, then each large tangle has a significant influence on a large grid-minor. This expands on the findings of Robertson and Seymour regarding Graph Minors.
411361	41136196	Supervised feature selection via dependence estimation	This article presents a new method for selecting features in machine learning by using the Hilbert-Schmidt Independence Criterion (HSIC) to measure the dependence between features and labels. The approach aims to maximize this dependence in order to identify the most relevant features. This framework can be applied to various supervised learning problems, such as classification and regression, and can be approximated using a backward-elimination algorithm. Results from experiments on both artificial and real-world datasets show the effectiveness of this method in feature selection.
411361	41136194	Learning to Explain: An Information-Theoretic Perspective on Model Interpretation.	Instancewise feature selection is a method used for interpreting models. It involves training a feature selector to extract the most informative subset of features for each individual example. This is done by maximizing the mutual information between the selected features and the response variable. The conditional distribution of the response variable given the input is used as the model to be explained. An efficient variational approximation to the mutual information is developed and the effectiveness of the method is demonstrated through both quantitative metrics and human evaluation on synthetic and real data sets. 
411362	41136266	A Linear-Time Kernel Goodness-of-Fit Test.	The proposed adaptive test of goodness-of-fit has a linear computational cost and uses Stein's method to construct features that distinguish observed samples from a reference model. The false negative rate is minimized and the test has higher efficiency compared to a previous linear-time kernel test under a mean-shift alternative. In experiments, the test performed better than the linear-time test and matched or exceeded the power of a quadratic-time kernel test. In high dimensions and with exploitable model structure, the test outperformed a quadratic-time two-sample test based on the Maximum Mean Discrepancy.
411362	41136212	A Test of Relative Similarity For Model Selection in Generative Models.	Probabilistic generative models are a useful way to represent data without the need for manual annotation, but model selection can be difficult when likelihoods are not readily available. To solve this problem, a new statistical test called the relative similarity test is introduced. This test uses the difference in maximum mean discrepancies (MMDs) between a reference dataset and each model to determine which model generates samples that are closer to the real-world dataset. The test is based on the joint asymptotic distribution of MMDs and is shown to be effective in ranking the performance of deep generative models in various training settings.
411363	4113636	A cubic kernel for feedback vertex set	This paper presents a polynomial time algorithm for the Feedback Vertex Set problem on unweighted, undirected graphs. The algorithm finds a smaller graph H with a feedback vertex set of size at most k′, if and only if the original graph G has a feedback vertex set of size at most k. The size of H is at most O(k3), which is an improvement over a previous result that had a size of O(k11). The algorithm can also be used as a preprocessing heuristic or as a first step in a fixed-parameter tractable algorithm. Additionally, the algorithm can be made constructive by transforming a minimum size feedback vertex set of H into a minimum size feedback vertex set of G. 
411363	41136376	Constructive Linear Time Algorithms for Small Cutwidth and Carving-Width	The problem of finding a tree with specific properties and a mapping between two graphs has applications in telephone network design. This problem is known as the MINIMUM ROUTING TREE CONGESTION problem. Similarly, a related problem called the MINIMUM CUT LINEAR ARRANGEMENT problem involves finding lines instead of trees and mapping all vertices of one graph to another. Recent research shows that both these problems can be solved efficiently using fixed parameter tractability, and a new constructive proof has been presented. This proof also has the ability to output the desired results in case of a positive solution.
411364	41136410	Using Wikipedia to boost collaborative filtering techniques	The sparsity of data is a major challenge for recommender systems, as it limits their ability to accurately predict user ratings. However, this problem can be overcome by utilizing publicly available user-generated information from Wikipedia. By mapping items to Wikipedia pages and analyzing text and links, we can identify similarities between items and use them to improve the recommendation process and ranking predictions. This method has been shown to be especially useful in cases where ratings are scarce or non-existent. Initial experiments on the MovieLens dataset have yielded promising results.
411364	411364103	An ensemble method for top-N recommendations from the SVD.	The singular value decomposition (SVD) technique is commonly used in recommender systems, but it has limitations when it comes to delivering top-N items online. To address this issue, an ensemble algorithm has been proposed that uses multiple compact trees to map items and quickly retrieve recommendations for users. This method is more efficient and accurate than the base SVD, as it only computes a small number of dot products for a given user vector. The algorithm has been tested on three public datasets and has shown to be effective in providing dynamic recommendations. 
411365	41136533	Forecasting Run-Times of Secure Two-Party Computation	Secure computation (SC) is a method used in cryptography to allow multiple parties to jointly perform computations while keeping their inputs private. Current methods for evaluating the performance of SC protocols rely on approximations of complexity, but these often fail to accurately predict performance due to differences in measures and constants. This study introduces a performance model (PM) that can forecast run-times of secure two-party computations. Through empirical testing on the problem of secure division, the correctness of the PM is demonstrated. The PM can also be used to choose the most optimal algorithm and cryptographic protocol combination, as well as determine security tradeoffs. This model can aid in designing more efficient and secure protocols.
411365	41136519	Improving Practical Performance on Secure and Private Collaborative Linear Programming	Supply chain management involves the sharing of information between parties, which can significantly lower costs. However, concerns about information leakage have made parties hesitant to share information. Secure multi-party computation solutions offer a way to ensure privacy and accuracy, but at the expense of increased communication and computation costs. The efficiency of these solutions is a major concern. The most well-known solution, developed by Li and Atallah, requires a secure permutation in each round of the protocol, which can be costly. To address this issue, a probability-based technique is proposed to reduce the number of secure permutations needed, making the solution more practical and efficient.
411366	4113660	Hitting Selected (Odd) Cycles.	The Subset Odd Cycle Transversal (Subset OCT) problem involves finding a k-sized vertex subset that intersects every odd cycle containing a vertex from a given subset of vertices in a graph. This problem is a generalization of the classic Odd Cycle Transversal problem and also includes the well-known Multiway Cut and Odd Multiway Cut problems. Previous solutions to this problem have had a triple exponential dependence on the parameter k, but a new algorithm has been proposed that has a polynomial dependence on k. This algorithm uses a recursive application of generalized important separators to reduce the problem to its standard version.
411366	41136648	Parameterized Complexity of Bandwidth on Trees.	The bandwidth of a graph is the smallest integer that allows for a bijective function to be created, called a layout, such that the difference between the values of adjacent vertices is less than or equal to the bandwidth. The BANDWIDTH problem involves determining if a given graph has a bandwidth less than or equal to a given integer. Two results related to the complexity of this problem on trees are presented. First, it is shown that any algorithm with a running time of f(b)n(o(b)) would violate the Exponential Time Hypothesis. Second, a polynomial time algorithm is presented that can either conclude that the bandwidth of a tree is greater than a given integer, or find a layout with a bandwidth of at most b(O(b)). This is the first approximation algorithm for the bandwidth of trees. 
411367	41136753	A Fast Parallel Algorithm for Selected Inversion of Structured Sparse Matrices with Application to 2D Electronic Structure Calculations	The article presents an efficient parallel algorithm for computing selected components of the inverse of a structured symmetric sparse matrix. These calculations are useful for various applications, such as electronic structure analysis of materials. The proposed algorithm is a direct method that utilizes a block $LDL^T$ factorization. The selected elements of the inverse are found in the nonzero positions of $L+L^T$. The algorithm is organized using the elimination tree associated with the factorization, and synchronization overhead is reduced by passing data level by level using local buffers and relative indices. The efficiency of the parallel implementation is demonstrated by applying it to a two-dimensional Hamiltonian matrix, and its performance is analyzed in terms of load balance and communication overhead. The algorithm shows excellent weak scaling on a large-scale high performance distributed-memory parallel machine.
411367	41136734	SCDM-k: Localized orbitals for solids via selected columns of the density matrix	The SCDM method is a recently developed approach for constructing localized orbitals from delocalized Kohn-Sham orbitals in insulators and semiconductors. It is simple, robust, efficient, and can be easily parallelized. However, it was previously limited to $\Gamma$ point sampling in the Brillouin zone. This work introduces a generalized version, called SCDM-k, which allows for k-point sampling. SCDM-k is gauge independent and does not require an optimization procedure, avoiding potential issues with local minima. It also has a computational complexity of O(N log N), making it efficient even with a large number of k-points. Numerical tests on 2D and 3D systems demonstrate the effectiveness of SCDM-k.
411368	41136836	Parallel Lagrange--Newton--Krylov--Schur Methods for PDE-Constrained Optimization. Part II: The Lagrange--Newton Solver and Its Application to Optimal Control of Steady Viscous Flows	This paper discusses the use of algorithms to optimize systems governed by partial differential equations (PDEs). In the first part, a Lagrange-Newton-Krylov-Schur method (LNKS) is proposed that uses Krylov iterations to solve the Karush-Kuhn-Tucker system of optimality conditions, with a preconditioner inspired by reduced space quasi-Newton algorithms. The second part focuses on the outer iteration and details how to obtain a robust and globally convergent algorithm. This involves the use of line search methods, a mix of quasi-Newton and Newton algorithms, and continuation techniques. The LNKS algorithm is tested on an optimal flow control problem and is found to have good parallelism and scalability. It is also shown to be significantly faster than reduced quasi-Newton SQP methods, and able to solve previously intractable problems. Implementation details and notation conventions are also discussed.
411368	41136827	Multigrid Algorithms for Inverse Problems with Linear Parabolic PDE Constraints	This article presents a multigrid algorithm for solving inverse problems involving linear parabolic partial differential equations with variable coefficients. The method is designed for problems where the unknown variable is only a function of space, and uses $L^2$ Tikhonov regularization. The algorithm has a mesh-independent convergence rate, making it robust to the choice of regularization parameter and suitable for high-fidelity reconstructions. The inverse problem is solved using a PDE-constrained optimization approach, with a reduced-space method that eliminates the state and adjoint variables. The Hessian matrix is preconditioned using a V-cycle multigrid scheme with a two-step stationary iterative smoother. Numerical experiments demonstrate the effectiveness of the algorithm for different coefficients and regularization parameters, and it is also shown to have the same complexity for problems with partial observations and variable coefficients. The algorithm can also be used as a preconditioner for a full-space solver.
411369	41136920	Activity analysis in crowded environments using social cues for group discovery and human interaction modeling	This paper presents a new approach for analyzing group activities in crowded scenes. It uses a graph-based algorithm with social cues to identify interacting groups. A novel descriptor is proposed to capture the motion and interaction of people within these groups. The advantage of using social cues is that it helps in better understanding of activities in crowded scenes. The proposed framework is evaluated on two public datasets and outperforms existing methods for group discovery and activity recognition. This approach effectively eliminates dataset contamination and achieves recognition rates comparable to state-of-the-art methods. Overall, the paper demonstrates the effectiveness of using social cues for group activity analysis.
411369	41136925	To track or to detect? an ensemble framework for optimal selection	This paper introduces a new method for multi-target tracking using an ensemble framework that combines the results of independent trackers and a detector. The ensemble model uses a scoring function that considers detection confidence, appearance similarity, and smoothness constraints to select the best target candidate. The scoring function parameters are trained using a max-margin framework. A hierarchical data association step is used to gradually associate candidates with targets. To increase robustness against unreliable detections, a second target classifier is introduced. The algorithm is able to track multiple objects in complex scenes with occlusions and has shown promising improvements over existing methods in evaluations on public datasets. 
411370	41137060	Network Topology Optimization for Data Aggregation	This paper explores the problem of optimizing data aggregation in data center networks by configuring the network topology. The authors prove that this problem is NP-hard even with only one aggregator. They also compare the approximation ratios of two different algorithms, finding that the Wang et al. algorithm may result in increased aggregation time with higher switch degrees. However, using the longest processing time (LPT) rule for topology configuration can improve this. By combining the LPT rule with the Wang et al. rule, the authors achieve better throughput and reduce total network traffic by up to 90%. Experimental results show that the LPT rule reduces aggregation time by up to 90% and when combined with the Wang et al. rule, total network traffic is reduced by up to 90%.
411370	411370185	Two Techniques for Fast Computation of Constrained Shortest Paths	The problem of computing constrained shortest paths is essential for network functions like QoS routing, MPLS path selection, ATM circuit routing, and traffic engineering. It involves finding the cheapest path that meets specific constraints, such as delay constraints for real-time data flows. Due to its complexity, heuristic algorithms have been developed to approximate the solution with adjustable accuracy. One common approach is to discretize link delay or cost, which simplifies the problem but introduces errors. This paper proposes two techniques to reduce these errors, allowing for faster algorithm design. This is crucial for high-throughput QoS routers, limited by processing power and memory. Simulations show that the new algorithms can improve execution time and memory usage significantly.
411371	41137120	Actuator selection for desired dynamic performance	The article discusses two methods for selecting actuators that result in a robot with desired dynamic performance. Dynamic performance is measured by the robot's acceleration and force capabilities, which are determined by the actuator torques. The dynamic capability equations are used to model this relationship in a consistent and physically meaningful manner. The article covers actuator selection for both single and multiple configurations.
411371	41137140	Dynamic loading criteria in actuator selection for desired dynamic performance	The article discusses two methods for selecting actuators based on dynamic loading criteria to achieve a desired level of dynamic performance in a robot. Dynamic performance is measured by the robot's acceleration and force capabilities, which are limited by the actuator torques. The Dynamic Capability Equations are used to model this relationship in a consistent and physically meaningful way. The article covers actuator selection for a single configuration and multiple configurations.
411372	41137246	Statistical Debugging Using Latent Topic Models	Statistical debugging is a method that uses machine learning to identify the root causes of bugs in software programs. This is done by using a Delta-Latent-Dirichlet-Allocation model, which models execution traces from failed runs of a program as being generated by two types of latent topics: normal usage topics and bug topics. By also modeling execution traces from successful runs, the model is able to identify weak bug topics that would otherwise go undetected. The model is evaluated on four real programs and is found to produce bug topics that are highly correlated to the true bugs. Domain experts also suggest that this model outperforms existing methods for bug cause identification and may have additional uses in software tasks. 
411372	41137257	A framework for incorporating general domain knowledge into latent Dirichlet allocation using first-order logic	Topic models, such as Latent Dirichlet Allocation (LDA), have been successfully applied to various problems by creating customized versions specific to the application. However, this process can be challenging and time-consuming. To address this, the Foldċall model was introduced, allowing users to incorporate domain knowledge in First-Order Logic (FOL). However, this combination can lead to inference difficulties beyond existing techniques. To address this, a scalable inference method using stochastic gradient descent was developed, which may also benefit the Markov Logic Network (MLN) research community. Experiments show the effectiveness and scalability of Foldċall and the proposed inference technique.
411373	41137317	Fast Global Minimization of the Active Contour/Snake Model	The active contour/snake model is a popular method for segmenting images by evolving a contour towards object boundaries. However, it has a drawback of getting stuck in local minima, making the initial guess crucial for good results. To solve this issue, a new approach is proposed that unifies three well-known image models and establishes the existence of a global minimum for the active contour model. A practical and efficient method is also introduced for propagating the contour, avoiding the need for frequent re-initialization. This method is tested on different types of images and shows better performance compared to other segmentation models.
411373	4113736	A level set method for segmentation of the thalamus and its nuclei in DT-MRI	In this paper, a method for segmenting white and gray matter structures from diffusion tensor magnetic resonance images (DT-MRI) is proposed. The method uses a set of coupled level set functions, with each function representing a surface in 3D and driven by a region-based force. This force is defined using a sensitive similarity measure between DT-MRI tensors. The method is applied to segment the thalamus and its nuclei. The paper also presents new strategies for efficient segmentation of complex objects in DT-MRI, including a sensitive similarity measure, selecting the most representative tensor for a group, and using multiple coupled level sets for background definition. The authors argue for the importance of considering surrounding structures in a region-based approach for accurate segmentation.
411374	41137422	Enforcing Architectural Styles In Presence Of Unexpected Distributed Reconfigurations	ADR is a formal framework used for modelling the development of distributed system architectures. It uses rules to refine ADR graphs and applies logic to these rules, adding pre- and post-conditions. These conditions limit when rules can be applied and specify the resulting graph properties. An algorithm is presented for computing the weakest pre-condition based on a rule and its post-condition. Using this algorithm, a methodology is created for selecting rules at the architectural level to reconfigure a system and restore its original architectural style when unexpected run-time changes occur. 
411374	41137450	On Recovering From Run-Time Misbehaviour In Adr	The proposed monitoring mechanism tracks changes in system evolution and stores them in a tree-like structure. It is developed within a variant of ADR, a rule-based framework for modeling system architecture evolution. The hierarchical nature of ADR allows for efficient use of the monitoring mechanism, which is also used to define new rewriting mechanisms for ADR reconfiguration rules. By monitoring the evolution, the mechanism can identify which part of the system is affected when unexpected behaviors occur. It also suggests potential reconfigurations to prevent errors.
411375	41137528	The Complexity Of Recognizing Incomplete Single-Crossing Preferences	The complexity of determining if a partial preference profile can be extended to a complete single-crossing profile is studied. This scenario represents situations where we have limited information about voters' preferences and want to know if the given profile follows a single-crossing pattern. When the order of votes is fixed and the input profile is made up of top orders, a polynomial-time algorithm can be used. However, if vote permutation is allowed and the input profile includes weak orders or independent-pairs orders, the problem becomes NP-complete. Certain practical situations of both problems have been identified that can be solved in polynomial time.
411375	41137519	Equilibria of plurality voting with abstentions	The traditional literature on voting manipulation assumes that a group of manipulators falsely represent their preferences to get a specific candidate elected, while the rest of the voters are truthful. However, this paper considers a different scenario where all voters are strategic and the election is seen as a game with Nash equilibria determining the outcomes. The authors analyze two forms of Plurality voting - simultaneous voting where all voters vote at once and sequential voting where voters express their preferences one by one. They find that it is difficult to determine if a given preference profile will lead to a pure Nash equilibrium in simultaneous voting, and that sequential voting with three or more candidates can lead to unexpected equilibria.
411376	41137672	Exploiting query reformulations for web search result diversification	This paper introduces a new method for diversifying web search results when the user's initial query is not specific enough. This probabilistic framework takes into account the different aspects of an underspecified query and diversifies the document ranking accordingly. The effectiveness of this approach is evaluated in the context of the TREC 2009 Web track diversity task, using reformulations from three major search engines. The results show that this framework outperforms other existing methods. Furthermore, by simulating an ideal query reformulation mechanism, the study reveals insights on the effectiveness of query reformulations from different search engines in promoting diversity. 
411376	41137643	Selecting effective expansion terms for diversity	Query expansion is a commonly used technique in Information Retrieval, but it can also fail when faced with ambiguous queries. To address this issue, diversifying search results has been found to be an effective strategy. In this study, the authors propose a new approach to tailor query expansion in order to improve the diversification of search results for ambiguous queries. This is achieved by selecting diverse expansion terms based on user feedback. The effectiveness of this approach is tested through experiments on TREC datasets from 2009 to 2011, showing promising results in improving the performance of existing query expansion techniques.
411377	41137714	Delay compensation in Shared Haptic Virtual Environments	Shared Haptic Virtual Environments (SHVEs) use a client-server architecture, with a physics simulation engine calculating object states based on position information from clients. Clients receive and update their local copy of the environment, with haptic forces computed based on object interactions. However, communication delay can lead to delayed updates and increased interaction forces, causing users to perceive increased object weight. To address this, the paper proposes an adaptive force rendering scheme that reduces object stiffness at clients based on delay, device velocity, and motion constraints to achieve perceptual transparency. Simulations and subjective evaluations show successful compensation for delays up to 150 ms, while preserving the perception of interacting with a rigid object.
411377	411377140	Compensating the Effect of Communication Delay in Client-Server--Based Shared Haptic Virtual Environments	Shared haptic virtual environments involve multiple users interacting in a virtual environment using haptic devices. This is achieved through a client-server architecture, where each client maintains a local copy of the virtual environment and a centralized physics simulation on the server calculates object states based on haptic device positions. However, communication delays can result in delayed object state updates and increased force feedback at the clients. This article analyzes the impact of communication delay on force feedback and proposes an adaptive force rendering scheme to compensate for this effect. Experimental results, including a user study, confirm the effectiveness of the proposed scheme in a multi-user shared haptic virtual environment.
411378	41137839	Planar and infinite hypohamiltonian and hypotraceable graphs	Chvatal and Grunbaum have debated the existence of planar graphs with specific properties. Chvatal questioned the existence of planar hypohamiltonian graphs, while Grunbaum suggested they do not exist. However, an infinite class of planar hypohamiltonian graphs has been identified. In addition, an infinite class of planar hypotraceable graphs with connectivity two or three has been described. The study of these infinite graphs has also resulted in the discovery of a new infinite family of finite hypotraceable graphs. This research sheds light on the properties and potential existence of certain types of planar graphs. 
411378	41137886	Cycles through all finite vertex sets in infinite graphs.	The Freudenthal compactification of an infinite locally finite graph G has a closed curve called a Hamiltonian curve if it meets every vertex of G exactly once and every end at least once. It is proven that G has a Hamiltonian curve if and only if every finite vertex set is contained in a cycle. This result can be used to extend results and conjectures on finite graphs to Hamiltonian curves in infinite locally finite graphs. For instance, Barnette's conjecture, which states that every finite planar cubic 3-connected bipartite graph is Hamiltonian, is equivalent to the statement that every one-ended planar cubic 3-connected bipartite graph has a Hamiltonian curve. However, there are some exceptions, such as 7-ended planar cubic 3-connected bipartite graphs that do not have a Hamiltonian curve.
411379	41137966	Efficient Sub-/Inter-Group Key Distribution for ad hoc Networks.	The paper discusses the importance of efficient communication in emerging networks, specifically in ad hoc networks. To ensure secure group communication, group key management has been proposed as a fundamental cryptographic tool. However, existing proposals have limitations in efficiently facilitating subgroup and intergroup communication. The paper introduces two group key distribution schemes that address these issues and do not require user interaction. Storage and computation analyses show that the proposed schemes are secure and efficient compared to existing ones. Furthermore, extensions for multipartite groups are presented, greatly improving efficiency in this scenario. 
411379	41137960	Fast transmission to remote cooperative groups: A new key management paradigm	Efficient and secure broadcasting to remote cooperative groups is a challenge in emerging networks due to limited communication, lack of a trusted key generation center, and dynamics of the sender. Existing key management methods are not effective in dealing with these challenges. To address this, a hybrid paradigm of traditional broadcast encryption and group key agreement is proposed. Each member has a public/secret key pair and a remote sender can securely broadcast to a chosen subgroup. The proposed scheme is secure, independent of group size, allows for easy member changes and rekeying, and does not rely on a trusted authority. It is a promising solution for many applications.
411380	41138037	Fast transmission to remote cooperative groups: A new key management paradigm	The problem of broadcasting securely to a remote cooperative group is a common issue in new networks. Overcoming challenges such as limited communication, lack of a trusted key generation center, and dynamic senders is crucial in designing effective systems. Existing key management methods are not effective in addressing these challenges. This paper proposes a new approach that combines traditional broadcast encryption and group key agreement. Each member has a unique public/secret key pair, allowing a sender to securely broadcast to a chosen subgroup. The system is proven secure and has minimal computation and communication costs, even if non-intended members collude. It also allows for easy member changes and rekeying strategies. This scheme is highly secure, efficient, and does not require a fully trusted authority, making it a promising solution for various applications.
411380	41138011	Secure One-To-Group Communications Escrow-Free Id-Based Asymmetric Group Key Agreement	Group key agreement (GKA) is commonly used to ensure secure communication within a group. However, there is a growing need for secure one-to-group communication in distributed computing applications. Asymmetric group key agreement (AGKA) addresses this need by allowing a group of members to establish a group public key while each member has a unique secret key. This paper presents a secure identity-based AGKA protocol that is resistant to active attackers and offers optimal round efficiency, sender dynamics, and escrow freedom. The protocol remains secure even if all participants or the key generation center is compromised. It is based on the k-Bilinear Diffie-Hellman exponent assumption and has comparable performance to current AGKA protocols.
411381	41138136	Simulating Non Stationary Operators in Search Algorithms.	This paper presents a model for simulating search operators that have continuously changing behavior during a search. These operators are often classified as either exploitation or exploration operators and their performance decreases when applied in these scenarios. The model is used to compare different operator selection policies and their ability to adapt to the changing behavior of these operators. An experimental study is conducted and reveals interesting results on the performance of operator selection policies in non-stationary search scenarios. This research provides valuable insights on how to effectively select operators in situations where their behavior is not constant.
411381	41138110	A Compass to Guide Genetic Algorithms	Parameter control is crucial for improving the performance of Genetic Algorithms (GA). However, there is a lack of general approaches for this issue, leading to manual adjustments of parameters in practice. Some generic methods have been tested by examining operators' improvements. In this paper, the authors propose a new approach called Compass, which considers the impact of operators on population diversity and computation time. Compass offers a way to adjust the balance between exploring and exploiting the search space, making it easier for users to control GA's parameters. The effectiveness of this approach is demonstrated through experiments on solving a well-known combinatorial problem (SAT). 
411382	41138246	Making preferences more active	The paper introduces the concept of pseudo-texts, a way to incorporate richer semantic structures into the Preference Semantics system. These pseudo-texts are consistent with previous accounts of the system and are used to handle extended use, where semantic preferences are broken in sentences. The authors argue that extended use is common in normal language and a language understanding system must be equipped to handle it. They propose a notion of sense projection, which involves altering semantic formulas to adapt to unexpected context. This idea could be implemented in a new semantic parser for the Preference Semantics system, which could serve as a test-bed for the usefulness of frames in language understanding. Overall, the authors suggest that this approach could be more effective than traditional question-answering systems and provide a better understanding of language.
411382	41138292	What would a Wittgensteinian computational linguistics be like	 learned thatThe paper explores the connections between Wittgenstein's ideas about language and the development of Artificial Intelligence (AI) and Computational Linguistics. It discusses how the shift towards statistical models in language processing has made these connections more plausible, but also notes that these models do not fully capture the concept of meaning. The author describes a recent effort to create a model of language based on the entire World Wide Web, but acknowledges the limitations of this approach. The paper also examines the influence of philosophers like Carnap and Wittgenstein on the development of linguistics, particularly in contrast to Chomsky's work. Overall, the paper highlights the importance of understanding the use of language in order to truly comprehend its meaning.
411383	41138312	Hierarchical-granularity holonic modelling	Designing distributed and pervasive intelligent systems, like Multi Agent Systems (MAS), is driven by breaking down the application-specific knowledge into functional components. Changes in problem semantics or granularity levels can greatly affect the system re-engineering process. To address these challenges, a new framework called Hierarchical-Granularity Holonic Model (HGHM) is proposed, which uses holons (agents with special features) for distributed intelligent systems modelling. Holons can be decomposed into other holons, allowing for complex systems to be modelled at different hierarchical-granularity levels. This approach has an advantage over traditional holonic systems and MAS as it directly derives the architecture from the problem ontology. The HGHM is applied in a case study for indoor air quality monitoring systems, showing improvements in system design compared to existing solutions. 
411383	41138368	A Methodology for Example-Based Specification and Design	The use of embedded systems is increasing in this decade, leading to the need for fast prototyping and time-efficient solutions. To address this, various algorithmic formalisms are available to describe and validate complex systems at a behavioral level, reducing development costs and simplifying design and implementation constraints. However, traditional development environments for embedded systems are not compatible with soft-computing paradigms, unless an algorithmic description is provided. This is typically achieved through a training procedure, which selects the most suitable soft-computing paradigm and configures it. The paper discusses the challenges of integrating soft-computing paradigms into traditional development environments for embedded systems, focusing on the behavioral level.
411384	41138449	A radix-8 wafer scale FFT processor	Wafer Scale Integration (WSI) technology offers the potential for significant enhancements in the performance of digital signal processing systems. This paper focuses on the development of a radix-8 systolic fast Fourier transform (FFT) processor, which is designed specifically for implementation with WSI. The use of a radix-8 FFT butterfly wafer, currently in the works, is expected to allow for continuous data rates of 160 million samples per second (MSPS) for FFTs of up to 4096 points using 16-bit fixed point data. This advancement in WSI technology could greatly improve the capabilities of digital signal processing systems.
411384	41138437	Generic signal processor implementation with VHSIC	This paper examines the connection between clock and data rates in different types of processor architectures. It finds that in signal processing, the performance of a processor is limited by the speed of memory access if the memory and arithmetic chips operate at the same clock rate. The paper uses a case study of the AOSP Macro Function Signal Processor to demonstrate the effectiveness of Very High Speed Integrated Circuit (VHSIC) technology in implementing a generic signal processor. VHSIC's high speeds and integration levels make it a suitable choice for this type of processor.
411385	41138516	Brain-computer interfacing in discriminative and stationary subspaces.	The non-stationary nature of neurophysiological measurements, such as EEG, makes it difficult to accurately classify motion intentions in Brain Computer Interfacing (BCI). Changes in brain processes can lead to unexpected variations in feature distribution, resulting in decreased accuracy. To address this problem, methods have been developed to adapt to changes or extract invariant features. A recent method called Stationary Subspace Analysis (SSA) has been applied to BCI data, which diminishes the impact of non-stationary changes by learning and classifying in a stationary subspace of the data. This paper proposes two extensions to SSA: a variant that can extract stationary subspaces from labeled data without disregarding class-related variations, and a discriminant variant that balances stationarity and discriminativity. Results show that learning in a discriminative and stationary subspace is more effective for BCI than the standard SSA method.
411385	41138526	Modelling non-stationarities in EEG data with robust principal component analysis	Modelling non-stationarities is a common challenge in neuroscience, and robust models can provide insights into the cause of changes observed in neuroscientific signals. One commonly studied signal is electro-encephalography (EEG), which is non-invasive, affordable, and easy to obtain. However, EEG is highly non-stationary, which can make it difficult to interpret. In this paper, a robust method is proposed to visualize non-stationarities in neuroscientific data. This method is unaffected by irrelevant noise sources, allowing for a better understanding of the neurological sources responsible for the non-stationarity. The method uses a robust version of principal component analysis and is applied to EEG data collected through a brain-computer interface. This demonstrates how the method can help to understand and address non-stationarities in EEG signals.
411386	411386229	What is happening right now ... that interests me?: online topic discovery and recommendation in twitter	The Social Web is becoming increasingly reliant on Twitter for real-time information and knowledge about current events. However, with the overwhelming amount of tweets, it can be difficult for users to find relevant and ranked information. To address this issue, the authors propose a new approach that focuses on the individual user's interests rather than the general perspective of what is happening. They introduce a method called RMFO, which creates personalized rankings for tweets based on the user's past interactions. Experiments show that this method outperforms other techniques, demonstrating its effectiveness in improving information discovery on Twitter. 
411386	41138686	Real-time top-n recommendation in social streams	The Social Web is a rapidly growing platform where people generate and consume data in real-time. This paper focuses on analyzing social streams for personalized topic recommendation and discovery. The proposed approach, Stream Ranking Matrix Factorization (RMFX), uses a pairwise matrix factorization method to optimize the personalized ranking of topics. It also utilizes a selective sampling strategy based on active learning principles to perform online model updates, making it suitable for large-scale applications. Experiments on a dataset of 476 million Twitter tweets show that RMFX outperforms recommendations based on Twitter's global trend and is able to deliver faster and more efficient recommendations compared to a state-of-the-art matrix factorization technique called Weighted Regularized Matrix Factorization (WRMF).
411387	41138744	Semantic program repair using a reference implementation.	Automated program repair techniques often rely on tests as the criteria for correctness, leading to the problem of overfitting. While various approaches have been proposed to address this, they are not guaranteed to produce patches that work beyond the given tests. This study proposes using a reference implementation to infer a specification of the intended behavior, which is then used to synthesize a patch that enforces conditional equivalence with the reference program. This approach reduces overfitting and allows for the generation of more accurate patches. Additionally, the use of semantic analysis allows for the patch to have a different implementation from the reference program. Experiments on repairing embedded Linux Busybox with GNU Coreutils as the reference showed promising results.
411387	411387101	An unfold/fold transformation framework for definite logic programs	The unfold/fold program transformation system takes a logic program P and derives a sequence of programs P0, P1, …, Pn by applying unfolding or folding steps. These transformations are commonly used to improve program efficiency and reason about programs. While unfolding is always semantics-preserving, folding may produce a different program. Existing transformation systems for logic programs have restrictions on folding, but the new system called SCOUT is more powerful and can be used for reasoning about concurrent systems. SCOUT uses a framework with a measure space and functions, allowing for more flexibility in transformation. It also includes a goal replacement transformation that can interchange semantically equivalent atoms. SCOUT has been successfully used for inductive proofs of temporal properties in parameterized concurrent systems.
411388	41138885	Dessy: Search and Synchronization on the Move	Smartphones today have the capability to store large amounts of information. As more and more data is stored on mobile devices, the need for effective information organization is becoming increasingly important. To tackle this challenge, people are turning to desktop search tools. With the rise of multiple devices, the synchronization of information between them has become crucial. Dessy is a framework that combines desktop search and synchronization for mobile devices. It allows users to sync search results, individual files, and entire directories. This paper also includes an evaluation of the system's energy usage. Dessy works closely with the Syxaw file synchronizer, which optimizes network usage for efficient file and metadata synchronization. 
411388	41138871	Dessy: Towards Flexible Mobile Desktop Search	In the future, mobile devices are expected to have similar storage capabilities as desktop computers. As the amount of information grows, traditional search tools become less effective. To address this issue, a desktop search system called Dessy has been developed for both mobile and desktop devices. It allows users to search for files based on their content, metadata, and context information. Dessy also supports separate synchronization of file metadata and data, and allows for easy customization and addition of new file types and storage methods. It uses virtual directories as a way to view and manage files, making it easy for users to browse and locate their desired files. 
411389	41138976	Degree condition and Z3-connectivity	The paper discusses a type of graph called a 2-edge-connected simple graph with 3 vertices, and an abelian group with 3 elements. It introduces the concept of A-reduction, where nontrivial A-connected subgraphs are repeatedly contracted until none are left, resulting in a simplified graph. The paper then proves that for a graph G with maximum degree of either n or 2 where the endpoints are not connected, G is not Z\"3-connected if and only if it is isomorphic to one of twenty two specific graphs or can be reduced to K\"3, K\"4, K\"4^- or G\"5. This result builds on previous work by Luo et al. and Fan and Zhou.
411389	41138974	A dual version of the Brooks group coloring theorem.	This article discusses the concept of A-connectedness in 2-edge-connected undirected graphs, where A is an additive Abelian group. It defines Λg(G) as the minimum size of A needed for a graph G to be A-connected, and shows that Λg(G) is less than or equal to the maximum length of a circuit containing a path of length i in G plus one, for any integer i and 2-connected graph G. The authors also investigate the graphs where this equality holds, and provide a complete determination for all graphs where Λg(G) is equal to the maximum length of a circuit containing a path of length two in G plus one. This research builds upon previous work by Fan et al. and others. 
411390	41139071	Hierarchical discriminant analysis for image retrieval	The Self-Organizing Hierarchical Optimal Subspace Learning and Inference Framework (SHOSLIF) is a system for object recognition that uses a hierarchical database structure for image retrieval. It utilizes optimal linear projection for feature derivation and a Space-Tessellation Tree generated using the Most Expressive Features (MEFs) and Most Discriminating Features (MDFs) to achieve a logarithmic retrieval complexity. By recursively deriving a better set of features for each level of the tree, it avoids the limitations of global linear features and improves generalization capability. The system also allows for perturbations in object size and position through learning. This approach has been successfully applied to a large image database with varying real-world objects. The focus of this paper is on the hierarchical partitioning of feature spaces.
411390	41139037	Discriminant analysis and eigenspace partition tree for face and object recognition from views	The SHOSLIF method utilizes linear discriminant projection to automatically select optimal features in each internal node of a Space-Tessellation Tree. This approach has been studied for its effectiveness in dealing with variability in position, size, and 3D orientation. The paper focuses on the application of this method to recognition tasks, specifically requiring "well-framed" images as input. Well-framed images have limited variations in object size, position, and orientation. The experimental results show a performance difference between linear discriminant analysis and principle component analysis subspaces, as well as the impact of using a tree rather than a flat eigenspace. 
411391	411391206	Crowdsourcing label quality: a theoretical analysis	Crowdsourcing has become a widely used method for obtaining labels for large amounts of data. While various methods have been developed for extracting labels from the crowd, there has been little theoretical analysis of this popular human-machine interaction process. This paper presents a theoretical study on the quality of labels obtained through majority voting and shows that the label error rate decreases exponentially with the number of workers selected for each task. The paper also addresses the issue of identifying and removing low-quality workers from the crowd, providing both conservative and aggressive conditions for doing so without eliminating any high-quality workers. 
411391	411391223	Crowdsourcing with Unsure Option.	In crowdsourcing, there is a balance between the number of workers needed for accurate results and the budget to pay them. To save money, high-quality labels from workers are important. One way to ensure this is to have workers only submit labels when they are confident, using an 'unsure' option. However, this can potentially waste budget. This study analyzes when using the unsure option can actually save money and how to set the confidence threshold. An online mechanism is also proposed as an alternative for setting the threshold when estimating worker abilities is difficult.
411392	41139219	Security testing of a secure cache design	Cache side channel attacks are a type of security breach that allows secret information to be leaked through the physical implementation of cryptographic operations. These attacks have gained attention in recent years, as software countermeasures have been found to be insufficient in protecting against them. Secure cache designs, such as Newcache, have been developed to address this issue and have shown to be more efficient in preventing cache side channels. However, these designs have not been extensively tested against various types of attacks. In this paper, the security of Newcache is evaluated using representative classes of cache side channel attacks and it is found to successfully defend against them. However, when specifically crafted attacks are targeted at Newcache, it may succumb to timing attacks due to a vulnerability in its replacement algorithm. This vulnerability is addressed by slightly modifying the replacement algorithm, resulting in an improved and simplified version of Newcache. 
411392	41139228	Disruptive prefetching: impact on side-channel attacks and cache designs	Caches are important components in modern computers that help bridge the speed gap between fast processors and slow memory. However, they can also be exploited by attackers to access critical information through cache-based side-channel attacks. Secure caches have been proposed to prevent such attacks, but they can complicate cache designs. This study suggests using specialized prefetching algorithms to protect against cache-based side-channel attacks. These prefetchers can be easily integrated with existing cache designs and have low hardware overhead costs. The study found that using a secure prefetching policy with a stride prefetcher can deliver original performance while protecting against attacks. Additionally, a disruptive prefetching scheme was shown to be effective in protecting against access-based side-channel attacks.
411393	41139341	A New Memory Monitoring Scheme for Memory-Aware Scheduling and Partitioning	The proposed memory monitoring scheme utilizes novel hardware counters to track the gain in cache hits as the cache size is increased, allowing for an accurate estimation of the cache miss-rate for each process under the LRU replacement policy. This information can be used for job scheduling and cache partitioning to minimize overall miss-rates. The collected data can also be used to improve an analytical model of cache and memory behavior, resulting in a more accurate overall miss-rate for a group of processes sharing a cache. This can further enhance scheduling and partitioning strategies. 
411393	41139350	High-performance parallel accelerator for flexible and efficient run-time monitoring	Harmoni is a highly configurable hardware accelerator architecture that supports a variety of run-time monitoring and bookkeeping functions. Unlike traditional custom hardware, Harmoni can be dynamically configured after fabrication, allowing for a wider range of monitoring functions to be added to a processing core. This architecture is more efficient than software implementations and other proposed monitoring platforms, as it closely matches the characteristics of run-time monitoring functions based on tagging. A prototype of Harmoni was implemented and tested with various monitoring functions for security and programmability, demonstrating its ability to support different functions with low overhead and high throughput. Overall, Harmoni offers moderate silicon area and high performance for run-time monitoring and bookkeeping tasks.
411394	41139491	Three-dimensional face pose detection and tracking using monocular videos: tool and application.	The paper discusses a real-time tracker that can simultaneously track the 3-D head pose and facial actions in low quality monocular video sequences. The proposed system has two main contributions: an automatic 3-D face pose initialization scheme using a 2-D face detector and eigenface system, and enhancing the human-machine interaction functionality of an AIBO robot by controlling the orientation of its camera through head pose estimation. The proposed techniques can be applied in various applications such as telepresence, virtual reality, and video games. Experiments on real videos demonstrate the effectiveness and usefulness of the proposed methods.
411394	41139428	Combined head, lips, eyebrows, and eyelids tracking using adaptive appearance models	The paper discusses the importance of accurately detecting and tracking human heads and faces in video sequences for various applications. It introduces a real-time tracker that can simultaneously track 3D head pose and facial actions associated with lips and eyebrows. The approach relies on Online Appearance Models, which learn facial texture during tracking. The paper presents an extension to this approach, showing that using a non-occluded facial texture model can improve accuracy and stability of 3D head pose parameters. Additionally, the paper demonstrates the use of Online Appearance Models for eyelid tracking, without relying on color information or intensity edges. Experiments on real videos validate the effectiveness of the proposed approach.
411395	41139587	Undecidability of Operation Problems for T0L Languages and Subclasses	The article examines the solvability of the operation problem for T0L languages and their subclasses. This refers to determining if applying a specific operation to languages from these families will result in a language that still belongs to the same family. The Lindenmayer language families, which include 0L and T0L languages, are not closed under common operations such as homomorphisms, inverse homomorphisms, intersection with regular languages, union, concatenation, and Kleene closure. The study also looks at the intersection and substitution operations, and finds that, except for Kleene closure, the operation problems for 0L and T0L languages and their propagating variants are not even semidecidable.
411395	41139586	Finite turns and the regular closure of linear context-free languages	This article explores different types of bounded pushdown automata and their relationship with closures of linear context-free languages under regular operations. The conditions for beginning a new turn in these automata are investigated, such as emptying the pushdown store or re-entering the initial state. This leads to the characterization of automata with an unbounded number of turns by the regular closure of linear languages, and automata that also have to re-enter the initial state by the Kleene star closure of linear languages. The article also introduces new language families and discusses their closure properties under AFL operations. Lastly, an algorithm is presented for parsing languages from these new families in quadratic time.
411396	41139645	Computing Reputation for Collaborative Private Networks	As collaborative network services become more popular, protecting the shared resources and relationships among participants is crucial. One important aspect is evaluating participant reputation, as access to network resources may depend on it. Previous reputation models, such as Ebay and Sporas, were either too simplistic or too complex to be suitable for privacy concerns. In response, the authors propose a new reputation model based on OWA and WOWA operators. This model allows for private computation using elGamal crypto-system and incorporates user preferences in the reputation calculation. The feasibility of this model is demonstrated in a scenario of a Web-based Social Network.
411396	411396125	Toward A Privacy Agent For Information Retrieval	In this paper, the private information retrieval (PIR) problem in Internet search engines is addressed. This problem arises when a user wants to retrieve information without the search provider learning about it. Traditional PIR protocols are not suitable for real-world search engines like Google due to their reliance on cooperation from the database and their high computational complexity. Recent approaches aim to distort server logs to protect privacy, but this can result in loss of information and hinder personalized results and targeted advertising. This paper proposes a noncooperative agent for private search that considers profiling as valuable data for both the user and search engine. The agent uses the user's browsing history to classify queries and reduce the risk of disclosure. Empirical results show the effectiveness of this approach.
411397	41139741	An alternative storage organization for ROLAP aggregate views based on cubetrees	ROLAP is a popular approach for data warehousing and decision support applications. It relies on creating summary tables to improve query performance. However, the traditional relational storage implementation of ROLAP views is inefficient and slow. This paper proposes the use of Cubetrees, a compressed and packed storage structure, for ROLAP views. An algorithm is also presented for mapping OLAP views to Cubetrees. Experiments using data from the TPC-D benchmark show that Cubetrees offer significant improvements in storage (2-1 reduction), query performance (10-1 faster), and update speed (100-1 faster) compared to the traditional relational storage approach. This highlights the superiority of Cubetrees as a storage and index organization for ROLAP views.
411397	41139768	A case for dynamic view management	DynaMat is a system designed to manage materialized aggregate views in a data warehouse. These views are redundant entities used to speed up OLAP processes. With the complex structure of data warehouses and the varied profiles of users, there is a need for tools to automate and simplify the view selection and management process. DynaMat utilizes a dedicated disk space to store computed aggregates, which are used to answer queries. It can handle single or grouped queries, taking advantage of dependencies among them to optimize execution. When updates occur, DynaMat selects and refreshes the most beneficial subset of materialized views within a given maintenance window, using an update plan that considers the maintenance window, update policies, and dependencies. 
411398	4113983	Implicit interaction profiling for recommending spatial content	GIS applications often return default maps with standard content when users request area maps. These maps may contain irrelevant information, making it necessary for users to customize them repeatedly. One solution is to ask for explicit input from users before generating a map, but this can be costly and rely on user input. Another solution is to store basic profile information, but this only captures limited customization. A better approach is to create personalized maps that only display the most relevant spatial content based on users' implicit interactions with maps. This does not require any extra effort from the user and allows the system to constantly learn and update their preferences. 
411398	41139830	Delivering personalized context-aware spatial information to mobile devices	In the digital age, finding relevant spatial information online can be challenging due to the presence of extraneous content. This is especially true for mobile users who face device limitations. To address this issue, a multimodal mobile GIS has been developed that profiles users' spatial content preferences by tracking their interactions with maps and movements in the field. This allows for personalized and context-aware recommendations of spatial information. A search engine is also incorporated to enhance the user experience by providing context-aware information. A case study focused on surveying tasks has been developed to demonstrate the effectiveness of this approach.
411399	41139995	Morphology analysis of 3D scalar fields based on morse theory and discrete distortion	The study focuses on analyzing and understanding 3D scalar fields using a morphological approach. This involves creating a discrete model of the field by dividing it into a tetrahedral mesh and using Morse theory to identify important morphological features. The researchers measure the distortion of this transformation and develop an algorithm to segment the field and control the number of regions. Experimental results demonstrate the effectiveness of this approach.
411399	41139953	Efficient computation and simplification of discrete morse decompositions on triangulated terrains	This article discusses the problem of efficient computing and simplifying Morse complexes on a Triangulated Irregular Network (TIN) using discrete Morse theory. The authors propose a compact encoding method for the discrete Morse gradient field, which is based on the terrain elevation and can be combined with any TIN data structure. They also explain how to compute the gradient field and effectively simplify it to reduce the number of critical elements. To demonstrate the effectiveness and scalability of their approach, they develop algorithms for extracting the cells of the Morse complexes and the graph joining the critical elements. The proposed approach is compared on two different data structures for TINs, showing promising results.
411400	41140032	Real-valued evolutionary multi-modal optimization driven by hill-valley clustering.	Model-based evolutionary algorithms (EAs) use an underlying search model to adapt to the features of a problem, such as the connection between variables. However, the performance of these algorithms can suffer when trying to model multiple modes in the fitness landscape with a unimodal search model. The number of modes is often unknown, making it difficult to adapt the search model. This study introduces a simple approach called Hill-Valley Clustering, which adapts to the multi-modality of the fitness landscape by dividing it into niches, with one mode in each niche. This approach, combined with an EA and restart scheme, is compared to other niching methods on a benchmark for multi-modal optimization. Results show that this approach is competitive within the limited budget of the benchmark and outperforms other methods in the long run.
411400	41140020	A Clustering-Based Model-Building EA for Optimization Problems with Binary and Real-Valued Variables	The proposed clustering-based model-building evolutionary algorithm is designed to solve optimization problems with both binary and real-valued variables. It uses a distance metric to cluster the search space each generation, taking into account the dependencies between variables of different types. Within each cluster, linkage learning is performed to capture and exploit dependencies between variables of the same type. This approach is compared to one that only considers dependencies between variables of the same type. The algorithm is also tested on problems with constraints, using different methods to handle them. Results show that this approach can significantly improve performance on problems with multiple optima, strong mixed dependencies, and constraints, compared to the Mixed-Integer Evolution Strategy.
411401	411401100	On the possibility and reliability of predictions based on stochastic citation processes	The author analyzes a statistical model for citation processes, which is a version of a non-homogeneous birth process. The model has been previously studied and demonstrated to be applicable through several examples. However, the practical aspects of predicting future citation rates and the statistical reliability of these predictions have not been addressed. The current study focuses on demonstrating the possibility of accurate predictions and analyzing the statistical reliability using the mean value function of citation processes. Data from 1980 to 1995 for papers published in 1980 and 1991 were used to show that the parameters estimated for earlier time periods can also be applied to more recent years. This model can also be used to validate the choice of citation windows in evaluation studies.
411401	41140119	Modelling and measuring multilateral co-authorship in international scientific collaboration. Part II. A comparative study on the extent and change of international scientific collaboration links	The present study examines international collaboration in science, specifically focusing on collaborations between more than two countries. The authors have developed a model, based on a series expansion approach, to measure and analyze the extent of multilateral co-authorship links. This model introduces a new indicator, the Multilateral Collaboration Index (ρ), which is related to the share of internationally co-authored papers (f). The study looks at changing collaboration patterns between 1983 and 1993 for 8 selected subfields as well as all fields combined for the top 38 most active countries. The results show an increase in international collaboration, particularly among former COMECON countries, and variations in behavior among countries and science subfields.
411402	41140297	Navigation techniques for small-screen devices: An evaluation on maps and web pages	The paper discusses three techniques for navigating large information spaces on small-screen devices like PDAs and Smartphones. The first technique, DoubleScrollbar, uses two scrollbars and zoom buttons. The second, Grab&Drag, allows users to directly drag the displayed portion of the information space. The third, Zoom-Enhanced Navigator (ZEN), is an adaptation of the Overview&Detail approach, displaying an overview and detail view of the space. A user study was conducted to compare these techniques in terms of performance and satisfaction for tasks involving maps and web pages. Results showed that ZEN was the most preferred and efficient technique, with the least number of user interface actions and highest accuracy in gaining spatial knowledge. 
411402	41140213	On the effectiveness of Overview+Detail visualization on mobile devices	Overview+Detail visualization is a popular method for displaying large amounts of information on computer screens. However, its use on mobile devices has not been extensively studied. This paper examines existing research on Overview+Detail visualization, comparing desktop and mobile studies to identify strengths and weaknesses. The analysis reveals areas that require further investigation and can guide interface design. The paper then presents an experiment that explores new aspects of using Overview+Detail on mobile devices, such as allowing users to manipulate the overview and highlighting important objects. The results show that both of these techniques improve user performance in search tasks on mobile devices, but do not have a significant impact on recall of spatial information.
411403	41140318	LPCEL Editor: A Web-Based Visual Authoring Tool for Learning Design	Educational Modeling Languages (EMLs) are used to represent learning processes in a formal way. One popular EML is the IMS-LD, and there are many tools available to help users create courses using this specification. However, these tools are limited by the level of expressiveness allowed by IMS-LD. To address this limitation, the LPCEL Editor was created. This visual tool allows for the authoring of courses while maintaining a broad level of expressiveness. This means that users can create more complex and dynamic courses without being restricted by the limitations of IMS-LD compliance. Overall, the LPCEL Editor provides a more user-friendly and versatile option for creating courses using EMLs.
411403	41140394	The collaborative development of didactic materials.	This paper discusses the development process of technology-supported didactic material. Didactic material is defined as the combination of content and instructional design used to guide teaching and learning. The authors argue that features such as reusability, semantic interoperability, and collaboration support are important to consider from the beginning stages of material creation. This makes didactic material development a complex task that requires a development methodology and a high-quality authoring environment. The paper introduces the MD2 research project, which offers a solution for creating didactic materials that addresses these concerns. This solution includes a collaborative creation method and a quality evaluation framework. The authors also introduce CASLO, a collaborative authoring tool designed to support these efforts.
411404	41140422	Note on Upper Density of Quasi-Random Hypergraphs.	In 1964, Erdos showed that for any alpha > 0, a hypergraph with n vertices and alpha(n^l) edges contains a large complete l-equipartite subgraph. This means that a sufficiently large hypergraph with density alpha>0 contains a large subgraph with density at least l!/l^l. In this study, we examine a similar problem for hypergraphs with a weak quasi-random property, and prove that a sufficiently large quasi-random hypergraph with density alpha>0 contains a large subgraph with density at least (l-1)!/l^l-1 - 1. This result also implies that every number between 0 and (l-1)!/l^l-1 - 1 is a jump for quasi-random l-graphs. For l=3, this interval can be improved based on a recent result. Specifically, we prove that every number between 0 and 0.3192 is a jump for quasi-random 3-graphs.
411404	41140445	Extremal problems on set systems	The article discusses the concept of k-uniform hypergraphs and their relationship to arithmetic progressions. The maximum number of k-tuples that a k-uniform hypergraph on n vertices can have without containing any member of a given family F(k) is denoted as ex(n, F(k)). The maximum cardinality of a set of integers Z that does not contain any arithmetic progression of length k is denoted as rk(n). The article introduces families F(k) = {F1(k), F2(k)} and proves that nk-2rk(n) ≤ ex(nk2, F(k)) ≤ Cknk-1 for k ≥ 3. It also discusses a conjecture that ex(n, F(k)) = o(nk-1), which would imply a result of Szemerédi stating that rk(n) = o(n). The article focuses on verifying this conjecture for k = 4 and explores related problems.
411405	41140566	Edge-Unfolding Nearly Flat Convex Caps.	This paper presents a proof that a convex cap, defined as the intersection of a convex polyhedron and a halfspace, can be unfolded into a non-overlapping polygon in the plane. The cap must be nearly flat, with each face normal forming a small angle with the z-axis. The required angle depends on the acuteness gap of the cap, with a smaller angle needed for more acute triangles. Even if the cap is closed by adding a convex polygonal base, it can still be unfolded without overlap. The proof uses angle-monotone and radially monotone curves and results in a polynomial-time algorithm for finding the necessary edge cuts.
411405	41140527	A note on reconfiguring tree linkages: trees can lock	A recent discovery has revealed that it is possible to rearrange any polygonal chain in a plane to lie on a straight line and any polygon to become convex. However, this does not hold true for tree linkages as there are cases where two configurations cannot be connected by a motion. This has been proven through the existence of trees with 2Ω(N) different configurations for an N-link tree. This shows that the concept of reconfiguration does not apply to tree linkages in the same way it does for polygonal chains and polygons.
411406	41140663	Blind separation of audio signals using trigonometric transforms and Kalman filtering	This paper addresses the issue of separating audio signals from noisy mixtures. Instead of separating the signals in the time domain, the authors propose using a blind separation algorithm on the Discrete Cosine Transform (DCT) or the Discrete Sine Transform (DST) of the mixed signals. This is because both transforms have an energy compaction property, which concentrates the signal energy in a few coefficients and leaves the rest close to zero. Additionally, the authors recommend using Kalman Filtering as a post-processing step for noise reduction. The simulation results demonstrate the effectiveness of this approach and the feasibility of using Kalman Filtering for further noise reduction.
411406	41140625	An efficient singular value decomposition algorithm for digital audio watermarking	This paper discusses the use of singular value decomposition (SVD) for audio watermarking in both time and transform domains. The audio signal is transformed into a 2-D format and the SVD algorithm is applied to add an image watermark with a small weight to the matrix of singular values. The paper compares the effectiveness of embedding the watermark in the time and transform domains and presents experimental results showing that embedding in the time domain leads to lower distortion levels and higher detection probabilities. The proposed algorithm also allows for the embedding of chaotic encrypted watermarks for increased security. A segment-by-segment implementation is also presented to improve the detectability of the watermark under severe attacks.
411407	41140716	Latent structured models for human pose estimation	This article discusses a new method for automatically reconstructing 3D human poses from single images using a discriminative approach with latent segmentation inputs. The approach utilizes a pool of figure-ground segment hypotheses and combines learning and inference to predict 3D human articular configurations. The authors also propose new augmented kernels to better encode dependencies between output variables and provide linear re-formulations based on Fourier kernel approximations to improve scalability. The effectiveness of the proposed models is demonstrated through experiments on the HumanEva benchmark and a clip from a Hollywood movie, showing their ability to infer human poses in complex environments.
411407	41140762	Twin Gaussian Processes for Structured Prediction	Twin Gaussian processes (TGP) is a structured prediction method that uses Gaussian process (GP) priors on both covariates and responses. It aims to minimize the Kullback-Leibler divergence between two GP models to ensure that similar inputs produce similar outputs. TGP takes into account the interdependencies between both inputs and outputs, resulting in better correlations. It has been successfully used for 3D human pose reconstruction from video sequences, achieving a 5 cm error on average per 3D marker. TGP is fast and automatic, requiring no manual adjustments or specific body models for training or testing. 
411408	41140813	Nearest hyperdisk methods for high-dimensional classification	In high-dimensional classification problems, it is often impossible to have enough training samples to densely cover all class regions. This leads to irregular decision boundaries for local classifiers like Nearest Neighbors and kernel methods. To address this issue, a method has been proposed where a convex model is built for each class using the training samples and examples are classified based on their distances to these models. This approach has been studied using various methods such as affine hulls and bounding hyperspheres. The proposed method uses bounding hyperdisks, which are the intersection of affine hulls and the smallest bounding hypersphere, and is shown to be more effective in tightly bounding classes and avoiding overfitting and computational complexity. This method can also be extended to non-Euclidean distance metrics through kernelization and has shown promising results in experiments. 
411408	41140836	Margin-based discriminant dimensionality reduction for visual recognition	Nearest neighbour classifiers and kernel methods struggle in high dimensional problems due to lack of dense training data. This leads to test samples falling into gaps between training samples, making it difficult to accurately determine class membership. One solution is to project the data onto a lower dimensional subspace. A new nonparametric method is proposed, which involves filling the gaps by building a convex model of each class region and finding highly discriminative directions using a scatter matrix. The weights used in this process prioritize narrow margin cases while still allowing for more diversity than the traditional linear SVM projection. Experimental results on face and object recognition datasets show the effectiveness of this method in reducing dimensionality and improving the performance of simple classifiers like nearest neighbours.
411409	4114096	Reducing frame rate for object tracking	Object tracking is often used in video surveillance, but sending full frame rate videos is not necessary. This paper presents an analytical framework for determining the critical frame rate needed to successfully track objects using two popular algorithms: frame-differencing-based blob tracking and CAMSHIFT tracking. The paper also explores ways to modify these algorithms to further reduce the critical frame rate. Results show that the frame rate can be reduced by up to 7 times for blob tracking and 13 times for CAMSHIFT tracking, without losing the tracked object. This information can be used to optimize object tracking in video surveillance systems and potentially save on bandwidth and storage costs.
411409	41140979	Video quality for face detection, recognition, and tracking	Distributed multimedia applications often use video analysis algorithms for automated processing, but it's unclear what video quality is needed for accurate results. To address this, researchers focused on commonly used face analysis algorithms and conducted experiments using standard datasets and live videos. They found that the algorithms remained accurate until the video quality was reduced to a critical point, much lower than what is acceptable for human vision. Since computer vision differs from human vision, traditional video quality metrics can't accurately predict the impact of reduced quality on algorithm accuracy. Researchers explored two alternative metrics, blockiness and mutual information, and found they could estimate the critical video qualities for face analysis algorithms.
411410	4114106	Automatic Generation of Tailored Accessible User Interfaces for Ubiquitous Services	Egoki is a tool that automatically creates user interfaces for people with disabilities, allowing them to access various services. It uses a model-based approach to determine the best interaction resources and modalities for each user's capabilities. A prototype was tested with accessibility experts and participants with disabilities in two scenarios: one for blind people and one for people with cognitive impairments. The interfaces generated for each scenario were evaluated and found to be operable and accessible. A user evaluation was also conducted, with all participants successfully completing tasks using the tailored interfaces. This demonstrates the effectiveness of Egoki in providing accessible services for people with disabilities.
411410	41141025	Providing universally accessible interactive services through TV sets: implementation and validation with elderly users	The challenge of Ambient Intelligence (AmI) is creating a user-friendly interaction concept for non-technical users. This paper presents a new approach to integrate interactive services from AmI environments with the widely used television set. This approach accommodates different TV configurations and allows for universally accessible solutions. The implementation focuses on creating a natural human-computer interface for the elderly, including simplified TV remote control and voice interaction. The user interface can also be adapted for other user groups. Testing with 83 users showed that the prototype was satisfactory and efficient to use, with features such as video conferencing and information services. 
411411	4114114	Semi-supervised Gaussian process latent variable model with pairwise constraints	Gaussian process latent variable model (GP-LVM) is a popular technique used in unsupervised dimensionality reduction in machine learning. However, when some supervised information is available, the traditional GP-LVM cannot effectively use it to improve its performance. In order to address this issue, a new semi-supervised GP-LVM framework has been proposed, which utilizes pairwise constraints to incorporate supervised information. By transferring these constraints to the latent space, the algorithm can optimize the latent variables using maximum a posteriori (MAP) algorithm. Experiments on different data sets have shown the effectiveness of this approach.
411411	41141169	Video denoising based on adaptive shrinkage in surfacelet transform domain	The proposed algorithm for video denoising uses an adaptively shrinking surfacelet transform (ST) coefficient. The algorithm first uses the Monte Carlo method to research the probability distribution feature of noise in the ST domain, which helps determine the threshold and mask classification for each ST coefficient. The energy ratio and prior ratio are then combined and used to shrink the ST coefficients using a shrinkage estimator. The denoised video is obtained by applying an inverse surfacelet transform to the shrunk ST coefficients. The algorithm also selects an ideal neighborhood shape to improve the prior ratio. Experimental results show that the proposed method outperforms other methods in terms of visual and quantitative performance for various levels of noise and motion.
411412	411412118	Teleoperation of Multiple Social Robots	The paper discusses the application of teleoperation, or remote control, of multiple social robots by a single operator. This concept has been extensively studied for search and navigation purposes, but has not been applied to social, conversational robots before. The paper outlines the challenges of remote operation of multiple social robots, including the need for the operator to multitask audibly. A system is proposed that allows a single operator to simultaneously control four robots in conversational interactions, using a control architecture, graphical interface, and a technique called "proactive timing control". Metrics for robot performance are also presented, and experimental results demonstrate the effectiveness of the system.
411412	411412200	Panel 2: social responsibility in human-robot interaction	The 2008 ACM/IEEE Conference on Human-Robot Interaction featured a panel discussion on the ethical issues surrounding the field. The panelists and audience highlighted the need for ongoing discourse on social responsibility in human-robot interaction. In 2010, a panel will be held at the conference to address the unique aspects of robotic interaction that require responsible action, such as autonomy, accountability, trust, and human dignity. As a growing field, there is a responsibility to conduct research that promotes the well-being of humans in society. The panelists will discuss what social responsibility looks like in HRI and how to conduct research that fulfills this obligation. This panel serves as a continuation of the conversation on social responsibility in human-robot interaction.
411413	41141327	Determining cylindrical shape from contour and shading	This paper presents an algorithm that can reconstruct the shape of a cylindrical object using only contour and shading information, without the need for surface albedo or lighting conditions. The algorithm segments the input image into different types of surfaces, such as spherical, cylindrical, or planar, by analyzing local shading. The direction of the generating lines on the cylindrical surface is determined using spatial derivatives in the image. By considering the brightest generating line, the equation representing the relationship between contour shape and shading can be simplified. While there is still one degree of freedom in the solution, the algorithm is able to accurately reconstruct the cylindrical shape (up to reflection). Experimental results using synthetic images are provided.
411413	411413164	Utilization of a stripe pattern for dynamic scene analysis	The paper proposes a new method for detecting moving objects and estimating 3-D motion parameters in a time-varying scene. It involves projecting a stripe pattern onto the scene and using a temporal difference method to detect objects moving against a complex background. The method then uses the slopes and intervals of stripes to estimate surface normals and create a 2(1/2)D representation of the moving objects. This representation is then used to identify planar or curved surfaces, and the rotational motion parameters of the objects are estimated by analyzing changes in these surfaces between consecutive frames. The paper also discusses the process of determining translational motion parameters.
411414	41141438	Evaluating spreading activation for soft information fusion	A soft-information fusion process refines natural language messages to produce more accurate estimates of soft-information. This information can then be used to retrieve related information from background knowledge sources using contextual cues, known as Context-Based Information Retrieval (CBIR). The performance of the CBIR process relies on the selection of appropriate algorithms and parameters for the given problem domain. This paper evaluates the performance of two spreading activation algorithms and their parameters in a counterinsurgency domain using an f-measure evaluation. The first phase determines how different parameter values affect the algorithms' performance and sets the parameters for future use. The second phase compares the results of the algorithms using the parameter settings learned in the first phase. 
411414	4114140	A knowledge engineering approach to natural language understanding	The paper examines a Knowledge Engineering approach to Natural Language Understanding, where a computer system is being developed to acquire, represent, and utilize linguistic knowledge. The system is rule-based and uses a semantic network for storing and representing knowledge. User interaction with the system is facilitated through natural language input and output. The system can handle various types of knowledge, such as syntactic and semantic, and supports both assertions and rules. An inference tracing facility is being developed as part of the rule-based system, with output in natural language. A detailed example is provided to demonstrate the current capabilities and features of the system.
411415	411415121	Toward Socially Assistive Robotics for Augmenting Interventions for Children with Autism Spectrum Disorders	Children with Autism Spectrum Disorders (ASD) often struggle with communication and social interaction, which can make it difficult for them to benefit from therapy and learn social skills. In recent years, research has shown that robots can be effective tools in promoting social behavior in children with ASD. The use of robots in therapeutic settings has led to the development of systems that act as catalysts for social behavior. A pilot experiment was conducted with children with ASD interacting with a socially assistive robot, and the results were promising. This suggests that robots have the potential to improve social skills and enhance the effectiveness of therapy for children with ASD. 
411415	41141516	A scalable approach to human-robot interaction	Current research in human-robot interaction mainly focuses on single systems and a small number of users. These systems have well-defined interfaces and are tightly-coupled. However, for large-scale applications where there may be multiple unknown systems, designing an interface for human interaction becomes challenging. In this paper, the authors propose an interaction infrastructure that can accommodate many-to-many and one-to-one interactions between users and robotic systems. The infrastructure is designed to be scalable and has been successfully tested in simulation and in a smaller-scale real robotic environment. The authors also emphasize the importance of autonomy in dealing with diverse systems and propose an interaction mechanism that offers extended functionality, such as task queuing and linking. The experiments conducted in this paper further validate the scalability and task-linking ability of the proposed infrastructure.
411416	41141645	Steiner tree packing number and tree connectivity.	The paper discusses the concept of S-Steiner trees in a graph G, where S is a set of at least two vertices. These trees are considered edge-disjoint or internally vertex-disjoint if their edges or vertices do not intersect. The maximum number of such trees, denoted by λG(S) and κG(S), are studied for different values of S. Kriesell conjectured that if λG({x,y})≥2k for any x,y∈S, then λG(S)≥k. This conjecture was proven for S with 3 or 4 vertices. The paper provides a concise proof for this conjecture and also establishes a relation between λk(G) and κk(L(G)), where L(G) is the line graph of G.
411416	41141610	Inverse Degree And Super Edge-Connectivity	A connected graph G with order n, minimum degree delta(G), and edge connectivity lambda(G) is maximally edge-connected if lambda(G) equals delta(G), and super edge-connected if every minimum edgecut contains edges incident with a vertex of minimum degree. The inverse degree of G, denoted as R(G), is defined as the sum of the reciprocals of the degrees of all vertices in G with no isolated vertices. If R(G) is less than 2 + (n-2 delta)/(n-delta)(n-delta-1), then G is super edge-connected. A similar result is also shown for triangle-free graphs.
411417	41141747	Chosen-ciphertext secure multi-hop identity-based conditional proxy re-encryption with constant-size ciphertexts.	Proxy Re-Encryption (PRE) is a method that allows one user to delegate the decryption rights of their encrypted data to another user. However, previous methods such as Multi-Hop Identity-Based PRE (MH-IBPRE) had the limitation of increasing ciphertext size and decryption complexity with the number of re-encryption "hops". In this paper, a new MH-IBPRE scheme is proposed that maintains a constant ciphertext size and computational complexity regardless of the number of hops. This scheme is also bidirectional and supports conditional re-encryption, and is proven secure against selective identity and chosen-ciphertext attacks. Additionally, the scheme can be extended to a set of conditions, making it of independent interest.
411417	41141724	Semantic keyword searchable proxy re-encryption for postquantum secure cloud storage.	Cloud computing has become increasingly popular, with consumers opting for the pay-as-you-consume model for cloud services. This allows for convenient storage and access to data through smart devices at any time and location. Data privacy is a top concern, and encryption is necessary to protect consumers' information. However, searching for data on encrypted files is a major challenge for effective data utilization. To address this, a new semantic keyword searchable proxy re-encryption scheme has been proposed in this paper. Notably, this scheme is resistant to quantum attacks, which is not the case for most existing searchable encryption schemes. It supports both exact and synonym keyword searches, and allows for delegation of search rights to other users through proxy re-encryption without the need for interaction. Additionally, the scheme is designed to be collusion resistant and has been proven secure under the learning with errors hardness problem in the standard model.
411418	41141833	Practice Sharing Paper: Motivating Computer Scientists to Engage with Professional Issues: A Technology-Led Approach	The authors of this paper discuss the challenges of incorporating professional issues modules into a computer science curriculum. These modules, which focus on discursive teaching, can be seen as different from the more typical knowledge and skills-based learning in computer science. The authors have experience teaching these modules and recently faced the challenge of consolidating two separate courses into one smaller course with less face-to-face contact time. This posed a challenge in motivating students who may have specialized in science, technology, and math to engage in a topic that may not directly relate to their chosen field. The paper discusses the motivations for designing the module, the approaches taken by the teachers and curriculum design team, and the impact of this change. The authors hope to encourage students to gain a deeper understanding of their own learning and technological preferences, and take ownership of their professional development.
411418	41141840	Bootstrapping a Culture of Sharing to Facilitate Open Educational Resources	This article discusses the benefits of having a large collection of reusable resources for teachers and students in the education community. The learning object community has been working towards this goal for over a decade by establishing infrastructure, standards, and specifications. However, there has been limited uptake in universities and higher education. The main issue is that teachers have not shared enough of their resources or made them openly available. Two initiatives, EdShare and Language Box, have focused on promoting sharing within institutional and subject-based communities. The paper examines the motivations, design decisions, and approaches used by these projects to encourage sharing and the impact they have had so far. It aims to contribute to a better understanding of how to promote sharing in educational communities.
411419	41141948	Enlarging learnable classes	The paper discusses the concept of Ex-learnable sets, which are sets of functions that can be learned through inductive inference. It is found that these sets are not closed under unions, leading to the question of which classes of functions can be combined with an Ex-learnable set to create another Ex-learnable set. The paper presents a criterion for this to be possible, as well as exploring the extension of learners to potentially learn infinitely more functions. It is shown that learners of non-dense sets of functions cannot be extended in this way, but a different split of learners into two sets allows for effective extension for all learners in each set. Similar concepts are also analyzed for other learning criteria.
411419	41141929	Learning with ordinal-bounded memory from positive data	A bounded example memory learner maintains a finite memory of previously processed data items and is known to coincide with set-driven learning. For each k, a hierarchy of stronger learning criteria has been obtained for iterative learners who can only store k items in their memory. This paradigm has been extended into the constructive transfinite using Kleene's universal ordinal notation system. The number of times a learner can extend its memory is bounded by an algorithmic count-down from the notation. A general hierarchy result has been proven, showing that learners with a higher Kleene notation can learn more than those with a lower notation. For learners with ordinal-bounded memory, the impact of requiring a learner to not discard elements without replacing them is also studied, known as cumulative learning. 
411420	41142034	An Agent-Based Platform for Cloud Applications Performance Monitoring	Virtualization in Cloud environments presents challenges when it comes to monitoring resources. To ensure scalability and reliability, applications are distributed across various resources like Virtual Machines and storage. This means that users can only access information about the Cloud infrastructure through monitoring services provided by the Cloud provider, requiring them to trust the provider for performance data. In response, a comprehensive architecture is proposed to cover all monitoring activities in a Cloud application's lifecycle. An agent-based implementation of a specific module is also suggested, offering greater customization and tolerance for network and resource failures. This approach aims to address the challenges of resource monitoring in virtualized Cloud environments.
411420	41142025	Agent-Based Intrusion Detection for Federated Clouds	The cloud services market has seen significant growth in recent years, leading to potential scalability issues. As a result, there has been a focus on federating multiple clouds as a solution. The publish-subscribe paradigm is commonly used to support interoperability in federated clouds. However, this paper highlights potential security vulnerabilities in this type of system and proposes an agent-based system to monitor and address these risks. This emphasizes the importance of considering security in the design and implementation of federated cloud solutions. 
411421	411421101	Using the mOSAIC's semantic engine to design and develop civil engineering cloud applications	The development of applications for the Cloud requires knowledge of programming models, APIs, and underlying infrastructures provided by Cloud vendors. The European Project mOSAIC aims to simplify this process by creating an API, Platform, and tools that allow for application development and deployment on various Infrastructure as a Service platforms. Within the project, ontologies, a knowledge base, and a Semantic Engine have been developed to assist developers in discovering necessary functionalities and resources in a vendor-independent manner. This paper demonstrates the use of the Semantic Engine, ontologies, and knowledge base in the design and implementation of a Finite Element Method-based application for structural analysis under static loading.
411421	41142152	Towards a Semantic Engine for Cloud Applications Development	The paper introduces a Semantic Engine for Cloud applications development, which will be a part of the mOSAIC project. This engine is designed to support developers in finding and using API functionalities and components for their cloud applications. It also manages semantic descriptions of resources and available mosaic elements, aiding in the selection of necessary components and resources from Cloud providers. This engine will assist developers in both the development and deployment stages of creating cloud applications. 
411422	4114225	Quasi-symmetric 2-(31, 7, 7) designs and a revision of Hamada's conjecture	The article discusses the existence of nonisomorphic quasi-symmetric 2-(31, 7, 7) designs with 2-rank 16, in addition to the design formed by planes in PG(4, 2). This disproves a conjecture by Hamada about the relationship between the incidence matrices of designs and their parameters. The five identified designs are extendable to nonisomorphic 3-(32, 8, 7) designs with 2-rank 16, one of which is made up of 3-flats in AG(5, 2). This also shows that designs arising from finite affine geometries cannot be characterized solely by their ranks. Additionally, a quasi-symmetric 2-(45, 9, 8) design is constructed using an extremal doubly even (48, 24) code, resulting in a pseudo-geometric strongly regular graph with parameters (r, k, t) = (15, 10, 6).
411422	41142213	Unitals and codes	The program described in this article focuses on enumerating unital 2-(28,4,1) designs using tactical decompositions based on weight vectors in the dual binary code of a design. The authors study a specific type of design with a spread that covers a code word of weight 12. Through this approach, they were able to construct 909 nonisomorphic designs, including well-known designs such as the hermitian and Ree unitals, as well as previously known 2-(28,4,1) designs. This program provides a comprehensive method for generating and analyzing these types of designs.
411423	41142331	Admission Control for Wireless Networks with Heterogeneous Traffic using Event Based Resource Estimation	The paper introduces a new admission control algorithm for wireless cellular networks that can handle different types of traffic. It estimates the resources needed for handoff calls by considering the probability of visiting specific cells during the call lifetime. This information is shared with surrounding base stations, and the reserved resources are updated during handoffs and call termination. The algorithm also maintains tuning parameters to ensure that handoff dropping probabilities meet certain constraints. The paper also provides QoS bounds for homogeneous traffic. Overall, the proposed algorithm aims to improve resource utilization and maintain quality of service for different types of traffic in wireless cellular networks.
411423	4114237	Event based resource estimation in admission control for wireless networks with heterogeneous traffic	The technique presented in the article aims to estimate the resources needed to maintain an upper bound on handoff dropping probability in wireless networks with heterogeneous traffic. This is done by collecting information from base stations in surrounding cells and using simple calculations. The results are used to propose a distributed adaptive admission control algorithm that has been evaluated for single traffic type cases. The performance of the algorithm is shown to be influenced by traffic descriptors, cell capacity, and a tuning parameter. By adjusting this parameter, the handoff dropping probability can be met while also maximizing cell capacity utilization and minimizing new call blocking probabilities.
411424	41142483	Learning on Graph with Laplacian Regularization	This article discusses transductive learning on graphs and how it can be improved using Laplacian regularization. The authors derive margin-based generalization bounds by examining the geometric properties of the graph. They also explore the impact of graph Laplacian matrix normalization and dimension reduction on the performance of the learning algorithm. The results highlight a limitation of the commonly used degree-based normalization and propose a solution based on their analysis. Empirical evidence shows that this remedy leads to better classification performance. Overall, this study contributes to a better understanding of transductive learning on graphs and provides practical insights for improving its performance.
411424	41142443	A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data	Machine learning is a rapidly developing field, and one of its key issues is whether incorporating unlabeled data can improve the performance of supervised learning algorithms. This is known as semi-supervised learning, and while many methods have been proposed, their effectiveness is not fully understood. This paper explores a related problem, using multiple learning tasks to learn predictive structures on hypothesis spaces. A general framework is presented for analyzing this problem and its relationship to learning with unlabeled data. The paper proposes algorithms for structural learning and investigates their computational aspects. Experiments are conducted to demonstrate the effectiveness of these algorithms in the semi-supervised learning setting.
411425	41142528	Automatic creation of a reference corpus for political opinion mining in user-generated content	The proposed method aims to automatically create a reference corpus for training text classification procedures to analyze political opinions in user-generated content. This is achieved by compiling a collection of highly opinionated comments from an online newspaper and using manually-crafted rules and a sentiment-lexicon to identify sentences expressing opinions about political entities. The identified opinions are then propagated to other sentences in the comment mentioning the same entities, resulting in a larger and more diverse set of opinion-bearing sentences. The method shows high precision in identifying negative opinions, but lower precision in identifying positive opinions due to issues with irony. Overall, the approach proves effective in creating a reference corpus for training text classification procedures for political opinion mining.
411425	41142536	Liars and saviors in a sentiment annotated corpus of comments to political debates	The study focuses on analyzing opinions about human entities in user-generated content (UGC). 2,800 online news comments were manually annotated using a specialized scheme. The main challenge in opinion mining from UGC is correctly identifying positive opinions due to their rarity and potential use of verbal irony. The use of pronouns, definite descriptions, and nicknames also poses difficulties in identifying human targets in the content. Overall, the study highlights the complexities involved in mining opinions from UGC and the need for a thorough annotation scheme to accurately analyze them.
411426	41142632	Dynamic Network DEA approach to basketball games efficiency.	Data Envelopment Analysis (DEA) has been widely used in sports, but there is limited research on its application in basketball. Two approaches have been developed to measure efficiency in basketball: player assessment and team performance. This paper introduces a new approach that focuses on measuring scoring efficiency of both teams in a game, as the number of points scored greatly impacts the appeal of a game. The proposed method, called Dynamic Network DEA, takes into account the performance of each team in each quarter and the carry-overs between quarters. This allows for a scoring efficiency to be calculated for each team in each quarter, overall, and for the entire game. The approach is applied to matches from the 2014-2015 NBA season. 
411426	41142617	Cell formation and scheduling of part families for reconfigurable cellular manufacturing systems using Tabu search	A reconfigurable cellular manufacturing system (RCMS) is a system made up of multiple reconfigurable machining cells, each containing reconfigurable machine tools (RMTs), a setup station, and an automatic material handling and storage system. In order to efficiently use a RCMS, similar parts are grouped into part families and RMTs are arranged in parallel cell configurations. This paper proposes a new approach that solves the cell formation and scheduling problems simultaneously, using a mixed integer linear programming model to minimize production costs. The model considers both reconfiguration and under-utilization costs. A Tabu search algorithm is also developed to find production schedules for part families in a RCMS. The approach is shown to be effective in solving real-world problems and maintaining cost-effectiveness in RCMS.
411427	41142734	Deterministic catalytic systems are not universal	A catalytic system (CS) is a language acceptor with evolution rules of the form Ca → Cv or a → v, where C is a catalyst, a is a noncatalyst symbol, and v is a string representing a multiset of noncatalyst symbols. A CS can be deterministic if there is a unique maximally parallel multiset of rules applicable at each step. It has been an open problem whether deterministic CSs are universal. This article shows that the membership problem for deterministic CSs is decidable, and the Parikh map of the language accepted by any deterministic CS can be effectively constructed. The results also generalize to multimembrane deterministic CSs.
411427	41142750	On deterministic catalytic systems	The article discusses a type of catalytic P system with evolution rules that can be used as language acceptors. The system uses catalysts and noncatalyst symbols to accept input strings and has the ability to nondeterministically select and apply rules in parallel to derive the next configuration. It is known that a 1-membrane catalytic system is universal, but the article explores the question of whether deterministic systems of this kind are also universal. The article shows that the membership problem for deterministic catalytic systems is decidable and provides examples of systems where the nondeterministic version is universal but the deterministic version is not. The article also discusses the use of prioritized rules in catalytic systems and their impact on universality.
411428	41142831	Network Inference From Co-Occurrences	The discovery of networks is a crucial issue in various fields such as communication, biology, sociology, and neuroscience. However, obtaining complete data to reveal network structure is often challenging or impossible. This paper focuses on inferring network structure from "co-occurrence" data, where the order of network components is not known. The number of possible networks that fit the data grows exponentially with network size, but certain principles suggest that some networks are more likely than others. The paper proposes a model and algorithm based on random walk and permutation to estimate network parameters. For networks with long paths, a polynomial-time Monte Carlo algorithm is suggested for efficient reconstruction. Experiments show promising results.
411428	41142823	Growing a Network on a Given Substrate	This article discusses the limitations of traditional studies on network growth models, which often only focus on the steady-state degree distribution and ignore the initial conditions. The authors shift the focus to the time evolution of the degree distribution and analyze two specific growth models with uniform and preferential attachment. They are able to obtain the degree distribution as a function of time for any initial condition, providing insight into the transient behavior and rate of convergence to the steady-state limit. This approach allows for a more comprehensive understanding of network growth dynamics.
411429	41142953	Discriminative model selection for object motion recognition	This paper focuses on the challenge of determining the optimal number of components in mixture-type models for trajectory classification using motion vector fields. The authors propose a discriminative criterion that considers the classification accuracy on a separate dataset to choose the best model for each class. The approach takes into account the task of classification and experiments with both synthetic and real data demonstrate its effectiveness in pedestrian activity classification. By incorporating this knowledge into the model selection process, the proposed method improves the performance of mixture-type models for trajectory classification. 
411429	411429111	Activity Recognition Using a Mixture of Vector Fields	The paper discusses the analysis of moving objects in videos and focuses specifically on pedestrian trajectories in video surveillance. The approach involves modeling these trajectories using a small set of motion fields and a space-varying switching mechanism. Despite the variability in motion patterns, the authors show that a small number of typical behaviors can be identified and modeled with simple motion fields. The model is made more flexible by allowing for switching between motion fields in a space-dependent manner. An expectation-maximization algorithm is used to learn the model parameters and the approach is applied to trajectory classification tasks. Experiments with synthetic and real data demonstrate the effectiveness of the proposed method. 
411430	41143010	A Topological Characterisation Of Belief Revision Over Infinite Propositional Languages	Belief revision is the process of updating one's beliefs in light of new evidence. The AGM framework, proposed by Grove in 1988, models belief revision as revising theories by propositions. This framework has been represented using systems of spheres, which have been extended to characterize multiple belief revision operators. However, there are still unresolved issues with this 'spheres' model. This paper introduces a topology on the set of all worlds in a propositional language and uses it to characterize systems of spheres. It also shows that there is a minimal system of spheres contained in all others for each AGM operator, and provides a topological characterization of these minimal systems. Furthermore, the paper proposes a method for extending an AGM operator to a multiple revision operator, and shows that this extension is not unique, answering a previously unresolved problem.
411430	41143053	A representation theorem for minmax regret policies	Decision making under uncertainty is a key task for artificial agents, and qualitative decision tools are commonly used in artificial intelligence due to their simplicity and easy specification. Brafman and Tennenholtz's model represents an agent's uncertain knowledge as their local state, consisting of possible states of the world. A policy is used to determine the agent's preference over actions for each local state. It has been proven that a policy is maximin representable if it satisfies specific conditions, such as being closed under unions and meeting certain acyclicity requirements.
411431	41143118	A note on near hexagons with lines of size 3.	We have determined a classification for all finite near hexagons that have certain properties, including three points on every line and a point at a specific distance from every other point. Additionally, we have found that every pair of points at a certain distance have a specific number of common neighbors. As a result, we have also identified all finite near hexagons that meet these criteria with a specific value for t(2). This information can be used to classify all finite near hexagons with t(2) equal to 4. 
411431	41143147	On the adjacency algebras of near hexagons with an order.	The article discusses a finite near hexagon of order (s, t) with v points, denoted as \(\mathcal {S}\). The adjacency matrix of the graph defined on the points by the distance i relation is denoted as \(A_i\). The real algebra generated by the \(A_i\)’s is studied and it is found that a certain number \(d_\mathcal {S}\) (based on s, t, and v) must be integral. This leads to the exclusion of certain near hexagons that have been a topic of discussion for 15 years. In the case where s=2, it is shown that the near hexagon has a minimum embedding rank of \(d_{\mathcal {S}}\), and its projective dimensions are not full with a vector dimension of \(d_{\mathcal {S}}\). 
411432	41143234	Enhanced Kinematic Model For Dexterous Manipulation With An Underactuated Hand	Recent studies have focused on underactuated manipulation and have used Kinematic Models (KM) to describe the system by adding external constraints to the standard manipulation analysis method. However, in real-world dexterous manipulation tasks, these external constraints are often violated, resulting in control errors. To address this issue, an Enhanced Kinematic Model (E-KM) has been proposed, which integrates the KM with the Sparse Online Gaussian Process (SOGP). The E-KM can compensate for the shortcomings of the KM by training the SOGP on the residual between the KM's prediction and the ground truth data in real-time. An optimal controller for underactuated manipulations has also been developed based on the E-KM and tested on the iCub humanoid robot. Real-world experiments showed that the E-KM controller achieved higher control accuracy than using the KM alone for a variety of objects.
411432	41143228	Object grasping using the minimum variance model.	The traditional view of reaching-to-grasp involves two separate processes: moving the hand to the object and gripping it. However, a new perspective suggests that grasping can be seen as pointing movements by the fingers to specific positions on the object. To test this, researchers used a minimum variance model, typically used for robot arm control, to explain human grasping movements. The model includes signal-dependent noise in motor commands, and planning the reach and grasp separately. The results showed that the model accurately reflects key characteristics of human grasping, such as the relationship between object size and maximum grip size. Further research is suggested based on this model.
411433	41143391	An improved multiple birth support vector machine for pattern classification.	Multiple birth support vector machine (MBSVM) is a new machine learning algorithm used for multi-class classification, based on the twin support vector machine (TSVM). It has a faster training speed compared to other multi-class classifiers based on TSVM, especially with a large number of classes. However, MBSVM may not perform well on certain datasets, such as the "Cross planes" dataset. To address this issue, an improved version of MBSVM is proposed, which includes a modified item to minimize the variance of distances between samples of a class and their hyperplanes. This improved MBSVM uses a smoothing technique and an interval-based approach for classifying new samples. Experimental results on artificial and UCI datasets demonstrate the efficiency and good performance of the proposed algorithm.
411433	411433100	Wavelet twin support vector machines based on glowworm swarm optimization.	The twin support vector machine is an improved version of the standard support vector machine, with better performance on datasets with complex regions. The recently proposed wavelet twin support vector machine combines wavelet analysis techniques with twin support vector machine, expanding the range of kernel function selection and improving generalization ability. However, both twin support vector machine and wavelet twin support vector machine struggle with parameter selection, which can greatly affect classification capability. To address this issue, a new method called wavelet twin support vector machine based on glowworm swarm optimization is proposed. This method uses glowworm swarm optimization to automatically select the parameters of wavelet twin support vector machine, resulting in improved performance and accuracy on benchmark datasets. 
411434	411434126	An Investigation on LTE Mobility Management	In LTE networks, the Mobility Management Entity (MME) is responsible for managing the movement of user equipment (UE). The MME is connected to a group of evolved Node Bs (cells) that are grouped into Tracking Areas (TAs). These TAs are further organized into TA Lists (TALs). When a UE moves to a new location, it reports this to the MME. If the network needs to connect to the UE, the MME will ask the cells in the TAL to page the UE. This paper explores the effectiveness of LTE paging and offers recommendations for the best sequence to page cells in.
411434	41143476	A dynamic paging scheme for long-term evolution mobility management	Long-term evolution (LTE) networks use tracking areas (TAs) to partition the service area, which consist of one or more cells. These TAs are grouped into a TA list (TAL). However, when a call arrives, the network pages all the cells in the UE's TAL, leading to high paging traffic and limited use of radio resources. To address this issue, a dynamic paging scheme is proposed that determines the paging sequence based on the UE's movement and call behavior. This paper compares the performance of dynamic paging with the previously proposed Cell-TA-TAL (CTT) paging and finds that dynamic paging is more effective when the UE's movement is regular and frequent. © 2013 John Wiley & Sons, Ltd.
411435	41143515	A process for continuous validation of self-adapting component based systems	This paper proposes a method to incorporate time-related stochastic properties into a continuous design process using runtime models. Time-related specifications of services are crucial in component-based architectures, especially in distributed and volatile networks. The models at runtime approach simplifies managing these architectures by keeping abstract models of the architecture in sync with the physical execution platform. For self-adapting systems, predicting delays and throughput is essential for making adaptation decisions and accepting changes that meet time specifications. The approach includes a new metamodel extension using stochastic Petri nets for internal time modeling and a library of patterns to specify and predict common time properties. The prediction engine can perform in real-time and validate the models, making synchronization of behaviors and structural changes easier.
411435	41143567	Towards the Automatic Generation of Self-Adaptive Robotics Software: An Experience Report	In this paper, the authors discuss their progress in using the DiVA Framework to design self-adaptive behavior for autonomous robots. They aim to translate these high-level models into a self-adaptive architecture for resource-constrained robots. Using a case study based on a Victim-Rescuer scenario, they manually created a self-adaptive component-based architecture using Cecilia and deployed it on two e-pucks (low-cost mobile robotics platform). Through this implementation, they aim to identify generic rules for automatically transforming DiVA design-time models into self-adaptive robotics software. The paper concludes with lessons learned and potential for future improvements in self-adaptive robotics. 
411436	41143629	Convex enclosures for the reachable sets of nonlinear parametric ordinary differential equations	This article discusses the use of convex enclosures to compute reachable sets for nonlinear parametric ordinary differential equations. These equations are commonly found in nonlinear control systems, and being able to accurately calculate reachable sets is important for control and verification purposes. The proposed method involves computing convex and concave approximations of the ODE solutions based on the parameters, which are then used to create a convex enclosure of the reachable set. This enclosure is expressed as an infinite intersection of halfspaces, allowing for the computation of a convex polyhedral enclosure by considering a finite number of these halfspaces. The approach involves solving convex dynamic optimization problems, ensuring accuracy even for nonconvex reachable sets. 
411436	41143630	Bounds on the reachable sets of nonlinear control systems	The article discusses the computation of rigorous enclosures for nonlinear control systems, focusing on applications that require fast results. Interval methods based on differential inequalities are a popular option due to their low computational costs. However, they often produce inaccurate and overestimated results. To address this issue, the authors propose a general bounding method that takes into account a known, but crude, set that contains the reachable set based on physical considerations. When this set is a convex polyhedron, an efficient implementation using interval computations is developed. Practical examples show that this method provides better efficiency and accuracy compared to other methods. 
411437	411437135	Improved Tardiness Bounds for Global EDF	The Earliest Deadline First (EDF) scheduling algorithm is not optimal for global scheduling on multiprocessor platforms. Previous studies have found bounds for the maximum tardiness of implicit-deadline sporadic tasks under global EDF scheduling, but these bounds are not accurate. This paper presents a new algorithm that can calculate better tardiness bounds for each task individually, rather than a single bound for the entire system. This algorithm is especially useful for task systems with diverse parameters, as it yields significantly improved bounds compared to previous methods. Additionally, the algorithm includes a simple test for verifying if a task system can be scheduled by global EDF without violating the maximum tardiness constraints.
411437	41143725	Uniprocessor scheduling of real-time synchronous dataflow tasks	The synchronous dataflow graph (SDFG) model is commonly used to model real-time applications in critical domains. To analyze recurrent workloads represented by this model, schedulability techniques from real-time scheduling are applied. A new version of the SDFG model is proposed that allows for specifying a real-time latency constraint between inputs and outputs. This model uses a polynomial-time algorithm to represent the computational requirements of each task, based on the concept of the demand bound function (dbf) used in real-time scheduling theory. This allows for applying existing dbf-centered methods to analyze SDFG models. An exact preemptive uniprocessor schedulability test is demonstrated using this approach for collections of independent recurrent processes represented by the enhanced SDFG model.
411438	41143843	Approximability and Fixed-Parameter Tractability for the Exemplar Genomic Distance Problems	This paper presents a survey of approximability and fixed-parameter tractability results for three Exemplar Genomic Distance problems: exemplar breakpoint distance, exemplar non-breaking similarity, and maximal strip recovery. The results discussed in the paper pertain to the simplest case of only two genomes, each with one sequence of genes. It was shown that the exemplar breakpoint distance problem is NP-hard and does not admit any approximation or FPT algorithm, even when a gene appears at most twice in a genome. The exemplar non-breaking similarity problem is also hard to approximate and is W[1]-complete. The maximal strip recovery problem was recently proven to be NP-complete, but also has a factor-4 approximation and a simple FPT algorithm with a running time of O(22.73k n + n^2).
411438	41143897	Lower bounds on the approximation of the exemplar conserved interval distance problem of genomes	This paper discusses the exemplar conserved interval distance problem for genomes and presents several lower bounds on its approximation. The authors prove that it cannot be approximated within a factor of clogn in polynomial time, unless P=NP. They also show that deciding whether the distance between two sets of genomes is zero or not is NP-complete. This implies that the problem cannot be approximated in polynomial time unless P=NP, even when a gene appears at most three times in each genome. The authors also demonstrate that there is no weak approximation for this problem within a factor of the maximum length of the genomes.
411439	41143923	QNoC: QoS architecture and design process for network on chip	In this text, we learn about the concept of Quality of Service (QoS) and its application in Systems on Chip (SoC). The authors propose a cost model for communication in SoCs and use it to design a Network on Chip (NoC) architecture. The inter-module communication traffic in SoCs is divided into four classes: signaling, real-time, RD/WR, and block-transfer. By analyzing the communication traffic, the authors determine the QoS requirements for each service class. They then modify a generic network architecture to create a customized Quality-of-Service NoC (QNoC) for the target SoC. This customization process includes minimizing network cost, optimizing placement of modules, removing unnecessary links and switches, and balancing link utilization. The end result is a low-cost QNoC that meets the QoS requirements of the target SoC.
411439	41143918	Routing table minimization for irregular mesh NoCs	NoC architectures typically use mesh topology and simple static routing to save power and space. However, this approach is not suitable for irregular mesh networks and application-specific NoCs. This paper presents a new technique for reducing the cost of routing tables by using a fixed routing function and minimal deviation tables for irregular mesh networks. Three hardware efficient routing methods are compared, and path selection algorithms are developed to minimize routing table costs. Simulations show that this approach significantly reduces costs compared to standard solutions, and the cost savings increase with larger NoC sizes.
411440	41144039	Time Optimal Synchronous Self Stabilizing Spanning Tree	This research introduces a time-optimal self stabilizing algorithm for constructing a synchronous distributed spanning tree. Previous algorithms had a stabilization time complexity of O(diameter), assuming that a larger message could be sent through each link in one time unit. However, the current algorithm stabilizes in O(diameter) time even with standard message sizes and without prior knowledge of the network size or diameter. The algorithm also does not depend on a given upper bound on the network size. Additionally, a new self stabilizing silent phase clock algorithm is presented, which is optimal in time. This research has potential applications in various distributed global tasks, such as distributed reset.
411440	4114404	Output stability versus time till output	In order to address networks with rapidly changing inputs or frequent faults, an algorithm is needed to quickly and accurately output the majority value of the inputs. This study focuses on stabilizing the output of majority consensus algorithms before their final stabilization, even if additional changes occur. The trade-off between output instability and time adaptivity (how fast the output stabilizes when faults occur) is discussed, with an optimal instability of Ω(log f) achieved for various versions of the majority consensus problem. Previous algorithms did not have this guarantee, and also had a limitation where stabilization was only adaptive if faults ceased for a certain amount of time. This study offers solutions to remove this limitation and improve the stability of the output.
411441	4114413	ON THE SUPPORTS OF THE WALSH TRANSFORMS OF BOOLEAN FUNCTIONS	 supports of Boolean functions This paper focuses on the structure of subsets of Fn2 that can be the Walsh supports of Boolean functions. These functions are important in cryptography for designing hash functions and ciphers. The Walsh transform is a key mathematical tool for studying these functions, as it measures the correlation between a Boolean function and all linear functions. The Walsh support of a Boolean function plays a crucial role in determining its cryptographic properties, such as resiliency and the existence of covering sequences. The paper also discusses the relationship between covering sequences and other important criteria for cryptographic Boolean functions, such as degree, non-linearity, and propagation criterion. While there are some known examples of subsets that can and cannot be Walsh supports, there is still limited knowledge on the overall structure of these supports. The paper presents new results in this area and provides an overview of what is currently known. 
411441	41144174	On Dillonʼs class H of bent functions, Niho bent functions and o-polynomials	In his thesis, John Dillon introduced a class of bent Boolean functions called family H. However, this class only included functions that were already known in the Maiorana-McFarland class. To expand this class, a slightly larger class, H', was created. It was found that the bent functions constructed using Niho power functions were the univariate form of the functions in class H. In addition, the bent functions with linear restrictions to vector spaces were characterized, and the duals of the Niho bent functions were shown to be affinely equivalent. This Niho function also belongs to the Maiorana-McFarland class, raising the question of whether H is a subclass of this class. It was discovered that the condition for a function in bivariate form to belong to class H is related to the function's definition being an o-polynomial. By utilizing existing classes of nonlinear o-polynomials, many new cases of bent functions in H were found, potentially differing from the known Maiorana-McFarland functions.
411442	41144215	Bayesian fuzzy clustering of colored graphs	The growing availability of interaction data from various fields highlights the importance of mining and understanding underlying graph structures. This includes data with different node types, represented by node color. To cluster nodes into communities, an unsupervised approach is used, where nodes of the same color are highly connected within but sparsely connected to the rest of the graph. A fuzzy extension of this clustering concept has been proposed, allowing nodes to have membership in multiple clusters. Two unresolved issues were the determination of cluster numbers and the integration of prior information. These issues are addressed by reinterpreting the factorization in a Bayesian framework, allowing for the inclusion of priors and automatic estimation of group sizes using automatic relevance determination. This approach is illustrated on a toy and protein-complex hypergraph, showing significant enrichment of distinct gene ontology categories in the resulting fuzzy clusters.
411442	41144217	Transforming Boolean models to continuous models: methodology and application to T-cell receptor signaling	Systems biology aims to understand the complex networks of regulation and signaling within organisms. These networks are usually described qualitatively, allowing for the creation of Boolean models. While these models can capture the overall behavior of a network, they cannot reproduce detailed concentration levels over time. With the increase in quantitative data from experiments, there is interest in using qualitative models to explain and predict these results. A method is presented for transforming Boolean models into continuous models using polynomial interpolation, allowing for the use of ordinary differential equations. This approach is compared to others and is demonstrated with a model of T-cell activation. This method will enhance the integration of modeling and experiments and allows for the application of quantitative analysis to qualitative systems.
411443	4114438	Compact Signed-Digit Adder Using Multiple-Valued Logic	The article discusses the challenges of interconnect complexity in future VLSI chips and how they can be solved using multivalued logic. The use of multiple-valued signals requires fewer interconnecting wires and can increase bandwidth. The paper introduces a new design for a signed-digit adder that utilizes multiple-valued logic and a combination of resonant-tunneling diodes and MOS transistors. This design allows for compact circuits and the use of current-mode logic for efficient addition of multiple signals. The proposed circuit eliminates ripple-carry effects and was verified through simulation and a prototype built using a standard 2-micron CMOS process. Although current technology does not integrate RTDs and MOS devices, there are efforts to develop such technology for their combined advantages.
411443	41144377	Digital circuit applications of resonant tunneling devices	Semiconductor quantum devices have become increasingly popular due to their ability to utilize a unique tunneling transport mechanism that allows for extremely fast device switching speeds. This is achieved through the use of resonant tunneling, which creates a negative differential resistance characteristic ideal for designing compact and self-latching logic circuits. This technology shows promise as an alternative for high-performance very-large-scale-integration design. Additionally, the bistable nature of the basic logic gates implemented using resonant tunneling devices has led to the development of nanopipelining, a technique that greatly improves the throughput and speed of pipelined systems. The use of multiple-peak resonant tunneling diodes has also allowed for the efficient design of multiple-valued circuits with reduced interconnect complexity and device count. This paper outlines the various circuit design advancements in both binary and multiple-valued logic using resonant tunneling diodes in conjunction with high-performance III-V devices such as heterojunction bipolar transistors and modulation doped field-effect transistors. It also introduces new bistable logic families that incorporate these devices and provide a single-gate, self-latching majority function along with the basic NAND, NOR, and inverter gates.
411444	411444243	Rate-distortion hint tracks for adaptive video streaming	The article introduces a new technique for adaptive video streaming called rate-distortion hint track (RDHT). This method involves storing precomputed characteristics of a compressed media source that are important for online streaming but difficult to compute in real time. It allows for low-complexity adaptation to variations in transport conditions such as data rate and packet loss. The RDHT-based streaming system has three components: information summarizing R-D attributes, an algorithm for predicting distortion, and a method for determining the best packet schedule. The article also presents distortion chain models which accurately predict distortion for various packet loss patterns. Experimental results show that the proposed techniques outperform traditional low-complexity streaming systems for both data rate and packet loss adaptation.
411444	411444183	Model-Based Enhancement Of Lighting Conditions In Image Sequences	This paper discusses the impact of illumination variability on computer vision algorithms and video coding methods. It introduces a 3-D model-based technique for estimating and manipulating lighting in image sequences, using 3-D model information and synthetic re-lighting. By tracking object motion and deformation, the algorithm can estimate the current scene lighting with low computational complexity and represent it using a small set of parameters. This representation can then be used to create video sequences with different lighting. The paper also shows how removing changing lighting effects before encoding can significantly improve coding efficiency, with PSNR improvements of up to 3 dB observed. 
411445	41144573	A Comparison of the LERS Classification System and Rule Management in PRSM	The LERS classification system and rule management in probabilistic rough set models (PRSM) are compared in terms of rule interpretation, quantitative measures, and conflict resolution when classifying new cases. The PRSM uses positive and boundary regions to interpret probabilistic rules, while LERS associates rules with different quantitative measures. These measures reflect different aspects of rules. Additionally, the rule conflict resolution method used in LERS can also be applied to PRSM. 
411445	41144590	The Usefulness Of A Machine Learning Approach To Knowledge Acquisition	This paper discusses the effectiveness of using machine learning methods for rule induction in expert systems. Four methods, including ID3 and LERS, were tested on six real-life data sets. The goal was to see how well an expert system could perform without certain attributes. The results showed that the knowledge acquisition options of LERS were much more successful in generating complete rule sets than the machine learning methods. Therefore, it is suggested that other rule induction methods should be used instead of machine learning when building knowledge bases. The knowledge acquisition options of LERS are recommended as a suitable alternative for this purpose.
411446	41144641	Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach	Recent research has shown that domain adaptation techniques can be beneficial in computer vision tasks. In this study, a new method called Domain Selection Machine (DSM) is proposed for recognizing events in consumer videos. This method utilizes a large number of loosely labeled web images from various sources to train a set of SVM classifiers, referred to as source classifiers. A parametric target decision function is then used to combine static SIFT features from web images/video keyframes and spacetime (ST) features from consumer videos. To select the most relevant source domains, a new data-dependent regularizer is introduced in the objective of Support Vector Regression (SVR). An alternating optimization algorithm is also developed to solve the target decision function and select the most relevant source domains. The proposed DSM method outperforms existing methods by up to 46.41% in real-world experiments on three datasets.
411446	41144682	Domain Adaptation From Multiple Sources: A Domain-Dependent Regularization Approach.	The paper introduces a new framework called Domain Adaptation Machine (DAM) for solving the multiple source domain adaptation problem. This framework aims to learn a robust decision function, known as the target classifier, for predicting labels of instances from the target domain. It does so by utilizing a group of pre-learned base classifiers trained on labeled instances from either the source domains or the target domain. The proposed framework outperforms existing methods and offers a more accurate and efficient solution for the domain adaptation problem.
411447	41144739	Reconstructing a 3D Line from a Single Catadioptric Image	The paper discusses a method for estimating the equation of a 3D line in axial non-central optical systems using only four points from a single image. This is due to the lack of vantage point and is supported by a result in enumerative geometry. The algorithm for reconstructing the equation is based on computing the Singular Value Decomposition (SVD) of the matrix of Plücker coordinates of the four corresponding rays. The paper also addresses the limitations of the method, such as when the four rays are nearly coplanar, and presents experimental results using a spherical catadioptric camera. The impact of calibration and numerical errors on the reconstruction algorithm is also discussed.
411447	4114475	Surround structured lighting: 3-D scanning with orthographic illumination	This paper introduces a new system for quickly capturing complete 3-D surface models using a single structured light projector, two mirrors, and one or more cameras. By projecting light patterns onto the object from different angles, the system can capture multiple views simultaneously. To ensure accuracy, the projector is constructed using a Fresnel lens and a commercial DLP projector. A single Gray code sequence is used to encode the projected light planes, and five views of the object are obtained from the mirrors. This allows for a dense 3-D point cloud to be generated using traditional structured light techniques. The system also includes mechanical alignment and calibration procedures, and eliminates the need for merging multiple scans or using multiple projectors. 
411448	41144823	Fast Individual Facial Animation	This article discusses a rapid method for creating personalized face models and generating facial animations. A frontal face image is captured and key points are automatically detected. These points are then used to deform a generic model into an individual model using RBF network. The texture is generated from the same image and texture coordinates are interpolated. The original motion vectors are then transferred to the individual model in real time. This process can be done with minimal or no manual adjustments and has been shown to be effective for various applications. Overall, this approach offers a fast and efficient way to generate personalized face models and animations.
411448	41144819	A video based personalized face model generation approach for network 3d games	Our team has created a speedy system for producing customized 3D face models that can be utilized in online 3D games. This system uses a single video camera to capture a frontal image of the player's face, eliminating the need for calibration and extensive manual adjustments. The 3D face model in the game is composed of a 3D geometry mesh and a 2D texture image. By automatically detecting the player's facial features from the frontal image, a personalized geometry mesh is created by altering the original mesh. The corresponding texture image is also generated from the same image. To minimize storage space and network bandwidth, only the feature and texture data for each player are transmitted to the game server and other clients. As a result, players can see their own faces in multiplayer games.
411449	41144937	Relax, but don't be too lazy	This paper discusses different types of algorithms for expanding the product of two formal power series, f and g. The two main types of algorithms are zealous, which first expands f and g and then truncates at a certain order, and lazy, which gradually computes the coefficients of f, g, and h, only using the necessary computations at each stage. The paper introduces a new type of algorithm, called relaxed, which balances the advantages and disadvantages of zealous and lazy algorithms. The paper also surveys various algorithms for manipulating formal power series and provides theoretical complexity bounds and benchmarks for these algorithms. It is intended for both those interested in the latest algorithms and those looking to implement a power series library.
411449	41144941	Computing with D-algebraic power series	This paper discusses algorithms for computing with D-algebraic power series, which are defined by algebraic differential equations and initial conditions. The focus is not on efficiently calculating the coefficients of these series, but rather on being able to determine if expressions involving them are equal to zero. The algorithms cover both univariate and multivariate series and include operations such as composition, implicit power series, and monomial transformations. The goal is to provide a comprehensive approach to working with D-algebraic power series.
411450	41145026	Modelling and multi-objective optimization of quality attributes in variability-rich software	Variability-rich software, like software product lines, offers optional and alternative features to meet the diverse needs of users. However, designers face the challenge of understanding how selecting these features affects the quality of the resulting software variant. To address this, attributed feature models have been proposed, but current variability modelling languages and tools do not fully support them. This paper introduces ClaferMoo, a language and tool that addresses these limitations. ClaferMoo uses type inheritance to organize feature attributes and allows for multiple optimization goals. The tool was evaluated on various attributed feature models, demonstrating its ability to handle small-scale models quickly.
411450	41145036	Efficient synthesis of feature models.	This paper discusses the problem of automatically creating feature models from propositional constraints in the context of variability modeling. The goal is to address the challenge of creating feature models from legacy code, which can be a time-consuming and difficult task. The paper presents three scenarios in which the algorithms proposed can be applied. The objective is to show that the decision version of the problem is NP-hard and to present two efficient algorithms for synthesis of feature models from CNF and DNF formulas. The results of an experimental evaluation show significant performance improvements compared to other approaches. The paper concludes that these algorithms have potential for use in creating reverse engineering and model management tools for variability models in real systems.
411451	4114510	Predictive blacklisting as an implicit recommendation system	The use of blacklists is a popular defense mechanism against malicious traffic on the Internet. These lists contain known attack sources and aim to predict and block future attacks. Current blacklisting techniques focus on highly active attack sources and collaborative efforts. This paper presents a new approach, called "predictive blacklisting," which uses shared attack logs as an implicit recommendation system. The authors compare existing methods to an upper bound for prediction and find room for improvement. They then propose a multi-level collaborative filtering model, inspired by the NetFlix competition, that considers various factors such as attacker-victim history and interactions. This model is evaluated on one month of logs from Dshield.org and shows significant improvement in prediction rate and resistance to poisoning attacks compared to other methods. 
411451	41145156	Optimal Filtering for DDoS Attacks	DDoS attacks are a major issue on the Internet, where a large number of compromised hosts overwhelm a victim's resources with unwanted traffic, preventing it from serving legitimate clients. One proposed solution is filtering, where routers block unwanted traffic. However, the success of this method depends on efficient allocation of filtering resources. This paper focuses on optimizing the allocation of filters on a single router, with a limited number available, to attack sources or domains. The problem is formulated as an optimization one and solved using dynamic programming. The optimal allocation is studied and a heuristic is also tested. The paper evaluates the solutions for various attack scenarios, including a two-tier problem with additional constraints.
411452	41145241	Discrete Gaussian Leftover Hash Lemma over Infinite Domains	The Leftover Hash Lemma (LHL) is a commonly used tool to show that certain distributions, particularly those related to modular subset-sums, are close to being uniform over a finite domain. However, its usefulness is limited in lattice-based cryptography for two reasons. First, the distributions of interest are often discrete Gaussians, not uniform. Second, these distributions are over an infinite domain, a lattice rather than a finite field. This work introduces a \"lattice-world\" version of LHL that applies to infinite domains, showing that certain generalized subset-sum distributions are statistically close to well-behaved discrete Gaussian distributions. This has applications in the construction of multilinear maps, where it is used to sample discrete Gaussians. The authors also suggest other potential applications for this new lemma.
411452	41145212	Toward basing fully homomorphic encryption on worst-case hardness	Gentry proposed a homomorphic public key encryption scheme using ideal lattices and based its security on two problems: an average-case decision problem and the sparse subset sum problem (SSSP). A key generation algorithm for Gentry's scheme was developed, generating ideal lattices according to a suitable distribution. A worst-case/average-case connection was established, linking Gentry's scheme to the quantum hardness of the shortest independent vector problem (SIVP) in the worst-case scenario. This connection is the first of its kind where the average-case lattice is an ideal lattice, which is crucial for the security of Gentry's scheme. The assumption that the SSSP is hard cannot be removed.
411453	41145340	Perfect Structure on the Edge of Chaos - Trapdoor Permutations from Indistinguishability Obfuscation.	This article discusses the construction of trapdoor permutations using indistinguishability obfuscation and one-way functions. This is the first time such a candidate has been proposed that does not rely on the hardness of factoring. The construction also shows that even complex primitives like trapdoor permutations can potentially be based on noisy structures similar to those used in indistinguishability obfuscation constructions. Additionally, this approach may provide resistance against quantum attacks and could potentially be based on problems outside the complexity class SZK. This eliminates the need for trapdoor permutations and injective one-way functions in many recent constructions based on indistinguishability obfuscation.
411453	41145319	Separating succinct non-interactive arguments from all falsifiable assumptions	A succinct argument system for NP has a communication complexity that is polylogarithmic in the instance and witness sizes. Kilian and Micali have shown that such arguments can be constructed with four rounds of interaction and made non-interactive in the random-oracle model. However, there is currently no construction of a succinct non-interactive argument (SNARG) in the standard model with a proof of security under a simple cryptographic assumption. This work presents a separation result, showing that black-box reductions cannot be used to prove the security of any SNARG construction based on a falsifiable cryptographic assumption. This applies to commonly used assumptions in cryptography, and an assumption is considered falsifiable if it can be modeled as an interactive game with an efficient challenger. The result also extends to designated verifier SNARGs and slightly succinct SNARGs.
411454	41145418	Continuous Amortization: A Non-Probabilistic Adaptive Analysis Technique	This paper discusses subdivision algorithms for isolating the roots of a univariate polynomial with real coefficients. These algorithms use algebraic techniques or numerical primitives, such as function evaluation. The paper introduces a form of continuous amortization for adaptive complexity analysis, which is a new challenge in computer science. The analysis is applied to an evaluation-based algorithm called EVAL, which is also a 1-dimensional analogue of other algorithms. The main result is an O(d3(log d + L)) bound on the subdivision-tree size of EVAL for isolating all real roots of a square-free integer polynomial. The proof uses several novel techniques, including an adaptive upper bound using integrals and two algebraic amortization techniques.
411454	41145410	Near optimal tree size bounds on a simple real root isolation algorithm	The problem of finding all real roots of a square-free integer polynomial within a given interval is a significant issue. The EVAL algorithm offers a practical and exact solution by using a recursive bisection method and a numerical predicate to determine the presence of roots. The size of the recursive tree used in this algorithm is O(d(L + r + log d)), where d is the degree of the polynomial, L is the absolute value of its coefficients, and r is the number of roots in the given interval. This bound is the most precise known, and has been proven to be optimal. The results of this algorithm are similar to those of other recent bounds and can also be used to solve the problem of ray-shooting.
411455	4114557	Coreflective Concurrent Semantics for Single-Pushout Graph Grammars	The paper discusses the challenge of applying Winskel's unfolding semantics to graph grammars, which has been attempted multiple times in the past but only with limited success. The focus is on single-pushout grammars, and the authors propose a complete extension of Winskel's approach that provides a concurrent semantics expressed as a coreflection between the categories of graph grammars and prime algebraic domains.
411455	41145566	Unfolding of Double-Pushout Graph Grammars is a Coreflection	The recent paper presents a way to define a concurrent semantics for DPO graph grammars by using Winskel's construction for Petri nets. The approach involves unfolding each graph grammar into an acyclic branching structure, which is a nondeterministic occurrence graph grammar that describes all possible computations of the original grammar. The paper aims to establish a closer correspondence with Winskel's result by proving that the unfolding construction can be seen as a coreflection between the categories of graph grammars and occurrence graph grammars. This result is only applicable to a specific type of graph grammars called semi-weighted graph grammars. However, the coreflection cannot be extended to the entire category of graph grammars and possible solutions to this issue are suggested. 
411456	41145652	Cooperability In Train Control Systems: Specification Of Scenarios Using Open Nets	Train control systems, such as the European Train Control Systems (ETCS), require different software components to work together effectively in order to achieve the desired system behavior. High-level Petri net techniques have been identified as a suitable formal specification technique for specifying the operational behavior of ETCS. However, traditional Petri nets lack the ability to interact with the environment, making them inadequate for modeling scenarios and achieving cooperability between system components. The concept of open nets, developed by the research group "Petri Net Technology", is a promising solution for this issue. In this paper, a simplified railway level crossing control system is studied using open nets, called scenario nets. Integration and composition techniques for open nets ensure the cooperability of system components, making them a valuable tool for train control systems, particularly in the area of ETCS.
411456	41145618	Control Synthesis For Discrete Event Systems: A Semantic Framework Based On Open Petri Nets	Open nets are Petri nets with interfaces that allow for composition and interpretation of concurrent behavior. The control synthesis problem, which involves creating a controller for a given plant based on an abstract description of its behavior, can be solved using open nets by modeling the plant as an open net with interfaces for sensors and actuators. The desired behavior is then specified as a set of processes for this net. The goal is to synthesize a controller net that, when combined with the plant net, results in the desired behavior. This approach allows for the synthesis of controllers with multiple components, and we explore the use of logical specifications to enable a compositional, component-wise synthesis.
411457	41145773	Blind sequential detection for Rayleigh fading channels using hybrid Monte Carlo-recursive identification algorithms	The detection of data transmitted over a Rayleigh fading channel, without prior knowledge of the channel, has been a topic of study for researchers. A new algorithm is proposed in this paper for joint detection and channel estimation, using a combination of Monte Carlo sampling and recursive identification methods. The channel is modeled as an autoregressive process, allowing for a dynamic state space representation of the communication system. The algorithm also takes into account fast fading and time diversity in the received signal for a more accurate channel modeling. Simulation results demonstrate the effectiveness of this approach.
411457	411457141	Hybrid Monte Carlo — Recursive identification algorithms for blind detection over a Rayleigh fading channel	 The proposed scheme is shown to outperform existing schemes in terms of bit error rate.The article discusses a new approach for detecting and estimating signals in a Rayleigh fading channel, where the channel is modeled as an autoregressive process and the communication system as a dynamic state space model. The method utilizes a hybrid filtering technique called hybrid Monte Carlo-recursive identification, which combines sampling-based filters with traditional recursive identification methods. This allows for more accurate estimation of both the transmitted symbol sequence and the channel fading. The proposed scheme is found to have superior performance compared to existing schemes, as evidenced by its lower bit error rate.
411458	41145819	Constructing Multiple Independent Spanning Trees On Recursive Circulant Graphs G(2(M), 2)	A recursive circulant graph G(N, d) is a graph with N vertices and a recursive structure, where two vertices are adjacent if they have a specific relation based on an integer k. These graphs are commonly used as interconnection networks for computing systems and have many attractive features. The design of multiple independent spanning trees (ISTs) on these graphs has various applications, such as fault-tolerant broadcasting and secure message distribution. Previous work has provided a scheme to construct k ISTs on G(cd(m), d) with d >= 3, but this cannot be applied to the case of d = 2. This paper addresses this issue by proposing new constructing rules for G(2(m), 2) and shows that they result in lower heights for ISTs compared to other known constructions. 
411458	41145855	Upper bounds on the queuenumber of k-ary n-cubes	A queue layout of a graph is a linear order of its vertices and a partition of its edges into queues, where no two edges in the same queue are nested. The minimum number of queues needed for this layout is called the queuenumber. Previous studies have shown that the boolean n-cube and ternary n-cube can be laid out using at most n-1 and 2n-2 queues, respectively. A recent study improved the upper bound for hypercubes to n-2. This paper focuses on a wider class of graphs called k-ary n-cubes, and shows that the queuenumber for ternary n-cubes can be improved. The paper presents three main results, including showing that the queuenumber for k-ary n-cubes is 3, and for certain values of k and n, it is 2 and 4.
411459	41145928	The lower and upper forcing geodetic numbers of block-cactus graphs	The geodetic number of a graph is the smallest number of vertices that must be included in a set for all vertices to be on a shortest path between two other vertices. A subset of this set is called a forcing subset if it is the only possible geodetic set containing it. The forcing geodetic number is the smallest size of a forcing subset, and the lower and upper forcing geodetic numbers are the smallest and largest forcing geodetic numbers among all minimum geodetic sets in the graph. The paper explores the lower and upper forcing geodetic numbers of block-cactus graphs.
411459	41145938	On the powers of graphs with bounded asteroidal number	In an undirected graph G with vertices V and edges E, the kth power Gk is a graph with the same vertex set as G, where two vertices are connected if their distance in G is at most k. A set of vertices A is called an asteroidal set if every vertex in A, when removed, leaves the remaining vertices in one connected component in G. The asteroidal number of G is the largest size of an asteroidal set in G. The class of graphs with asteroidal number at most s is denoted by A(s). The paper shows that if Gk belongs to A(s) for s greater than or equal to 2, then Gk+1 also belongs to A(s). This extends a previous result for AT-free graphs. Additionally, forbidden configurations for powers of graphs with bounded asteroidal number are considered, leading to the conclusion that every proper power of AT-free graphs is perfect.
411460	41146074	A feasible intrusion detector for recognizing IIS attacks based on neural networks	Log files are used by website administrators to locate problems after a network intrusion. However, due to the large amount of data in these files, it is difficult for administrators to determine their hidden meanings. One solution is to use neural networks, which can effectively identify abnormal data and lighten the administrator's workload. This paper discusses the use of a hybrid intrusion detection system for IIS, validated through the Internet scanner system. The system was trained using different data sets and experimental designs, with the best results achieved with a training set size of 1400 or 2000. It is important to regularly retrain the detector to maintain accuracy, with the frequency and size of retraining dependent on the downgrade percentage of the detection rate.
411460	41146058	Linguistic object-oriented web-usage mining	Web mining is a crucial research area in data mining due to the abundance of web services. Fuzzy and object concepts have gained popularity and are used for complex data description. This paper presents a new algorithm that combines fuzzy and object-oriented techniques for web mining. Each web page is considered as a class and each page visited by a client is considered an instance. The algorithm has two phases: the first phase identifies linguistic large itemsets for each page, and the second phase finds large sequences to represent relationships between pages. The proposed algorithm is illustrated with an example and its effectiveness is demonstrated through experimental results.
411461	41146122	Extended Temporal Logic Revisited	Model-checking tools are important in verifying the correctness of software systems. One key issue in their design is choosing the formal language for specifying properties. A good language should be able to handle all ω-regular properties and be familiar to designers who use finite-state machines. Early extensions of linear temporal logic with automata used nondeterministic Büchi automata, but they have limitations. In this study, an extension called ETL2a is proposed, which uses two-way alternating automata as temporal connectives. These automata have the ability to refer to the past and are exponentially more succinct than one-way automata. ETL2a is shown to be powerful, convenient, and efficient, with the same space complexity as other extensions with automata. The use of alternating automata in the industry and the development of symbolic procedures make ETL2a a promising choice for practical use.
411461	41146155	Model Checking Linear Properties of Prefix-Recognizable Systems	The article presents an automata-theoretic framework for reasoning about linear properties of infinite-state sequential systems. The framework is based on representing states of such systems as nodes in an infinite tree and simulating transitions between states using finite-state automata. The authors introduce path automata on trees, which can only read a single path of a tree and cannot split into copies. They demonstrate the effectiveness of this approach by solving various versions of the model-checking problem for LTL specifications and prefix-recognizable systems. The algorithm is shown to be optimal and can handle systems with regular labeling. 
411462	41146213	Almost all k-cop-win graphs contain a dominating set of cardinality k.	In the binomial random graph G(n,1/2), we study k-cop-win graphs and find that the majority of them contain a universal vertex. This result is extended to show that for any given k, almost all k-cop-win graphs have a dominating set of size k. As a result, the number of labelled k-cop-win graphs of order n approaches (1+o(1))(1-2^-k)^-knk2n2/2-(1/2-log2(1-2^-k))n.
411462	4114627	Elimination schemes and lattices.	Perfect vertex elimination schemes are important in characterizing different types of graphs, such as chordal and cop-win graphs. Partial elimination schemes also play a role in reducing a graph to a significant subgraph, such as k-cores and robber-win graphs. In particular, we are interested in partial elimination schemes where once a vertex is ready to be eliminated, it remains in that state regardless of the elimination of other vertices. We demonstrate that in such schemes, the sets of subsets of eliminated vertices form an upper locally distributed lattice when ordered by inclusion. We also explore cop-win orderings and the process of cleaning graphs, both of which can lead to upper locally distributed lattices. Finally, we pose the question of whether there is an elimination scheme that is associated with distributive lattices. 
411463	41146332	Analysis of algorithms on the cores of random graphs	The k-core of a graph is the largest subgraph where every vertex has a degree of at least k. This subgraph can be found by repeatedly deleting vertices with a degree less than k. The threshold for when the k-core appears in a random graph was originally determined by Pittel, Spencer, and the speaker using differential equations. Recently, other papers have provided alternative derivations using a pseudograph model introduced by Bollobás and Frieze. This model is useful for analyzing algorithms on the k-core of sparse random graphs. The model was used to rederive the threshold for the k-core and has also been used to analyze algorithms for off-line load balancing and to determine lower bounds on the longest cycle lengths in random graphs. These analyses use differential equations to approximate random variables during the algorithm and have shown to be accurate in determining the algorithms' performance.
411463	41146390	Avoidance of a giant component in half the edge set of a random graph	 The article discusses a procedure for selecting a subset of edges from a complete graph in order to avoid the formation of a giant component. The edges are chosen randomly and with replacement, and the goal is to choose a subset of size m that results in a graph with a maximum component size that is less than o(n). This process must take place online, meaning that decisions to accept or reject an edge are based on previously seen edges. The article describes an algorithm that can successfully achieve this goal with a high probability, and also determines a threshold for the existence of a subset of 2m random edges without a giant component. This threshold is found to be m = c*n, where c* is a value between .9792 and .9793. The article also presents new upper bounds for related processes.
411464	41146454	A Cooperative Intrusion Detection Model Based on Granular Computing and Agent Technologies	This paper examines different attack types, such as Probing, DoS Denial of Service, R2L Remote to Local, and U2R User to Root. The attacks are then categorized based on the number of hosts involved and the resource and destination addresses of network packages. Using a granular computing methodology and agent technologies, a cooperative intrusion detection model is proposed. An intrusion detection agent is also constructed, and experiments show that the proposed method is effective in detecting slow scanning attacks that traditional scanning detectors may miss. 
411464	41146451	Fuzzy Multi-Class Support Vector Machines for cooperative network intrusion detection	The article discusses the issue of noise data affecting the performance of intrusion detection in network systems. To address this issue, the authors propose a new method that combines fuzzy theory and Support Vector Machine (SVM) in a cooperative network intrusion detection system. This method involves preprocessing the data and using a unique Multi-Class SVM for each network protocol. The proposed approach also includes a multi-agent architecture with agents for different protocols and a statistic-based agent. The authors conducted simulation experiments using the KDD CUP 1999 data set and found that their method can significantly reduce training time, storage space requirements, and improve classification accuracy.
411465	41146594	Bichromatic Quadrangulations with Steiner Points.	The paper discusses quadrangulations of k-colored point sets, where k is greater than or equal to 2. A quadrangulation is a family of quadrilaterals with disjoint interiors that covers the entire point set and has edges connecting points of different colors. Not all k-colored point sets can be quadrangulated, but those that can are called quadrangulatable. The paper explores the question of how many Steiner points (points added to the interior of the convex hull) are needed to make a point set quadrangulatable. It is shown that for a bichromatic point set with equal numbers of red and blue points, at most (n-1)/3 + n/2 + 1 Steiner points are needed, and for monochromatic point sets, the convex hull can be partitioned into a set of star-shaped polygons. However, there are some 3-colored point sets that cannot be made quadrangulatable.
411465	41146557	The Number of Geometric Bistellar Neighbors of a Triangulation	The theory of secondary and fiber polytopes suggests that regular triangulations of configurations with n points in R d have a minimum of n-d-1 geometric bistellar neighbors. This has been proven to be true for all triangulations of n points in R 2 and for three-dimensional configurations with no collinear points. However, there are some configurations, such as those with a single interior point, where the number of geometric bistellar flips is less than the minimum. A technique called lifting can be used to create a triangulation of a simplicial convex 4-polytope with even fewer neighbors. Finally, a family of point configurations in R 3 is presented that has an unlimited number of flip deficiencies.
411466	41146610	Image annotation via learning the image-label interrelations	Image annotation is the process of assigning relevant labels to digital images using machines. This is beneficial for image search and sharing on social networks. While several methods have been proposed in the past decade, most are not accurate or fast enough for practical use. In this paper, a new image annotation method is proposed, which uses the relationship between images and labels to predict labels quickly. The method involves a model that learns this relationship through regression and incorporates a label-biased regularization for better results. The model can be solved quickly, and experiments on three datasets show that it performs as well as other methods while having a faster learning time.
411466	41146664	Kernel sparse representation with pixel-level and region-level local feature kernels for face recognition	 Face recognition has been a popular topic in pattern recognition for many years, but it remains a challenging task due to image distortions. A new approach called sparse representation based classification (SRC) has shown great success in image classification, but it struggles when there are limited training samples. To improve the performance, this paper proposes a novel kernel SRC framework that uses effective local image features and appropriate nonlinear kernels. The framework includes a kernel coordinate descent (KCD) algorithm for the LASSO problem and utilizes both pixel-level and region-level kernels for robust face recognition. Experiments on three public face databases demonstrate the effectiveness of this approach in handling various challenges such as illumination variations, occlusions, and registration errors.
411467	41146742	Fast image upsampling via the displacement field.	The paper introduces a new method for fast image upsampling, which ensures sharpness in both large-scale edges and small-scale structures. This is achieved through a two-scale framework, where a low-frequency image is recovered using a displacement field and a sharpness preserving interpolation technique. The model preserves the distances of pixels on edges, resulting in sharp edges in the final high-resolution image. Additionally, a sharpness preserving reconstruction algorithm is used to reconstruct local high-frequency structures. The method outperforms existing approaches in terms of quantitative and qualitative evaluations, as well as user perception. It is also noted for its speed, making it suitable for practical applications. 
411467	41146775	An integrated graph-based face segmentation approach from Kinect videos	IntSSG is a new approach for automatically segmenting faces from color-depth video. It uses a combination of skin color detection and online SIFT matching to initially label face and non-face pixels. These labels are then refined using adaptive depth thresholding. IntSSG then uses a semi-supervised graph framework to propagate these refined labels to other pixels and accurately segment faces in challenging scenarios such as pose changes and lighting variations. Experimental results demonstrate the effectiveness of IntSSG in accurately segmenting faces from video.
411468	41146837	Decorating surfaces with bidirectional texture functions.	The presented system allows for the decoration of arbitrary surfaces with bidirectional texture functions (BTF). The system generates BTFs in two steps: first, a BTF sample is synthesized over the target surface, and then the user can interactively paint BTF patches onto the surface to seamlessly integrate with the background patterns. The system utilizes a patch-based texture synthesis approach called quilting and includes a graphcut algorithm for BTF synthesis on surfaces. This algorithm works well with a wide variety of BTF samples, including those that present challenges for existing algorithms. Additionally, a graphcut texture painting algorithm is described for creating imperfections on surfaces from existing BTF samples. This system allows for the realistic decoration of surfaces with textures that have varying reflectance, detailed geometry, and imperfections. Examples are provided to demonstrate the effectiveness of the system.
411468	41146829	Parallel Strategies Of Occlusion Culling On Cluster Of Gpus	Parallel rendering, visibility culling, and level-of-detail are techniques used to improve rendering performance for large geometric data sets. While there have been extensive studies and systems developed to combine these techniques, parallel occlusion culling has received less attention. This is due to the difficulty in parallelizing existing occlusion culling algorithms or their lack of scalability when parallelized. A novel occlusion culling algorithm using GPU occlusion queries and a visibility prediction technique is introduced. Different strategies for parallelizing the algorithm on GPU clusters are proposed, including data parallelism and functionality parallelism. Solutions to issues such as data dependency and load balancing are also presented. Experimental results demonstrate the effectiveness of these parallelism strategies.
411469	41146985	Automatic recognition of touch gestures in the corpus of social touch.	This paper discusses the importance of automatic detection and recognition of touch in human-robot interactions. The authors collected data for the Corpus of Social Touch (CoST), which includes 7805 captures of 14 different touch gestures in three variations (gentle, normal, and rough) using a pressure sensor grid. They found that gentle gestures are more difficult to classify than normal and rough gestures, and different classifiers and interpersonal differences also affect recognition accuracy. The paper concludes with suggestions for future research in this area to improve the touch modality in human-robot interactions.
411469	41146958	The multiLis corpus - dealing with individual differences in nonverbal listening behavior	Computational models that aim to predict when a virtual human should provide a backchannel are often based on studying face-to-face conversations between humans. However, using a corpus for this can be problematic as people may behave differently in various contexts. To better understand this variability, a study was conducted where three listeners interacted with one speaker, with the speaker only seeing one of the listeners. This allowed for the collection of data on cases where different individuals showed similar behaviors and cases where they behaved differently. This data will be used to build a more comprehensive model that takes into account individual differences in backchannel behavior for virtual humans.
411470	4114702	Multi-target ray searching problems.	This article discusses the problem of searching for multiple targets in a set of concurrent rays, with the exception of a common point. The goal is to design efficient search strategies for finding a specified number of targets. This problem is a generalization of the well-known ray search problem, and is applicable in various scenarios such as interleaved heuristic algorithms. The efficiency of the search strategies is evaluated using two different measures, and the article presents optimal strategies for both measures. Interestingly, the results show that the difficulty of finding multiple targets is comparable to finding a single target in a slightly smaller number of rays. 
411470	41147029	Robot Localization without Depth Perception	In this study, the problem of placing reflectors in a 2-D environment to help a robot determine its location is explored. The robot has a basic laser that can swivel and detect the angles of visible reflectors, but no distance information. A polygonal map is provided to the robot, and it is shown that there is always a way to place the reflectors so that the robot can localize itself from any point in the environment. This placement can be computed in polynomial time and is an improvement over previous techniques. The problem is also found to be equivalent to an art-gallery problem with a constant factor.
411471	41147113	Visibility Constrained Surface Evolution	The paper discusses the problem of feature-based surface reconstruction and introduces a method that takes into account visibility constraints, such as points, curves, and silhouettes, in the surface fitting process. These constraints not only improve the initial surface estimate and speed up convergence, but also aid in determining surface topology. The method uses a variational approach within the level set framework to evolve the surface while satisfying the visibility constraints. The theory is applied to various geometric primitives, including points, curves, and visual hulls, and is tested on real image sequences, showing promising results.
411471	41147148	Surface Reconstruction using Learned Shape Models	The problem of reconstructing 3D surfaces from images is difficult to automate, but humans can do it easily. A new framework has been introduced, combining level set surface reconstruction and shape models, to efficiently and accurately reconstruct surfaces of a specific object category. The shape model includes surface cues such as points, curves, and silhouettes, and incorporates ideas from Active Shape Models to consistently model geometry and appearance in a multi-view context. The complete surface is obtained by evolving a level set driven by a PDE, with an a priori 3D surface model used to regulate the solution in areas with sparse surface features. The effectiveness of this method is demonstrated through experiments on real face images.
411472	41147250	Set covering with ordered replacement: additive and multiplicative gaps	The article discusses set covering problems in which the set system follows a specific replacement property with respect to a given partial order. This means that if a set is in the system, a set obtained by replacing an element with a smaller one will also be in the system. Many variants of Bin Packing problems fall into this category. The article provides a detailed analysis of the integrality gap and approximability of set covering with replacement, including a polylogarithmic upper bound on the additive integrality gap. Additionally, the article lists several covering problems that fit this framework and have a polylogarithmic additive gap. 
411472	41147225	Max-sum diversity via convex programming.	Diversity maximization is a crucial concept in various fields such as information retrieval, computational geometry, and operations research. It involves selecting a feasible subset from a ground set, subject to constraints, in order to maximize a diversity measure function. One popular diversity measure is the sum-dispersion function, which calculates the sum of pairwise distances in the subset. Recently, there has been a focus on designing constant-factor approximation algorithms for diversification problems with this function under a matroid constraint. This paper presents a PTAS (polynomial-time approximation scheme) for the max-sum diversification problem under a matroid constraint for distances of negative type, which includes commonly used similarity metrics in web and image search. The algorithm uses techniques from geometric algorithms and convex optimization to compute a fractional solution and then applies a rounding approach to achieve a PTAS. This technique can also be applied to other variants of the max-sum dispersion function, leading to improved constant-factor approximation algorithms. 
411473	411473173	Ontology-Based Data Sharing in P2P Databases	The article discusses the use of schema mappings in peer-to-peer systems, where peers share structured data and express queries on their local schema. The existence of ontologies is assumed to describe the domain of interest. One major issue is the difficulty in determining the semantic relevance of interests and answers between peers with different schemas. To address this, the authors propose a semantic similarity measure that can evaluate the semantic relativeness between peer schemas and queries. It is first introduced for a shared ontology among peers and then extended to support comparison across multiple ontologies. The measure uses recall and precision from Information Retrieval to identify relevant peers and evaluate the quality of answers based on semantic annotations, mappings, and queries. 
411473	41147318	Optimizing XML queries: Bitmapped materialized views vs. indexes	The paper addresses the issue of optimizing XML queries using materialized views within the inverted lists evaluation model. The proposed approach involves materializing sublists of inverted lists, stored as compressed bitmaps, to reduce space and processing costs. This differs from traditional methods which rewrite queries using materialized views. Experimental results show that this approach is the most efficient and stable, with significant performance savings and negligible optimization time. A comparison with other approaches also shows its superiority. 
411474	41147412	Two tricks to triangulate chordal probe graphs in polynomial time	A chordal probe graph is a type of graph where the vertices can be divided into two sets, one being a stable set and the other being probes. By adding edges between non-probe vertices, the graph can be extended to a chordal graph. These graphs were initially studied as a generalization of interval probe graphs used in DNA mapping. However, they also have applications in computational biology, specifically in constructing phylogenies for genetic mutations. This study provides characterizations for chordal probe graphs in cases with a fixed partition and without any partition. Recognition algorithms for these graphs have been developed with polynomial time complexity.
411474	4114745	Chordal Probe Graphs	This paper introduces a new type of graph called chordal probe graphs, which are a combination of interval probe graphs and chordal graphs. A graph is considered chordal probe if its vertices can be divided into two sets: probes and non-probes, where the non-probes form a stable set. These graphs can be extended to chordal graphs by adding edges between non-probes. It is proven that chordal probe graphs do not contain odd-length chordless cycles or their complements. The paper also presents efficient algorithms for recognizing a subfamily of chordal probe graphs known as weakly chordal graphs, both with and without a predetermined partition of probes and non-probes.
411475	41147540	Incomplete Directed Perfect Phylogeny	The perfect phylogeny model is an important tool for studying evolution. A variant of this model involves a species-character matrix where some species have unknown character states. The goal is to determine if these missing states can be filled in to create a perfect phylogeny. This problem arises in phylogenetic studies and in studies using DNA repeat elements. Current solutions have a time complexity of O(n2m). A new graph theoretic approach has been developed which has a near-optimal time complexity of $\tilde{O}(nm)$. Another problem addressed is finding a general solution tree, which can be used to obtain any other solution by node splitting. An algorithm has been developed for constructing this tree or determining if none exists.
411475	41147580	On the Generality of Phylogenies from Incomplete Directed Characters	The article discusses the Incomplete Directed Perfect Phylogeny (IDP) problem, which arises in computational biology when trying to reconstruct the evolutionary relationships between a group of species. In IDP, some characters are unknown and the goal is to complete them in a way that is consistent with a perfect phylogenetic tree. This problem can occur in both traditional and modern phylogenetic studies. The article presents a polynomial algorithm to find a general solution for an IDP instance, or determine if none exists. This solution is important as it is consistent with the data and can be used to obtain any other consistent solution through node splitting. 
411476	41147651	Predicting the performance of component-based software architectures with different usage profiles	Performance predictions are important during the design phase of software architecture in order to improve its overall quality. To make accurate predictions, specifications of the performance properties of individual components within the architecture are necessary. However, existing prediction approaches often overlook the impact of component configuration and data on response times. This paper proposes extensions to the Palladio Component Model, a performance specification language for components, to account for these factors. The extended model allows for predicting response times for different architectural options. A case study on a web portal architecture demonstrates the effectiveness of this approach in aiding design decisions.
411476	41147628	Parametric Performance Contracts: Non-Markovian Loop Modelling and an Experimental Evaluation	Software systems often experience performance problems, even with advancements in hardware. To address this issue in component-based architectures, a method has been proposed to predict performance during early development by combining performance specifications of pre-made components. However, many existing methods overlook important factors that affect component performance. This paper presents a new method that considers external services and varying usage patterns when calculating component service performance. It uses stochastic regular expressions and probability mass functions to model the control flow and time consumption of internal and external services. An experimental evaluation on a webserver shows that this approach can accurately predict service response time with less than 2 percent deviation from actual measurements. 
411477	411477138	Beyond the Touchscreen: An Exploration of Extending Interactions on Commodity Smartphones.	Smartphones have a variety of sensors that are not fully utilized in terms of input methods. The BeyondTouch project aims to explore the potential of using built-in sensors to enhance the input experience on smartphones. By implementing a variety of machine learning and rule-based methods, users can now interact with their phones through additional inputs such as tapping on the case or surface adjacent to the device. This article is an extended version of a previous work, now including rule-based methods which do not require training data and work well with different users and devices. The effectiveness and usability of these interaction techniques are demonstrated, along with their practicality for various application scenarios. 
411477	411477130	Supporting parents for in-home capture of problem behaviors of children with developmental disabilities	Ubiquitous computing has potential in healthcare, particularly in the home setting. Selective archiving technology, which captures data before and after an event, can be used to support behavioral health research and practice. Parents of children with autism and related disabilities can use this technology to record video clips of problem behavior, which can then be viewed by behavior analysts for functional assessment. This is a cost-effective alternative to direct observation, which is labor-intensive and can influence behavior. A study was conducted using a research tool called CRAFT in eight households of children with developmental disabilities, which showed promise but also highlighted challenges in implementing this technology. Careful design is necessary to make ubicomp systems user-friendly for non-technical users and to advance research in this field.
411478	41147815	A PSO approach for robust aircraft routing	In order to minimize the effects of unexpected disruptions, airlines are implementing strategies to create more resilient schedules. In this study, a new simulation-optimization method using a particle swarm optimization algorithm is proposed for solving the problem of ensuring robust aircraft routing and flight retiming. The approach involves adding buffer times between flight departures to improve the reliability of both aircraft and passenger connections. The process utilizes a combination of discrete-event simulation and a Particle Swarm Optimization routine to find optimal solutions. Results from experiments using real data demonstrate the effectiveness of this approach in improving schedule robustness.
411478	41147818	A benders decomposition approach for an integrated airline schedule design and fleet assignment problem with flight retiming, schedule balance, and demand recapture.	Effective flight schedules are crucial for an airline's profitability. By offering flights at desired times in competitive markets and matching demand with suitable aircraft, an airline can significantly impact its profits. This can be achieved by considering optional flight legs and optimally selecting among them, as well as examining itinerary-based demands and multiple fare-classes. Allowing flexibility in departure times can increase connection opportunities and save fleet assignment costs. Balancing flight schedules throughout the day and considering recapture considerations can also help capture an adequate market share. A model is proposed that integrates schedule design and fleet assignment processes, taking into account flexible flight times, schedule balance, recapture issues, optional legs, itinerary-based demands, and multiple fare-classes. Valid inequalities and solution approaches are used to tighten the model and computational results demonstrate the effectiveness of the proposed procedures.
411479	41147928	Group-Oriented Undeniable Signature Schemes without the Assistance of a Mutually Trusted Party	In a (t, n) undeniable signature scheme, a group of n members shares a single public key and at least t members work together to sign a message. All signers must cooperate to prove the signature's validity to an outsider. If there is a dispute, a judge can intervene. This scheme has four main properties: the signature is generated by at least t members, verification is simplified with one public key, all signers must agree for the signature to be valid, and signers are responsible for the messages. Two proposed schemes have a threshold value of either n or 1, meaning all or just one member must sign. No mutually trusted party is needed, and the group public key is determined by all members.
411479	41147923	Secure universal designated verifier identity-based signcryption	In 2003, Steinfeld et al. introduced the concept of Universal Designated Verifier Signature (UDVS), which allows a signature holder to convince a designated verifier that they possess a signer's signature without being able to transfer this conviction to others. This protects the receiver's privacy by allowing them to prove possession of a signature without disclosing it to anyone else. In traditional UDVS schemes, a secure channel is needed between the signer and signature holder to transfer the signature. This paper presents a new approach, Universal Designated Verifier Signcryption, which combines UDVS and signcryption to eliminate the need for a secure channel. A concrete construction and formal security proofs are provided for this scheme.
411480	4114802	Data Lakes, Clouds and Commons: A Review of Platforms for Analyzing and Sharing Genomic Data.	Data commons are a type of data sharing platform that combine cloud computing infrastructure, commonly used software services, tools, and applications to create resources for managing, analyzing, harmonizing, and sharing large-scale biomedical data. These platforms have been particularly useful for analyzing genomics datasets. Data ecosystems, which involve connecting multiple data commons, have also been used for this purpose. However, the curation and analysis process in data commons can be time-consuming. As an alternative, data lakes provide access to data without requiring immediate curation and analysis. This article reviews different software platforms for managing, analyzing, and sharing genomic data, with a focus on data commons, but also discussing data ecosystems and data lakes.
411480	41148066	The Open Cloud Testbed: Supporting Open Source Cloud Computing Systems Based on Large Scale High Performance, Dynamic Network Services.	A variety of cloud platforms and services have been created for data intensive computing, such as Hadoop, Sector, Cloud Store, HBase, and Thrift. To assess their performance, compatibility, and potential for new services, the Open Cloud Testbed (OCT) was created. With 120 nodes spread across four data centers connected by a high-speed 10Gb/s network, the OCT is a large-scale, wide area testbed. Unlike other cloud testbeds, which rely on commodity internet services, the OCT is based on dedicated lightpaths. This testbed is capable of handling large data streams that would be difficult for other distributed systems. Various utilities have also been developed to support the development of cloud computing systems and services. In this paper, the OCT's concepts, architecture, infrastructure, benchmarks, interoperability studies, and results are discussed. 
411481	411481163	An architecture for highly available wide-area service composition	Service composition allows for the quick addition of new application functionalities in next generation networks. This is achieved by "composing" the services of other providers through an overlay network of service clusters. The architecture includes algorithms for detecting and recovering from failures in client sessions, with a focus on quick recovery. To evaluate this architecture, the researchers developed an emulation platform that allows for a realistic and controlled design study. The experiments showed minimal control overhead and resource usage, with failure recovery within 1 second using alternate service replicas. Trace data also demonstrated fast failure detection within 1.8 seconds, a significant improvement over existing mechanisms. 
411481	411481161	An active service framework and its application to real-time multimedia transcoding	The idea of "active networks" proposes placing user-defined computation within the network to enable new applications and protocols. However, this presents difficult research problems, and although some progress has been made, there are still many challenges to overcome. "Active services" offer an alternative approach, where user-defined computation is restricted to the application layer and does not require changes to the Internet architecture. The proposed "Media Gateway" active service addresses six key problems and has been successfully implemented and tested on the UC Berkeley campus. This prototype shows potential for a flexible and programmable platform for network computation that can be incrementally deployed in the current Internet.
411482	41148210	General Model for Infrastructure Multi-channel Wireless LANs	This paper presents an integrated model for managing requests and data transmission in multi-channel wireless local area networks. The model considers noisy channel conditions and calculates performance parameters for both single and multi-channel networks. It is a general model that can be applied to various wireless networks including IEEE802.11x, IEEE802.16, CDMA, and Hiperlan\2. This model provides a comprehensive approach for optimizing network performance in the presence of noise.
411482	41148252	Quality of service support and backoff strategies in wireless networks with error control protocol	The paper presents a Quality of Service model for wireless networks, along with an error control protocol for transmission states. Three backoff strategies are utilized and analytical models are developed. The study measures performance metrics such as throughput, acceptance probability, delay, and energy. The error control protocol also measures the average number of retransmissions and efficiency. These models can be applied to various wireless network standards, including Hiperlan\2 and WiMAX. Overall, the paper offers a comprehensive approach for improving the performance of wireless networks by considering various metrics and strategies.
411483	411483172	Personalizing product rankings using collaborative filtering on opinion-derived topic profiles	Product review sites like Trip Advisor, Yelp, and Amazon use a non-personalized ranking system to evaluate products. This makes it difficult to personalize recommendations based on sparse review data. Collaborative Filtering, which uses review texts to identify user profiles, can improve the accuracy of personalized recommendations by carefully using available data and separating users into classes. This method has shown significant improvements in metrics such as MAE, RMSE, and Kendall tau compared to previous methods. However, not all users benefit equally from personalization, and a new approach has been proposed to switch between personalized and non-personalized methods based on the user's opinion profile. The user's level of "opinionatedness" can be a good indicator of whether personalization will be effective or not.
411483	411483102	Reporting incentives and biases in online review forums	Online reviews have become a popular way for consumers to judge the quality of products and services. However, research has shown that the lack of incentives for reporting can lead to biased reviews that do not accurately reflect the true quality. In this paper, the authors investigate the factors that influence users when reporting feedback on the popular review site, Tripadvisor. They consider numerical ratings, comments, and time patterns in reviews and find that longer discussions about a feature lead to more agreement in ratings. They also find that users are more likely to provide feedback when they perceive a higher risk in a transaction and that their ratings may be influenced by prior expectations. The authors suggest that using the median instead of the mean to summarize ratings may provide a more accurate representation of the true quality.
411484	4114848	Experiments on user experiences with recommender interfaces	Recommender systems are popular tools used in e-commerce to personalize the browsing and purchasing experience for users. However, most research has focused on improving the algorithm's performance rather than analyzing user experience with the interface. This article discusses two studies comparing two recommender interfaces: the organization-based interface and the standard ranked list. The first study used an eye tracker and found that the organization interface attracted more user attention and led to more product choices. The second, larger-scale study showed that the organization interface was perceived as more useful and easier to use by users from different cultures. These findings suggest that the design of the recommender interface can impact both user attention and their attitudes towards the system.
411484	41148445	Users' eye gaze pattern in organization-based recommender interfaces	This paper discusses the eye-movement patterns and preferred layouts of users on three different recommender interfaces. The first is a standard list layout commonly used in most recommender systems, while the other two are organization layouts with categorized and annotated recommendations. The results show that users are more likely to pay attention to and choose recommendations from the organization layouts, particularly the quadrant layout. The study also suggests design guidelines and practical implications for future work based on these findings and previous research.
411485	41148524	On Dillonʼs class H of bent functions, Niho bent functions and o-polynomials	Dillon's thesis introduced a class of bent Boolean functions called family H, which is a construction of bent functions in bivariate form. However, these functions were found to already belong to the well-known Maiorana-McFarland class. A larger class, denoted by H', was then discovered by extending H. It was also observed that the bent functions constructed using Niho power functions are the univariate form of those in class H. Further, the bent functions whose restrictions to certain vector spaces are linear were characterized. The question of whether the duals of Niho bent functions are affinely equivalent to them was answered by calculating the dual of one of these functions. It was also shown that this Niho function belongs to the Maiorana-McFarland class, raising the question of whether H (or H') is a subclass of this class. The condition for a function in bivariate form to belong to class H was found to be equivalent to a polynomial being an o-polynomial. By using existing classes of nonlinear o-polynomials, a significant number of new bent functions in H were discovered, potentially not equivalent to known bent functions.
411485	41148532	Bent vectorial functions and linear codes from o-polynomials	This paper explores the connections between symmetric cryptography, coding theory, and finite projective geometry. It discusses the use of bent vectorial functions in maximally nonlinear Boolean functions and their involvement in achieving optimal resistance to attacks in symmetric cryptosystems. The paper also presents new connections between bent vectorial functions and hyperovals in the projective plane, as well as a link between hyperovals and q-ary simplex codes. It introduces a general construction for linear codes and shows that the hyperovals in finite projective geometry can be used to create minimal codes and multiples of 2r-ary simplex linear codes over an extension field F2r of F2. The diagram provided in the paper illustrates the main topics and interconnections discussed.
411486	41148655	Data transmission and base-station placement for optimizing the lifetime of wireless sensor networks	This paper discusses the optimization problem of positioning base-stations in wireless sensor networks to enable energy-efficient data transmission. The focus is on a scenario where sensors transmit data directly to the base-station or through one other node, resulting in low duty-cycling and end-to-end delay. The objective is to maximize network lifetime considering the limited battery capacity of sensors. The paper presents efficient algorithms for computing transmission schemes in a distributed manner with a constant number of messages per sensor. It also shows that the problem becomes NP-Hard when sensors can transmit data through more than 2 hops. Furthermore, the paper proposes algorithms for simultaneously locating the base-station and finding a transmission scheme. Simulations show the effectiveness of these algorithms compared to linear-programming based approaches for more general settings.
411486	41148628	Collecting data in ad-hoc networks with reduced uncertainty	This article discusses the problem of data gathering in wireless ad-hoc networks. A data mule is used to collect data from sensors with important information in its surroundings. The mule's objective is to collect the maximum amount of data while minimizing its travel distance. The problem is solved using a generalized version of the Prize Collecting Steiner Tree Problem and an algorithm with a 6-approximation ratio. Simulation results demonstrate the effectiveness of this approach for different network topologies. 
411487	41148719	Correcting noisy exponentiation black-boxes modulo a prime.	This article discusses the problem of correcting a noisy exponentiation black-box modulo a prime number. The black-box takes in an integer and outputs another integer with a small error, representing the result of raising a fixed base to the input power. Howgrave-Graham, Nguyen, and Shparlinski developed an algorithm for this problem that works within a certain range of error and a multiplicative order of the base modulo the prime. This study expands upon their work by extending the range of error and order for which the algorithm can successfully recover the original result with high probability. This is achieved by utilizing a bound on character sums in finite fields and combinatorial techniques.
411487	411487113	On Polynomial Approximation of the Discrete Logarithm and the Diffie - Hellman Mapping	The article discusses obtaining lower bounds for the degrees of polynomials and algebraic functions that coincide with discrete logarithms modulo a prime number p. These lower bounds are exponential in terms of the logarithm of p and require a minimum of p^(1/2+e) points. Improved lower bounds are also found for the degree and sensitivity of Boolean functions that determine whether a number x is a quadratic residue. Similar results are found for the Diffie—Hellman mapping g^x → g^(x2) using a primitive root g of a finite field of q elements. These findings can be used to determine lower bounds for the complexity of computing discrete logarithms and breaking the Diffie—Hellman cryptosystem, based on character sums and polynomial equations.
411488	411488129	Visualization techniques for mining large databases: a comparison	Visual data mining techniques have become highly valuable for exploratory data analysis and have great potential for use in mining large databases. This article introduces a new visualization-based approach to mining large databases, which involves representing as many data items as possible on the screen at once by mapping data values to pixels and arranging them appropriately. The main focus of the article is to evaluate and compare these visual data mining techniques with other commonly used techniques for multidimensional data, such as parallel coordinate and stick figure visualizations. The evaluation considers the perception of data properties as the primary factor, followed by CPU time and secondary storage accesses. A testing environment has also been developed to generate test data sets with predefined characteristics for comparing the perceptual abilities of different visual data mining techniques.
411488	41148812	Incremental Visual Text Analytics Of News Story Development	The amount of news articles produced online daily can make it difficult for readers to understand the context of current events. To address this issue, a visual analytics system has been developed that combines interactive visualization and text mining techniques. This system allows for the exploration of news topics as they change over time, including how they are connected and how they may merge or split. By using text clustering, stories can be automatically extracted from news streams and displayed in a way that represents their temporal characteristics and minimizes clutter. Through interaction, readers can filter and explore the stories in detail. Case studies using real news data demonstrate the effectiveness of this system.
411489	4114891	Binary optimization using hybrid particle swarm optimization and gravitational search algorithm	The PSOGSA is a hybrid optimization algorithm that combines the strengths of PSO and GSA, resulting in improved exploration and exploitation. While the original version is suitable for continuous search spaces, this paper introduces a binary version called BPSOGSA to handle problems with binary parameters. Adaptive values are also integrated to balance exploration and exploitation. The efficiency of BPSOGSA is evaluated on 22 benchmark functions, divided into three groups. The results show that BPSOGSA outperforms other binary algorithms such as BGSA, BPSO, and genetic algorithm in avoiding local minima and achieving faster convergence.
411489	41148951	Training feedforward neural networks using hybrid particle swarm optimization and gravitational search algorithm.	The Gravitational Search Algorithm (GSA) is an optimization method that uses the law of gravity and mass interactions to search for the best solution. While it has been found to be effective in finding the global optimum, it can be slow in the later iterations. To solve this issue, a hybrid of GSA and Particle Swarm Optimization (PSO) is proposed. This hybrid, called PSOGSA, is tested as a training method for Feedforward Neural Networks (FNNs) and compared to a standard PSO-based learning algorithm. The results show that PSOGSA outperforms both PSO and GSA in terms of speed and avoiding local minima, and also leads to better accuracy for FNNs.
411490	41149015	High-Power-Factor Single-Phase Diode Rectifier Driven by Repetitively Controlled IPM Motor	This paper introduces a new method for power factor correction using an inverter-driven interior permanent magnet (IPM) motor. This system utilizes a three-phase pulsewidth modulation inverter and an IPM motor to achieve a high power factor in a single-phase diode rectifier. The proposed converter includes a small film capacitor, a voltage-source inverter, and an IPM motor. The inverter serves two functions: controlling the motor's speed and regulating the source-side current waveform. To achieve a unity-power-factor operation, a new control method is implemented, where the inverter output power is regulated by a proportional-integral and repetitive controller. Experimental results show that the maximum power factor achieved by this method is 98.7% under rated load conditions.
411490	4114903	Anti-windup robust controller considering motor dynamics for speed servo system	A robust servo system is crucial for improving motion control performance in various industries. Typically, a speed servo system uses a controller with an integrator, such as a PI controller, but issues like wind-up and unstable responses can arise when the output is saturated by current and voltage limiters. To address this, an anti-windup algorithm has been proposed that considers voltage saturation for speed servo systems, ensuring a stable current response. Additionally, a similar algorithm has been developed for position servo systems to regulate speed response, but it may still result in overshoot due to current saturation and the speed controller not maintaining performance. This paper presents a new anti-windup algorithm that takes into account motor dynamics and current saturation for speed servo systems using SPM synchronous motors. Experimental and simulation results demonstrate the effectiveness of this algorithm in achieving smooth and stable motor speed control.
411491	41149115	Estimation of Vector Fields in Unconstrained and Inequality Constrained Variational Problems for Segmentation and Registration	Vector fields are commonly used in computer vision, especially in non-rigid registration problems. This study introduces a method for estimating vector fields that describe the deformation between objects and the segmentation of these objects. The authors also investigate the use of inequality constraints in solving variational problems in computer vision, such as estimating deformation fields and tracking. Their approach utilizes the Kuhn-Tucker theorem from optimization theory to solve inequality constrained vector field estimation problems. This method differs from other joint segmentation and registration algorithms by using a set of coupled partial differential equations derived from the same energy terms for both registration and segmentation. The paper presents the theory and results of this approach.
411491	41149146	Efficient segmentation based on Eikonal and diffusion equations	Segmentation of regions of interest in an image is crucial for medical image analysis, especially in computer aided diagnosis. It allows for further quantitative analysis of anatomical structures. This can be achieved by solving partial differential equations (PDEs) for each known image region, which represents the weighted distance from a region with a known segmentation label. To do this, two separate PDEs are used - the Eikonal equation and a diffusion equation. The segmentation labels are determined through a competition between the solutions of these PDEs. This concept of information propagation is applied in both methods to segment both two-region and multi-region problems. The effectiveness and accuracy of these methods are demonstrated through experimental results on various medical images.
411492	4114926	A rational reconstruction of nonmonotonic truth maintenance systems	This paper presents a detailed description of the inferences made by nonmonotonic truth maintenance systems (TMSs) using two commonly used formalisms: logic programming with stable set semantics and autoepistemic logic. It also examines the use of dependency-directed backtracking in handling contradictions and proves that implementing a nonmonotonic TMS is a computationally complex task. This paper provides a comprehensive understanding of the workings of nonmonotonic TMSs and their role in reasoning with incomplete and inconsistent information.
411492	41149255	Incremental, Approximate Planning	This paper discusses the benefits of using a nonmonotonic logic in planning to quickly discover plausible plans and then refine them. This approach allows for candidate plans to depend on unproved assumptions, with the nonmonotonic logic determining which conditions are considered default. By only producing plausible plans, the strategy is more efficient. These plans can then be refined by justifying the assumptions they depend on. The approach has been successfully implemented and has shown good results in experiments. Overall, using nonmonotonic logic in planning can lead to more effective and efficient plan discovery and refinement.
411493	41149358	On illuminating line segments in the plane	The concept of illumination in a family of convex sets in the plane is explored, where a set of light sources is defined as illuminating if they can make every point on the boundary of each set visible. It is proven that for a family of n line segments, at least 2n/3 light sources are needed to illuminate the sets if n is greater than or equal to 11. Additionally, if all the sets in the family are parallel to the x or y-axes, then the minimum number of light sources needed is (n+1)/2.
411493	41149318	Separating convex sets in the plane	A line in a 2-dimensional space is said to separate a set of points from a collection of plane sets if the points are all on one side of the line and the plane sets are all on the other side. This means that the line divides the space into two parts, with all the points in one part and all the plane sets in the other. This concept is useful for organizing and categorizing sets and can be applied in various fields such as geometry and data analysis.
411494	41149412	A unified information-theoretic framework for viewpoint selection and mesh saliency	Viewpoint selection is a growing area in computer graphics that has various applications like scene exploration and image-based modeling. It involves finding the most optimal views of an object or scene with the least number of images. In this article, a unified framework for viewpoint selection and mesh saliency is presented, using mutual information as a tool to deal with various aspects like viewpoint stability, object exploration, and saliency. The framework can be applied to any set of viewpoints in a closed scene and has been tested through various experiments, showing its robustness and effectiveness. The incorporation of saliency as an importance factor further enhances the framework's capabilities.
411494	41149421	Information measures for terrain visualization.	Quantitative and qualitative geoscience studies often utilize digital elevation models (DEMs) and 3D surfaces to better understand natural and human-influenced topography. DEMs not only aid in quantitative analysis, but can also provide valuable visual information for identifying topographic features. However, choosing viewpoints and rendering styles can be challenging, especially when incorporating digital image texture. This paper proposes an information-theoretic framework for terrain visualization and view selection. By calculating information measures for each polygon in the terrain model, the visual representation of the terrain's shape is enhanced. Furthermore, a method for selecting representative views of the terrain is introduced. The effectiveness of these techniques is demonstrated using example datasets, and a publicly available framework for terrain analysis is provided.
411495	41149559	A New Adaptive Sampling Technique for Monte Carlo Global Illumination.	The paper discusses the use of Monte Carlo methods for solving the problem of global illumination in realistic image synthesis. It explains how adaptive sampling can be used to reduce noise in these algorithms and introduces the concept of entropy from information theory to measure pixel quality. The use of the nonextensive Tsallis entropy, which includes a parameter q to represent the degree of nonextensivity, is explored as a means of evaluating pixel quality and conducting adaptive sampling. The paper also describes a method for systematically determining the appropriate value of q using least-squares design. Implementation results demonstrate that this approach outperforms existing methods and may be the first attempt to systematically choose an appropriate entropic index for Tsallis entropy in engineering applications.
411495	41149586	A new way to re-using paths	The Monte Carlo method is considered the most accurate way to solve global illumination problems in realistic image synthesis. However, noise can be a major issue in Monte Carlo-based algorithms, such as ray tracing. One solution to reduce noise is reusing light transport paths, but this can also result in noise patches in the final image. In this paper, the authors propose a new approach to path reuse, which effectively reduces noise without creating patches. Experimental results demonstrate the effectiveness of this method. 
411496	41149618	Using internal sensors and embedded detectors for intrusion detection	The article discusses the use of internal sensors for intrusion detection in computer systems. It presents a classification of data collection mechanisms for intrusion detection, categorizing them as direct and indirect monitoring. Internal sensors are considered more advantageous in terms of reliability, completeness, timeliness, volume of data, efficiency, and resistance against attacks. The ESP architecture is introduced as a framework for building intrusion detection systems using internal sensors. A prototype implementation based on this architecture is described, along with the concept of embedded detectors for localized data reduction. Performance and detection testing of the implementation show promising results, demonstrating the potential of embedded detectors in detecting new attacks.
411496	4114967	Prototyping experiences with classical IP and ARP over signaled ATM connections	This paper discusses the development of a prototype for the classical internet protocol (IP) and address resolution protocol (ARP) over asynchronous transfer mode (ATM) model. It explains the key features of these protocols and their relevance to ATM. The paper also explores different scenarios for the interaction between IP and ATM. A prototype driver for an ATM network adapter is designed and implemented, which uses and manages signaled ATM connections in a transparent manner for user processes. The authors share their experiences and lessons learned from the design and implementation process, including unexpected problems and how they were addressed. The insights from this paper can be applied to other software prototyping projects, not just the one described.
411497	41149730	Information flow security in Boundary Ambients	Boundary Ambients is a variation of the Mobile Ambient calculus that allows for the modeling of multi-level security policies. Boundaries, which are specific ambients, act as resource access managers for confidential data and ensure its protection. By satisfying certain syntactic conditions, direct information leakage can be prevented. A new notion of non-interference is also introduced to capture indirect flows of information. Additionally, a Control Flow Analysis is developed to identify ambients that may be affected by high-level data at runtime, and it is shown that this analysis can be used to enforce non-interference and detect any potential information leakage at runtime. 
411497	41149740	Security boundaries in mobile ambients	The concept of a security boundary is proposed to represent multilevel security policies in mobile systems. This is based on Cardelli and Gordon's mobile ambients calculus. The potential for unauthorized access to sensitive data by hostile ambients is seen as information leakage, and can be detected through a control flow analysis that is an improvement on the Hansen-Jensen-Nielsons's CFA. This allows for the identification of direct information leakage, improving the overall security of mobile systems.
411498	41149898	Relating Theories of Actions and Reactive Control	This paper focuses on the formalization of reactive control using action theories. The concept of a reactive control program being correct in relation to a specific goal, set of initial states, and action theories is defined. The paper also presents conditions that ensure correctness and offers an automated method for developing control modules that are proven to be correct. Further, the approach is extended to incorporate action theories about both the agent/robot and external actions. 
411498	4114980	Approximate reasoning about actions in presence of sensing and incomplete information	In planning with incomplete information, the ability to sense actions is crucial. To address the frame problem for sensing actions, Scherl and Levesque proposed a solution that integrates the knowledge of possible worlds with the situation calculus. In this paper, the authors introduce a high-level language that allows for sensing actions, inspired by the language A. They also present two approximations of this language and their translation to logic programs. This approach differs from A, which only considers two possible states, by allowing for a more nuanced understanding of the world.
411499	41149928	Theoretical analysis of differential microphone array beamforming and an improved solution	Differential microphone arrays (DMAs) have gained popularity due to their frequency-invariant beampatterns, small size, and maximum directivity potential. Traditionally, DMAs are designed in a multistage way using time delays to achieve the desired beampattern. However, a recent study showed that DMAs can be designed using a linear system of equations based on the nulls of the desired beampattern. This paper focuses on beamforming with linear DMAs and makes four main contributions. Firstly, it presents theoretical analysis showing that traditional and null-constrained DMAs are equivalent. Secondly, it introduces a two-stage approach to robust DMA beamforming based on maximizing white noise gain. Thirdly, it addresses the issue of extra nulls in the beampattern at high frequencies in robust DMA beamforming. Lastly, it proposes a method to solve this problem while maintaining a frequency-invariant beampattern.
411499	41149920	Design of Directivity Patterns with a Unique Null of Maximum Multiplicity	Differential beamforming is a popular technique for creating frequency-invariant directivity patterns. This paper focuses on designing beampatterns with multiple nulls in the same direction, which is different from the usual approach of designing distinct nulls. The authors present a method for constraining multiple nulls to the same direction and designing the desired beampattern using both traditional and robust approaches. They also derive an explicit formula for the white noise gain (WNG) of the traditional approach, showing that the cardioid beampattern is optimal in terms of WNG. The authors also prove that the WNG improvement of the robust approach is not highly dependent on the null direction at low frequencies. To address the issue of frequency-dependent beampatterns in the robust approach, they propose a weighted-norm approach that balances robustness to white noise and frequency-invariant beampatterns. Simulations confirm the effectiveness of this approach.
411500	41150023	Noise reduction algorithms in a generalized transform domain	Noise reduction for speech applications involves using a filter or transform to reduce noise in speech signals without distorting the speech. The goal is to design an optimal filter that can effectively suppress noise while maintaining speech quality. This can be done in either the time or transform domain, with the latter being advantageous as it allows for better separation of speech and noise signals. While the Fourier and Karhunen-Loève transforms are commonly used, there has been no formal study to determine which is more effective. This paper proposes a generalized transform domain for noise reduction, which allows for easy comparison of different transforms and the design of optimal and suboptimal filters. 
411500	411500140	Optimal single-channel noise reduction filtering matrices from the pearson correlation coefficient perspective	This paper focuses on the problem of reducing noise in single-channel speech signals, using a rectangular filtering matrix to estimate the desired clean speech. The main challenge lies in determining the optimal filtering matrix, which is achieved through the use of the squared Pearson correlation coefficient (SPCC). Different optimal matrices can be obtained by either maximizing or minimizing the SPCC between different signals. For example, maximizing the SPCC between the enhanced signal and the filtered speech results in the reduced-rank Wiener and minimum distortion (MD) matrices, while minimizing the SPCC gives the minimum noise (MN) and another reduced-rank Wiener matrices. Simulation results are provided to demonstrate the effectiveness of these filtering matrices.
411501	41150128	Indexing Mobile Objects Using Duality Transforms	This article discusses techniques for efficiently indexing mobile objects in order to answer range queries about their future positions. This is a common problem in real-life applications, such as predicting traffic congestion or allocating bandwidth for areas with high concentrations of mobile phones. The authors propose dynamic solutions for both one-dimensional and two-dimensional cases, using external memory. Their approach involves transforming the problem into a dual space that is easier to index. They also compare the advantages and disadvantages of different indexing schemes proposed in literature for mobile objects. 
411501	41150157	Indexing Animated Objects Using Spatiotemporal Access Methods	The article presents a new method for indexing and querying animated objects in a spatiotemporal context, specifically focusing on animated movies. The approach treats the movie as a sequence of frames, each containing 2D space occupied by objects. Queries of interest include finding objects in a specific area between frames or finding the nearest objects to a given position between frames. The proposed approach reduces the problem to a partial-persistence problem, using a 2D access method that is partially persistent. This leads to faster query performance and uses storage proportional to the number of changes in the frame evolution. The article also discusses how this approach differs from traditional temporal indexing methods and provides an optimal solution for linearly moving objects. The approach can be applied to other spatiotemporal applications, not just animated movies.
411502	41150248	Storing and querying multiversion XML documents using durable node numbers	Managing multiple versions of XML documents is a significant issue for traditional applications like software configuration control and newer ones such as maintaining web document links. Research is being conducted to develop effective methods for storing, retrieving, and querying these documents. This paper proposes a new method for version management using Durable Node Numbers and timestamps for XML document elements. The paper discusses efficient storage and retrieval techniques as well as indexing and clustering strategies to support complex queries on document content and evolution.
411502	41150211	Copy-Based versus Edit-Based Version Management Schemes for Structured Documents	The paper discusses the problem of managing multiple versions of XML documents and semi-structured data. Traditional version control methods, like RCS, use edit scripts to track changes in the document. However, these methods have limited flexibility and cannot handle certain types of changes. To address this, the paper proposes a copy-based versioning scheme called UBCC, which is based on page usefulness. The paper also presents algorithms to improve the performance of the copy-based approach and compares it with the edit-based UBCC approach through various experiments. The results show that the copy-based approach is a simpler and more efficient way to manage multi-version documents.
411503	41150353	Up-front interaction design in agile development	This paper discusses the relationship between interaction design and agile development, specifically addressing the issue of whether interaction design should be done before or during software development. The study involved conducting interviews with interaction designers and software developers on agile teams and using a qualitative approach to analyze the results. The findings suggest that there are benefits to both pre-development interaction design and incorporating it throughout the development process. The paper concludes that a balance between these two approaches is ideal for successful integration of interaction design and agile development.
411503	41150375	How much up-front?: a grounded theory of agile architecture	The tension between software architecture and agility is often overlooked by those practicing or studying agile methods. Teams that do not prioritize upfront architecture design may face increased risk and failure, while those who spend too much time on it may delay delivering value to customers and struggle with adapting to change. A grounded theory of agile architecture, based on research with 44 participants, identifies six factors that impact a team's context and five strategies they use to determine the appropriate level of upfront design effort. This theory sheds light on how agile teams address the balance between architecture and agility.
411504	41150439	Interpolating vertical parallax for an autostereoscopic three-dimensional projector array	This article discusses a new technique for achieving vertical parallax for multiple users using different types of autostereoscopic projector setups, including front- and rear-projection and curved displays. This technique allows for both horizontal and vertical parallax, creating a more realistic 3-D experience for viewers. It uses a low-cost RGB-depth sensor to track multiple viewer head positions, updating the imagery sent to the array accordingly. Unlike previous methods, this technique does not assume a single vertical perspective for each projector, allowing for a hybrid parallax approach. The authors demonstrate this technique using a horizontal array of pico-projectors aimed at a vertical diffusion screen, resulting in a clear and seamless viewing experience for multiple users. 
411504	4115043	Achieving eye contact in a one-to-many 3D video teleconferencing system	The article presents a set of algorithms and a display system that can accurately render eye contact between a remote participant and a group of observers in a 3D teleconferencing system. The participant's face is scanned in 3D and transmitted in real-time to a 3D display with a wide field of view for multiple observers. A fast vertex shader is used to project the 3D scene onto a curved display surface, allowing for correct perspective. Two-way eye contact is achieved by capturing 2D video and displaying it on a screen in front of the participant, replicating their virtual self's viewpoint. The system also tracks the position of each observer's eyes to ensure correct vertical perspective. This technology aims to improve the experience of teleconferencing by replicating the effects of gaze, attention, and eye contact.
411505	41150520	A novel remote user authentication scheme through dynamic login identity	Password authentication is a widely used method for verifying a user's identity and preventing malicious activity. In 1981, the first password-based remote user authentication scheme was introduced by Lamport. However, the problem with static login-ID based schemes is that they can be easily compromised if the login-ID is shared or intercepted. To address this issue, a dynamic login-ID based scheme using smart cards was proposed. This scheme assigns a unique login-ID for each login session, preventing unauthorized access. The use of smart cards also restricts the distribution of login-IDs. This scheme is particularly useful for applications like digital libraries and uses secure methods like RSA and one-way hash functions for login request verification. It also allows for easy password changes without assistance from the remote system.
411505	41150543	A novel remote user authentication scheme using bilinear pairings	The paper proposes a new method for remote user authentication that utilizes bilinear pairings. This allows the remote system to verify login requests and only allow valid users to log in. The scheme also prevents multiple users from using the same login ID and allows registered users to change their password without assistance from the remote system. This approach offers greater security and flexibility for remote user authentication.
411506	41150647	Hybrid Scheduling of Dynamic Task Graphs with Selective Duplication for Multiprocessors under Memory and Time Constraints	This paper introduces a hybrid scheduling method for multiprocessor embedded systems that can handle dynamic task graphs with conditional tasks and unpredictable execution times. The methodology involves three phases: static mapping of task nodes to processors, selective duplication of critical nodes for possible rescheduling at runtime, and an online scheduling algorithm based on the current schedule. The proposed technique has shown to improve schedule length by up to 20% compared to previous methods, with low overhead and complexity similar to other online techniques. The paper also explores the impact of different parameters on performance, such as the number of processors and memory.
411506	4115067	Thermal analysis of multiprocessor SoC applications by simulation and verification	Computer chip overheating can cause performance and reliability issues, making it a major challenge to prevent in the face of increasing performance demands. Due to the rising cost of cooling, designers use architecture and application techniques to prevent overheating. One such technique is temperature-aware task scheduling, which involves analyzing the thermal behavior of a multiprocessor system-on-chip task schedule during the early stages of design. To accurately analyze non-deterministic execution behaviors, a model checking-based approach is proposed and formulated as a hybrid automata reachability problem. This approach uses power profiles of tasks, learned from simulation or emulation runs, to infer task power consumption. An algorithm for reachability analysis and a simulation methodology using this approach have been implemented in a tool called HeatCheck. Experimental results show the effectiveness of this approach in analyzing the thermal behavior of a set of task schedules on a multiprocessor system-on-chip system.
411507	4115077	Multigrid Reinforcement Learning with Reward Shaping	Potential-based reward shaping is a technique used in reinforcement learning to improve the speed at which agents learn. It allows for the incorporation of background knowledge into the learning process in a systematic manner. However, the challenge lies in determining the potential function that shapes the agent's reward. This paper proposes a solution to this problem by learning the potential function in parallel with the reinforcement learning process. By using a grid-based approach, a lower-resolution V-function can be learned simultaneously with the Q-function, allowing for the approximation of the potential function. The algorithm is introduced and its effectiveness is demonstrated through experiments.
411507	41150731	Online learning of shaping rewards in reinforcement learning.	Potential-based reward shaping is a technique used in reinforcement learning to improve the speed at which agents learn. It allows for the incorporation of background knowledge into the learning process in a systematic way. However, the challenge lies in determining the potential function that shapes the reward for the agent. This paper presents two solutions for learning the potential function in real-time, without prior knowledge. The first solution is designed for model-free reinforcement learning, while the second is for the model-based R-max algorithm. Both approaches are evaluated and shown to be effective in improving the learning process.
411508	41150851	Agent-Based Coalitions In Dynamic Supply Chains	Coalition formation is a significant issue in multi-agent systems, and recent research has focused on simplifying the process. This involves agents actively seeking out potential coalition members before engaging in negotiations. A framework has been proposed for coalition formation among agents in a dynamic supply chain setting, which includes a negotiation protocol and a decision mechanism. The negotiation protocol allows for effective communication between agents from different sectors (buyers, sellers, and logistics providers), while the decision mechanism involves two steps: forming loosely-coupled coalitions within sectors to reduce complexity, and forming cross-sector coalitions to deliver goods to end customers. An example is provided to demonstrate the success of this approach in helping agents form coalitions.
411508	41150898	Supporting dynamic supply networks with agent-based coalitions	This work builds upon previous research to enhance collaboration within supply networks. Three types of agents, buyers, sellers, and LPs, play different roles in forming coalitions. The process involves two steps: primary coalitions are formed within sectors to increase bargaining power or efficiency, followed by secondary coalitions across sectors to complete the deal. A negotiation protocol and deliberation mechanism are proposed to facilitate communication and decision-making among agents. These tools help agents identify potential coalition members and attractive payoffs, leading to successful coalition formation. Several examples are provided to illustrate the effectiveness of these methods. 
411509	411509129	Mixed Neighbourhood Local Search For Customer Order Scheduling Problem	COSP is a difficult problem with practical applications in industries like paper and pharmaceuticals. Current algorithms struggle with finding quality solutions for large problems. In this paper, a new heuristic called RBM is proposed, along with a mixed neighbourhood local search (MNLS) algorithm. MNLS includes various move operators to improve local exploitation and a greedy diversification method for plateau situations. Experiments on 960 instances show that MNLS outperforms existing algorithms and finds new best solutions for 721 instances. This highlights the potential of using MNLS for solving COSP.
411509	41150930	The road not taken: retreat and diverge in local search for simplified protein structure prediction.	The protein structure prediction problem is a crucial and challenging task in computational biology, where the goal is to find the three-dimensional structure of a protein that has the correct energy level. Local search methods are often used to efficiently find good solutions, but they suffer from re-visitation and stagnancy. In this paper, a new local search algorithm is proposed that addresses these issues by storing unexplored candidates and using a non-isomorphic encoding to prevent repetition. The algorithm outperforms existing methods on standard benchmark proteins and is particularly effective for Hydrophobic-Polar energy models and Face Centered Cubic Lattice.
411510	4115100	Relating weight constraint and aggregate programs: Semantics and representation	This paper discusses the relationship between weight constraint and aggregate programs, which are two commonly used types of logic programs with constraints. The stable model semantics for weight constraint programs and the answer set semantics for aggregate programs are compared and shown to be closely related. The paper also explores the possibility of representing aggregate programs using weight constraints, which can be more compact and efficient. Experimental results show that using weight constraints to handle aggregates can be a competitive approach for computing answer sets compared to more traditional methods. Overall, the paper provides insights into the connection between these two types of programs and offers a potential improvement for handling aggregates in logic programming.
411510	41151050	Weight Constraint Programs with Functions	This paper introduces a new type of logic program, called weight constraint programs with functions, which incorporate functions over non-Herbrand domains. The authors define answer sets for these programs and propose a computational mechanism based on loop completion. They first formulate loop formulas for lparse programs without functions, which improves upon previous formulations by not introducing new variables or requiring translation to nested expressions. They then extend this work to weight constraint programs with functions and show that their loop completion can be transformed into a Constraint Satisfaction Problem (CSP), allowing for the use of CSP solvers for answer set computation. Preliminary experimental results are also presented.
411511	41151124	An Efficient Protocol to Study the Effect of Flooding on Energy Consumption in MANETS.	Mobile Ad-hoc network (MANET) is a network of mobile nodes that communicate with each other through wireless channels. Unlike fixed wired networks, MANET has nodes with limited transmission range and mobility, which introduces challenges such as energy consumption and finding the location of the nodes. Most MANET routing protocols use flooding to locate nodes, but this can be costly in terms of energy consumption. In this paper, the Ring routing protocol is introduced as an alternative to flooding, and its effectiveness is compared to other protocols (DSDV, AODV, DSR) that use flooding. The results of the comparison, which were obtained using the ns-2 simulator, show that the Ring protocol is a viable option for reducing energy consumption in MANETs.
411511	41151127	Multipath Routing to Provide Quality of Service in Mobile Ad Hoc Networks	Providing quality of service routing in mobile ad hoc networks is a critical issue due to frequent topology changes and node failures. One solution is routing data along multiple paths, which improves reliability, load balancing, and reduces delay. However, there are two challenges: obtaining accurate topology information and selecting the best paths for reliability and network throughput. To address these challenges, a proposed algorithm minimizes overhead while constructing an accurate network topology. Additionally, a proper metric is chosen based on residual power, traffic load, and surrounding medium to select reliable paths, and links are shared between paths to further improve reliability.
411512	41151230	Chosen ciphertext secure authenticated group communication using identity-based signcryption	Secure group communication requires efficient access control and scalable rekeying to ensure the protection of dynamic and large groups. In addition, sender authentication is crucial in a many-to-many communication environment where each participant can send and receive messages. This study presents an authenticated group communication scheme that uses identity-based signcryption to prevent adaptive chosen ciphertext attacks. The scheme allows multiple senders to dynamically send messages to a chosen group of receivers, and group members can be stateless receivers. This ensures data confidentiality and sender authentication in the group communication.
411512	41151211	Removing escrow from ciphertext policy attribute-based encryption.	Attribute-based encryption (ABE) is a cryptographic technique used for controlling access to distributed data. In ciphertext policy attribute-based encryption (CP-ABE), users are associated with specific attributes and data is encrypted based on access policies for these attributes. This allows users to decrypt data only if their attributes match the access policy. However, one drawback of ABE is the key escrow problem, where a central key generation center has the power to decrypt all ciphertexts. This study proposes a solution to this problem by separating the power of issuing user keys between the key generation center and attribute authority. Through a secure two-party computation protocol, the two parties issue different parts of the secret key components to users, preventing either of them from having complete control over user keys. This approach can be applied to existing CP-ABE schemes, effectively solving the key escrow problem.
411513	41151337	Entity Based Peer-to-Peer in a Data Grid Environment	The past decade has seen a growing interest in Grid technologies, resulting in various Grid projects being launched with different visions. While all these visions aim to facilitate resource sharing, they vary in terms of functionality, grid characterization, and programming environments. A new Grid system, called DGET, has been introduced to specifically address data-related issues. DGET utilizes a peer-to-peer communication system and an entity-based architecture, combining the key features of both P2P and Grid systems. Currently in development, DGET's main components are being tested in a prototype. This paper focuses on the system's architecture and highlights its uniqueness compared to other systems.
411513	41151367	Toward a Distributed Knowledge Discovery system for Grid systems.	In the past decade, there has been an influx of data from various fields such as science, industry, and commerce. However, our ability to handle this increasing amount of data is becoming more challenging. Extracting useful knowledge from these datasets has become a major economic need, leading to the development of large-scale data mining techniques. This chapter introduces a new system that combines dataset-driven and architecture-driven strategies to efficiently mine these huge datasets. The system utilizes Grid middleware tools to handle large data and allows for more dynamic and autonomous mining, integration, and processing. By considering both data size and distribution, this system aims to improve the discovery of valuable insights from datasets in science and economic applications.
411514	41151441	Flattening single-vertex origami: The non-expansive case	 and closed origami.The Single-Vertex Origami Problem involves determining if a piece of paper with creases emanating from a fold vertex can be flattened without tearing or stretching the paper. Streinu and Whiteley showed that this problem can be reduced to the Carpenter's Rule Problem for spherical polygons. They were able to solve this problem by using spherical expansive motions and applied it to both open and closed origami cases.
411514	41151437	The Number of Embeddings of Minimally Rigid Graphs	The content discusses the number of distinct planar embeddings of minimally rigid graphs with $n$ vertices in Euclidean space. It is shown that, modulo planar rigid motions, this number is at most ${{2n-4}\choose {n-2}} \approx 4^n$. This upper bound is obtained using techniques from complex algebraic geometry, specifically by sectioning the (projective) Cayley-Menger variety. Lower bounds of the order of $2^n$, $2.21^n$ and $2.28^n$ are also demonstrated through inductive constructions of minimally rigid graphs using Henneberg sequences. The approach is also applicable in higher dimensions, leading to an upper bound of $2 D^{3,n}= {({2^{n-3}}/({n-2}})){{2n-6}\choose{n-3}}$ for the number of spatial embeddings of the $1$-skeleton of a simplicial polyhedron, up to rigid motions. This technique can also be extended to the non-Euclidean case.
411515	41151573	Fast, memory efficient flow rate estimation using runs	Per-flow network traffic measurements are crucial for effective network traffic management, performance assessment, and detection of anomalous events like DoS attacks. However, tracking the large number of flows in backbone networks requires high-speed memories, making explicit measurement difficult. To reduce overhead, random sampling has been proposed and used in commercial routers. The goal is to develop a new scheme with low memory requirements and quick convergence to a specified accuracy. This is achieved through a novel approach of sampling two-runs, which automatically biases the samples towards larger flows for more accurate estimation. This leads to smaller memory requirements compared to random sampling and is simple to implement and performs well. 
411515	41151561	ACCEL-RATE: a faster mechanism for memory efficient per-flow traffic estimation	Per-flow network traffic measurement is crucial for managing network traffic, evaluating network performance, and detecting unusual events like DoS attacks. In a previous study, the authors developed a method called RATE to efficiently estimate per-flow traffic rates with a specified level of accuracy. However, certain applications require faster estimation times, such as detecting worm activity or tracking transient traffic. This paper introduces a new scheme called ACCEL-RATE, which significantly reduces per-flow rate estimation times using a hashing method to split traffic into sub-streams. The estimated rates in each sub-stream are then related back to the original per-flow rates. The authors demonstrate through theoretical and experimental analysis that ACCEL-RATE can achieve estimation times one to two orders of magnitude faster than RATE, without significantly increasing memory size.
411516	41151668	Routing restorable bandwidth guaranteed connections using maximum 2-route flows	Restorable routing is crucial for maintaining reliable connections in MPLS and optical networks. It involves having both an active path and a backup path for each connection, allowing for quick restoration in case of failure. In order to optimize bandwidth usage, backups can be shared, but this requires knowledge of the backup bandwidth distribution among nodes. One approach for restorable routing in optical networks is simultaneous transmission on both paths, with the receiver choosing the stronger signal. However, this does not allow for backup sharing. This article focuses on efficient routing without sharing, using minimum-interference techniques to improve path selection. Two new algorithms are proposed, both of which outperform previous methods.
411516	41151646	Fast network re-optimization schemes for MPLS and optical networks	This paper discusses algorithms for improving network routing in connection-oriented networks like MPLS. The goal of re-optimization is to increase network traffic without adding more capacity. This is necessary due to dynamic connection routing, where connections are routed one-by-one as they enter the network, leading to inefficiencies. The proposed re-optimization scheme constantly monitors the network and identifies opportunities for improving efficiency. It then calculates the most cost-effective connections to be re-routed and computes new routes for them. The paper presents efficient algorithms and simulations that show significant improvements in network performance metrics through re-optimization.
411517	41151731	Refuse to crash with Re-FUSE	Re-FUSE is a framework designed to support restartable user-level file systems. It continuously monitors the file system and in the event of a crash, it automatically restarts the file system and restores its state. This process is transparent to applications, ensuring smooth operation. Re-FUSE achieves this through various techniques such as request tagging, system-call logging, and non-interruptible system calls. It has been tested with three popular FUSE file systems and has shown minimal impact on performance. Additionally, Re-FUSE requires minimal changes to existing FUSE file systems, making it a non-intrusive solution for improving crash tolerance.
411517	41151717	Split-level I/O scheduling	Split-level I/O scheduling is a new approach that divides I/O scheduling tasks into three layers: block, system call, and page cache. This framework addresses the limitations of traditional block-level schedulers in meeting throughput, latency, and isolation goals. By using this framework, various novel schedulers are developed that achieve these goals, such as the Actually Fair Queuing, Split-Deadline, and Split-Token schedulers. The framework is shown to work effectively with different file systems and is also useful for databases, hypervisors, and distributed file systems. This results in improved performance and isolation for these important applications.
411518	41151840	ViewBox: integrating local file systems with cloud storage services	Cloud-based file synchronization services have gained popularity for their ability to sync files across devices and provide automatic cloud backups. However, the loose coupling between these services and local file systems can make data vulnerable. Local corruption can spread to the cloud, causing issues on other devices, and crashes can lead to inconsistency between local and cloud copies. ViewBox is a solution that integrates synchronization and local file systems to prevent data corruption and inconsistency. It uses ext4-cksum and a user-level daemon to detect and recover from problems, and employs a view manager to create consistent snapshots of the file system. Experiments show that ViewBox has minimal overhead and effectively addresses these issues.
411518	4115189	Optimistic crash consistency	Optimistic crash consistency is a new approach to crash consistency in journaling file systems. It uses unique techniques to create an optimistic commit protocol that can recover from crashes and maintain high performance. This is implemented in a Linux ext4 variant called OptFS, which introduces two new file-system primitives, osync() and dsync(), to separate the ordering of writes from their durability. Experimental results show that OptFS significantly improves performance for various workloads, and robustness tests confirm its correctness in recovering to a consistent state after crashes. Additionally, osync() and dsync() are beneficial in achieving application-level consistency in atomic file system and database updates. 
411519	41151925	Exploitation of human shadow perception for fast shadow rendering	This paper discusses an experiment that was conducted to understand how the human visual system perceives shadows. Shadows are important for conveying the spatial structures of objects and enhancing the realism of rendered images. However, algorithms used in computer graphics to create realistic shadows are often computationally expensive. The experiment aimed to simplify the shadow casting process in order to improve performance, while still maintaining a high level of accuracy. Test subjects were asked to indicate the point at which they could notice a difference in the quality of the shadows. Initial results suggest that a simplified mesh with only 1% of its original complexity can produce soft shadows that are satisfactory to 90% of the participants.
411519	41151926	Interactive Exploration of Large Event Datasets in High Energy Physics	High energy physics uses particle accelerators to study the structure of matter by creating collisions that produce new particles. However, analyzing the billions of events that occur during these experiments can be challenging. To address this issue, researchers developed a visual browsing technique inspired by image databases. This technique allows for efficient navigation through large collision event datasets and includes features such as intuitive selection methods and a clustering-based approach. The system has been integrated into an event display application for the COMPASS experiment at CERN, demonstrating its potential for visual inspection in high energy physics.
411520	41152044	Synchronized extension systems	SE-systems are 4-tuples consisting of an alphabet, two languages, and a synchronizing set. They are used to generate new languages by combining the two languages and synchronizing on specific words. These systems are commonly found in various structures such as stacks, queues, and grammars, and are also used in coding systems. 
411520	41152034	On the Complexity of a Problem on Monadic String Rewriting Systems	The process of determining the descendants of a regular language L using a monadic string rewriting system has proven to be beneficial for decision algorithms related to finitely presented monoids and context-free grammars. Recent research by Esparza et al. has resulted in time and space complexities of O(ps3) and O(ps2) respectively for this problem, where p is the number of rules in the rewriting system and s is the number of states in the automaton accepting L. By utilizing synchronized extension systems, a new approach has been proposed with time and space complexities of O(pr), where p is the number of rules in the grammar generating L and r is the number of rules in the grammar.
411521	411521176	Efficient Face Image Deblurring via Robust Face Salient Landmark Detection.	In recent years, there has been significant progress in image deblurring. However, the deblurring of face images has not been well studied. Existing methods for deblurring faces rely on constructing exemplar sets and matching candidates, which is time-consuming and can be affected by complex or exaggerated face variations. To address these issues, a new method is proposed that integrates a classical L-0 deblurring approach with face landmark detection. A specialized landmark detector is used to identify key facial features, which are then used to guide the deblurring process. Experimental results show that this method is effective in handling complex face poses and reduces computation time compared to other approaches.
411521	41152132	Robust frontal face detection in complex environment	Our team developed a quick and efficient system for detecting human faces in complex environments. This system has two key features: 1) it uses a rapid image segmentation technique based on connected components labelling to identify potential face regions, and 2) it utilizes a positive-negative attractor template to confirm the presence of a face in these regions. The system also utilizes a valley detector to locate the eyes and mouth on the face. We tested our system on images with complicated backgrounds and distracting objects, and the results showed a strong ability to accurately detect faces with minimal false positives. 
411522	41152214	On the complexity of pure-strategy nash equilibria in congestion and local-effect games	Congestion games are a type of noncooperative game where players aim to route flow from their origin to their destination at minimal cost. This cost is determined by the total number of players using a certain path. Weighted congestion games allow for players to send varying amounts of flow, making it more complex. While it has been shown that pure-strategy Nash equilibria may not exist in these games, it is also proven that it is difficult to determine their existence. This holds true for both unsplittable flow (must be routed on one path) and splittable flow. Another related game is the local-effect game, where the cost of taking an action depends on the actions of other players. It is shown that determining whether a bidirectional local-effect game has a pure-strategy Nash equilibrium is a computationally difficult problem, and even finding a pure-strategy Nash equilibrium in a bidirectional local-effect game with linear local-effect functions is a complex task. This is due to the use of a tight PLS-reduction, which means that finding a pure-strategy Nash equilibrium can take exponential time.
411522	4115223	Encouraging Cooperation in Sharing Supermodular Costs	The study focuses on the computational complexity and algorithms for calculating the least core value of supermodular cost cooperative games. These games have applications in optimization problems, such as machine scheduling. The study shows that computing the least core value is NP-hard and presents approximation algorithms based on determining maximally violated constraints. The results are applied to schedule planning games, where the costs come from weighted completion times on a single machine. The study improves upon previous results and provides an explicit formula for the least core of schedule planning games, as well as a polynomial time approximation scheme for computing its value.
411523	41152340	A proposal to detect errors in Enterprise Application Integration solutions	Enterprise Application Integration (EAI) solutions are used to keep different applications' data in sync and to create new functionality. However, these solutions can be prone to errors due to their distributed nature and integration challenges. To address this issue, many researchers have focused on adding fault-tolerance capabilities to EAI solutions. This article examines EAI solutions from two perspectives: orchestration versus choreography and process- versus task-based execution models. It is found that current proposals have limitations or are limited to a specific viewpoint or execution model. To overcome this, an error monitor has been developed that can be used to add fault-tolerance to EAI solutions. The monitor has been proven to have efficient algorithms and has shown promising results in high workload situations.
411523	41152347	A Domain-Specific Language To Design Enterprise Application Integration Solutions	Enterprise Application Integration (EAI) solutions address the challenge of keeping multiple applications' data in sync and creating new functionality on top of them. Enterprise Service Bus (ESB) is a technology that enables cost-effective implementation of EAI solutions, but there is still a need for domain-specific tools to reduce integration costs. Guarana is a proposed solution that uses a graphical model and a domain-specific language (DSL) to design EAI solutions based on enterprise integration patterns. It offers multiple inputs and outputs for processes and tasks, and has a more efficient task-based execution model. A graphical editor and transformation scripts have been developed to generate Java code for the solution, and adapters can be configured to communicate with the integrated applications.
411524	4115242	Schema integration and transaction management for multidatabases	This paper proposes a technique for integrating multiple independent database systems using a unified model, which combines relational and object-oriented features. The advantages of this model are discussed, along with the methods for integrating databases with different models and resolving conflicts. The paper also addresses how queries on the integrated schema are handled in a heterogeneous environment and presents a transaction management scheme that allows for semantic recovery without restricting transactions or violating local autonomy. This paper was published in 1998 by Elsevier Science Inc.
411524	41152421	Integration of a Relational Database with Multimedia Data	The paper proposes a method for integrating a traditional database system with a multimedia server in a multidatabase environment. This integration allows for content-based retrieval of multimedia data using a high-level semantic description modeled with the enhanced entity-relationship (EER) model. The EER design is translated into a schema for the existing database system and then integrated with the preexisting database schema. However, this simple schema representation may not provide high levels of recall and precision in query results. To address this, the paper also introduces a modified cooperative query answering mechanism to improve query processing. Overall, this method allows for the efficient integration and retrieval of multimedia data in a multidatabase environment.
411525	41152531	Incremental Design Debugging in a Logic Synthesis Environment	Today's VLSI design process is complicated and can lead to multiple logic errors due to human mistakes and bugs in CAD tools. This can be challenging for designers to correct, but a new study presents a simulation-based solution for debugging combinational circuits with multiple errors. Unlike other techniques that identify all errors at once, this method works incrementally by fixing one error at a time using linear time algorithms. The study also includes the use of theorems, heuristics, and data structures to guide the search for a solution in a large space. Experiments on benchmark circuits show that this incremental approach is effective for logic debugging.
411525	41152548	Design error diagnosis and correction via test vector simulation	The complexity of digital VLSI circuit design can lead to logic errors during synthesis. To address this issue, a test vector simulation-based approach is proposed for diagnosing and correcting multiple design errors. This approach uses implicit enumeration to identify erroneous lines and minimize the need for resynthesis during correction, preserving the design effort already invested. The method is applicable to circuits without global binary decision diagram representation and has been successfully tested on ISCAS'85 benchmark circuits. The results demonstrate the effectiveness and robustness of this approach for diagnosing and correcting multiple design errors in digital VLSI circuits using test vector simulation.
411526	411526174	Resource description framework: metadata and its applications	The Web's universality, which allows for easy access to vast amounts of data and information, also hinders the development of a uniform organization scheme. To fully utilize the potential of Web data, a semantic web is needed, where applications and websites can exchange information. This requires detailed and structured metadata, which can be achieved through the Resource Description Framework (RDF). The success of RDF and the semantic web will depend on the development of applications, interfaces, and databases that can utilize it effectively. Practical issues such as security and compatibility will also play a crucial role. This survey provides an overview of the past, present, and future of RDF and the Semantic Web, emphasizing its potential for improved organization, accessibility, and usefulness. It is believed that knowledge discovery and data mining will benefit from this technology.
411526	41152660	Query Selection Techniques for Efficient Crawling of Structured Web Sources	 research areaThe use of structured data from Web sources is crucial for various applications. However, much of this data is hidden in databases that cannot be easily accessed by traditional Web search engines. Recent research has focused on understanding and efficiently acquiring data from these hidden Web databases through iterative querying. This paper specifically addresses the issue of selecting optimal queries to quickly gather data from Web databases. The authors propose a theoretical framework that models each database as a graph and show that finding the best query selection plan is a difficult problem. They present a range of techniques to optimize query selection and demonstrate their effectiveness through experiments and simulations. These findings can inform future research in this area.
411527	4115279	Acquiring a radiance distribution to superimpose virtual objects onto a real scene	This paper presents a new method for superimposing virtual objects onto real scene images with accurate shading. Unlike previous methods, this approach automatically measures the radiance distribution of the scene and uses it for appropriate virtual object placement. The method begins by constructing a geometric model of the scene from omni-directional images using a stereo algorithm. Then, radiance is computed from a sequence of images with different shutter speeds and mapped onto the geometric model. This information is then used to render virtual objects onto the real scene image, resulting in convincing shading and shadows. The effectiveness of this method was tested using real images. Keywords include computer graphics, computer vision, augmented reality, and illumination distribution measurement. 
411527	41152716	Illumination distribution from shadows	The image brightness of a three-dimensional object depends on the distribution of light sources, the shape, and reflectance of its surface. Previous research has focused on recovering the shape and reflectance from recorded image brightness, but little progress has been made in recovering the illumination. This paper proposes a new method for estimating the illumination distribution of a real scene from the observed image brightness on an object's surface. By using occlusion information, the method can accurately estimate the illumination in complex lighting environments. It does so by analyzing the radiance distribution in shadows cast by a known-shaped object onto another object with a known shape and reflectance. This allows for reliable estimation of illumination in real scenes.
411528	4115286	Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain	Hyper spectral imaging has a wide range of applications, but current methods do not take into account the fluorescent effects present in everyday items such as paper, clothing, and food. This paper presents a method for efficiently separating and recovering reflective and fluorescent emission spectra using high frequency illumination in the spectral domain. This allows for the estimation of fluorescent absorption spectra from the emission spectra, which is the first of its kind. Unlike conventional methods, this approach only requires two hyper spectral images and has been evaluated through simulations and real experiments. An application of this method to synthetic relighting of real scenes is also demonstrated.
411528	411528102	Reflectance Estimation from Motion under Complex Illumination	This paper introduces a new method for recovering the reflectance properties of a moving Lambertian object from a sequence of images taken by a stationary camera with unknown and complex lighting. The method utilizes the spherical-harmonic representation of Lambertian reflectance and combines it with the geometry reconstructed using Shape-From-Motion (SFM) to estimate the object's albedo and illumination distribution. This allows for the synthesis of realistic images of the object in different poses and lighting conditions. Experiments with both synthetic and real images demonstrate the effectiveness of the proposed method. 
411529	411529148	Modeling sensors: toward automatic generation of object recognition program	This paper discusses the process of automatically generating object recognition programs from given geometric models, which is a key method in building model-based vision systems. It explains the necessary components for this process, including object and sensor models, strategy generation, and program generation. The focus is on sensor modeling, as it is seen as the main obstacle in automatic program generation. The paper specifically looks at two aspects of sensor characteristics: detectability and reliability. A configuration space is defined to represent these characteristics, and a representation method is proposed. The paper concludes by exploring how this sensor model can be used in the automatic generation of object recognition programs.
411529	411529169	Appearance Sampling for Obtaining A Set of Basis Images for Variable Illumination	Previous studies have shown that the appearance of an object in different lighting conditions can be represented by a low-dimensional linear subspace. This can be achieved by using principal component analysis (PCA) on a large number of images taken under varying lighting conditions. However, it is unclear how many images are needed to accurately obtain the basis images. This study introduces a new method that can analytically obtain the basis images of an object for any lighting condition using images taken under a point light source. The key contribution of this work is the use of a set of lighting directions based on the object's BRDF spectrum in the angular frequency domain, allowing for the analytical generation of harmonic images without the need for the object's 3D shape and reflectance properties.
411530	41153013	Identification without randomization	The theory of identification via noisy channels involves using randomization in encoding to reduce the optimal code size, which experiences a double-exponential growth in blocklength. However, when considering more robust channels such as compound and arbitrarily varying channels, randomization is not necessary as the jammer already knows the input sequence. In these cases, the identification capacity is equal to the logarithm of the number of different row-vectors in the channel. This holds true for compound channels, but for arbitrarily varying channels, the identification capacity is different from the transmission capacity. A lower bound on the identification capacity is presented, and it is shown that randomization in decoding does not increase the capacity. This work is related to source coding.
411530	41153059	On lossless quantum data compression with a classical helper	Boström and Felbinger discovered that lossless quantum data compression is impossible without the decoder knowing the lengths of codewords. To solve this issue, they introduced a classical noiseless channel to inform the decoder of these lengths. This paper presents a further analysis of their codes, including a sufficient and necessary condition for their existence and an optimal compression rate. The main contribution is a more efficient way to use the classical channel, resulting in a more general coding scheme that always achieves optimal compression. Lower bounds and necessary conditions for achieving them are also discussed, along with a sharper lower bound for a specific type of quantum source. The paper ends with suggestions for future research.
411531	41153119	The Architect's Role in Software Ecosystems Health	Software ecosystems have become increasingly important for businesses to achieve success and maintain good health. The role of a software architect is crucial in this process, as they are responsible for implementing the organization's business strategy. Unlike traditional two-sided markets, software ecosystems operate as multi-sided markets, requiring different strategies. As a result, software architects must focus on flexibility and quick response to meet the demands of these markets. This shifts the understanding of their role and highlights the importance of collaboration and cooperation in fostering competition within the ecosystem. This paper examines how the actions of software architects impact the health of the ecosystem, as measured by productivity, niche creation, and robustness. 
411531	41153145	Educating to achieve healthy open source ecosystems.	Open source software ecosystem projects are governed by various authorities, including architects, maintainers, and committers. These governing bodies have established practices that are incorporated into training materials for newcomers to the project. These materials aim to guide contributors and promote the quality of the software product and the overall health of the open source ecosystem. This research focuses on the architectural practices used to train newcomers and their impact on the ecosystem. By analyzing data from various training sources, the study identifies these practices and their contribution to a healthy ecosystem. The findings highlight the importance of education for newcomers and its influence on the success of open source projects.
411532	41153256	Interval Valued Versions of T-Conorms, Fuzzy Negations and Fuzzy Implications	The article discusses the various ways in which classical prepositional connectives can be extended to the set [0,1] while preserving their behavior in extreme cases. However, it is generally agreed upon that simply preserving classical properties is not enough, leading to the introduction of t-norms, t-conorms, fuzzy negations, and fuzzy implications. This paper focuses on generalizing these concepts to the set II = {[a, b] : 0 les a les b les 1}, known as interval t-norms, and provides constructions to obtain the best interval representation of these connectives. The paper also explores the relationships between these concepts, such as obtaining an interval fuzzy t-conorm from an interval t-norm and an interval fuzzy negation. Various properties of these constructions are also proven.
411532	41153250	Formal Aspects of Correctness and Optimality of Interval Computations	Interval analysis, introduced by R. Moore in the 1950s, is a method for calculating bounds on the accuracy of numerical results using intervals, which are continuous sets of real numbers defined by their end-points. The main goal of interval analysis is to guarantee the correctness of numerical computations, and this is achieved through the "Fundamental Theorem of Interval Arithmetic". This theorem led to the development of interval representations, which serve as a language for expressing computations with real numbers. This paper explores the topological aspects of intervals and defines the canonical interval representation of a real function. It also shows how correct interval algorithms relate to other types of functions and preserve the continuity of real functions in the Scott and Moore topologies.
411533	41153341	Using gaze data in evaluating interactive visualizations	The importance of proper evaluations in publications presenting new visualization techniques is increasing. However, there have been many challenges and reluctance in conducting these evaluations, such as the difficulty and time-consuming nature of the task. To address this, a simple evaluation approach is proposed, which involves a set of tasks performed in a controlled environment with the use of eye tracking to estimate the user's focus. Additionally, three methods for visualizing gaze data are discussed, along with examples from a study evaluating a parallel coordinate browser. This approach offers valuable insights into the distribution of user attention, aiding in the understanding and improvement of visualization techniques.
411533	41153351	Static visualization of temporal eye-tracking data	Current methods for visualizing eye-tracking data do not allow for easy comparison of temporal information, specifically gaze paths. After reviewing existing techniques, a new approach is proposed that prioritizes time as the main attribute to be visualized. This new technique was successfully used to analyze the visual scanning of web search results listings. It is particularly useful when the main focus of the analysis is the temporal order of visiting Areas of Interest (AOI), the AOIs have a natural linear order, there are numerous AOIs producing interesting patterns, and the AOIs fill most of the space being studied. 
411534	4115342	Emotions and heart rate while sitting on a chair	Researchers are seeking new ways to monitor computer users' emotions in human-computer interaction research. In this study, participants were presented with emotionally provocative audio, visual, and audiovisual stimuli while their heart rate was measured using a special office chair and traditional earlobe photoplethysmography. The results showed a strong correlation between the two methods and indicated that emotional stimulation leads to a deceleration of heart rate, with the most significant changes occurring in response to negative stimuli. The researchers suggest that the EMFi chair could be a useful tool for unobtrusive measurement of emotional reactions in human-computer interaction.
411534	41153450	Ballistocardiographic Responses to Dynamic Facial Displays of Emotion While Sitting on the EMFi Chair	The aim of the study was to examine changes in heart rate during a video stimulation featuring two actors displaying happy, sad, and neutral facial expressions. The experiment used an unobtrusive measurement device called the EMFi chair to record ballistocardiographic responses. Subjective ratings of emotional valence and arousal were also collected. The results showed that the video stimuli elicited different ratings of emotional valence and arousal, with the strongest response being for negative stimuli. The male actor's stimuli resulted in higher arousal ratings and heart rate responses compared to the female actor's stimuli. Female and male participants also showed differential responses. The findings support the hypothesis that heart rate decreases in response to negative facial expressions in films and suggest that the EMFi chair can detect emotional responses during interactions with technology.
411535	41153535	Study on two privacy-oriented protocols for information communication systems	The privacy of users in information communication systems is crucial, especially for mobile systems. Many cryptographic tools have been developed to protect users' privacy, but some have been found to have security vulnerabilities. In this paper, two privacy-oriented cryptographic protocols are reviewed and their weaknesses are exposed. The first protocol, proposed by Hsu and Chuang, allows users to log into a system anonymously and establish a secret key, but it is not secure against session key attacks. A solution to this issue is proposed. The second protocol, proposed by Harn and Ren, aims to protect the privacy of message senders but has potential vulnerabilities. These vulnerabilities are discussed and potential solutions are suggested. 
411535	4115358	Appearance-based nude image detection	The authors propose a new system for detecting nude images based on their appearance. This system utilizes shape information in addition to skin color to accurately classify images as nude or not. The process involves identifying skin regions using texture characteristics, creating a skin likelihood image which contains both color and shape information, and using this as a feature to classify images with a nonlinear-SVM. Experimental results demonstrate the system's high performance in classification and ability to detect small nude images within larger images.
411536	411536106	Comparative modeling and evaluation of CC-NUMA and COMA on hierarchical ring architectures	Parallel computing performance on scalable shared-memory architectures is influenced by the interconnection networks connecting processors to memory modules and the efficiency of the memory/cache management systems. CC-NUMA and COMA are two effective memory systems, and the hierarchical ring structure is a efficient interconnection network in hardware. This article compares the performance of CC-NUMA and COMA on a hierarchical ring shared-memory architecture, using analytical models and performance measurements on a shared-memory machine. The results show that COMA balances work load well, but the frequent data movement overhead may offset the gains. CC-NUMA allows for explicit data locality management, potentially improving performance. These findings can be applied to other hierarchical network architectures.
411536	41153690	Comparative performance evaluation of hot spot contention between MIN-based and ring-based shared-memory architectures	Hot spot contention is a common issue in shared-memory architectures, where multiple processors try to access a globally shared variable through a network. Two main architectures, MIN and HR, have been used to build large shared-memory multiprocessors, but they respond differently to network bottlenecks. This paper compares the performance of hot spots on these two architectures. Analytical models are used to understand and evaluate the impact of hot spots, with the conclusion that HR-based architectures are better equipped to handle hot spot contention. Performance tests were conducted on two machines, BBN TC2000 and KSR1, and the results aligned with the analytical models. Practical observations and evaluations of hot spots were also made on the two types of architectures, particularly regarding synchronization lock algorithms.
411537	41153721	A personalized search engine based on Web-snippet hierarchical clustering	SnakeT is a meta-search engine that queries over 18 search engines and presents the results in two views: a flat-ranked list and a hierarchical organization of folders with clear labels. Users can browse this hierarchy for various purposes, including knowledge extraction and personalization of results. The personalization is done by users themselves, making it adaptive, private, and scalable. SnakeT has been extensively tested and compared to other Web-snippet clustering engines, showing its efficiency and effectiveness. The quality of the ranking of the underlying search engines directly affects the relevance of the results in the folder hierarchy, and vice versa. This work was published at the 14th International World Wide Web Conference in 2005 and contains a detailed description and experiments of the SnakeT software system. 
411537	41153742	The BioPrompt-box: an ontology-based clustering tool for searching in biological databases.	High-throughput molecular biology is producing a vast amount of data, leading to a rapid increase in the size of biological databanks. This presents challenges in indexing and retrieving information, as well as in browsing and understanding query results. To address these issues, a new generation of Web search engines, such as Vivisimo, are being developed. In the field of biology, the BioPrompt-box (Bpb) software system utilizes ontology-driven clustering strategies to improve the efficiency and effectiveness of searches. Bpb allows users to search for keywords within specific fields and also offers tools like Blast to find documents related to homologous sequences. The software also clusters search results into groups with similar content, using metadata like Gene Ontology and taxonomy information. Bpb is currently used for the UNIPROT databank, but could potentially be applied to larger databases like GenBank or EMBL. 
411538	41153836	Compressed representations of sequences and full-text indexes	This article discusses a method for representing a sequence of integers using a small amount of storage space, based on the zero-order empirical entropy of the sequence. This allows for constant time operations on the sequence, such as finding specific integers or answering queries, even for large sequences. The method is also applied to designing compressed full-text indexes that can handle larger input alphabets, with improved query times compared to previous methods. The article also mentions the ability to handle even larger alphabets with a slight increase in storage space and query times. Overall, this method provides a more efficient and scalable approach for handling sequences and full-text indexes.
411538	4115385	Efficient and Compact Representations of Some Non-Canonical Prefix-Free Codes.	This paper discusses efficient and compact alternatives to traditional tree-based codes, which can only be used if the order of codeword assignment is chosen. The paper presents a method for storing a nearly optimal alphabetic prefix-free code in a small number of bits, allowing for constant time encoding and decoding of characters. It also explores a type of code that was introduced to reduce the space usage of wavelet matrices, and shows how to store this code in a limited amount of bits while still allowing for constant time encoding and decoding. In cases where the assumptions do not hold, a method for encoding and decoding in linear time is also presented. 
411539	4115398	Cross-Language Relevance Assessment and Task Context.	An experiment was conducted to explore how users evaluate relevance in a foreign language that they are proficient in. The findings revealed that this process takes longer and is prone to mistakes compared to assessing relevance in their native language. These results were found to be influenced by the task and context. The study also proposes a new methodology for conducting context-sensitive studies. It was noted that people are naturally multilingual and many have proficiency in multiple languages, especially those engaged in intellectual activities. These individuals are also skilled at making relevance assessments.
411539	4115393	SICS at iCLEF 2002: Cross-Language Relevance Assessment and Task Context	The study looked at how well users are able to determine relevance in a foreign language they are familiar with. The results showed that this process takes longer and is more prone to mistakes compared to assessing relevance in their first language. The findings suggest that the task and context play a role in this difference. The study also presents a new method for conducting studies that take into consideration the context of the task.
411540	41154048	Pattern Matching Algorithms with Don't Cares	This paper discusses algorithms for pattern matching with "don't care" characters in either the pattern or the text. If the pattern has don't care characters, the problem can be solved in O(n + m + ) time, where is the total number of occurrences of component subpatterns. Online queries can also be handled with O(n) preprocessing time and O(m + ) time per query. If the text has don't care characters, the problem can be solved in O(n + m + |occ(P)|) time, assuming the length of each component sub-text is longer than the pattern.
411540	41154051	Finding patterns with variable length gaps or don’t cares	This paper discusses new algorithms for solving the pattern matching problem, specifically when the pattern contains variable length gaps. The proposed algorithm has a time complexity of O(n + m + α log(max$_{\rm 1bi–ai))) where n is the text length, m is the combined length of subpatterns, α is the number of occurrences of subpatterns in the text, and ai and bi represent the minimum and maximum number of allowed "don't care" characters between subpatterns. The paper also presents a second algorithm that can determine if the pattern occurs in the text using a suffix array in O(m + α loglogn) time. These algorithms also have the ability to report all occurrences of the pattern in the text. The techniques used in these algorithms have potential applications in other contexts as well.
411541	41154116	Drawing Directed Graphs Using One-Dimensional Optimization	The presented algorithm efficiently draws directed graphs by solving optimization problems for each axis. This results in a clear representation of the graph's hierarchy structure, with nodes not confined to fixed horizontal layers. The layout naturally conveys the symmetries of the graph. The algorithm can be used for cyclic or acyclic digraphs, as well as those with a mix of directed and undirected edges. Additionally, a hierarchy index is derived from the input graph to measure its level of hierarchy. 
411541	41154137	Combining hierarchy and energy for drawing directed graphs	The article discusses an algorithm for creating visually appealing layouts of directed graphs. The algorithm involves solving a one-dimensional optimization problem for each axis, resulting in a clear depiction of the graph's hierarchy. This allows for nodes to be placed in a more natural and symmetrical layout. The algorithm can be used for both cyclic and acyclic digraphs, as well as graphs with both directed and undirected edges. The article also introduces a hierarchy index that quantitatively measures the amount of hierarchy present in a given graph. 
411542	41154256	Applying the PLEX framework in designing for playfulness	Interactive products now not only need to be functional and usable, but also provide enjoyable experiences for users. However, playfulness can manifest in various forms, making it challenging to design for. To address this, a framework called Playful Experiences (PLEX) has been developed. It serves as a conceptual tool to understand playful aspects of user experience (UX) and a practical tool to design for such experiences through user-centered design (UCD) methods. This paper presents an overview of the PLEX framework and its development between 2008-2010. It also discusses the creation and evaluation of PLEX Cards and their use in a design case, as well as the development of PLEX Design Patterns for practical design solutions. The PLEX framework is proposed as a powerful tool for understanding and designing for playful experiences in interactive products.
411542	41154248	Social and privacy aspects of a system for collaborative public expression	This paper discusses a study on how a new technology, which combines mobile phones and public displays, is used in real-world social situations. The system allows for collaborative creation and sharing of comic strips in places like pubs or cafes. The study was conducted in three sessions, one at a social event and two in a bar, to observe how the larger social setting and location influenced the interaction with the system. The researchers also looked at how different roles, such as actor, spectator, and bystander, were taken on by participants and the privacy concerns that arose. The findings suggest that the social context plays a significant role in the use of this technology.
411543	4115435	Generative Adversarial Nets for Information Retrieval: Fundamentals and Advances.	Generative adversarial nets (GANs) are a popular method in deep learning and unsupervised learning for training a generative model to fit an unknown data distribution. This is achieved through an adversarial training mechanism, where a discriminative model guides the generative model to produce realistic data instances. While GANs were originally designed for continuous data like images, they have been adapted for discrete data in information retrieval scenarios, such as IDs, text, and graphs. This tutorial covers the fundamentals of GANs, their applications in discrete data generation, and their use in information retrieval tasks like text and graph generation. It also introduces relevant open-source platforms for conducting research experiments with GANs and discusses future research directions for GANs in information retrieval.
411543	41154358	AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods.	Adam, a popular adaptive learning rate method, has been shown to have issues with non-convergence in certain cases. Previous attempts to address this issue have not been effective in practice. This paper proposes a new insight into the non-convergence issue, specifically the correlation between the gradient and the second moment term in Adam. This correlation leads to unbalanced step sizes, where large gradients may have small steps and small gradients may have large steps, resulting in non-convergence. The paper presents a solution, AdaShift, which decorrelates the gradient and second moment term using temporal shifting. Experimental results show that AdaShift successfully addresses the non-convergence issue of Adam while maintaining competitive performance.
411544	41154446	Adaptive diversification of recommendation results via latent factor portfolio	This paper discusses the importance of result diversification in collaborative filtering for recommendation systems. The authors argue that the level of diversification in a recommended list should be tailored to the individual interests and needs of the target users. This is because users with different interests may have varying levels of uncertainty in their preference models. To address this, the authors propose a Latent Factor Portfolio (LFP) model that considers both the user's interest range and the uncertainty in their preference model. This model utilizes correlations between items and latent factors to reduce computation load and improve diversity in recommendations. Experimental results support the effectiveness of the LFP model in adapting to individual user needs and improving upon traditional latent factor models. 
411544	41154465	Dynamic Information Retrieval Modeling	Dynamic Information Retrieval modeling involves using techniques from artificial intelligence and reinforcement learning to model systems that change over time or a sequence of events. This approach is particularly relevant to current open problems in IR research, such as session search and computational advertising. The goal of this tutorial is to provide an up-to-date introduction to dynamic IR modeling, including a conceptual model for understanding dynamics in IR, algorithms and techniques for modeling dynamics, and a review of current research and applications in the field. Attendees will learn how to identify dynamics in an IR system, model them using AI techniques, and gain knowledge of state-of-the-art research in this area. This content is relevant to both IR practitioners and researchers, and will be particularly beneficial for those working in statistical modeling, personalization, and recommendation. 
411545	411545252	Explorable Volumetric Depth Images from Raycasting.	View-dependent image-based rendering techniques have become popular for their ability to combine high quality images with interactive exploration. However, previous approaches for volume rendering have limitations such as being limited to surfaces, expensive generation, and issues with depth perception. In this paper, the authors propose a solution called Volumetric Depth Images (VDI) which overcomes these issues by using a compact representation that is independent of the original data structure. This representation, generated with low overhead, allows for efficient rendering with arbitrary camera configurations. The results produced by VDIs are identical to the original ray casting, making it useful for preview rendering and analysis in various contexts. A prototype implementation and data from different fields demonstrate the effectiveness of this approach.
411545	41154565	Visualization of Temporal Similarity in Field Data	In this paper, the authors propose a visualization method for identifying and analyzing similarity in temporal variations of field data. The approach involves using interactive techniques to extract correlations from similarity matrices, which capture the temporal similarity of single-variable functions. This method can identify periodic and quasiperiodic patterns at individual points in the field, as well as similarities between different locations and datasets. The extracted correlations can then be visually explored to understand both temporal and spatial relationships. The entire process allows for flexibility and interaction, making it suitable for analyzing time-dependent data. The effectiveness of this approach is demonstrated through its application to both simulated and measured data.
411546	41154649	Visualization of Documents and Concepts in Neuroinformatics with the 3D-SE Viewer.	A new interactive visualization tool has been proposed for mining text data from neuroscience research. This tool utilizes the spherical embedding (SE) algorithm to visually represent complex relationships between pairs of lexical entities such as terms, keywords, posters, or papers' abstracts. By embedding these entities on spheres in a 3-D space, the tool can convey more information than traditional planar or linear displays. It can also extract and visualize different types of information, such as keywords, indexing terms, or topics, allowing for interactive browsing of various fields of research. This tool operates as an applet on top of existing resources and provides direct access to document sources in databases.
411546	41154635	Exploration of a collection of documents in neuroscience and extraction of topics by clustering.	This paper discusses a preliminary analysis of the neuroscience field using cluster analysis, specifically focusing on identifying topics within neuroscience. The study uses a collection of posters presented at the Society for Neuroscience Annual Meeting in 2006 and extracts terms from the posters' abstracts and titles. These terms are then compared to existing thematic categories in neuroscience to determine their relevance. The study found that using Log-Entropy scores for term ranking resulted in a better performance, with 31.0% of original title terms being retrieved in clustered documents and 37.1% in original thematic categories. This approach has potential for identifying and organizing topics in neuroscience.
411547	41154789	A safe, efficient regression test selection technique	Regression testing is a crucial but costly process used to ensure that modifications made to software do not negatively impact other parts of the program. A new technique for regression test selection has been developed, which involves creating control flow graphs for the original and modified versions of a procedure or program and using these graphs to select tests that execute the changed code. This technique has been proven to be safe and can potentially reduce the cost of regression testing. It also has the advantage of handling all types of program modifications and language constructs. Initial studies have shown promising results.
411547	41154781	A Safe, Efficient Algorithm for Regression Test Selection	Regression testing is a necessary but expensive process that ensures code remains unaffected by changes. A selective approach to regression testing involves choosing specific tests from an existing test suite to be applied to a modified program. A new technique for selective regression testing has been proposed, which uses control dependence graphs to identify tests from the existing suite that may exhibit changed behavior on the new version. Unlike previous methods, this algorithm selects all potentially error-exposing tests without prior knowledge of program changes, making it easily automated and able to handle all types of programming languages and modifications.
411548	411548112	Test Case Prioritization Based on Information Retrieval Concepts	Regression testing involves running all the test cases of a system, which can be time consuming and resource intensive. Test case prioritization (TCP) aims to schedule test cases in a way that achieves specific goals, such as higher coverage or faster fault detection. While most TCP methods focus on code coverage, recent research has explored using additional information to improve effectiveness. This study focuses on using Information Retrieval (IR) techniques to improve TCP, particularly for testing rarely used code. By considering both the frequency of testing and code coverage, our approach utilizes linear regression modeling to balance these factors. Our empirical study shows that this approach is more effective than random or traditional code coverage-based methods, with a potential increase in fault detection rate of up to 4.7%.
411548	41154837	Prioritizing JUnit Test Cases: An Empirical Assessment and Cost-Benefits Analysis	Test case prioritization is a method of organizing and running test cases based on their level of importance. Studies have shown that this technique can improve the detection of faults in a test suite, but it has only been tested on the C language and specific types of test suites. This raises the question of whether it can be effective on other languages, such as Java, which is widely used in software development. To address this, a controlled experiment was conducted to examine the effectiveness of test case prioritization on Java programs tested with the JUnit framework. The results showed a significant improvement in fault detection, but also highlighted differences compared to previous studies due to the language and testing approach. Cost-benefit models were also presented to demonstrate the practical implications of these findings.
411549	41154945	Type-2 fuzzy logic-based classifier fusion for support vector machines	SVMs have become popular as a machine-learning tool due to their promising performance, but their generalization abilities depend on the suitability of selected kernel functions for real classification data. To address this issue, a fuzzy fusion model is proposed in this paper to combine multiple SVMs classifiers. This model uses interval type-2 fuzzy sets to handle uncertainties in real classification data and membership functions in traditional fuzzy logic systems. It considers both the classification results and accuracy information of individual SVMs to generate a more robust combined classification decision. Experiments show that this type-2 based SVM fusion model outperforms individual SVM classifiers and the type-1 based SVM fusion model in most cases. 
411549	41154947	Genetic fuzzy classification fusion of multiple SVMs for biomedical data	Biomedical data classification is challenging due to the limited amount of data and high number of features. Improving classification performance in this domain is a key research area. A potential solution is to combine multiple classifiers using a fusion model, resulting in a more effective overall classifier. This paper proposes a fusion model for support vector machine (SVM) classifiers using fuzzy logic and genetic algorithms. The fuzzy logic system is created based on SVM accuracies and distances to SVM hyperplanes in feature spaces, and the genetic algorithm is used to optimize the fuzzy membership functions. The proposed model was tested on two biomedical datasets, and results showed that the fusion of multiple SVM classifiers outperformed individual classifiers in terms of robustness and reliability. 
411550	411550135	More Efficient Topological Sort Using Reconfigurable Optical Buses	Topological sort is a method used to order the vertices of an acyclic graph in a linear sequence. This has various practical applications, including job scheduling and network analysis. Many models have been developed to solve this problem efficiently, such as the hypercube and shuffle-exchange networks, CRCW PRAM model, and LARPBS model. Dekel et al. proposed an O(log2 N) time algorithm on the hypercube or shuffle-exchange networks with O(N3) processors, while Chaudhuri gave an O(log N) algorithm on the CRCW PRAM model using O(N3) processors. Li et al. showed that the problem can be solved in O(log N) time on the LARPBS model with N3 processors. This paper proposes a more efficient topological sort algorithm on the LARPBS model, with a time complexity of O(log N) and a processor complexity of N3/log N, outperforming previous algorithms on the hypercube and CRCW PRAM models.
411550	41155093	Efficient parallel algorithms for distance maps of 2D binary images using an optical bus	Distance maps, also known as distance transforms, are a common operation used in image processing and computer vision. They convert a 2D image of black and white pixels into an image where each pixel contains the distance or location of the nearest black pixel. This paper explores the use of an optical bus to efficiently compute a distance map for an image. The proposed method, called the linear array with a reconfigurable pipelined bus system (LARPBS), can solve the problem in O(log n log log n) or O(log n) bus cycles with high probability. It is also scalable and outperforms previous parallel algorithms for the same problem.
411551	41155150	Mining for Structural Anomalies in Graph-based Data	This paper discusses the use of graph-based methods for detecting anomalies in data where the anomalies involve unexpected changes in entities and relationships. The authors introduce three new algorithms that use the minimum description length principle to identify anomalous substructures in graphs, and evaluate their effectiveness using both synthetic and real-world data. They define anomalies in the context of graphs and present examples and results from their algorithms. This research highlights the potential of using graph-based representations for detecting fraud and suggests avenues for future work in this area. 
411551	41155151	Discovering Anomalies to Multiple Normative Patterns in Structural and Numeric Data	Traditional methods of detecting anomalies in data have limitations when it comes to complex, structural data. One solution is to use graph-based anomaly detection, which allows for analysis of relationships between elements rather than just data values. However, previous attempts at this approach have encountered two problems: not incorporating numeric values in the analysis and only detecting anomalies that deviate from the most common pattern. This paper introduces improvements to graph-based anomaly detection that address these issues and provides experimental evidence of their effectiveness. These enhancements allow for a more comprehensive and accurate identification of anomalies in complex data.
411552	41155244	Enabling Incremental Query Re-Optimization.	As declarative query processing expands to various platforms, there is a growing need for adaptive techniques to handle unanticipated performance changes. This requires innovation in both cost estimation algorithms and search algorithms. The authors propose a cost-based optimizer that can continuously update the optimal query plan based on new cost information, similar to a stream engine updating its outputs with new data. Their implementation is particularly effective for stream processing workloads and serves as the foundation for future adaptive optimization algorithms. They utilize a recursive datalog query approach and develop novel optimization methods for both static and incremental cases. These techniques can also be applied to traditional optimizer implementations.
411552	41155279	Recursive Computation of Regions and Connectivity in Networks	The data management community is focusing on scenarios where data access is closely tied to network routing and distributed acquisition. This includes sensor networks, declarative networks, and distributed and peer-to-peer stream systems. The main challenge is maintaining a view over dynamic network state, which is often domain-specific, expensive, and incomplete. This paper proposes a solution to this problem by considering incremental recursive view maintenance in the presence of distributed streams of updates. The approach involves maintaining compact information about tuple derivability or data provenance, as well as reducing communication through techniques such as aggregate selections and provenance-aware operators. The work is validated through experiments with sensor and network router queries, showing improved communication overhead without sacrificing performance.
411553	411553107	TinyDB: an acquisitional query processing system for sensor networks	We are discussing the design of a query processor that collects data in sensor networks. This processor addresses the "acquisitional" issues of where, when, and how often data is acquired and delivered to query processing operators. By considering the locations and costs of data acquisition, we can reduce power consumption compared to traditional passive systems. Simple extensions to SQL are proposed for controlling data acquisition, and the impact of acquisitional issues on query optimization, dissemination, and execution is discussed. The effectiveness of this design is evaluated through the example of TinyDB, a distributed query processor for smart sensor devices, which demonstrates significant power consumption reductions.
411553	411553136	Multiresolution Cube Estimators for Sensor Network Aggregate Queries	This work focuses on improving the efficiency of spatial aggregate queries in a sensornet setting. The proposed approach involves constructing and maintaining multi-resolution cube hierarchies within the network, which can be done in a distributed manner. In case of failures, recovery can also be performed within the network. The paper demonstrates how these cube hierarchies can be used to summarize sensor data and improve the efficiency of spatial aggregate queries. It also presents algorithms for computing query plans in polynomial time and handling optimization over multiple queries. The work also discusses ways to enrich cube hierarchies with additional summary information and presents a distributed cube construction algorithm. Finally, the paper investigates failure scenarios and recovery algorithms for query results.
411554	41155431	Towards realistic benchmarks for virtual infrastructure resource allocators	The rise of virtual infrastructures has sparked interest in developing resource allocation systems to reserve and use different combinations of nodes, switches, and network links. However, there is a lack of realistic benchmarks in this relatively new area. This paper proposes a method for creating realistic benchmarks for virtual infrastructure and introduces a workload generator based on a 5-year trace of experiments from the Emulab testbed. The importance and potential uses of these benchmarks are demonstrated through various evaluation scenarios. This research suggests a way to improve resource allocation in virtual infrastructures through the use of realistic benchmarks.
411554	41155447	BFT protocols under fire	Recent research in Byzantine state machine replication has focused on improving protocol performance under benign conditions, but there has been limited evaluation under practical conditions such as WAN delays, packet loss, and shared resources. This makes it challenging for system designers to choose the appropriate protocol for a real deployment. Additionally, there is a lack of direct protocol comparisons due to varying runtime environments, crypto libraries, and transports used in implementations. To address this, a simulation environment has been developed that combines a declarative networking system with a network simulator. Protocols can be quickly implemented in the declarative language, and network conditions and communication costs can be easily adjusted. The simulator accurately predicts the performance of native protocol implementations, and allows for rapid comparison and exploration of different scenarios. Results show that Zyzzyva outperforms other protocols under most conditions, highlighting the difficulty in designing a one-size-fits-all protocol.
411555	411555226	Plugging Taxonomic Similarity in First-Order Logic Horn Clauses Comparison	Horn clause logic is a representation language used in Logic Programming and Inductive Logic Programming. It is used to express relationships among objects in order to capture relevant information. While the predicates used in this language are defined by the knowledge engineer and handled by interpreters, they may not fully capture all underlying relationships. To address this, a taxonomic background knowledge can be used to better assess the similarity between two descriptions. This approach extends an existing distance framework by incorporating information from the taxonomic background knowledge. The effectiveness of this solution is demonstrated through sample problems.
411555	411555115	Plugging numeric similarity in first-order logic horn clauses comparison	Horn clause logic is a language used in Logic Programming and Inductive Logic Programming to represent relationships among objects. This language is defined by the knowledge engineer and interpreted by computer programs. However, in some cases, the information expressed by these predicates may require additional background knowledge in order to fully capture the underlying relationships. This is especially true when dealing with numerical information, as simple syntactic matching is not enough. To address this issue, a new framework has been proposed that can handle numeric information in Horn clauses and has been successfully demonstrated on sample problems.
411556	41155631	Simulating empathic behavior in a social assistive robot.	The paper discusses the importance of social robots in Ambient Assisted Living (AAL) and their role in establishing a social empathic relationship with the user. The NAO robot has been equipped with the ability to recognize the user's emotions through communicative signals from speech and facial expressions. This helps the robot to trigger appropriate empathic behaviors, which have been evaluated by experts and elderly users. The results are encouraging and suggest the potential for further development of the robot's social and affective capabilities.
411556	411556100	Automatic topics identification for reviewer assignment	Scientific conferences involve many complex tasks, such as assigning papers to suitable reviewers. This can be a time-consuming and tedious process for all involved, including authors, reviewers, and conference chairs. To make this process easier, a web-based management system is desirable. Authors must submit a form with their paper title, abstract, and conference topics. Reviewers must register and declare their expertise on the conference topics. The conference chair then manually assigns the papers based on the information provided by both authors and reviewers. This paper proposes the use of intelligent techniques to automatically extract paper topics and reviewer expertise from their titles and publications. These methods were evaluated on a real conference dataset and showed good results in terms of quality, user satisfaction, and time efficiency compared to manual assignments. 
411557	41155768	A General Similarity Framework for Horn Clause Logic	First-Order Logic formulæ, which use relations, can lead to computational problems due to indeterminacy. To address this issue, a framework for comparing and assessing similarity between two descriptions is needed. This could have many applications in Artificial Intelligence, such as guiding subsumption procedures and implementing flexible matching. However, there is limited research on this topic. This paper focuses on Horn clauses, the basis of Logic Programming, and proposes a new similarity formula and evaluation criteria for identifying similar components in descriptions. Experiments on real-world datasets demonstrate the effectiveness and efficiency of this approach.
411557	41155747	Similarity-Guided Clause Generalization	The literature lacks sufficient information on defining similarity between First-Order Logic formulæ due to the presence of relations, which can cause mapping difficulties. As a result, there is a need for general criteria to compare formulæ. This paper focuses on the comparison of two descriptions, such as a definition and an observation, and how similarity criteria can aid in identifying similar subparts based on their syntactic structure. Real-world experiments demonstrate the success and efficiency of this approach in a generalization procedure.
411558	41155845	A Logic Programming Framework for Learning by Imitation	This paper discusses the use of imitation as a way for humans to acquire knowledge, by following instructions or demonstrations from others. The authors propose a logic programming framework for teaching agents to learn from relational demonstrations, where the demonstrations are received incrementally and used as training examples while the agent interacts in a stochastic environment. This framework allows for the representation of domain-specific knowledge and complex relational processes in a compact and declarative way. The effectiveness of this framework is demonstrated through experiments in simulated agent domains. 
411558	41155848	Relational Sequence Clustering for Aggregating Similar Agents	Many clustering methods only consider flat descriptions of data, but real-world domains often involve heterogeneous objects that are related in multiple ways. For example, in Multi-Agent Systems, agents interact with their environment and with other agents. To be effective, an agent must be able to recognize the behaviors of other agents. As agents' actions are sequential, their behavior can be expressed as a sequence of actions. This paper proposes a clustering method that can identify patterns in the behavior of competing or companion agents by observing their actions. The algorithm was tested on a real-world dataset and was found to be effective.
411559	41155915	Translation and optimization for a core calculus with exceptions	Source languages need to have a balance of complexity and conciseness for programmer use. However, this complexity can make it difficult to analyze and some features may be overlooked. Exception handling is important for robust software, but is often not considered in program analysis and optimization. Traditional methods of converting high level languages to machine code can result in code that is too cryptic for analysis. To address this issue, an intermediate, minimal but expressive, core calculus has been designed to handle major language features. This allows for easier analysis and optimization without sacrificing the flexibility and richness of the source language. 
411559	41155912	An Interval-Based Inference of Variant Parametric Types	Variant parametric types combine subtype and parametric polymorphism to create a more versatile subtyping system for Java-like languages. The use-site variance feature allows for different subtyping behaviors, such as covariant, contravariant, invariant, or bivariant, depending on how the fields are used. By annotating variance properties on type arguments, programmers can choose the desired subtyping behavior for each use of a parametric class. However, this process can be challenging, leading to criticism of these mechanisms. While algorithms exist to automatically refactor legacy Java code to use variant parametric types, they do not fully support the flexibility of use-site variance subtyping. To address this issue, this paper proposes an interval-based approach for inferring variance annotations and type arguments in a constraint-based inference algorithm. Each variant parametric type is treated as an interval type with lower and upper bounds, allowing for a per-method summary-based analysis. 
411560	41156029	Peering and provisioning of differentiated Internet services	The article discusses the importance of maintaining consistent and stable service level agreements (SLAs) for differentiated network services across multiple networks. A game theoretic model is used to investigate the feasibility of this, taking into account raw-capacity sellers, brokers, and users in a two-tier market. The model provides a necessary and sufficient condition for the stability of the game and determines whether a set of SLA configurations among peering ISPs is sustainable. Simulations using a distributed progressive second price auction as the spot market mechanism in a scenario with three interconnected networks and two services validate the analytical results. Overall, the study emphasizes the significance of considering network dynamics and market mechanisms in ensuring stable and consistent SLAs for differentiated network services.
411560	41156058	Programming telecommunication networks	The telecommunications industry is undergoing major changes with the move towards market deregulation and open competition. This has led to a need for telecom providers and operators to open up their services to competition. The success of these providers now depends on the level of sophistication, flexibility, and speed of deployment of their services. The software architecture in place plays a crucial role in achieving this. The service architecture of the telephone network and the Internet are examined to identify their strengths and weaknesses. A new service architecture is proposed to overcome the limitations of existing networks and create a more open and flexible environment. This approach involves developing a service model that reflects the future market structure and implementing it through a broadband kernel called xbind, which incorporates IP, CORBA, and ATM technologies. The article also addresses key issues such as quality of service, performance, scalability, and implementation.
411561	4115611	Deaccumulation techniques for improving provability	The paper discusses the limitations of induction theorem provers in verifying functional programs with accumulating arguments. To address this issue, the authors propose automatic transformations from accumulative to non-accumulative functional programs using concepts from tree transducers and previous research. This approach aims to reduce the need for generalizing induction hypotheses in (semi-)automatic provers, and also has potential applications in reducing the need for inventing loop invariants in the verification of imperative programs. The paper highlights the potential of this approach to improve the efficiency and accuracy of mechanized verification for functional and imperative programs.
411561	41156144	Context-Moving Transformations for Function Verification	Induction theorem provers are used to verify the correctness of functional programs, but they often struggle with verifying tail recursive functions, which are commonly used in imperative programs. To address this issue, an automatic transformation has been developed that can transform these difficult-to-verify functions into ones that can be verified using existing techniques. This transformation allows for the use of induction theorem provers in practical settings where imperative programs are prevalent.
411562	41156212	Certification of Termination Proofs Using CeTA	The paper discusses the use of automated tools to prove termination of term rewrite systems, which often involve complex criteria. However, the generated proofs can be too large for humans to easily check for accuracy. To address this issue, the authors propose using the theorem prover Isabelle/HOL to automatically certify termination proofs. They first formalize the necessary theory and develop executable checks for three major termination techniques. These checks not only ensure correct application of the techniques, but also provide readable error messages if a proof is not accepted. Additionally, a certified Haskell program, CeTA, is generated for efficient certification of termination proofs without requiring Isabelle installation. 
411562	4115620	Proving and Disproving Termination in the Dependency Pair Framework	The dependency pair approach is a powerful technique for automatically proving the termination of term rewrite systems. It has traditionally been seen as just one method among many, but it can actually be used as a general concept to integrate various techniques for termination analysis. This new approach, called the "dependency pair framework," allows for the combination of different techniques and makes the development of new methods for termination analysis easier. This approach also allows for the verification of non-termination, which was previously not possible. By combining termination and non-termination techniques, the framework is able to identify potential causes of non-termination and detect "dead ends" in termination proofs. This approach has been successfully implemented in the automated termination prover AProVE, which was the winning tool in the International Competition of Termination Provers 2005.
411563	41156323	Termination Analysis by Dependency Pairs and Inductive Theorem Proving	Current techniques and tools used for automated termination analysis of term rewrite systems (TRSs) are highly effective. However, they are not able to handle algorithms that rely on inductive reasoning for their termination. To address this issue, we propose a method that combines the dependency pair approach for TRS termination with inductive theorem proving. This approach has been implemented in the tool AProVE and has shown success in analyzing the termination of TRSs that rely on inductive arguments. With this new approach, TRS termination techniques can now handle a wider range of algorithms.
411563	4115630	Proving and Disproving Termination in the Dependency Pair Framework	The dependency pair approach is a powerful technique for automatically proving the termination of term rewrite systems. However, it was previously seen as just one of several methods for proving termination. This paper introduces the concept of the "dependency pair framework" which integrates various termination analysis techniques, increasing their effectiveness and modularity. This framework allows for the development of methods for both proving and disproving termination, which was previously not possible. The authors implemented and evaluated this framework in the automated termination prover AProVE, which was successful in both proving and disproving termination in the International Competition of Termination Provers 2005. Overall, the dependency pair framework greatly improves the ability to analyze and verify termination of term rewrite systems.
411564	41156433	A Rounds vs. Communication Tradeoff for Multi-Party Set Disjointness	The set disjointness problem involves k players with private inputs who communicate over a shared blackboard to determine if their sets have a common element. The trade-off between the number of rounds of interaction and the total number of bits sent is studied, with a lower bound of Ω̃(nk <sup>1/R</sup> /R <sup>4</sup> ) bits for R rounds. This is nearly tight. The same proof is used to show that wellfare maximization with unit demand bidders cannot be efficiently solved in a small number of rounds. The lower bound for this problem is improved to Ω(log k/log log k), which is known to be tight up to a log log k factor.
411564	41156446	From information to exact communication	The authors introduce a new local characterization of the zero-error information complexity function for two-party communication problems, using it to compute the exact internal and external information complexity of the 2-bit AND function. This leads to a tight characterization of the communication complexity of the set intersection problem, with a randomized communication complexity tending to the information complexity of AND multiplied by the size of the sets. The authors also prove that an infinite number of rounds is necessary for an information-optimal protocol for AND. Using this analysis, they calculate the exact communication complexity of the set disjointness function and obtain a tight bound for sets of size ≤ k.
411565	41156595	Information Theoretical Limit of Media Forensics: The Forensicability.	This paper discusses the potential limitations of forensic techniques used to detect the processing history of multimedia content. While there have been advancements in this field, it is important to consider if there is a fundamental limit to the capability of forensics. The author suggests that it is important to not only focus on what investigators can do, but also understand the boundaries of their capabilities. The paper explores the fundamental principles of forensics and proposes ways to improve its effectiveness in detecting tampering and manipulation of multimedia content. 
411565	41156516	On antiforensic concealability with rate-distortion tradeoff.	The compression history of a signal is important for forensic analysis as it can reveal information about its origin and authenticity. Antiforensic techniques have been developed to hide manipulation fingerprints, but they can also introduce distortions or increase data size. This creates a tradeoff between concealability, data rate, and distortion for a forger. To measure this tradeoff, the concept of concealability is introduced and a flexible antiforensic attack is proposed. An antiforensic transcoder is also suggested to efficiently complete the tasks of decoding, applying antiforensics, and recompressing in one step. Through simulation, it is shown that using a lower quality factor in the second compression can increase concealability and decrease data rate, and using a lower secondary quality factor can achieve the same concealability and distortion values at a lower data rate. Therefore, forgers have an incentive to always use a lower secondary quality factor.
411566	411566107	Energy-Efficient Base-Station Cooperative Operation with Guaranteed QoS	The development and deployment of energy-efficient communication technologies has become a key focus in the information and communications technology sector due to the increasing energy costs and environmental concerns. To improve the energy efficiency of mobile cellular networks, shutting off base stations during off-peak periods is proposed. This paper introduces a strategy for energy-efficient base station switching, using cooperative communication techniques among base stations to extend network coverage. Both path-loss and fading effects are considered in the system model, and closed-form expressions for call-blocking probability and channel outage probability are derived. By identifying user equipment at worst-case locations, the proposed scheme ensures quality of service. Results show that this strategy, with BS cooperation, can significantly reduce energy consumption while maintaining quality of service. 
411566	411566289	On the energy efficiency of cooperative communications in wireless sensor networks	Cooperative communications have the potential to improve wireless sensor networks by mitigating channel fading and increasing energy efficiency. However, this comes with added complexity. In sensor networks, receiving and processing power can consume a significant portion of total power. To address this, an analytical framework is proposed to study the energy efficiency trade-off in cooperative networks, taking into account factors such as receive and processing power, quality-of-service requirements, and power amplifier loss. Results show that direct transmission is more energy efficient than relaying for short distances, and equal power allocation can perform as well as optimal power allocation in certain scenarios. The impact of relay location and number of relays on energy efficiency is also analyzed, and experimental results are presented to validate the proposed channel model.
411567	411567106	Monkey-in-the-browser: malware and vulnerabilities in augmented browsing script markets	As more applications move from desktop to web-based platforms, power users have found ways to customize and enhance them to better suit their needs. This has been made possible through tools like the Greasemonkey extension, which allows users to write scripts that can manipulate the content of any web page. However, the popularity of script-sharing markets has introduced new security risks, with potential for malicious scripts to be installed by unsuspecting users. A study of the most popular Greasemonkey script market reveals numerous vulnerabilities, including some that could be used to access sensitive user data. The practicality of these attacks is demonstrated through a proof-of-concept exploit against a vulnerable script with over 1 million users. 
411567	411567123	Better security and privacy for web browsers: a survey of techniques, and a new implementation	The web browser is an essential tool for accessing various applications and services, but it also poses a security risk when visiting untrustworthy sites. Therefore, it is crucial for browsers to prioritize privacy and security to prevent malicious content from compromising user data or interfering with other sites. Over the past decade, there has been significant research on improving browser security, with proposals for new techniques and architectures. This paper provides a survey of these problems and solutions, specifically focusing on the security of JavaScript scripts. It discusses three main techniques for enhancing script security and describes a new implementation of one of these techniques in the Mozilla Firefox browser. The paper also includes preliminary results from experiments with this implementation.
411568	41156837	A Survey on Mobile Social Networks: Applications, Platforms, System Architectures, and Future Research Directions	Mobile social networks (MSNs) have gained popularity in recent years and offer a variety of applications and services that are of interest to service providers, developers, and users. This paper provides a comprehensive survey of MSNs, distinguishing them from conventional social networks and discussing their platform, solutions, and system architecture. The popular MSN platforms and experimental solutions for existing applications are reviewed, along with the dominant mobile operating systems on which MSNs are implemented. The overall architectural designs of current and future MSN systems are analyzed and proposed, considering both client-server communication and wireless data transmission. The unique features, services, and key technologies of two generations of MSN architectural designs are compared. The paper also introduces the concept of a vehicular social network and its challenges, and concludes with an overview of the major challenges and potential future research directions for MSNs.
411568	41156828	Variations on Using Propagation Cost to Measure Architecture Modifiability Properties	Structural metrics are commonly used to measure the modifiability and impact of changes in a system's architecture. However, these metrics only consider the dependencies among system elements and do not account for the semantics of architectural transformations. To address this limitation, a study was conducted using a structural metric called 'propagation cost' on different architectural transformations. The results showed that the original form of the metric did not consistently indicate the health of the architecture. By enhancing the metric with the knowledge of architectural patterns and tactics used in the transformations, it was able to detect modifiability properties that were previously undetectable. This highlights the importance of incorporating semantic information in architectural metrics to better understand the impact of changes on a system.
411569	41156975	Context-aware regulation of context-aware mobile services in pervasive computing environments	The paper presents a context-aware policy system for controlling mobile services in pervasive computing environments. This system considers the various elements of a pervasive environment, including users, services, contexts, policies, and roles. Depending on the context of a user, such as their location, activity, and role, appropriate policies are selected to determine which services they can see and access. This approach offers a generic model for executing mobile services in different pervasive domains, such as a campus setting. The design and prototype implementation of this system are also discussed.
411569	41156928	Implementing context-aware regulation of context-aware mobile services in pervasive computing environments	This paper discusses the concept of pervasive computing, which is a collection of users, services, contexts, policies, and devices. Context-aware policies are used to determine what services can be accessed in different contexts, such as location, activity, and user role. The paper introduces an infrastructure called Mobile Hanging Services (MHS) that utilizes policy-based access and execution of services. MHS provides abstractions for delivering, downloading, and executing services using a policy mechanism. The paper also presents a case study of a Mobile Windows Media Player application in a pervasive campus environment to illustrate their approach using web services, compact mobile code, and policies. 
411570	41157031	A New Symmetry-Based Method For Mid-Sagittal Plane Extraction In Neuroimages	The estimation of the mid-sagittal plane (MSP) in neuroimage analysis has several applications but has been a challenging problem. In this study, a new method was developed using bilateral symmetry maximization and a more suitable error metric to compare different MSP extraction methods. The method was evaluated on a dataset of 164 clinical images and was found to outperform three other state-of-the-art approaches in terms of accuracy and precision. It also does not have limitations regarding imaging protocol or initial head position and is one of the fastest methods, taking only 30 seconds on a regular workstation. This new method shows promise for accurate and efficient MSP estimation in neuroimage analysis.
411570	41157056	Translation-symmetry-based perceptual grouping with applications to urban scenes	Perceptual grouping is an important aspect of the human vision system, where visual elements are organized into coherent groups. However, in computer vision, there is a lack of mid-level cues due to difficulties in constructing feature detectors. A new method has been proposed that uses the 2D translation subgroup of a wallpaper pattern to group visual elements. This method is different from previous algorithms and can detect multiple, semantically relevant patterns in a scene simultaneously. The experimental results demonstrate the effectiveness of this method in tasks such as building facade super-resolution and orientation estimation from a single view.
411571	41157155	Learning Ground CP-Logic Theories by Leveraging Bayesian Network Learning Techniques	Causal Probabilistic Logic (CP-logic) is a language used to express causal relations in different domains. This paper presents an algorithm called SEM-CP-logic that learns CP-theories using Bayesian network (BN) learning techniques. The algorithm transforms the CP-theories into BNs and modifies the BN parameter learning and structure search to ensure that the constructed networks represent valid CP-theories. The paper also compares the learning of CP-theories and BNs, showing that simple CP-theories can be represented with noisy-OR nodes while more complex theories require fully connected networks. In experiments, SEM-CP-logic shows better performance in learning CP-theories with fewer training data compared to BN learning. It is also applied in a medical domain for HIV research and shows competitive results with other methods. 
411571	41157116	Relational Reinforcement Learning	Relational reinforcement learning combines reinforcement learning with relational learning or inductive logic programming. This technique uses a more expressive representation language for states, actions, and Q functions, allowing it to be applied to a wider range of learning tasks. One task that has been explored is planning in the blocks world, where the effects of actions are assumed to be deterministic. This approach has the potential to improve the performance of traditional reinforcement learning methods by incorporating relational knowledge into the learning process.
411572	411572102	Estimating Size of Search Engines in an Uncooperative Environment	The size of a search engine is determined by the number of documents it indexes. This information is crucial for metasearch engines to select and merge results from multiple search engines. Accurately estimating the size of search engines is important for efficient functioning of metasearch engines. A new algorithm has been proposed in this paper that has better accuracy and efficiency compared to existing methods. It also shows better tolerance to unfavorable environments compared to the Sample-Resample approach, which is the most widely used method. This new algorithm can help improve the performance of metasearch engines that use multiple autonomous search engines. 
411572	41157231	A Fuzzy Search Engine Weighted Approach to Result Merging for Metasearch	 This paper discusses the challenge of combining result lists from different search engines in metasearch engines to achieve optimal performance. The authors propose a new result merging model, called IGOWA (Importance Guided OWA), which uses Yager's fuzzy aggregation OWA operator and incorporates the concept of importance guided aggregation. This model allows for weights to be applied to the result lists of each search engine, which is an improvement over a previous OWA-based model proposed by Diaz. The authors also introduce a scheme for computing these weights, called Query-System Weights, and compare it to another scheme proposed by Aslam and Montague, known as System Weights. 
411573	41157326	Design of a remote procedure call system for object-oriented distributed programming	This paper discusses the development of an RPC system specifically designed for building object-oriented distributed software systems. The main goals of this system are to support inheritance, polymorphism, dynamic binding, and modular development in the implementation of distributed software. The paper outlines the features of this RPC system that meet these requirements and briefly discusses the lessons learned from previous versions that influenced the design of the final system. The final system was implemented as part of the Nexus distributed operating system, and this paper was published in 1998 by John Wiley & Sons, Ltd. 
411573	4115737	Exception Handling in Object Oriented Systems	Exception handling is a difficult problem in object oriented system design due to the increasing complexity of software systems. Different application systems require specific models for handling exceptions, and factors such as concurrency, distribution, and code mobility add new challenges to this area. To ensure robust and reliable operation, it is important to integrate exception handling mechanisms based on well-founded principles and formal models. This should be considered from the beginning of the design process and throughout all stages, including design specification, implementation, operation, maintenance, and evolution. A workshop was held to address the research issues in this field and develop a shared understanding of the current and future research directions. 
411574	4115740	Dimensional Affect Recognition from HRV: an Approach Based on Supervised SOM and ELM	Dimensional affect recognition is a complex area that has yet to achieve the necessary accuracy for effective use in human-computer interaction (HCI) applications. In this study, two new methods are proposed to address this issue. The first method is a self-organizing model that learns from the similarity between features and affects and produces a graphical representation of the multidimensional data. The second method uses extreme learning machines, which only require heart rate variability data recorded by a small set of sensors to minimize intrusiveness. These methods were evaluated using two datasets and outperformed existing models on a classification task and dimensional affect estimation. The results demonstrate the potential for these models to improve affect analysis and inform future HCI applications.
411574	41157431	Network Analysis Improves Interpretation of Affective Physiological Data	Understanding how individuals respond physiologically to emotional stimuli is important for the fields of Affective Psychophysiology and Affective Computing. Network analysis is used to model this variation by analyzing individuals' galvanic skin responses (GSRs) to emotionally charged images. Each image is represented as a network, with individuals as nodes and connections between them if their GSRs are similar. Different network inference and clustering techniques are used to group images with similar network structures. These network-based image partitions are compared to a known arousal/valence-based partition, showing that network analysis accurately captures the underlying signal and provides more semantically meaningful results. This suggests that network analysis can improve the interpretation of affective physiological data. 
411575	41157511	Three-Receiver Broadcast Channels With Common and Confidential Messages	This paper presents a study on the secrecy capacity regions of a three-receiver broadcast channel with one common and one confidential message. Two scenarios are considered: sending the confidential message to two receivers while keeping it secret from the third, and sending it to one receiver while keeping it secret from the other two. The authors propose achieving the secrecy capacity using indirect decoding, Wyner wiretap channel coding, and a novel method of generating secrecy from a publicly available codebook. The inner bounds are shown to be tight for specific types of channels and noise levels. The study provides insights and potential solutions for secure communication in complex broadcast channels.
411575	41157517	The capacity of a class of 3-receiver broadcast channels with degraded message sets	Korner and Marton's capacity region for the 2-receiver broadcast channel with degraded message sets has been widely accepted as optimal. However, recent research and conjectures have proposed that this region can be extended to more than 2 receivers. This paper challenges this idea and establishes the capacity region for a specific class of 3-receiver broadcast channels with 2 degraded message sets. It is shown that this region can be larger than the straightforward extension of the Korner-Marton region. The paper introduces the concept of indirect decoding, where receivers can decode satellite codewords to indirectly find the cloud center. This idea is then used to establish new bounds for the capacity region of the general 3-receiver broadcast channel with 2 and 3 degraded message sets. The results indicate that determining the capacity of the 3-receiver broadcast channel with degraded message sets is at least as difficult as finding the capacity of the general 2-receiver broadcast channel with common and private messages.
411576	41157683	Design of an efficient VLSI architecture for non-linear spatial warping of wide-angle camera images	Endoscopic images can be distorted due to the wide-angle camera lenses, making it difficult to use them for diagnostics. This paper presents a digital architecture that can correct the distortion in real-time. The technique uses a polynomial mapping model based on least-squares estimation to map the distorted images onto a corrected image space. Experiments were conducted to test the algorithm, and a hardware implementation was developed using linear processing modules. The architecture was simulated and showed a computation time of 1.8 ms for a 256x192 pixel image, making it suitable for real-time applications in endoscopy systems.
411576	41157628	A Fully Pipelined Architecture for Barrel-Distortion Correction Based on Back Mapping, Linear Interpolation	This paper presents a high-speed architecture for correcting barrel distortion in wide-angle camera images in real-time. The method uses a least-squares estimation model to correct the non-linear distortion, with parameters including the image sizes, mapping coefficients, and distortion and corrected centers. The design, based on CORDIC hardware, can handle 8-bit input images up to 2056 x 2056 pixels and operates at a clock frequency of 33 MHz, producing corrected images at a rate of 30 frames per second. This is 4 times faster than previous designs. The proposed VLSI system can be integrated with the camera unit for efficient distortion correction.
411577	41157793	Retrieval effectiveness of an ontology-based model for information selection	Technology in digital media produces a vast amount of information including audio, video, and images, in addition to traditional text. Retrieving this information efficiently and accurately is challenging, and the traditional method of using keywords often results in either irrelevant information being included or relevant information being overlooked. To overcome this issue, a concept-based model using domain-specific ontologies was developed. This model indexes documents based on their meaning and context rather than just keywords, and a scalable disambiguation algorithm helps to filter out irrelevant concepts. Additionally, an automatic query expansion mechanism is proposed, which utilizes knowledge from the ontology to generate relevant queries. This model has been successfully tested on audio data and has shown to significantly improve both precision and recall compared to keyword-based search. These techniques can be applied to all types of media for efficient information retrieval.
411577	41157742	Extending metadata definitions by automatically extracting and organizing glossary definitions	This paper discusses the importance of metadata descriptions for databases in order to efficiently access and deliver data. When multiple databases with different metadata formats are combined, integration of the metadata is necessary and can be a time-consuming task. The paper presents a method for faster integration of new metadata by utilizing web pages with natural language glossaries. The demonstration will showcase this method by having the user interact with the system to retrieve data and then introducing a new topic that is not yet in the database. The system will then automatically acquire and place the new metadata concept, which can be verified and examined by the user. This demo will showcase the latest work by the authors and highlight the effectiveness of their system for integrating metadata.
411578	41157848	Update Conscious Bitmap Indices	Bitmap indices are widely used in data warehousing and scientific applications because they are efficient in answering certain types of queries on large data sets. However, their use is limited to read-only data or static snapshots due to the cost of updating or adding new data. Each attribute in a table has multiple bitmaps associated with it, making the insertion of a single record very expensive. To address this issue, a new type of bitmap index called "update conscious" has been proposed. This index only updates the relevant bitmaps for a new record insertion, reducing the cost and improving performance. A cost model is provided to compare it with traditional bitmaps in terms of storage, update time, and query execution.
411578	41157820	A general framework for modeling and processing optimization queries	An optimization query is used to find data objects that maximize or minimize a function over a data set. A new type of optimization query, called model-based optimization, uses a generic model to define various types of queries involving an optimization objective function and constraints on attributes. This model can handle linear and nonlinear expressions as well as other types of queries studied in database research. A significant subset of this model deals with convex optimization, which is efficiently processed using a unified framework that accesses data and space partitioning index structures. Experiments show that this technique is effective and versatile compared to specialized optimization queries, providing optimal solutions and handling constraints.
411579	41157952	A survey on ear biometrics	Ear recognition has become a popular topic in recent literature due to its advantages over other non-contact biometric methods, such as face recognition. It is particularly useful for multi-pose face recognition and can be used in surveillance videos where the face may be obstructed. The ear also shows minimal signs of aging, making it a reliable method for human recognition. However, current ear detection and recognition systems are limited to controlled indoor conditions. Challenges such as variations in lighting, hair obstruction, and ear individuality still need to be addressed. This article provides a comprehensive overview of ear detection and recognition research, discussing current advancements and potential areas for further development. It also highlights available databases for researchers to use in their studies.
411579	41157976	Automatic facial makeup detection with application in face recognition	The use of facial makeup can alter a person's appearance, which can cause problems for automated face recognition systems and methods for estimating age and beauty. To address this issue, a method has been developed to automatically detect the presence of makeup in face images. This algorithm takes into account the shape, texture, and color characteristics of the face, as well as specific areas like the eyes and mouth. Experiments on two datasets show that this approach can achieve a makeup detection rate of up to 93.5% with a low false positive rate. Additionally, a pre-processing technique is proposed to improve the accuracy of face matching when makeup is present.
411580	41158053	Examining the robustness of sensor-based statistical models of human interruptibility	The paper discusses the limitations and potential solutions for current systems that often cause socially awkward interruptions or demand attention when a person is busy. Previous studies have explored using sensors and statistical models to estimate human interruptibility in an office setting, but there are concerns about the robustness of this approach. The paper presents findings that show sensors can be constructed with enough accuracy to drive predictive models, and that these models can be applied to a wider range of people. The effects of training data quantity on accuracy are also examined, along with the tradeoffs of using different sensor combinations. Overall, the paper suggests that incorporating these models into systems can improve human-computer interaction and communication.
411580	4115804	Presence versus availability: the design and evaluation of a context-aware communication client	Electronic communication is essential in today's workplace, but it can also be disruptive when used at the wrong time. Previous research suggests that signaling when someone is busy can help prevent interruptions and allow people to wait until they are available to communicate. A new communication client has been developed that uses laptop microphones to detect nearby speech and combines this with location, computer, and calendar data to model availability. In a 4-week study with 26 participants, the use of this additional context was examined. It was found that people mainly used this information as an indication of presence rather than availability, raising the question of whether indicating unavailability will actually reduce interruptions. This study is the first to look at how people use automatically sensed context and availability information to make communication decisions in a real-world setting.
411581	41158146	Configuration analysis and recommendation: Case studies in IPv6 networks.	This paper focuses on studying the configurations of IPv6 networks and compares them to the more well-researched configurations of IPv4 networks. The study was conducted on two networks - a pure IPv6 network and a dual-stack network. The authors found that IPv6 configurations are more complex due to the complexity of IPv6 addresses, but have fewer command lines and less reliance on references compared to IPv4. The rapid development of IPv6 has led to a higher growth rate in configurations. The authors also propose a framework for network configuration recommendation based on their analysis. Understanding and managing configurations is crucial for the development of IPv6 networks.
411581	41158127	Towards Centralized And Semi-Automatic Vlan Management	Virtual local area networks (VLANs) are commonly used in enterprises, campuses, and data-centre networks to restrict broadcast domains and organize hosts into separate networks. However, managing VLANs can be a challenging and error-prone task. To address this issue, a centralized and semi-automatic system for VLAN management (CSS-VM) has been designed and implemented. This system utilizes information about the physical network topology and user groups to determine the number of VLANs needed and can monitor device and link status. It also has the ability to calculate an optimal spanning tree for each affected VLAN in the event of a link or device failure. Evaluation of CSS-VM on an operational enterprise network showed its effectiveness in reducing broadcast traffic, efficiently configuring VLANs, and quickly responding to failures while utilizing network resources effectively. 
411582	41158230	A Channel-based Coordination Model for Components	This paper presents a coordination model for component-based software systems using mobile channels, which allow for anonymous and dynamic communication between components. The model supports mobile components and promotes efficient interaction between them. It also separates the computational and coordination aspects of a system, allowing for transparent and external development of the coordination structure. The Java implementation of this model is self-contained and suitable for developing component-based systems in object-oriented languages. However, it can also be extended to incorporate other aspects of components beyond composition and coordination.
411582	4115822	Interacting Via The Heap In The Presence Of Recursion	Modern imperative programming languages have operations for manipulating the heap, such as allocating and deallocating objects and updating reference fields. However, in the presence of recursive procedures and local variables, the interactions with the heap can become complex as an unlimited number of objects can be allocated on the call stack or the heap. This makes it difficult to statically analyze the behavior of the program, as it becomes undecidable. This paper focuses on verifying recursive programs with unbounded object allocation in a simple imperative language. The authors present a precise abstraction that represents the behavior of the program, even though an unlimited number of objects can appear in the state. This makes model checking decidable for programs with a bounded visible heap. Additionally, a specification language for temporal properties of the heap is introduced, and the authors discuss how to use model checking to verify these properties against heap-manipulating programs.
411583	4115839	Load balancing in MapReduce environments for data intensive applications.	Distributed computations are a common method for processing large scale tasks in modern technology. The Hadoop framework, based on the Google MapReduce model, has gained popularity for its high processing power and user-friendly nature. However, in a heterogeneous computing environment, the lack of load management can lead to a decline in performance. To address this issue, a load balancing algorithm has been developed and tested using the Hadoop simulator HSim. The results show a significant improvement in cluster performance with the implementation of this algorithm. This paper discusses the details of the algorithm and its successful impact on enhancing the performance of the Hadoop framework.
411583	41158356	A MapReduce based distributed LSI	Latent Semantic Indexing (LSI) is a popular text mining technology that is effective in handling issues such as synonymy and polysemy. However, the computational intensity of LSI can be a challenge when dealing with large amounts of data. To address this, the use of multiple computing nodes through a MapReduce based distributed LSI using Hadoop architecture is proposed. This approach involves using the K-means algorithm to cluster documents and then applying LSI on the clustered results. The performance of this method is compared to standalone LSI, and the results show a significant improvement in terms of speed.
411584	41158417	Self-similarity in World Wide Web traffic: evidence and possible causes	The concept of self-similarity has been applied to both wide-area and local-area network traffic. In this paper, the authors examine the reasons behind the self-similarity of network traffic. They propose a possible explanation for this phenomenon by studying a subset of wide area traffic - traffic from the World Wide Web (WWW). Using data from over half a million requests for WWW documents, they analyze the dependence structure of WWW traffic and find evidence that it exhibits characteristics of self-similarity. This is attributed to factors such as the distribution of document sizes, caching and user preferences, user "think time", and the overlapping of multiple transfers in a local network. The authors support their findings with empirical data from their own traces and from other sources.
411584	41158454	Diagnosing network-wide traffic anomalies	This paper discusses the issue of anomalies in network traffic and the importance of diagnosing them for network operators and end users. The difficulty of this task is due to the large amount of data and the need to extract and interpret patterns. The authors propose a method that uses Principal Component Analysis to separate the high-dimensional space of network traffic measurements into normal and anomalous subspaces. They demonstrate the effectiveness of this method in detecting, identifying, and quantifying volume anomalies using simple traffic measurements from links. The method is evaluated on real traffic from two backbone networks and consistently performs well with a low false alarm rate. 
411585	4115857	Optimizing Queries with Universal Quantification in Object-Oriented and Object-Relational Databases	This study focuses on optimizing and evaluating queries with universal quantification in object-oriented and object-relational data models. The queries are classified into 16 categories based on referenced variables. The authors propose new query evaluation plans, utilizing anti-semijoin, division, generalized grouping with count aggregation, and set difference. Performance analysis was conducted on various database configurations, showing that anti-semijoin-based plans were the most effective, even when using advanced division algorithms. Additionally, the study found that anti-semijoin plans can be derived in object-oriented models where they are not possible in relational models. This highlights the benefits of leveraging object-oriented features for query optimization.
411585	41158524	On the Complexity of Generating Optimal Left-Deep Processing Trees with Cross Products	The task of producing optimal left-deep trees is considered difficult for general join graphs and a complex cost function. However, a dynamic programming approach can be used, with a known number of alternatives. Certain cost functions can be solved in polynomial time for acyclic query graphs, but others, such as sort merge, have not been addressed. The paper presents a method for optimizing intermediate result sizes, which has the ASI property and motivates focusing on the cost function called C-out. The paper then explores the complexity of producing left-deep trees that may contain cross products, which can result in cheaper plans. It is shown that this problem is NP-complete for star queries, but an algorithm is presented which is more efficient than dynamic programming. This result also implies the NP-completeness for general tree queries.
411586	41158678	The wave kernel signature: A quantum mechanical approach to shape analysis	The Wave Kernel Signature (WKS) is a method for characterizing points on non-rigid three-dimensional shapes. It measures the average probability of finding a quantum mechanical particle at a specific location, with varying energy levels. This allows for separation of information from different Laplace eigenfrequencies, making the WKS useful in various applications. The WKS is shown to be more effective in feature matching than the commonly used Heat Kernel Signature (HKS), both theoretically and in experiments. As an example, the WKS is used in shape analysis for shape matching.
411586	411586182	A simple and effective relevance-based point sampling for 3D shapes	The proposed method is a significant points sampling technique that can be applied to various tasks. Control parameters allow for fine-tuning the selection of points. It is easy to implement and fast to compute, but still effective. Experiments have shown that the sampling performs well in practical scenarios where surfaces have distinct regions with different features. Depending on the task, certain region properties can be helpful or harmful, so it is important to have a suitable surface sub-sampling. The method aims to be general enough to be useful for a wide range of tasks and it focuses on points in compact neighborhoods. Its effectiveness is demonstrated in three different shape processing scenarios.
411587	411587129	An open software architecture for virtual reality interaction	OpenTracker is an open software architecture that simplifies the development and maintenance of hardware setups for virtual reality and augmented reality applications. It uses an object-oriented design based on XML and features a data flow concept for multi-modal events. The engine is multi-threaded for optimal performance and allows for transparent network access. The developer's interface includes both time-based and event-based models for versatile use. OpenTracker aims to be a "write once, input anywhere" solution for virtual reality development and has already been integrated into an augmented reality system. It also demonstrates how consumer input devices can be used for mobile augmented reality with the help of OpenTracker. The software will be made available to the public under an open source license once it is fully developed.
411587	41158726	OpenTracker: A flexible software design for three-dimensional interaction	Tracking is an essential aspect of virtual reality and augmented reality applications. While there has been much research on improving the quality and performance of tracking, there is a lack of attention given to the software engineering aspects of tracking software. To address this issue, a software design and implementation using the pipes-and-filter architectural pattern has been developed. This allows for customizable and flexible handling of tracking data and configurations. The result of this work is the creation of a data flow network library called OpenTracker, specifically designed for tracking data. The flexibility of this approach is showcased through various development scenarios and prototype applications in the realm of mobile augmented reality.
411588	41158845	Statistical sampling of microarchitecture simulation	Current microarchitecture simulators are significantly slower than the hardware they simulate, leading to inaccurate and misleading conclusions in microarchitecture design studies. This article introduces the Sampling Microarchitecture Simulation (SMARTS) framework, which allows for fast and accurate performance measurements of full-length benchmarks by selectively measuring a subset of instructions. Through analysis of the SPEC CPU2000 benchmark suite, it is shown that SMARTS can estimate CPI and energy per instruction (EPI) with 99.7&percnt; confidence by measuring fewer than 50 million instructions per benchmark. Despite some uncertainty in microarchitectural state initialization, SMARTS achieves an average error of only 0.64&percnt; on CPI and 0.59&percnt; on EPI, with significant speedups compared to detailed simulation.
411588	41158848	TurboSMARTS: accurate microarchitecture simulation sampling in minutes	Recent research suggests a new method for speeding up processor microarchitecture simulation by using statistical sampling. This approach, called functional warming, involves continuously warming large microarchitectural structures while emulating the instructions between measurements. However, this process takes hours to warm up the structures, while the actual simulation only takes minutes. To address this bottleneck, a new framework called TurboSMARTS is proposed, which stores functionally-warmed state in small, reusable checkpoints. This allows for the creation of thousands of checkpoints, resulting in an accurate simulation with a minimal amount of warmed state. TurboSMARTS has been shown to match the accuracy of previous techniques and can estimate the performance of an 8-way out-of-order super-scalar processor running SPEC CPU2000 in just 91 seconds per benchmark, using a 12 GB checkpoint library.
411589	41158943	Exploiting reference idempotency to reduce speculative storage overflow	Recent proposals for multithreaded architectures use speculative execution to allow threads to run in parallel, even if their dependencies are unknown. This is achieved through hardware speculative storage that tracks data dependencies and corrects incorrect executions through roll-backs. However, the use of small memory structures for speculative storage results in performance loss due to overflow when a thread's speculative state exceeds the storage capacity. To address this issue, the article introduces the concept of memory reference idempotency, which identifies references that can be eventually corrected and do not need to be tracked in speculative storage. The article presents a formal framework, a compiler-assisted model and an algorithm to label idempotent memory references for the hardware. Experimental results show that a large number of references in nonparallelizable program sections are idempotent, reducing the demand for speculative storage space in larger threads.
411589	41158953	Reference idempotency analysis: a framework for optimizing speculative execution	Recent proposals for multithreaded architectures have suggested allowing threads with unknown dependencies to execute speculatively in parallel. These architectures use hardware speculative storage to buffer uncertain data and track data dependences, and have small memory structures for fast access. However, the limited capacity of this storage can lead to performance loss when threads exceed the storage capacity. In this paper, a new program property called memory reference idempotency is introduced. This property allows for certain memory references to directly access non-speculative storage, reducing the demand for speculative storage space. A formal framework for reference idempotency is defined and a new compiler-assisted speculative execution model is presented. Experimental results show that over 60% of memory references in non-parallelizable program sections are idempotent.
411590	41159027	Race detection for event-driven mobile applications	Mobile systems, like Android, use an event-based model of concurrent programming, which is supported by the devices' sensors and user input options. However, most existing tools for detecting concurrency errors focus on a thread-based model, leading to inaccuracies when applied to event-based programs. In this paper, the authors introduce CAFA, a race detection tool specifically designed for event-driven mobile systems. This tool uses a causality model that accounts for the causal order of events in the Android system, which is often ignored by other data race detectors. By checking for races between high-level operations instead of low-level memory accesses, CAFA reduces the number of false positives and has successfully identified harmful races in open-source Android applications.
411590	41159037	Can deterministic replay be an enabling tool for mobile computing?	Deterministic record and replay technology is becoming increasingly important in desktop and server computing, but its potential for use on mobile devices like cell phones has not been fully explored. This paper discusses the various ways in which replay can be beneficial for mobile phones, such as using cloud or cloudlet computers to reduce latency and save energy, using operation shipping for file synchronization, and offloading security and reliability checks to remote servers. The authors also address the challenges of implementing replay on phones. This technology has the potential to greatly improve the performance and efficiency of mobile devices.
411591	41159144	The future is cloudy: Reflecting prediction error in mobile applications	Mobile applications rely on predicting the future to make decisions in the present, but these predictions are often uncertain. However, applications typically assume they are completely accurate, which can lead to incorrect decisions and negative consequences. To address this issue, prediction error should be considered as a fundamental aspect in mobile systems. This can be achieved by taking into account uncertainty when evaluating alternatives and using redundant strategies when there is no clear superior option. A system has been developed to quantify uncertainty and allow applications to balance the tradeoff between redundancy and resource usage. This approach has been shown to significantly improve application performance compared to other strategies.
411591	41159126	Bobtail: avoiding long tails in the cloud	Highly modular data center applications, such as Bing, Facebook, and Amazon's retail platform, are prone to long response times. Amazon's EC2 has become a popular platform for building similar applications, but virtualization used in these platforms can make the long tail problem worse. Surprisingly, poor response times in EC2 are not due to the network, but rather a widespread and persistent issue with individual nodes. This is caused by co-scheduling of tasks that require a lot of CPU and tasks that are sensitive to latency. A solution called Bobtail has been developed to detect and avoid these problematic nodes without impacting node creation. This has resulted in up to a 40% reduction in response times for common communication patterns.
411592	411592144	A formal requirements engineering method for specification, synthesis, and verification	This paper introduces a formal requirements engineering method that combines multiple established formal methods to capture specification, synthesis, and verification. The approach uses temporal logics to express abstract specifications and Statecharts to support the development of more detailed specifications. The two layers are connected through a semi-automatic synthesis process and automatic formal verification through model checking. The method is demonstrated through a detailed user session.
411592	41159248	Backtracking-Free Design Planning by Automatic Synthesis in METAFrame	The article discusses an environment that supports the creation of inflexible, application-specific design plans that prevent the occurrence of unsuccessful plans during the design phase. This is achieved through the automatic synthesis of complete and executable plans based on simple constraints and a library of tools. The user is aided in choosing the best plan through a user-friendly graphical interface and hypertext support. The application of this environment is demonstrated in the generation of design plans for hardware design in a CAD environment.
411593	4115938	Encoding Asynchronous Interactions Using Open Petri Nets	The article introduces an encoding method for representing asynchronous CCS processes with replication using open Petri nets. In this method, ordinary Petri nets are used with a designated set of open places to model the interactions between processes and their environment. The encoding is able to maintain the strong and weak bisimilarities of CCS, allowing for a precise correspondence between the two formalisms. The goal of this work is to promote the transfer of technology between these two methods, and it is shown how results on expressiveness can be transferred back and forth between the calculus and the nets.
411593	41159342	Modular encoding of synchronous and asynchronous interactions using open Petri nets	The paper examines the relationship between process calculi and Petri nets, two commonly used approaches for modelling concurrent and distributed systems. A framework is proposed for encoding process calculi into Petri nets, specifically using a reactive variant of Petri nets. Two calculi, CCS and CSP, are used as examples to demonstrate the encoding process for asynchronous and synchronous communication. The proposed encoding is shown to maintain and reflect the operational semantics of the original systems. This allows for a transfer of technology between the two formalisms, resulting in insights into the decidability of properties such as reachability and deadlock-freedom.
411594	41159431	The effects of asymmetry on TCP performance	This paper examines the impact of network asymmetry on TCP performance and suggests methods to enhance it. The study focuses on two types of networks commonly found in mobile ad hoc networks - a wireless cable modem network and a packet radio network. Previous research has primarily looked at bandwidth asymmetry, but this study expands the concept to include other forms of asymmetry such as latency, media-access, and packet error rate. By conducting experiments and simulations, the researchers analyze TCP performance in these networks where the reverse direction (from receiver to sender) plays a significant role in achieving desired throughput. They propose and evaluate various techniques, including ack congestion control, ack filtering, TCP sender adaptation, and acks-first scheduling at the reverse bottleneck router, to improve end-to-end performance. 
411594	41159426	Bandwidth estimation in broadband access networks	This paper discusses the challenges of estimating network capacity and available bandwidth in broadband access networks, such as cable modem and 802.11-based wireless networks. Previous techniques have focused on point-to-point links with defined bandwidth and FIFO packet ordering, but these broadband networks break this model in various ways. They may use rate regulation, non-FIFO packet scheduling, and support multiple rates. The paper presents a new method, Probe-Gap, for estimating available bandwidth in these networks, and evaluates it through experiments on 802.11a and cable modem links. This technique overcomes some of the difficulties posed by the unique characteristics of broadband access networks.
411595	41159543	Chimpp: a click-based programming and simulation environment for reconfigurable networking hardware	Reconfigurable network hardware offers a convenient way to experiment with and prototype high-speed networking systems, but it can be difficult to program and requires co-development with software. To address this issue, Chimpp is a development environment modeled after the Click modular router system. It uses a modular approach and a simple configuration language, similar to Click, to design hardware-based packet-processing systems. Chimpp can be combined with Click at the software layer and allows for integrated simulation with the OMNeT++ network simulator. The goal of Chimpp is to make experimentation easy by providing a toolbox of reusable, modular elements and allowing for customization and incorporation of existing hardware modules. Initial evaluations show that it simplifies the implementation, simulation, and modification of packet-processing systems on the NetFPGA platform.
411595	41159529	Condor: Better Topologies Through Declarative Design	Designing large and complex datacenter networks requires trade-offs between cost, reliability, and maintainability. Architects often have limited exploration of the design space. To address this, we introduce Condor, which uses a Topology Description Language (TDL) to express requirements and generate candidate topologies quickly through constraint-based synthesis. TDL can describe various topologies, including fat-trees, BCube, and DCell. Simple changes to the TDL file can generate both known and novel variants of fat-trees. Condor can also handle the challenging task of designing multi-phase network expansions on live networks. The approach allows for a rapid and efficient design cycle, enabling architects to create cost-effective, reliable, and maintainable networks.
411596	41159657	Improved Scene Reconstruction From Range Images	Modeling real scenes is difficult and using laser rangefinders is a popular method. However, it is often not feasible to capture all surfaces in a scene due to occlusions and accessibility limitations. This can result in incomplete or inaccurate models. The paper proposes a pipeline and system that can enhance model reconstruction using incomplete range images. This approach aims to improve the overall quality of the reconstructed models by utilizing available information effectively. The system implementation provides a practical solution for dealing with incomplete data, making it a valuable tool for modeling real scenes.
411596	41159615	Modeling and Rendering of Real Environments	The use of detailed geometric models is crucial for creating realistic computer graphics. There has been an increasing demand for accurate representations of real scenes in applications such as virtual environments. This technology has potential uses in entertainment, training, simulation, special effects, forensic analysis, and remote walkthroughs. However, traditional modeling techniques are not suitable for creating models of real scenes. Therefore, image-based techniques, particularly the combination of laser rangefinders and color images, have been proposed as a promising approach due to their independence of sampled geometry and quick acquisition time. To achieve photorealism in renderings of these virtual environments, challenges must be addressed. This tutorial provides an overview of the main issues and techniques for modeling and rendering real environments using laser rangefinders.
411597	41159732	Turing Trade: A Hybrid of a Turing Test and a Prediction Market	Turing Trade is a web-based game that combines elements of a Turing test and a prediction market. The game involves a mystery conversation partner, known as the "target," who is either a human or a bot. Multiple judges participate in the game by interrogating the target and placing bets on whether it is a human or a bot. The game collects detailed data on the judges' changing beliefs throughout the conversation, providing insights into specific moments where the target's response influenced their belief. This game offers advantages over traditional Turing tests, such as more precise data and a more enjoyable experience for participants. This paper describes how Turing Trade works, provides example logs, and analyzes its effectiveness through real user data. 
411597	4115976	Using a Memory Test to Limit a User to One Account	The paper discusses the issue of users creating multiple accounts on Web-based applications using false names and the negative effects it has on system performance. The traditional method of using CAPTCHAs to prevent automated account creation does not work for human users. The proposed solution is to use a memory test where users are asked to recall colors associated with items, which are randomly redrawn each time. If there is a significant correlation between the user's answers and previous correct answers, it can be determined that the user is the same and another account should not be granted. A small study with human subjects was conducted and a game-theoretic analysis was also presented. An alternative test was proposed in the appendix, but the results were not favorable. 
411598	41159897	A new approach to countering ambiguity attacks	Watermarking schemes for resolving ownership disputes are vulnerable to ambiguity attacks, which exploit the high false-positive rate of the scheme. To address this issue, a new scheme is proposed that embeds multiple watermarks and detects a randomly selected subset during ownership proof. This is achieved through the use of one-way functions for watermark generation and selective detection to inject uncertainty. The false-positive probability is reduced compared to single watermark embedding, as shown through numerical analysis. The security level of the proposed scheme is also examined with different parameters, using a modified notion of security that is more practical. This allows for a better understanding of the achievable security with typical parameters. 
411598	41159843	Cryptanalysis of the Yeung - Mintzer fragile watermarking technique.	The increase in digital multimedia content has sparked concerns about how to verify its authenticity. Various methods using digital watermarks have been proposed, but a study of the Yeung-Mintzer technique found it vulnerable to impersonation and substitution attacks. These attacks allow an attacker to create or modify images that appear authentic, and can be executed without knowledge of the secret watermark insertion function. The first attack can reconstruct the watermark and insertion function in a short amount of time, while the second attack, called the "collage attack," produces high-quality counterfeits using a dithering process. These findings were reported in a 2002 paper by SPIE and IST.
411599	41159983	Evaluating Human-Robot Interaction Focusing On The Holistic Interaction Experience	The paper discusses the challenges in designing and evaluating human-robot interaction (HRI) due to the unique experience of interacting with a robot. Unlike other technologies and artifacts, robots have a strong social and emotional component, which is influenced by their dynamic presence in the real world and their ability to invoke a sense of agency. This complexity of interaction requires careful consideration when applying HCI evaluation methods to HRI. The paper proposes a holistic view of social interaction with robots and introduces three perspectives for exploring this interaction: visceral factors, social mechanics, and social structures. The authors also present a heuristic for brainstorming different types of interaction experiences, called the interaction experience map. These perspectives and heuristic can be used to guide the evaluation process of HRI.
411599	41159964	You're capped: understanding the effects of bandwidth caps on broadband use in the home	Bandwidth caps are limits on the amount of data that can be used for home and mobile Internet access. Each bit of data used counts towards a monthly quota or running tab, making it important for users to monitor their usage. However, there has been little research on how this usage-based pricing affects the user experience. This paper presents findings from a study on households with bandwidth caps, showing that users struggle with three uncertainties: not knowing their balance, not understanding the processes behind the caps, and multiple users sharing the same quota. The authors suggest the need for better tools to manage and monitor data caps for users under cost constraints.
411600	41160042	The complexity of the free space for a robot moving amidst fat obstacles	The article introduces a new definition of fatness for geometric objects and compares it to other definitions. It demonstrates that, based on certain assumptions, the complexity of free space for a robot with a set number of degrees of freedom moving in a d-dimensional Euclidean workspace with fat obstacles is directly related to the number of obstacles present. This has implications for the complexity of motion planning algorithms, which can be very high in theory. The findings suggest the potential for developing efficient motion planning algorithms in realistic situations.
411600	4116008	Efficient Algorithms for Exact Motion Planning Amidst Fat Obstacles.	The level of difficulty for exact motion planning algorithms varies based on the complexity of the robot's free space, which refers to the area where the robot can move without colliding with obstacles. Theoretically, the free space can be very complex, but in reality, it is usually much simpler. Recent research has shown that the free space of a robot surrounded by large obstacles can be characterized as linear, meaning that the number of obstacles directly affects the complexity. This discovery has important implications for the development and implementation of motion planning algorithms.
411601	41160129	Competitive on-line coverage of grid environments by a mobile robot	The paper discusses two algorithms, called Spanning Tree Covering (STC), for covering planar areas using a square-shaped tool attached to a mobile robot. The algorithms divide the area into a grid of cells and use a spanning tree of a grid graph to guide the robot's movements. The first algorithm, Spiral-STC, creates spiral-like patterns while the second, Scan-STC, creates scan-like patterns along a specific direction. Both algorithms can cover any planar grid using a path no longer than (n + m)D, where n is the total number of cells and m is the number of boundary cells. The paper also shows that any on-line coverage algorithm will have a path length of at least (2 - ε)lopt, where lopt is the length of the optimal off-line path. The STC algorithms are worst-case optimal and generate close-to-optimal paths in practical environments.
411601	41160115	Spiral-STC: an on-line coverage algorithm of grid environments by a mobile robot	The authors present an algorithm, called Spiral-STC, for covering planar areas with a square-shaped tool attached to a mobile robot. The algorithm divides the work-area into disjoint cells of size D, following a spanning tree of the resulting grid. It covers general grid environments using a path with a length of at most (n + m)D, where n is the number of D-size cells and m is the number of boundary cells. The algorithm is shown to be worst-case optimal, with a covering path length of (2 - ε)lopt, where lopt is the length of the optimal path. Simulation results demonstrate the effectiveness of the algorithm in practical environments.
411602	41160220	An adaptive method for image registration	The paper discusses a new technique for registering images with non-linear local geometric distortions. This involves dividing the image into smaller subregions based on the type of distortion and applying a simple local transformation to each subregion. This method allows for registration with varying levels of accuracy and has been compared to other existing methods in terms of accuracy and computational complexity. A practical application of this method is also demonstrated.
411602	41160252	On the independence of rotation moment invariants	This paper focuses on the issue of independence and completeness of rotation moment invariants. It introduces a method for creating invariants of any order using complex moments. The paper also highlights a significant finding that there is a small basis that can generate all other invariants. The construction of this basis and its independence and completeness are explained. The authors also mention some practical implications of these results.
411603	41160313	Fair Division under Ordinal Preferences: Computing Envy-Free Allocations of Indivisible Goods	This article discusses the issue of dividing a set of goods fairly among a group of individuals. The problem arises when the individuals have preferences for different bundles of goods, but this information is incomplete and expressed in a compact language. The article explores the algorithmic problem of determining if an allocation can be made that satisfies envy-freeness and economic efficiency, given the limited preference information. The authors present simple characterizations and algorithms for some cases, while also showing the complexity of the problem in other instances. 
411603	4116035	First-Order Logic Formalisation of Impossibility Theorems in Preference Aggregation.	Preference aggregation is the process of combining individual preferences into a collective preference. This is done through social welfare functions, which are used to aggregate preferences expressed as orders. According to classical results in social choice theory, it is impossible to aggregate preferences under certain conditions. To address this issue, a first-order language has been defined for social welfare functions and a complete axiomatisation has been provided. This language allows for the expression of axiomatic requirements and the formal derivation of classical theorems, such as those by Arrow, Sen, and Kirman and Sondermann. While positive results have been obtained for Sen's and Kirman-Sondermann's theorems, an additional axiom is needed for Arrow's theorem in cases of infinite societies. This approach aims to eventually enable a fully automated proof of both classical and new theorems in social choice theory.
411604	41160447	Automatic Verification of Safety and Liveness for XScale-Like Processor Models Using WEB Refinements	This article discusses the use of Well-founded Equivalence Bisimulation (WEB) refinement to automatically verify that complex pipelined machine models, similar to the XScale architecture, adhere to the same safety and liveness properties as their corresponding instruction set architecture models. This is achieved by converting the WEB-refinement proof obligation into a formula in the logic of Counter arithmetic with Lambda expressions and Uninterpreted functions (CLU), which is then checked using a SAT solver. The models being verified include advanced features such as out of order completion, precise exceptions, branch prediction, and interrupts. Two types of refinement maps are used for this verification process: flushing and commitment. Experimental results and verification times are also presented, with liveness being found to account for approximately 5% of the overall verification time.
411604	41160432	A Refinement-Based Compositional Reasoning Framework for Pipelined Machine Verification	Our study introduces a refinement-based compositional framework that can demonstrate the safety and liveness of pipelined machines compared to their non-pipelined counterparts. This framework includes a set of easy-to-use and comprehensive proof rules. We demonstrate its effectiveness in verifying both abstract and executable microprocessor models. Our approach allows for the verification of complex machine models that go beyond the capabilities of current automated decision procedures. For instance, we were able to verify a 32-bit, 10-stage, executable pipelined machine model using our framework. Moreover, our compositional framework offers significant improvements in design debugging compared to traditional monolithic approaches, as it isolates bugs and generates smaller counterexamples.
411605	4116054	A distributed index for efficient parallel top-k keyword search on massive graphs	In this paper, the authors propose a new distributed disk-based index for optimizing keyword search on graphs. This index can be constructed in a MapReduce manner and utilizes heuristics to track and prune irrelevant vertices before search. Additionally, a parallel search algorithm is developed, which runs multiple asynchronous search instances to incrementally find the top-k answers. Experiments on synthetic and real graphs demonstrate that this approach significantly improves search efficiency on massive graphs while keeping indexing overheads affordable.
411605	41160524	Efficient keyword proximity search using a frontier-reduce strategy based on d-distance graph index	 The current keyword proximity search methods for general graphs are not efficient in reducing the search space, which leads to low efficiency when dealing with large search spaces. To address this issue, the authors propose a new approach that uses a best-effort frontier-reduce strategy to find a set of subgraphs containing the best answers. This allows for a more efficient search by focusing on smaller subgraphs. To enable this strategy, the authors define a d-distance subgraph with a size limit and create a new index structure that maps keywords, vertices, and subgraphs. They also develop an algorithm to find the top-k answers while overcoming the subgraph overlap problem. Experiments show that this approach is more effective and efficient than existing methods. 
411606	411606121	Word sense disambiguation with pictures	This article discusses the use of images for word sense disambiguation, either alone or in combination with traditional text-based methods. The approach is based on a statistical model that automatically annotates images using a joint probability for image regions and words. The model is learned from a database of images with associated text. The predicted words are constrained to be possible senses for the word being considered, making the word prediction more reliable. The article reports on experiments using this method, both on its own and in conjunction with a text-based disambiguation algorithm. A new corpus, ImCor, was created to evaluate the effectiveness of this approach, using a portion of the Corel image dataset with disambiguated text from the SemCor corpus. The results suggest that incorporating visual information can greatly aid in disambiguating word senses and help to ground language meaning.
411606	411606128	Probabilistic Methods for Finding People	The task of finding people in pictures is challenging for object recognition. To overcome this difficulty, a method is proposed that involves identifying segments of the body and then assembling them based on constraints determined by kinematic properties. However, due to the complexity of the problem, it is not feasible to inspect every possible group. To address this, two approaches are suggested. The first involves using a classifier to prune the search, while the second utilizes a probabilistic framework to draw samples of human-like assemblies. Although segmentation remains a performance issue, both approaches have shown promising results in identifying people in real images.
411607	41160714	Binary and Multi-Class Learning Based Low Complexity Optimization for HEVC Encoding.	HEVC is a video coding technology that improves compression efficiency but requires high computational complexity. To address this issue, a fast HEVC encoding algorithm is proposed in this paper. It uses binary and multi-class support vector machine (SVM) to optimize the processes of recursive CU decision and PU selection, reducing the need for intensive rate distortion (RD) cost calculation. The algorithm also incorporates a learning method and optimal parameters determination scheme for better prediction performance. Experimental results show that it outperforms previous fast coding algorithms in terms of time-saving and RD performance. 
411607	411607190	Machine learning based fast H.264/AVC to HEVC transcoding exploiting block partition similarity.	This paper discusses the upper bound of complexity reduction for transcoding from H.264 to HEVC, a process of converting compressed video streams. The authors propose a machine learning-based approach that considers the similarities between the two formats, specifically the block partition correlations, to reduce computational complexity. They use three-level binary classifiers to predict quad-tree Coding Unit (CU) partition in HEVC and introduce a feature selection algorithm and adaptive probability threshold determination scheme to improve prediction accuracy and achieve a balance between complexity and compression efficiency. The proposed method is shown to achieve an average complexity reduction of 50.2% and 49.2% under different configurations, with minimal rate-distortion degradation, making it more effective than current benchmarks. 
411608	41160813	Multi-modal learning for affective content analysis in movies	Affective content analysis is an important topic in video content analysis with various applications. However, designing a computational model for predicting emotions induced by videos is challenging due to the subjectivity of emotions. To address this, a multi-modal learning framework is proposed that uses motion keypoint trajectory and convolutional neural networks to depict visual emotions, and a global audio feature from the openSMILE toolkit for audio emotions. Linear support vector machines and support vector regression are used to learn affective models. Comparing these features to five baseline features, it is found that they are significant in describing affective content and complement each other. The proposed framework also achieves state-of-the-art results on two challenging datasets.
411608	41160822	Joint Compression of Near-Duplicate Videos.	With the rise of social media and multimedia technologies, more and more people are choosing to store and share information in visual formats like images and videos. However, this convenience comes at a cost for traditional video servers, which are at risk of becoming overloaded. One major issue contributing to this problem is the large number of near-duplicate videos (NDVs) in the vast amount of online videos. While there have been efforts to detect NDVs, little has been done to compress them more efficiently than individual video compression. In this study, the authors propose a framework for jointly compressing NDVs by exploring their data redundancy. This involves using a graph-based method to group similar videos and designing preprocessing functions to identify correlations among NDVs. Experimental results show that this approach is effective in compressing NDVs and saving storage space.
411609	411609100	Low-Complexity Adaptive Block-Size Transform Based on Extended Transforms.	This paper proposes a low-complexity Adaptive Block-size Transform (ABT) scheme for the Chinese Audio and Video coding Standard (AVS)1. The scheme uses an integer 8x8 transform derived from the 4x4 transform used in AVS, with a focus on high energy compacted property and efficient implementation. The 8x8 transform shares the same scale matrix with the 4x4 transform, leading to efficient use of hardware and storage resources for both encoder and decoder. Experimental results on various sequences demonstrate the significant performance improvement achieved by the proposed ABT scheme for AVS. 
411609	41160930	FGS Coding Using Cycle-Based Leaky Prediction Through Multiple Leaky Factors	This paper proposes a new method for fine granularity scalable (FGS) coding using cycle-based leaky prediction. The method uses multiple leaky factors to improve the prediction of enhancement layers and balance coding efficiency with drift error. The paper analyzes the error propagation of two leaky factors and investigates how to effectively incorporate enhancement-layer information into the prediction loop. A coefficient scaling approach in the transform domain is also proposed to reduce decoding complexity when reconstructing multiple partial enhancement layers. Additionally, an encoder optimization approach is introduced to further improve coding performance. Experimental results show that this method outperforms AR-FGS in JSVM at various bitrates.
411610	411610330	How many users should be turned on in a multi-antenna broadcast channel?	This paper discusses broadcast channels with a base station having L antennas and m single-antenna users. Partial channel state information is available through finite rate feedback. The optimal number of on-users (s) is found to be dependent on the signal-to-noise ratio (SNR) and feedback rate. An asymptotic analysis is used for large values of L, m, and feedback rate, yielding an optimal feedback strategy and a criterion for turning on users. The resulting spatial efficiency, defined as asymptotic throughput per antenna, is also dependent on s. A scheme is proposed for systems with finite antennas and users, achieving a significant gain compared to previous studies with a constant s. The analysis and scheme are applicable to heterogeneous systems with varying path loss coefficients and feedback rates.
411610	411610573	Optimal entropy-constrained non-uniform scalar quantizer design for low bit-rate pixel domain DVC	This paper proposes an optimal entropy-constrained non-uniform scalar quantizer for the pixel domain in distributed video coding (DVC). While the uniform quantizer is efficient for hybrid video coding, it is not optimal for DVC as it is not adaptive to the joint distribution of the source and side information (SI). The proposed quantizer is designed based on the joint distribution, leading to decreased error between the source and SI and improved R-D trade-off. The quantizer is optimized using a Lagrangian function and a Gaussian mixture model is used to approximate the conditional probability density function of the SI. An iterative Lloyd-Max algorithm with a novel quantization partition updating algorithm is used for optimization. Experimental results show a 0.5 dB average improvement compared to the uniform scalar quantization.
411611	41161152	Supremum preserving upper probabilities	This article discusses the relationship between possibility measures and the theory of imprecise probabilities. It argues that possibility measures play a significant role in this theory and shows that a possibility measure is a coherent upper probability if and only if it is normal. The differences between the possibilistic and natural extension of an upper probability are examined, and it is proven that a possibility measure can be seen as the restriction of the natural extension of a special type of upper probability. The concept of possibilistic extension is also explained and related to natural extension. The article then moves on to discuss the connection between upper probabilities and upper previsions, showing that a coherent upper prevision must take the form of a Shilkret integral associated with a possibility measure if it is supremum preserving. However, it is also shown that such a supremum preserving upper prevision is never coherent unless it is the vacuous upper prevision with respect to a non-empty subset of the universe of discourse.
411611	41161121	Independent natural extension	The authors propose a definition of independence for finite-valued variables, based on personal judgements of irrelevance. This definition, called the independent natural extension, is shown to encompass existing notions of independence. It is also proven to satisfy key properties such as marginalisation, associativity, and strong factorisation. This allows for connections to be made with more traditional approaches to defining independence. 
411612	41161239	A New Definition of the Subtype Relation	Hierarchy plays a crucial role in object-oriented design as it enables the use of type families, where higher level supertypes encompass the shared behavior of their subtypes. It is essential to have a thorough understanding of the relationship between subtypes and supertypes for this approach to be successful. A new definition of the subtype relation is proposed in this paper, ensuring that any property proven for supertype objects also applies to subtype objects. The implications of this definition on the design of type families are also discussed. 
411612	41161258	Specifications and their use in defining subtypes	This paper discusses the importance of specifications and type hierarchies in designing and reasoning about objects. It introduces a method for specifying types and highlights the need for additional information, beyond just the methods of the objects, to support reasoning. The paper also presents a new approach for demonstrating that one type is a subtype of another, utilizing the information in the type specifications. This technique is applicable in a wide range of computational environments, even when multiple users are sharing mutable objects. Overall, the paper emphasizes the crucial role of specifications and type hierarchies in facilitating effective reasoning about objects.
411613	41161390	Combining OWL ontologies using E-Connections	In this paper, the authors address two unresolved issues in the standardization of the Web Ontology Language (OWL): representing and reasoning with multiple linked ontologies, and promoting knowledge sharing and reuse on the Semantic Web. They propose using E-Connections as a solution to these problems, providing a modular approach for developing web ontologies and an alternative to the owl:imports construct. This involves a syntactic and semantic extension of OWL-DL, which can be applied in different modeling situations and optimized using reasoning algorithms for logics such as SHIN(D), SHON(D), and SHIO(D). The authors also present support for E-Connections in two tools: an ontology editor, SWOOP, and an OWL reasoner, Pellet.
411613	41161354	HTN planning for Web Service composition using SHOP2	Automated composition of Web Services can be achieved by utilizing AI planning techniques, specifically Hierarchical Task Network (HTN) planning. This paper explains how the SHOP2 HTN planning system can be applied to OWL-S Web Service descriptions. The authors provide an algorithm for translating OWL-S service descriptions to a SHOP2 domain, which has been proven to be sound and complete. A system was implemented to plan and execute OWL-S descriptions using SHOP2, while also being able to use information-providing Web Services during the planning process. The authors also discuss the challenges and complexities of using planning in the context of Web Services, which are information-rich and human-oriented.
411614	41161423	Annotating and searching web tables using entities, types and relationships	Tables are a commonly used method for displaying relational data, and billions of tables on the web contain information about entities, attributes, and relationships. These structured representations are often more useful than unstructured text. However, since web tables are not manually created, they lack a formal, uniform schema, making it difficult for search engines to utilize them effectively. In this paper, the authors propose new machine learning techniques to automatically label table cells with entities, column types, and relationships, using a graphical model. Experiments with various datasets show the effectiveness of this approach, and the authors also demonstrate the benefits of these annotations for a prototype relational web search tool. 
411614	41161452	Answering table augmentation queries from unstructured lists on the web	The article discusses the design of a system that can assemble a table using a small number of example rows from unstructured lists found on the web. The system uses an unsupervised approach that involves retrieving relevant HTML lists, segmenting the list records, mapping them to a query schema, consolidating the results, and presenting them to the user ranked by their estimated relevance. The main challenges of this task include creating new rows from few examples and dealing with noisy and irrelevant lists. The authors propose modifications to statistical models and present new techniques for consolidation and ranking that do not require human supervision. Experiments show that the system is effective at recreating Wikipedia tables, with a mean runtime of 20 seconds.
411615	41161591	Resource-Efficient Routing and Scheduling of Time-Constrained Network-on-Chip Communication	Network-on-chip-based multiprocessor systems-on-chip (NoC MPSoCs) are becoming increasingly popular as embedded systems platforms. A key aspect of mapping applications onto these parallel platforms is scheduling communication on the NoC. This paper introduces various scheduling strategies that take advantage of the flexibility of NoCs to minimize resource usage. Results from experiments demonstrate that these strategies lead to improved resource utilization compared to existing techniques. Overall, this research contributes to the development of more efficient and optimized NoC MPSoCs, which are expected to play a significant role in future embedded systems.
411615	41161589	A Predictable Multiprocessor Design Flow for Streaming Applications with Dynamic Behaviour	Embedded systems are becoming more complex as more functionality is integrated, making it crucial to have a predictable design flow to deal with this complexity. Synchronous Dataflow Graphs (SDFGs) are often used to model time-constrained streaming applications on a multiprocessor platform, providing predictability. However, the dynamic behaviour of applications is not considered, leading to overestimation of resource requirements. To address this, a design flow is presented that takes into account the dynamic behaviour and available resources. It generates a set of mappings that provide a trade-off in resource usage, which can be adapted by a run-time mechanism. Experimental results show a 66% decrease in resource requirements for an MPEG-4 decoder compared to a state-of-the-art design flow using SDFGs.
411616	41161620	Representation Learning via Semi-Supervised Autoencoder for Multi-task Learning	Multi-task learning is a method that aims to learn multiple related tasks simultaneously. There are two approaches to multi-task learning: one focuses on shared feature space for knowledge sharing, while the other focuses on sharing model parameters among tasks. However, these approaches have limitations. To address this, the paper proposes a feature representation learning framework, called Semi-Supervised Autoencoder for Multi-Task Learning (SAML), which combines the benefits of both approaches. SAML uses autoencoders to learn feature representations from unlabeled data and a regularized multi-task softmax regression method to learn distinct prediction models for each task. Experiments on real-world data sets show the effectiveness of SAML, and its feature representations can also be used by other methods for improved results. 
411616	41161655	Rebalancing Bike Sharing Systems: A Multi-source Data Smart Optimization	Bike sharing systems are becoming popular in urban cities as they aim to fill the gaps in public transportation. A key factor in their success is the effectiveness of rebalancing operations, which involves restoring the number of bikes in each station to its target value by strategically routing vehicles for pick-up and drop-off tasks. However, there are two main challenges for this bike rebalancing problem: determining the station inventory target level and optimizing the routing of multiple vehicles for large-scale systems. This paper proposes a method for predicting bike demand using a Meteorology Similarity Weighted K-Nearest-Neighbor regressor and a model for predicting inter-station bike transition. It also presents a Mixed Integer Nonlinear Programming formulation and an Adaptive Capacity Constrained K-centers Clustering algorithm to solve the routing optimization problem. Experimental results on the NYC Citi Bike system demonstrate the effectiveness of this approach.
411617	41161716	On Subjective Trust for Privacy Policy Enforcement in Cloud Computing	Cloud computing has become a crucial part of the technology ecosystem, forcing clients to interact with cloud service providers, even if they do not fully trust them. To address this issue, a policy-based approach is proposed to implement subjective trust in privacy policy enforcement within a cloud computing environment. This approach involves acknowledging each client's individual trust assessment and providing services accordingly. An abstract model is presented, consisting of computational, storage, and monitoring units with configurable elements. Algorithms are described to show how changes in trust affect the configuration of these elements. A prototype storage middleware is also developed based on this model and its performance is benchmarked.
411617	41161748	Using strategy trees in change management in clouds	Managing changes in a cloud environment can be complicated due to the different needs of the clients. Changes are not applied all at once and may be scheduled based on SLAs. This is especially important for Platform-as-a-Service instances, as different clients may have different requirements for when changes can be made. Changes are not always successful and can result in a return to a previous state, potentially impacting SLAs and resulting in penalties for the cloud provider. To mitigate this, it may be necessary to adjust the change deployment schedule based on factors such as operator skill level and change complexity. This paper proposes using strategy trees in an autonomic change management system to dynamically adjust the deployment schedule based on the cloud provider's policy sets. Experiments have shown that this approach can save time and minimize SLA violations.
411618	41161814	Policy-driven autonomic management of multi-component systems	Policies are proposed to guide the behavior of systems and applications and manage violations. In complex systems like e-commerce, different policies are used to manage individual components. The autonomic manager uses these policies to make decisions when a violation occurs. However, this can result in multiple directives, so heuristics are used to determine the most appropriate action. This study focuses on the design and implementation of an autonomic manager using these heuristics, which was tested on a dynamic web server. Results showed the effectiveness of the heuristics in managing violations.
411618	41161866	An Adaptive Reinforcement Learning Approach to Policy-Driven Autonomic Management	The use of policies as a basis for autonomic management has been studied, and it has been found that policy-driven autonomic systems need the ability to adapt policies based on past experience. This is important in dealing with human error and unpredictable workloads. Learning approaches, specifically reinforcement learning, have been explored as a way to provide autonomic systems with the ability to identify preferred policies or learn new ones. The question of whether a model learned from one set of policies can be applied to similar policies or if a new model must be learned from scratch when policies change is addressed. The paper demonstrates how a reinforcement learning model can be adapted to accommodate policy changes.
411619	411619106	Hypermedia presentation generation in Hera	Hera is a methodology used to design Semantic Web Information Systems (SWIS) by separating different aspects into models represented using RDF. It has two phases: data collection and presentation generation. The focus is on the presentation generation phase, which has two variants: a static one that creates a full Web presentation at once and a dynamic one that allows the user to influence the next page. The dynamic variant includes new models to capture user interactions. The implementation involves applying data transformations to the models to produce a hypermedia presentation. Overall, Hera aims to create efficient and user-friendly SWIS designs.
411619	41161933	HPG: the Hera presentation generator	This paper discusses the Hera Presentation Generator (HPG), a model-based design and development environment for Web Information Systems (WISs) that utilizes Semantic Web technologies. HPG integrates various software tools for constructing WIS input specifications and implementing data transformations. There are two versions of the HPG engine: HPG-XSLT and HPG-Java. HPG-XSLT uses XSLT stylesheets and generates a complete Web presentation, while HPG-Java utilizes Java code and generates one page at a time, allowing for richer user interaction. However, HPG-Java lacks the declarativity, simplicity, and reuse capabilities of HPG-XSLT. HPG fills the gap for tool support in designing WISs with Semantic Web technologies.
411620	41162093	Interweaving Trend and User Modeling for Personalized News Recommendation	This paper explores user modeling on Twitter and how personal interests and public trends interact. The authors propose a framework that enriches the semantics of Twitter messages to create meaningful user profiles, which can be used for personalized applications. They analyze user and trend profiles using a large Twitter dataset and evaluate their effectiveness in a personalized news recommendation system. The results show that personal interests are more significant than public trends in the recommendation process, but combining both types of profiles can lead to even better recommendations. This study highlights the importance of considering both personal interests and public trends in user modeling on Twitter.
411620	411620108	Analyzing user modeling on twitter for personalized news recommendations	This paper discusses the potential for utilizing micro-blogging activities on Twitter to improve user modeling and personalization. The authors propose a framework for user modeling on Twitter that utilizes semantic enrichment to identify topics and entities mentioned in tweets. They compare the effectiveness of different user modeling strategies, such as hashtag-based, entity-based, and topic-based profiles, and analyze the impact of incorporating temporal profile patterns on personalization in a news recommendation system. The results show that semantic enrichment can improve the diversity and quality of user profiles, and considering temporal patterns can further enhance recommendation performance. 
411621	41162153	Compiler/Interpreter Generator System LISA	The paper discusses the LISA system, a versatile tool for developing programming languages. LISA uses formal language specifications to create a language-specific environment including an editor, compiler/interpreter, and graphic tools. The paper highlights the design choices, implementation challenges, and integration of various tools in LISA. The main motivations for developing LISA were to support incremental language development, enable visual language design, and ensure portability. LISA comprises scanner, parser, and compiler generators, as well as graphic tools, an editor, and conversion tools, all connected through well-designed interfaces. It is written in Java, making it highly portable across different platforms. Overall, LISA offers the benefits of both a single system and a federated environment.
411621	41162131	LISA: An Interactive Environment for Programming Language Development	The LISA system is a programming environment that is tailored to specific programming languages. It includes editors, a compiler/interpreter, and other graphic tools that are designed based on the formal language specifications of the language. LISA also offers a variety of integrated tools such as scanner and parser generators, compiler generators, and conversion tools, all connected by well-designed interfaces. It provides users with a comprehensive and interactive platform for developing and working with programming languages. 
411622	4116221	Preference Between Allocentric and Egocentric 3D Manipulation in a Locally Coupled Configuration.	This study explores user preference for manipulating 3D objects on mobile devices using allocentric and egocentric methods. The experiment evaluates preference for translation, rotation, and full 6-DOF manipulation, and also considers the impact of context by testing in different 3D scenes. The influence of each manipulation axis is also examined. The results offer guidance for interface designers on selecting a default mapping in this type of setup.
411622	41162219	Exploring 3d Interaction In Alternate Control-Display Space Mappings	Research is focused on finding new ways to improve 3D interaction, but we often default to using Brunelleschi's perspective without considering its impact. To fully explore the possibilities, user studies are needed to examine different mappings of the control space onto the display space. Previous options have mainly looked at control-display ratio or gain, but this paper introduces a more comprehensive framework that includes flip, rotation, skew, and scale. A user study comparing these mappings to the perspective mapping shows interesting differences in interactions and preferences, indicating that all options could be considered viable. This framework and study open up opportunities for further exploration of 3D interaction.
411623	41162311	Towards Discourse Meaning	The article discusses issues related to dependencies at the discourse and sentence level. It describes the Penn Discourse Treebank (PDTB) corpus, which annotates discourse connectives and their arguments, as well as attributions and their relationship to dependencies. The PDTB is a valuable resource for natural language processing tasks and follows a theory-neutral approach to annotation. It also provides sense labels for each relation and separately annotates attributions. The PDTB has been released in two versions, with the latest one being available through the Linguistic Data Consortium (LDC).
411623	41162358	Centering: a framework for modeling the local coherence of discourse	The paper discusses how focus of attention, choice of referring expression, and perceived coherence are related within a discourse segment. It introduces a framework and theory of centering, which aims to model the local aspect of attentional state. The paper also explores the connection between local coherence and choice of referring expressions, suggesting that differences in coherence can be attributed to the inference demands of different types of referring expressions. It provides evidence that the attentional state properties described by centering can explain these differences.
411624	41162415	Detection of Significant Sets of Episodes in Event Sequences	The authors propose a method for detecting "unusual" sets of event sequences within a large event stream. This includes investigating permutations of the same sequence, where the order of events does not matter. To ensure reliability, the method compares measurements to a probabilistic reference model. The authors provide a precise analysis and determine a threshold for detecting unusual episodes, taking into account the potential occurrence of multiple patterns within the same window. The method is tested on a real-world dataset and shows good agreement with the theoretical analysis. This paper builds upon previous work and extends it to the detection of multiple episodes simultaneously.
411624	411624113	Multiple choice tries and distributed hash tables	In 1960, Fredkin introduced tries as an efficient way to search and sort digital data. There has been renewed interest in tries in recent years due to their usefulness in various applications such as dynamic hashing, conflict resolution, and distributed hash tables. In this paper, the authors explore the construction of balanced tries by considering different scenarios, such as choosing from a pool of k strings generated by a discrete i.i.d. source. They analyze parameters such as height and fill-up level and show that for two-choice tries, the height can be reduced by 50% compared to regular tries. They also design a more refined on-line algorithm to reduce the height by another 25%. Additionally, they demonstrate that for large but fixed k, the height is asymptotically equal to the typical depth in a trie. Finally, they show that for unbiased memoryless sources, highly balanced tries can be constructed with a simple greedy algorithm. This has implications for distributed hash tables and allows for a randomized ID management algorithm in peer-to-peer networks with a balanced load distribution. 
411625	41162532	Modeling ill-structured optimization tasks through cases	CABINS is a framework used for modeling optimization tasks in ill-structured domains where exact models are not available and the user's idea of optimality is subjective and context-dependent. It uses case-based reasoning to iteratively revise a solution. Task structure analysis is used to create an initial model and generic vocabularies are specialized into case feature descriptions for specific problems. Through experimentation on job shop scheduling problems, CABINS has been shown to effectively operationalize and enhance the model by accumulating cases. 
411625	41162526	Construction of a reusable knowledge base - Generating operation sequences for the accident restoration of primary substations based on the reusable knowledge base.	Expert systems (ES) have historically been created to solve specific problems, making it difficult to share or reuse the gained knowledge. This has hindered efficient construction of ES. However, as the practical application of ES is gaining momentum and its usefulness is being recognized society-wide, it is important to focus on sharing and reusing knowledge. The authors sought to address this by constructing a reusable knowledge base for aiding in the restoration of primary substations after accidents. They analyzed conventional ES used for this purpose and identified limitations in knowledge sharing. A methodology for constructing a reusable knowledge base was then presented and applied to the accident restoration process. The knowledge of an expert in problem-solving was divided into three categories: object model, general rules, and knowledge specific to particular primary substations. The paper also describes a procedure-generation system for accident restoration in primary substations and its successful implementation in 10 accident cases in 2 substations.
411626	411626166	Large-scale malware indexing using function-call graphs	The AV industry faces a challenge in processing the large number of malware samples received daily. One solution is to quickly determine if a new sample is similar to known malware. The paper presents SMIT, a malware database management system that uses function-call graphs to efficiently make this determination. Each malware program is represented as a graph, allowing for a nearest-neighbor search in a graph database. To speed up the search, a method to compute graph similarity and a multi-resolution indexing scheme are developed. Results from testing on a database of over 100,000 malware show the effectiveness and scalability of SMIT's search mechanisms.
411626	41162617	Efficient and Automatic Instrumentation for Packed Binaries	In order to add security features to modern software, executable binaries often need to be transformed. This transformation relies on accurate and efficient disassembly, but many application binaries are now distributed in a packed form, which poses challenges for existing disassembly tools. To address this issue, a new tool called Uncover has been developed, specifically designed for packed Win32/X86 binaries. Uncover utilizes two unique techniques - computing entropy to identify packed binaries and accurately tracking the unpacking process during runtime - to efficiently and automatically instrument packed binaries for disassembly. These techniques allow Uncover to disassemble packed binaries as if they were never packed, overcoming the limitations of existing tools. 
411627	41162733	Supporting revisitation with contextual suggestions	Web browsers often lack features that allow users to easily revisit pages they have not recently visited. To address this issue, a browser toolbar has been developed that reminds users of previously visited pages related to the one they are currently viewing. This toolbar uses a combination of ranking and propagation methods to make recommendations. In a user evaluation, it was found that on average, 22.7% of revisits were triggered by the toolbar, significantly changing the participants' revisitation habits. This paper discusses the effectiveness of the recommendations and the implications of the evaluation results. 
411627	41162729	Methods for web revisitation prediction: survey and experimentation	The majority of websites we visit are ones we have already visited before. Browsers have tools such as bookmarks and history to help revisit frequently and recently visited pages, but they do not support less frequently visited pages. Browser plugins and extensions have been developed to improve revisiting less popular pages using recommendation and prediction techniques. This article provides an overview of these techniques and how they can be combined to increase accuracy. The performance of various workflows is also analyzed, and factors affecting accuracy are discussed. An upper limit for accuracy is also provided using an 'oracle' that ignores non-revisited pages. 
411628	41162812	xTagger: a new approach to authoring document-centric XML	The process of creating document-centric XML documents in humanities disciplines differs from the typical approach used by standard XML editing software, which focuses on data-centric views of XML. Document-centric encoding involves starting with the content and then adding markup, while data-centric XML involves first creating a tree structure and then adding content. This paper discusses the development of a tool called xTagger, originally designed for the Electronic Boethius project and later enhanced for the ARCHway project. Both projects aim to create methods and software for preparing electronic editions of historic manuscripts using image-based techniques.
411628	41162813	Towards traceable test-driven development.	The Grand Challenges in Traceability aim to incorporate traceability into the software development life cycle. This paper focuses on test-driven development (TDD), a practice where automated tests and corresponding code are created in rapid iterations. TDD offers potential for traceability information to be gathered throughout the process, which can improve the development process and final software product. The paper explores the opportunities and challenges of combining TDD and traceability, and discusses plans for their synthesis. TDD has the potential to naturally incorporate traceability into the development process, leading to more effective and efficient software development.
411629	41162915	Informing the Requirements Process with Patterns of Cooperative Interaction	The RE community recognizes the importance of understanding the social context in which computer-based systems will be used. Ethnographic studies have been used to inform the requirements process from a social perspective. However, there has been little focus on how these findings can be generalized and reused in new settings. This paper introduces a resource called Patterns of Cooperative Interaction, which compares and contrasts ethnographic findings, discusses their relevance to design, and introduces the analytic capabilities of such studies. The paper explains how these interaction patterns were developed from a collection of ethnographic studies and provides examples of how they can be used by requirements engineers to identify potential social issues and generate requirements that support social interaction.
411629	41162913	Exploring bluetooth based mobile phone interaction with the hermes photo display	The use of personal mobile phones has shown promise in supporting user interaction with public displays. By utilizing Bluetooth technology, users can interact with displays without incurring personal financial connectivity costs. Despite the widespread use of Bluetooth in mobile phones, there has been limited exploration in this area. This paper discusses the results of a study involving the Hermes Photo Display, which allows users to send and receive pictures over Bluetooth. The study explores the technical challenges of using Bluetooth and presents insights into user acceptance and the potential for creating a sense of community through this type of display.
411630	41163026	Context sharing in a 'real world' ubicomp deployment	The article discusses the use of ubicomp systems for context sharing, which has gained interest but has not been extensively studied outside of lab settings. It focuses on the Hermes interactive office door display system, which allows for asynchronous messaging between users in an office setting. The article presents an analysis of the context sharing behaviors that emerged during the deployment of the Hermes system. It identifies a range of issues related to context sharing, including privacy concerns and ease of use, based on qualitative data from user interviews and questionnaires.
411630	4116306	Exploring awareness related messaging through two situated-display-based systems	The article discusses two systems, Hermes and SPAM, that were designed to address awareness issues in a university department and two residential community care facilities. These systems allowed users to post messages related to awareness on displays located outside office doors or in staff offices. The placement of these displays had a significant impact on the design and use of the systems, providing context for the messages being sent. The article also highlights the importance of user-friendly interaction methods and the need for appropriate levels of expressiveness and user control. Examples are given of users using these systems for both serious coordination purposes and playful communication. 
411631	41163151	Computing machine-efficient polynomial approximations	Polynomial approximations are commonly used in computing systems, but often the coefficients of the polynomial cannot be represented exactly with a finite number of bits. This is due to the limitations of floating-point representations in software and the need for fast and inexpensive multipliers in hardware. To address this, polynomial approximations with coefficients that have a maximum number of fractional bits are considered. This means that the coefficients are rational numbers with denominators that are powers of 2. A method is proposed for finding the best polynomial approximation under this constraint, and it can also be applied to other constraints such as minimizing relative error or having some coefficients equal to predefined constants.
411631	41163111	On the Componentwise Accuracy of Complex Floating-Point Division with an FMA	This paper discusses the accuracy of complex division in radix-two floating-point arithmetic. It focuses on ensuring high relative accuracy by using Kahan's compensated algorithm for 2 by 2 determinants. However, in the case of complex division, two of the expressions have the same sign, making it possible to use cheaper schemes. The paper presents a detailed accuracy analysis for the sum of two nonnegative products and provides sharp bounds for both absolute and relative errors. It also introduces two new division algorithms, with the first having a maximum relative error of 5u+13u^2 and the second having a maximum relative error of 4.5u+9u^2 when tests are allowed. These results are shown to be close to the best possible.
411632	41163238	Health education in rural communities with locally produced and locally relevant multimedia content	Health education is crucial in addressing common health issues in developing countries. In rural communities, Community Health Workers (CHWs) play a vital role in spreading health education information. This paper discusses important principles for designing solutions to create and distribute digital health content in rural areas, based on previous work in health education and CHW training. The model presented involves providing tools for rural health professionals to independently create health content within their communities. Lessons learned from implementing this model in Lesotho highlight the benefits of using locally produced and relevant digital content in health education. This approach has the potential to greatly improve health outcomes in rural communities.
411632	41163213	Extending Dynamic Queries to Handle Uncertain Data	Dynamic querying is a useful method for novice users to access and understand data in databases. However, certain multimedia archives, like those containing African art, have data with vague locations in time and space, making it difficult for users to query and display the data. This study extends dynamic querying techniques to work with uncertain African art data. The researchers developed methods for storing, visualizing, and querying this uncertain data, which were tested by users and found to be successful. This approach can also be applied to other domains with uncertain one-dimensional attribute data. Overall, this study shows how dynamic querying can help novice users query large databases with complex uncertain attributes.
411633	4116333	On a New Method for Dataflow Analysis of Java Virtual Machine Subroutines	The Java Virtual Machine's bytecode verifier is responsible for checking the type safety of Java bytecode, making it a key component of Java's security model. However, there are technical issues with the type system for bytecode, especially when it comes to handling subroutines. This paper proposes a new type system for Java Virtual Machine subroutines, building on the work of Stata and Abadi and Qian. The new type system includes types like last(x) and return(n), which allow for purely type-based analysis of instructions and simplify the bytecode verification process. Additionally, this method is more powerful and has no limitations on the number of subroutine entries and exits.
411633	41163350	A typed &lgr;-calculus for proving-by-example and bottom-up generalization procedure	The article discusses a generalization procedure for reconstructing mathematical inductions in a proof of an example. This procedure is based on a newly defined typed lambda-calculus, which extends the logical framework and allows for recursions and inductions on natural numbers. The type system also includes inferences on linear arithmetical terms. The generalization procedure works in a bottom-up manner and can construct nested inductions, as well as identify inductions with a limited form of bounded quantification. Overall, this procedure aims to increase the applicability of mathematical induction in proofs.
411634	41163410	Mechanising Partiality With Re-implementation	Partial functions are often overlooked, but they are important in practical deduction systems. Kleene's three-valued logic allows for rejecting faulty formulae that the simpler two-valued logic would accept. However, existing theorem provers cannot easily be adapted to this approach. Recently implemented calculi for many-valued logics are also not suitable, as they do not exclude undefined elements. In this work, the authors show that a two-valued theorem prover can be enhanced with a simple strategy to generate proofs for three-valued logic theorems. This allows for the use of an existing theorem prover for a large part of the language. For more information, see [Far90].
411634	41163438	System Description: LEO - A Higher-Order Theorem Prover	Higher-order logic is a powerful tool for expressing mathematical problems, but it can be difficult for automated theorem provers to handle due to undecidability and the need for primitive substitution. However, some problems that cannot be solved by first-order theorem provers can be easily solved by a higher-order theorem prover like Leo. This is because Leo is based on a higher-order logic that handles comprehension axioms implicitly and uses a higher-order resolution calculus that interleaves the search for empty clauses and higher-order pre-unification. Unlike other higher-order theorem provers, Leo's unification process includes extensionality principles, making it Henkin complete without the need for additional axioms.
411635	41163554	Events and Controversies: Influences of a Shocking News Event on Information Seeking	The internet has been criticized for creating a "filter bubble" where users are only exposed to information that aligns with their existing beliefs, leading to intellectual isolation and limited exposure to alternative viewpoints. This phenomenon is particularly harmful when it comes to discussions on emotionally charged topics, such as gun control, where people tend to become even more entrenched in their beliefs. Researchers seek to study the impact of large-scale news events, like mass shootings, on people's information-seeking behavior and access to diverse perspectives. Using measures of information diversity, they analyze the browsing patterns of users before and after such events to understand the influence of the news on their search and browsing habits. 
411635	41163527	Predicting content change on the web	Accurate prediction of web page changes is important for improving various retrieval and web-related tasks. This includes designing a better crawling strategy that only recrawls pages when necessary and a proactive personalization mechanism that delivers relevant content to users upon revisiting a page. While past techniques have focused on change frequency, this study also considers the page's content, observed changes, relatedness to other pages, and types of changes. A new expert prediction framework is proposed, which incorporates this information more effectively than standard techniques. Results from an empirical analysis show that using page content and related pages greatly enhances prediction accuracy. Different similarity metrics are also discussed, with a focus on temporal content similarity, and their impact on prediction performance is examined.
411636	411636117	Using latent semantic analysis to improve access to textual information	This paper presents a new method for addressing the vocabulary problem in human-computer interaction. Traditional approaches rely on exact word matching, but this is limited due to the vast range of words people may use to describe the same thing. The proposed solution is to use latent semantic indexing, which organizes text objects into a semantic structure to better match user requests. This is achieved by breaking down a large word by text-object matrix into smaller, orthogonal factors, resulting in a 50 to 150 dimensional representation for terms and objects. Initial tests have shown this approach to be effective in improving access to various types of textual materials and descriptions.
411636	41163633	Information retrieval using a singular value decomposition model of latent semantic structure	A new method for automatic indexing and retrieval has been introduced, which utilizes implicit higher-order structure to improve the accuracy of term-document association. This method involves decomposing a large matrix of terms and documents into 50-150 orthogonal factors using singular-value decomposition. These factors are then used to approximate the original matrix and represent both documents and terms as vectors in a 50-150 dimensional space. Queries are represented as pseudo-documents, formed by combining weighted terms, and documents are ranked based on their similarity to the query. Initial tests have shown this method to be highly effective.
411637	41163781	Two-Party Privacy Games: How Users Perturb When Learners Preempt.	Internet tracking technologies and wearable electronics generate a large amount of data that is used by machine learning algorithms. This amount of data is expected to increase with the emergence of the internet of things and cyber-physical systems. While these technologies offer many benefits, they also pose a risk to privacy. To address this, machine learning algorithms can add noise to outputs using the concept of differential privacy. Users can also protect their privacy by injecting noise into the data they report. This paper examines the relationship between privacy and accuracy, and user and learner perturbation in machine learning. It proposes a solution for the case of an averaging query, where either users or learners perturb the data, but not both. The threshold for learner perturbation increases as incentives become more misaligned. The paper also suggests future directions for this topic and hopes to encourage further research in the area of differential privacy games. 
411637	41163774	A Dual Perturbation Approach for Differential Private ADMM-Based Distributed Empirical Risk Minimization.	The rapid growth of data has emphasized the need for privacy protection in distributed machine learning. This paper presents a privacy-preserving method for a certain type of machine learning problem. The method utilizes the alternating direction method of multipliers and a dual variable perturbation technique to ensure dynamic differential privacy. The algorithm is applicable under certain conditions and its performance is evaluated in terms of the number of data points needed for a desired level of accuracy. The paper also discusses the tradeoff between privacy and accuracy and provides guidelines for choosing privacy parameters. Numerical experiments using a real-world database validate the results and demonstrate the effectiveness of the proposed method. 
411638	41163842	Using Attribute-Based Access Control to Enable Attribute-Based Messaging	Attribute Based Messaging (ABM) allows message senders to create a targeted list of recipients based on their attributes from an enterprise database. This can reduce unnecessary communications and enhance privacy, but there are challenges with access control. In this paper, the authors propose an approach to ABM that uses the same attribute database for addressing and access control. They demonstrate a manageable access control system, show how it can be integrated with existing messaging systems, and prove its efficiency for mid-size enterprises. Their implementation can dispatch ABM messages for a large number of users with minimal latency.
411638	41163845	Defeasible security policy composition for web services	This paper discusses the importance of being able to automatically combine security policies from multiple organizations in order to create scalable security systems. The diversity of policies often leads to conflicts, which require a way to prioritize rules. The paper introduces the concept of defeasible policy composition, where policies are represented in defeasible logic and combined using non-monotonic inference rules. This approach allows policy writers to assert rules tentatively, with the strongest policy taking precedence. The structure of these policies also allows for fully automated composition using a single operator. The paper argues that this system is practical, easily understood by policy writers, and efficient for computers to automate. An application of this concept to web services is used to validate these claims.
411639	41163943	Not so greedy: Randomly Selected Naive Bayes	The attribute selection approach is one way to improve the performance of Naive Bayes, and it has been particularly successful. There are two main types of algorithms for attribute selection: filters and wrappers. Filters use general data information to select attributes before the learning algorithm is applied, while wrappers use the learning algorithm itself to evaluate the selected attributes. This paper focuses on the wrapper approach and introduces a new algorithm called Randomly Selected Naive Bayes (RSNB). Three versions of RSNB are designed for classification, ranking, and probability estimation tasks. Experiments on various datasets show that RSNB is effective in terms of classification accuracy, area under the ROC curve, and conditional log likelihood.
411639	4116397	Learning naive bayes for probability estimation by feature selection	Naive Bayes is a popular classification algorithm, but it has poor probability estimation. This is a problem in many applications where accurate probability estimation is needed for optimal decision-making. To address this issue, researchers have proposed various algorithms, but they often have high computational complexity. In this paper, the authors suggest using feature selection to improve the probability estimation of naive Bayes. They conduct a search process to select a subset of attributes and then apply naive Bayes on the selected attributes. The authors also propose an improved feature selection algorithm, SBC-CLL, which directly uses the conditional log likelihood score for attribute selection. Experiments show that both SBC and SBC-CLL significantly improve probability estimation over naive Bayes, with SBC-CLL outperforming SBC. This approach provides a simple and effective solution for improving the probability estimation of naive Bayes.
411640	41164049	MFQE 2.0: A New Approach for Multi-Frame Quality Enhancement on Compressed Video	The past few years have seen significant progress in using deep learning to improve the quality of compressed images and videos. However, most existing methods only focus on enhancing individual frames and do not consider the similarities between consecutive frames. This paper introduces a new task called Multi-Frame Quality Enhancement (MFQE) which utilizes the similarities between frames to enhance the quality of low-quality frames using neighboring high-quality frames. The proposed approach includes a BiLSTM-based detector to locate Peak Quality Frames (PQFs) and a Multi-Frame Convolutional Neural Network (MF-CNN) to compensate for motion and reduce compression artifacts. Experiments show that this approach is effective and can improve the quality of compressed videos. 
411640	41164043	MW-GAN+ for Perceptual Quality Enhancement on Compressed Video	Deep learning has greatly contributed to the development of video quality enhancement. However, current methods focus on improving the objective quality of compressed videos and neglect the importance of perceptual quality in determining the overall viewing experience. To address this issue, the paper proposes a novel generative adversarial network (GAN) called multi-level wavelet-based GAN+ (MW-GAN+) that utilizes a multi-level wavelet packet transform (WPT) to enhance the perceptual quality of compressed videos. The method includes a multi-level wavelet pixel-adaptive (MWP) module for extracting temporal information, a wavelet reconstruction network with wavelet-dense residual blocks (WDRB) to recover high-frequency details, and a 3D discriminator for temporal coherence. Experimental results show that MW-GAN+ outperforms existing methods in improving perceptual quality. The code is available on GitHub.
411641	41164166	A Morphological Approach to the Generalised 2-stage Stock-Cutting Problem	This paper discusses the two-stage stock-cutting problem, where rectangular pieces need to be cut from a general shape object with holes or defects. Mathematical morphological operators are used to find the best shifting for the cutting pattern, and simulated annealing is used to determine the optimal cutting pattern. The paper also presents experimental results.
411641	411641104	A peak preserving algorithm for the removal of colored noise from signals	The simulated annealing method is used to restore signals that contain desired peaks that are smooth and buried in colored noise. This is achieved through a global optimization problem and a piecewise linear model for the noise-free signal. An iterative algorithm is used to estimate model parameters and improve the signal estimation until self-consistency is achieved. The method is applied to simulated and real-world data, showing success in restoring the underlying signal. Results are compared to another algorithm and shown to be better at preserving peaks in the signals.
411642	41164213	Abstractive Document Summarization With A Graph-Based Attentional Neural Model	Abstractive summarization, the goal of document summarization research, has received less attention due to limitations in text generation techniques. However, advancements in neural models have led to progress in abstractive sentence summarization. Despite this, abstractive document summarization is still in its early stages and has shown worse evaluation results compared to extractive methods. To address this issue, a new graph-based attention mechanism is proposed in this paper to consider the saliency factor of summarization, which has been overlooked in previous works. Experimental results show a considerable improvement in the proposed model compared to previous neural abstractive models, making it competitive with state-of-the-art extractive methods. The data-driven approach of this method further strengthens its effectiveness.
411642	41164233	Towards Constructing Sports News From Live Text Commentary	This paper explores the possibility of automatically generating sports news from live text commentary scripts. The task is approached as a type of document summarization using a supervised learning to rank framework. The method utilizes both traditional sentence features for generic document summarization and new task-specific features. To address the issue of local redundancy, a probabilistic sentence selection algorithm is proposed. Experiments on data from football live commentary scripts and corresponding sports news demonstrate the effectiveness of this approach. Results show that the proposed methods outperform several baseline methods in various aspects, indicating their suitability for this task. 
411643	41164343	Manifold learning for biomarker discovery in MR imaging	The authors propose a framework for extracting biomarkers from low-dimensional manifolds of MR image data. This method captures information about the brain's structural shape and appearance, as well as the subject's clinical state. A key contribution is the incorporation of longitudinal image information into the learned manifold. This is achieved by simultaneously embedding baseline and follow-up scans into a single manifold, rather than using separate manifolds for inter-subject and intra-subject variation. The proposed method is applied to data from 362 subjects enrolled in the Alzheimer's Disease Neuroimaging Initiative, and shows promising results in classification of healthy controls, subjects with Alzheimer's disease, and subjects with mild cognitive impairment. The biomarkers identified using this method are data-driven and may provide an alternative to traditional biomarkers derived from manual or automated segmentations.
411643	41164355	Automatic morphometry in Alzheimer's disease and mild cognitive impairment.	This paper introduces a new public repository containing brain images of healthy individuals and those with mild cognitive impairment and Alzheimer's disease. The images were obtained from the ADNI database and processed using a multi-atlas based MAPER procedure, resulting in labels for 83 brain regions. The segmentations were visually assessed and found to be consistent, with strong agreement between images acquired at different field strengths. Comparisons between diagnostic groups showed significant differences in specific brain regions, particularly in the temporal and parietal lobes. An automatic measure of white matter disease was also found to be effective. This repository provides a valuable resource for researchers using ADNI data.
411644	4116445	Automatic anatomical brain MRI segmentation combining label propagation and decision fusion	Automated classification of brain regions in three-dimensional magnetic resonance (MR) images is challenging due to the time and expertise required for manual segmentation and labeling. To address this, individual segmentations can be propagated to other individuals using an anatomical correspondence estimate. However, the accuracy of these propagated segmentations is limited. To improve accuracy, multiple segmentations can be combined using decision fusion. In a study using 30 normal brain MR images, the average similarity index (SI) for individual propagations was 0.754±0.016, while decision fusion with 29 input segmentations increased SI to 0.836±0.009. Additionally, a model was developed to predict the improvement in accuracy based on the number of individual propagated segmentations combined. This automated method shows promise in accurately classifying brain regions and can compete with manual delineations.
411644	41164458	Multi-atlas based segmentation of brain images: Atlas selection and its effect on accuracy.	Quantitative research in neuroimaging relies on precise segmentation of brain MR images. Multi-atlas based methods use multiple atlases to accurately segment the brain. The number of atlases available for this purpose is increasing. To address the issue of scale in this approach, a framework was developed to select a custom subset of atlases for each subject. This method resulted in more accurate subcortical segmentations compared to using non-selective random subsets. Using a database of 275 atlases, the study tested different selection criteria such as image similarity and age. Results showed that image-based and age-based selection improved the accuracy of the segmentations. This study highlights the importance of selecting atlases from large databases for optimal brain image segmentation.
411645	411645164	Intervention in gene regulatory networks via phenotypically constrained control policies based on long-run behavior.	The main purpose of studying gene regulatory networks is to find strategies for intervening and designing gene-based therapies. Previous studies have focused on using transition probability matrices to develop intervention strategies for probabilistic Boolean networks. However, it is important to also consider minimizing collateral damage when designing these strategies. This paper proposes two new control policies that take into account the long-term effects on the network and aim to reduce the risk of entering undesirable states while limiting collateral damage. Experiments on random networks and a melanoma network demonstrate the effectiveness of these policies in reducing the risk of entering undesirable states and their potential use in designing genetic therapies.
411645	411645165	Intervention in gene regulatory networks via greedy control policies based on long-run behavior.	Gene regulatory networks are studied in order to develop strategies for intervention, such as identifying drug targets and designing gene-based therapies. One approach is through optimal stochastic control, which minimizes a cost function to reduce the probability of undesirable network states. However, this method is computationally complex for large networks. In this paper, three new greedy stationary control policies are proposed that directly consider the long-term behavior of the network. These policies do not rely on a cost function and can be used as an alternative to dynamic programming algorithms. They also have the potential to predict the best control gene with reduced computational complexity. The proposed policies show better performance and have potential for future gene therapeutic intervention strategies. 
411646	4116467	Learning-Based Approach to Real Time Tracking and Analysis of Faces	This paper presents a trainable system that can track faces and facial features in real time. The system is able to accurately estimate features such as eye and nostril locations, as well as mouth parameters like openness and smile. The development of this system focused on addressing the challenges of image representation and learning algorithms. The system uses Haar wavelets for image representation, which allows for robust detection of facial features. Unlike previous methods, this system is entirely trained using examples and does not rely on pre-existing models. The system works in stages, using supervised learning techniques such as support vector machines for classification. The estimation of mouth parameters is based on regression from a subset of Haar wavelet coefficients. 
411646	41164631	A General Framework for Object Detection	This paper introduces a trainable framework for detecting objects in cluttered images. The method is based on a wavelet representation of an object class, which is learned from a statistical analysis of the class instances. This representation addresses the issue of variability within a class and results in a low false detection rate in diverse environments. The framework is demonstrated in two different domains - face detection and people detection - and is shown to be effective in both cases. Unlike previous approaches, this method does not rely on hand-crafted models or motion-based segmentation. The paper also includes a motion-based extension for improved detection in video sequences. Overall, the results suggest that this approach can be applied to a wide range of object detection tasks.
411647	4116471	A System for Automatic Notification and Severity Estimation of Automotive Accidents	New communication technologies in modern vehicles can greatly improve assistance for those injured in traffic accidents. Studies have found that integrating artificial intelligence into these technologies can automate decision making for emergency services, reducing response time and adapting resources to the severity of the accident. To further enhance the rescue process, accurately estimating the severity of the accident is crucial. A new intelligent system has been proposed that uses data mining and knowledge inference to automatically detect accidents, notify them through vehicular networks, and estimate their severity based on relevant variables such as vehicle speed, type of vehicles involved, impact speed, and airbag status. Testing at a research facility has shown promising results in reducing response time for emergency services.
411647	4116477	SEED: scalable, efficient enforcement of dependences	Instruction issue logic is a crucial element in modern high-performance out-of-order processors. To overcome the increasing latencies caused by memory accesses and longer pipelines, large issue queues are used. However, conventional designs that rely on atomic wakeup-select cycles require heavy use of broadcasting, compaction, and heavily-ported structures, leading to scalability and power consumption issues. To address this, a new design called "Scalable, Efficient Enforcement of Dependences (SEED)" is proposed. SEED uses an out-of-order, broadcast-free instruction wakeup block feeding an in-order scheduler, along with multi-banked, index-based structures for efficient dependence tracking. SEED offers high performance and energy efficiency, with only a small performance cost compared to conventional designs. It also consumes significantly less power and occupies less area.
411648	411648132	Evaluating Bluetooth performance as the support for context-aware applications.	This article discusses an experiment that explores the use of Bluetooth wireless technology in supporting context-aware applications. By combining wireless and wired network technologies, the authors propose a method for network interconnection. They also outline the process of creating a context-aware application using Bluetooth. The technology's performance is then evaluated through a test-bed and simulation. The results suggest that Bluetooth can effectively support context-aware applications in this area.
411648	41164836	UbiqMuseum: A Bluetooth and Java Based Context-Aware System for Ubiquitous Computing	 This paper discusses the use of Bluetooth and Java technologies in ubiquitous computing, which relies on contextual information to cater to user preferences and surroundings. The authors present UbiqMuseum, a context-aware application for museum visitors that utilizes both Bluetooth and Java for productivity and connectivity. They detail the architecture and implementation of the application and demonstrate its practicality by integrating multiple technologies such as Bluetooth, WLAN, and Ethernet LAN. Experiments in a small testbed were conducted to evaluate the system's performance and behavior, including throughput and inquiry delay with varying packet size, coding types, and device separation distance. The results show that Bluetooth can maintain a steady throughput up to 10 meters and that inquiry delay does not significantly increase with distance.
411649	41164963	An Adaptive Anycasting Solution for Crowd Sensing in Vehicular Environments	Vehicular networks have the potential to greatly enhance our society by acting as mobile sensors and collecting various types of information while vehicles travel. This can enable new services such as environment monitoring, traffic management, and urban surveillance. In this paper, the authors introduce AVE, a message delivery protocol that utilizes both geographical and topological information to dynamically adapt to network conditions. This protocol is specifically designed for V2I connectivity for cloud services, where vehicles transmit individual messages to a cloud service through any available nearby Road Side Unit (RSU). Simulations show that AVE outperforms other protocols in terms of delivery ratio, with a 10% improvement over DYMO in sparse scenarios and DTN techniques in dense scenarios. 
411649	41164949	DTB-MAC: Dynamic Token-Based MAC Protocol for reliable and efficient beacon broadcasting in VANETs	The paper discusses the issues with using broadcasting as the main communication mechanism in vehicular environments, particularly in the widely used IEEE 802.11p protocol. The strict delay limit and high reliability requirements for safety messages make it challenging for random access MAC protocols like IEEE 802.11p to ensure reliable transmission. To address this issue, the paper proposes a hybrid MAC protocol called DTB-MAC, which combines token passing and random access mechanisms to improve transmission reliability and reduce channel contention. Simulation experiments show that DTB-MAC outperforms IEEE 802.11p in terms of channel utilization and beacon delivery ratio. 
411650	41165068	Studying the feasibility of IEEE 802.15.4-Based WSNs for gas and fire tracking applications through simulation	Wireless Sensor Networks (WSNs) have become increasingly popular in various fields such as military, environmental, and industrial applications due to their reliability and low latency. For time-critical WSN applications, it is important to quickly respond to changes in the environment and ensure accurate data collection. This paper proposes a near real-time monitoring system using binary detection sensors for both indoor and outdoor environments. The system utilizes a routing protocol that minimizes control traffic to reduce end-to-end data delivery delay. The performance of gas and fire tracking applications is evaluated using IEEE 802.15.4 technology and a routing scheme that relies on sink announcements for route discovery. The accuracy of the monitoring process is determined by simulating emergency events and developing gas and fire propagation models. 
411650	41165010	VEACON: A Vehicular Accident Ontology designed to improve safety on the roads	Vehicles are now equipped with advanced sensors that can collect data about the vehicle and its surroundings. In the near future, vehicles will be able to share this information with other vehicles and connect with emergency services in case of accidents. To ensure interoperability, a standardized structure is needed to enable data sharing among different entities in transportation systems. This paper focuses on traffic safety applications and proposes the VEhicular ACcident ONtology (VEACON) to improve safety. The ontology combines data from accident occurrences and the General Estimates System (GES) accidents database. Realistic crash tests and simulations demonstrate the effectiveness of the proposal in quickly notifying nearby vehicles and infrastructure about accidents, improving emergency service response time.
411651	41165160	A Practical Framework for Privacy-Preserving Data Analytics	The availability of large amounts of user-generated data has transformed our society. This data is used for various purposes, such as disease outbreak detection and traffic control, as well as for commercial interests like smart grid and product recommendation. However, this data can also be used to identify individuals, as seen in the AOL search log release incident. To address this issue, the authors propose a framework that provides differential privacy guarantees to individual data contributors. This framework allows for accurate data analytics while protecting user privacy, through the use of differentially private aggregates and sampling techniques. Empirical studies show that this solution effectively balances privacy and data analysis needs.
411651	41165146	Exploring online social activities for adaptive search personalization	The internet has become a highly social environment, with people actively participating in various social activities such as blogging and social bookmarking. In this paper, the authors propose using a user's public social activities to personalize internet services, as this data is more acceptable than private data like search histories. They have developed a framework that learns about a user's preferences from their activities on multiple online social systems. This personalized approach is demonstrated through an example of search result personalization. The system is also adaptive, adjusting its interest profile based on user choices and integrating information from multiple social systems. Experiments on real-world data have shown the effectiveness of this personalized search approach and the benefits of integrating information from multiple social systems.
411652	411652123	Parallel Maximum Weight Bipartite Matching Algorithms for Scheduling in Input-Queued Switches	An input-queued switch with virtual output queuing can achieve a maximum throughput of 100% by using advanced scheduling strategies. This can be seen as a maximum flow problem and a maximum weight bipartite matching (MWBM) algorithm has been proposed for input-queued switches. The goal is to maintain fairness and stability while achieving 100% throughput. The algorithm has a sublinear parallel run time complexity and can obtain the MWBM in sublinear time by using the previous time slot's matching. This algorithm outperforms all previously proposed MWBM algorithms for input-queued switches. A linear time complexity MWBM algorithm has also been developed for a general bipartite graph that outperforms the best known sublinear algorithm for graphs with less than 1015 nodes. 
411652	411652111	Subsequence matching on structured time series data	Subsequence matching is a technique used in time series databases to analyze and compare data. It has various applications such as pattern matching, prediction, and rule discovery. This paper focuses on utilizing the internal structure of time series data to improve these tasks and gain insight into the problem domain. Specifically, it introduces a solution for analyzing and predicting respiratory motion in cancer radiation treatment using subsequence similarity matching. The system captures real-time motion data and uses a piecewise linear representation to query for subsequence matching. To address the unique needs of matching breathing patterns, a new similarity measure is introduced. The results of the matching can be used for offline and online applications such as motion prediction and correlation discovery. This approach can be applied to other domains with structured time series data.
411653	41165329	A Context-Based Approach to Reconciling Data Interpretation Conflicts in Web Services Composition	The article discusses a method for detecting and resolving data interpretation conflicts in Web services composition. This is done through the use of a lightweight ontology with added modifiers, contexts, and atomic conversions. By annotating the WSDL descriptions of the Web services, correspondences to the ontology are established. The proposed approach can automatically identify conflicts in the Business Process Execution Language (BPEL) specification and generate a mediated BPEL. A prototype is also developed to test the effectiveness of the approach.
411653	41165338	KNOWLEDGE INTEGRATION TO OVERCOME ONTOLOGICAL HETEROGENEITY: CHALLENGES FROM FINANCIAL INFORMATION SYSTEMS	The shift towards global networking presents both opportunities and challenges. This paper discusses the key technologies needed to achieve global semantic interoperability among various information systems, including traditional and web-based sources. The significance of this capability is highlighted, and the authors propose an extension of the context interchange framework to address ontological heterogeneity, a common issue in financial information systems. They also present a solution for handling ontological heterogeneity in source selection, using a combination of symbolic solvers and mixed integer programming techniques. The integration of these approaches with emerging Semantic Web technologies such as Web-Services, DAML+OIL, and RuleML can offer scalable solutions for global semantic interoperability. This has implications for financial services and other e-business tasks, making a valuable contribution to the financial knowledge integration problem. 
411654	41165490	The PowerPC Backend Molen Compiler	This paper discusses a C compiler that was created to work with the Virtex II Pro PowerPC processor and incorporate the Molen architecture programming paradigm. To test the efficiency of the compiler, a multimedia video frame M-JPEG encoder was used with the Discrete Cosine Transform (DCT*) function mapped onto an FPGA. The results showed a speedup of 2.5, which is 84% efficient compared to a maximal theoretical speedup of 2.96. This was achieved using a non-optimized DCT* hardware implementation that was automatically generated. Overall, the backend C compiler proves to be effective in targeting the Virtex II Pro PowerPC processor and incorporating the Molen architecture programming paradigm.
411654	4116549	Instruction Scheduling For Dynamic Hardware Configurations	The current Field-Programmable Gate Array (FPGA) platforms have a known issue of long reconfiguration times, which can negatively impact performance. However, little research has been done on instruction scheduling to address this problem. This paper presents a new algorithm that reduces the number of hardware reconfiguration instructions by considering conflicts between different FPGA configurations. The algorithm uses compiler analyses and feedback-directed techniques and can switch from hardware to software execution when needed. It was tested on the M-JPEG encoder application and real hardware implementations, resulting in up to 16 times faster speed for the encoder. Overall, the proposed scheduling algorithm significantly improves performance compared to a simple scheduling approach.
411655	4116556	Types for safe locking: Static race detection for Java	This article discusses a static race-detection analysis for multithreaded shared-memory programs in Java. The analysis is based on a type system that can identify common synchronization patterns and supports different types of classes. The authors have implemented the type system in a checker and applied it to over 40,000 lines of hand-annotated Java code, finding multiple race conditions. They also present two improvements that allow for checking larger programs, including an annotation inference algorithm and a user interface for clarifying warnings. These extensions have allowed the checker to be used on software systems with up to 500,000 lines of code. The overall effectiveness of the type system is demonstrated by the relatively low number of additional annotations required per 1,000 lines of code.
411655	41165552	Types for atomicity	Multithreaded programs can be difficult to ensure correctness in, as unexpected and nondeterministic interactions between threads can occur. Previous approaches focused on detecting race conditions, where two threads access the same data variable simultaneously, but this is not enough to guarantee error-free operation. The proposed solution is a type system that checks for atomicity, meaning that code blocks are executed without interleaving with other threads. This allows for higher-level reasoning about program behavior and simplifies both formal and informal analysis. The type system has been successfully applied to verify methods in java.util. Vector that were previously rejected by race detection type systems.
411656	41165621	Exploiting fact verbalisation in conceptual information modelling	Many information modelling approaches now use verbalisation techniques to create a model for a specific problem domain. This involves using verbal descriptions of facts from the domain to understand the relevant concepts and their relationships. These verbalisations also help validate the resulting model for user familiarity. This approach is present in various modelling techniques such as ER, Object-Role Modelling, and Object-Oriented Modelling. However, after the modelling process, the verbalisations are not utilized further. This article discusses the potential of using these verbalisations in different ways, including a conceptual query language, describing database contents, and formulating queries in a computer-supported environment. An example is also provided to demonstrate how verbalisations can be used in an end-user query tool.
411656	41165616	The modelling and retrieval of documents using index expressions	This paper introduces Index Expressions as a way to represent document content. These expressions can then be used to derive the Power Index Expression, a tool for information retrieval. The document content is described using formal logic and represented by a set of axioms, with the document serving as a model. To determine the relevance of a document to a query, the query is proven using the document's axioms. If a proof is not possible, two rules of inference for plausible deduction are introduced. 
411657	411657145	Efficient Lattice Boltzmann Solver for Patient-Specific Radiofrequency Ablation of Hepatic Tumors	Radiofrequency ablation (RFA) is a well-established treatment for liver cancer when surgery is not an option. However, its effectiveness can be limited by the presence of large blood vessels and the varying thermal properties of tissue. This can lead to incomplete treatment and a higher risk of recurrence. To address this, a new method has been developed that uses patient-specific images and a computational model to accurately plan the extent of ablation needed. This method was verified and evaluated on 10 patients, showing promising results. It also highlights the importance of considering liver perfusion in the simulation. The method can be implemented in real-time, making it a valuable tool for RFA treatment planning.
411657	411657158	Parameter Estimation for Personalization of Liver Tumor Radiofrequency Ablation	Mathematical modeling can be used to assist in the radiofrequency ablation (RFA) of tumors by predicting the extent of ablation. However, the accuracy of the simulation is affected by patient-specific material properties that are dependent on temperature and space. This paper presents a framework for patient-specific RFA modeling of multiple lesions in metastatic diseases. The framework includes a computational model of heat diffusion, cellular necrosis, and blood flow through vessels and liver, using patient images. The most sensitive material parameters are estimated using inverse modeling, and personalized parameters are used to predict the ablation of remaining lesions. The framework was tested on seven lesions from three patients, with good correlation between predicted and actual ablation extents.
411658	41165812	Practical Techniques for Searches on Encrypted Data	Data storage servers, such as mail and file servers, often store data in encrypted form to mitigate security and privacy risks. However, this can limit functionality, such as performing searches on the data without compromising confidentiality. In this paper, the authors propose cryptographic schemes that allow for searching on encrypted data while maintaining provable security. These schemes offer several advantages, including secrecy of the plaintext, isolation of search queries, controlled searching, and support for hidden queries. Additionally, the proposed algorithms are efficient, with minimal space and communication overhead, making them practical for use today.
411658	41165822	Timing analysis of keystrokes and timing attacks on SSH	.SSH is a secure communication protocol used between two hosts. However, it has two weaknesses that can reveal sensitive information about users' passwords and typing patterns. First, the padding of transmitted packets reveals the size of original data, and second, the timing of keystrokes can be used to predict key sequences and passwords. These vulnerabilities can be exploited by an eavesdropper, who can use statistical techniques and a program called Herbivore to learn passwords by monitoring SSH sessions. The authors propose countermeasures and caution against designing similar protocols without considering timing leaks. These risks apply not only to SSH, but also to other protocols for encrypting interactive traffic.
411659	41165978	A new fuzzy interpolative reasoning method based on the areas of fuzzy sets	In sparse fuzzy rule-based systems, fuzzy interpolative reasoning is an important inference technique. This paper introduces a new method for fuzzy interpolative reasoning based on the areas of fuzzy sets. The method ensures the normality and convexity of the conclusion and can handle complex membership functions such as hexagons, polygons, and Gaussians. It is also capable of dealing with fuzzy rules that have different types of membership functions for the antecedents and consequents. The proposed method is compared to existing methods using various examples, and the results show that it is more effective for fuzzy interpolative reasoning. This method provides a useful approach for handling fuzzy interpolative reasoning in sparse fuzzy rule-based systems.
411659	4116598	Multicriteria fuzzy decision making based on interval-valued intuitionistic fuzzy sets	This paper introduces a new method for multicriteria fuzzy decision making using interval-valued intuitionistic fuzzy sets. The method utilizes interval-valued intuitionistic fuzzy values to represent the decision-maker's evaluations of alternatives. A new ranking method for these values is proposed, which is then used in a multicriteria fuzzy decision making method. This method performs better than a previous method by Ye (2009) as it overcomes a drawback in Ye's method where the ranking order between alternatives cannot be distinguished in certain situations. The proposed method is a useful approach for dealing with multicriteria fuzzy decision making problems.
411660	41166059	Characterization of Efficiently Solvable Problems on Distance-Hereditary Graphs	In the literature, various algorithms have been developed to solve problems in distance-hereditary graphs using techniques based on the properties of the graph. By examining the structural properties of these graphs, a general problem-solving approach has been identified. This approach involves using a decomposition tree representation of the graph to construct sequential dynamic-programming algorithms for fundamental graph-theoretical problems. These algorithms can also be efficiently parallelized using the tree contraction technique. This unified approach allows for systematic solving of problems on distance-hereditary graphs and has been shown to be effective in solving multiple problems in this type of graph.
411660	41166066	A Scripting Language for Automating Secure Multiparty Computation	This paper discusses a new scripting language designed to automate the creation of complex protocols for a commodity-based approach to secure multi-party computation (SMC). The language models each participating party in a peer-to-peer manner, with private data and intermediate results being shared jointly. The authors propose a three-level security attribute system (public, private, and shared) for users to express their security requirements by associating variables with these attributes. The language also directs the compiler to perform security checks and generate code accordingly. The paper demonstrates how the scripting language can be used to express complex protocols and how it works with a distributed SMC runtime environment. 
411661	41166172	Invited talk: towards declarative programming for web services	The World Wide Web (WWW) is seeing two emerging trends - the rise of Web Services and the development of the Semantic Web. Web Services are self-contained, accessible software applications that are used for distributed systems. The Semantic Web is the next generation of the WWW, designed to be easily understood by computers. To achieve reliable, large-scale automation of Web Services, a declarative language for describing their properties and capabilities is needed. This enables tasks such as Web service discovery, invocation, composition, and monitoring. To address the issue of automated Web service composition, researchers have proposed reasoning techniques and a logic programming language that allows for generic and customizable programs. A middle-ground interpreter is also suggested for information gathering and search. This work has been done in collaboration with others, and related research can be found in various sources.
411661	41166151	Bringing Semantics to Web Services with OWL-S	The current industry standards for Web Services focus on ensuring compatibility between different platforms, but they do not provide a strong foundation for automating the use of Web Services. To address this issue, representational techniques from the Semantic Web can be incorporated into these standards. This results in the creation of Web Service specifications that allow software programs to interpret and utilize unfamiliar Web Services to fulfill user goals. OWL-S, also known as "OWL for Services," is a set of notations based on the Semantic Web ontology language OWL that allows for the expression of these specifications. It includes a profile ontology, process ontology, and grounding ontology, which can be used to automate service-related tasks such as discovery, interoperation, and composition. A significant amount of research has been conducted on OWL-S, leading to the development of various open-source tools for creating, reasoning about, and using Web Services. 
411662	41166228	Designing Learning by Teaching Agents: The Betty's Brain System	Teaching others is an effective way to learn, backed by research. Teachable Agents, computer-based tools, allow students to teach using visuals and monitor their own learning and problem-solving. This motivates students to learn more and improve their agent's performance. Betty's Brain is a Teachable Agent that combines teaching with self-regulated learning feedback to enhance science understanding. A study in a 5th grade classroom compared three versions of the system: one where students were taught by an agent, a baseline learning by teaching version, and a version where students received feedback on self-regulated learning and domain content. Results showed all groups made learning gains, but the two learning by teaching groups performed better. The self-regulated learning group showed better preparation for learning in new domains even without access to the self-regulation environment. 
411662	41166268	Towards Using Coherence Analysis to Scaffold Students in Open-Ended Learning Environments.	Scaffolding students in open-ended learning environments (OELEs) is a challenging task due to the nature of these environments, which allow students to pursue, modify, and abandon various approaches to completing tasks. To address this challenge, a new approach called coherence analysis has been developed, which focuses on students' ability to interpret and use information in OELEs. This approach has been successful in providing teachers and researchers with measures of students' problem-solving processes. The next step in this research is to create a scaffolding framework using coherence analysis to support students in OELEs. This paper presents initial ideas and guidelines for constructing this framework.
411663	41166319	An adaptive meta-heuristic approach using partial optimization to non-convex polygons allocation problem	The two-dimensional packing problem involves placing non-convex polygons of various shapes on a sheet in a way that minimizes waste. To solve this problem, a new method is proposed using an adaptive meta-heuristics approach with partial optimization. This algorithm divides objects into subgroups and uses meta-heuristic techniques to allocate them, resulting in improved search efficiency. The method was tested through simulation experiments, which showed superior performance compared to using adaptive meta-heuristics alone. 
411663	411663110	Crisp and fuzzy methods of optimal clustering on networks of objects	The paper proposes two methods for optimal clustering using general dissimilarity measures on a network of objects. The first method utilizes meta-heuristic techniques such as local search, simulated annealing, genetic algorithms, and tabu search to optimize the clustering. The effectiveness of these methods is evaluated through numerical experiments and a parameterized objective function is used to determine the most suitable option. The second method is a variation of fuzzy c-means for Euclidean space, which is developed through a relatively simple algorithm. Numerical examples are provided to demonstrate the effectiveness of this method. Overall, the paper presents two approaches for optimal clustering on a network of objects, one using meta-heuristic techniques and the other using fuzzy clustering. 
411664	41166431	From Cluster Monitoring to Grid Monitoring Based on GRM	GRM, or Grid Resource Management, was initially created as a component of the P-GRADE program development environment for supercomputers and clusters. However, in the DataGrid project, the potential for using GRM as a grid application monitoring tool was explored. This led to the redesign of GRM to function independently as a grid monitoring tool. This paper discusses the changes made to GRM's architecture to enable its standalone use in monitoring grid applications.
411664	41166434	P-GRADE: A Grid Programming Environment	P-GRADE is a graphical tool that allows users to develop parallel applications for both parallel systems and the Grid. It supports interactive execution of programs and can create jobs for execution in the Grid using Condor, Condor-G, or Globus. P-GRADE can generate PVM or MPI code based on the underlying Grid. PVM applications generated by P-GRADE can move between Grid sites, ensuring reliable and fault-tolerant execution. The tool also includes GRM/PROVE performance monitoring, which has been extended for use on the Grid, allowing for remote monitoring and analysis of parallel applications. P-GRADE also supports workflow definition and multi-job execution on the Grid, with automatic checkpointing to support fault-tolerant execution. This paper describes all these features and their implementation in P-GRADE.
411665	41166517	Database systems: achievements and opportunities	Database system research in the U.S. has had a significant impact on the economy, with a $10 billion per year information services industry that has been growing at an average rate of 20 percent since 1965. This success is due to the support of federal funding in universities and industrial research labs. The advancements in database research have not only contributed to the growth of the information services industry, but also to other areas such as communications, transportation, finance, and science. These achievements also serve as a basis for progress in various fields, including computing and biology. Despite being a relatively young field, the history of database system research in the U.S. has been marked by exceptional productivity and significant economic impact.
411665	41166529	Querying websites using compact skeletons	The integration of information from multiple websites or XML documents is a common requirement for commercial applications. This has led to a significant amount of research and development in the area, resulting in various prototypes and commercial implementations. One approach to this problem is using wrappers, which provide structured interfaces to websites. However, manually constructing wrappers for each website can limit scalability. To address this, a mechanism called compact skeletons has been introduced, which automatically generates wrappers from website structures. This approach has been studied and tested, showing promising results and being considered a natural extension of existing wrapper construction techniques.
411666	41166690	Perception of Springs With Visual and Proprioceptive Motion Cues: Implications for Prosthetics.	A study has found that controlling objects with an upper limb prosthesis requires more visual attention than using an intact limb. To address this issue, researchers investigated the difference between visually seeing a virtual prosthetic limb move and feeling one's real limb move. Fifteen participants controlled a virtual prosthetic finger using a haptic device and were tested on their ability to discriminate between different levels of stiffness. The study found no significant performance differences between using visual or proprioceptive information, but participants perceived proprioceptive information to be more useful. This suggests that conveying proprioceptive information through a nonvisual channel could improve the experience of using an upper limb prosthesis by reducing the need for visual attention.
411666	411666123	Sensory augmentation of stiffness using fingerpad skin stretch	Interacting with everyday objects involves experiencing kinesthetic force and tactile feedback. Skin stretch, caused by friction between the skin and objects, is a type of tactile feedback. Stiffer objects result in more force and skin stretch. Researchers believe that adding artificial skin stretch to kinesthetic force feedback can enhance the perception of stiffness. The Skin Stretch Stylus was created to combine these two types of feedback. A study was conducted using a two-alternative forced-choice paradigm to compare the stiffness perception of virtual springs with and without added skin stretch feedback. Results showed a significant increase in stiffness perception with the addition of skin stretch, which was greater with higher levels of skin stretch. This suggests that skin stretch feedback could be used to improve stiffness perception in situations where force feedback gains are limited, such as in teleoperation systems or haptic devices with low actuator force.
411667	41166714	Slip classification for dynamic tactile array sensors.	The article discusses the ability of humans to distinguish between different events that occur during the manipulation of objects with their hands, such as contact and slippage. This is because humans have specialized mechanoreceptors in their hands that respond differently to these events. The article proposes two features that can be extracted from dynamic tactile array data to help robots discriminate between hand/object and object/world slips. These features are effective for a wide range of frequencies and grasp conditions and do not require extensive training. Testing on different sensors and object textures shows that these features have a high accuracy in identifying the location of slip.
411667	41166713	Manipulation with soft fingers: contact force control	This article discusses the issue of controlling contact forces in dextrous devices when the fingers first close on an object. The authors propose the use of active fingertips to improve performance and prevent instability problems caused by noncollocated sensors and actuators. The first step in addressing this issue is studying an ideal fingertip in contact with an object, using analytical and simulation studies to determine the optimal characteristics for desired performance. The article then explores more realistic fingers with limited controllability and presents preliminary experimental results for a prototype semi-active fingertip that utilizes an electrorheological fluid for active damping control. The authors also briefly mention future plans to implement these active fingertips in manipulation tasks.
411668	41166848	Power Optimization in Fault-Tolerant Mobile Ad Hoc Networks	This paper discusses the problem of optimizing the lifetime of k-connected Mobile Ad hoc NETworks (MANETs) by minimizing power consumption. The proposed solution is a fully distributed model-based transmission power adaptation strategy that uses model-predictive control. This involves using a stochastic model and state estimator to predict the network's future connectivity and energy levels, and then using an optimizer to derive an optimal transmission power assignment sequence. Results from experiments on a simulated wireless sensor network with 100 nodes show that this localized topology control algorithm is nearly as effective as a globalized approach, but with less communication and energy requirements, making it more scalable.
411668	41166855	Focusing on Mobility	In this paper, the authors discuss the significance of mobile computing and review current practical and theoretical methods. The main focus is to establish and clarify the key principles of mobile systems through a precise and mathematical framework. The model proposed is an extension of existing network models, incorporating the concept of locations as spaces containing components. The model allows for dynamic changes in the containment relationship to represent the migration of components between locations. Using this formal model, the authors define essential properties and features such as network transparency. This paper contributes to the understanding and formalization of mobile systems.
411669	411669211	Relay Selection for OFDM Wireless Systems under Asymmetric Information: A Contract-Theory Based Approach	User cooperation is beneficial for wireless systems, but it requires incentives for nodes to act as relays. The problem is that potential relays have more information about their transmission costs than the source, causing asymmetry. This paper uses contract theory to address relay selection in OFDM-based systems with decode-and-forward relaying. Incentive-compatible contracts are designed and broadcasted to nearby nodes. These nodes can then accept contracts for specific subcarriers. The problem of relay selection is a nonlinear non-separable knapsack problem, but a heuristic solution is proposed. Numerical results show that this solution performs well compared to a simple relay selection scheme. The overall mechanism is simple to implement and has minimal signalling overhead.
411669	41166981	Power Allocation for Decode-and-Forward Cellular Relay Network with Channel Uncertainty	The paper presents power allocation algorithms for a multi-user, multi relay cellular network using decode-and-forward cooperation strategy. The objective is to minimize total uplink power while ensuring each user's target data rate as the quality of service constraint, even with imperfect channel state information. A centralized and a distributed algorithm are proposed, both based on the worst-case optimization approach. The centralized algorithm optimally allocates power among users and relay nodes, while the distributed algorithm allows relays to independently minimize their own power. These algorithms use second order cone programming and simulation results show that the distributed solution is near-optimal and more efficient in reducing power and handling channel uncertainty compared to non-cooperative systems.
411670	4116700	FlowNet: A Deep Learning Framework for Clustering and Selection of Streamlines and Stream Surfaces.	The paper discusses the problem of identifying representative flow lines and surfaces in flow visualization. While this has been studied before, no existing method can effectively solve the problem for both lines and surfaces. The authors propose a single deep learning framework called FlowNet, which uses an autoencoder to learn latent feature descriptors for streamlines and stream surfaces. These descriptors are used for error estimation and network training. Through clustering and dimensionality reduction, the framework allows for easy exploration and selection of representative flow lines and surfaces. The effectiveness of FlowNet is demonstrated through various flow field data sets and comparison with other selection algorithms. The paper also provides intuitive user interactions for visual reasoning of the flow lines and surfaces.
411670	41167057	A deformation framework for focus+context flow visualization.	This paper presents a new approach for visualizing large and complex three-dimensional flow fields. The goal is to balance coverage, occlusion, and complexity in order to better understand the data. The proposed method uses a deformation framework to manipulate the positions of streamlines, rather than varying their densities. This is achieved by dividing the flow field into blocks and deforming them to guide streamline repositioning. The method also includes energy terms and constraints to minimize distortion and allow for interactive visualization. The results show that this approach is more effective than existing techniques, as it can magnify multiple streamlines in different regions simultaneously while reducing occlusion and clutter. Automatic and manual focus selection options are also provided for flexibility in visualization.
411671	41167148	Effects of short-time plasticity on the associative memory	This study examines the impact of short-term synaptic plasticity on associative memory networks. Through analysis and simulations, it is determined that synaptic dynamics do not affect the retrieval of information, but do affect its stability. This can lead to a lower storage capacity in cases of depression. Additionally, the network operates in an oscillatory manner, which is not seen in networks with static connections. The study also suggests that the dynamics and diversity of synapses can aid in switching between memory states and storing patterns in sequence.
411671	41167115	Synaptic Depression in Associative Memory Networks	This study examines how synaptic depression impacts the stability of patterns stored in neural networks with low activity levels. By using mean-field theory, the researchers find that the stationary states are not affected by synaptic depression. However, the stability of memory patterns is significantly reduced, resulting in a lower memory capacity. The study also shows that the network becomes more sensitive to changes in input. Numerical calculations support these findings. Overall, the results suggest that synaptic depression can have a significant impact on the stability and capacity of memory patterns in neural networks.
411672	41167211	Approximate Boolean Reasoning: Foundations and Applications in Data Mining	Boolean algebra, introduced by George Boole in the mid-1800s, has become an important tool in mathematics, science, engineering, and artificial intelligence research. It has proven to be effective in decision-making and approximation optimization. In recent years, it has also been recognized as a useful technique in rough set theory for developing concept approximation methods. This paper presents a general framework, called Rough Sets and Approximate Boolean Reasoning (RSABR), which combines rough set theory, Boolean reasoning, and data mining. It addresses the need for a general framework in machine learning and data mining and provides solutions for various data mining problems such as feature selection, feature extraction, data preprocessing, and classification. The paper discusses the theoretical foundation of RSABR and its practical applications in knowledge discovery in databases.
411672	41167210	Rough Set Approach to KDD (Extended Abstract)	This tutorial provides an overview of rough set theory and its applications in Knowledge Discovery from Databases (KDD). It includes a practical guide for using rough set methods to analyze real life problems and introduces the Rough Set Exploration System (RSES) as a preliminary resource for conferences and workshops. Rough set theory, introduced by Zdzisław Pawlak in the 1980s, has gained significant visibility and maturity over the years. The theory is based on the concept of indiscernibility and discernibility of objects and was initially used for concept approximation under uncertainty. However, it has since been expanded and many data analysis methods have been developed using rough set theory. 
411673	41167354	On the spanning fan-connectivity of graphs	The connectivity of a graph G, @k(G), is the maximum number of paths that can be drawn between two distinct vertices without overlapping. A spanning container is a set of paths that covers all the vertices in G. A graph is k^*-connected if there is a spanning k-container between any two vertices. The spanning connectivity of G, @k^*(G), is the maximum k for which G is connected. A spanningk-(x,U)-fan is a set of paths connecting a vertex x to a subset of vertices U. A graph is k^*-fan-connected if there is a spanning F"k(x,U)-fan for every choice of x and U. The spanning fan-connectivity, @k"f^*(G), is the maximum k for which G is k^*-fan-connected. The paper discusses the relationship between @k(G), @k^*(G), and @k"f^*(G), and presents conditions for a graph to be k"f^*-connected and k^*-pipeline-connected.
411673	41167326	On the spanning connectivity and spanning laceability of hypercube-like networks	In an undirected graph G, u and v are two distinct nodes and G is k-connected. A w-container C(u,v) is a set of w-disjoint paths joining u and v. If C(u,v) contains all the nodes of G, it is a w^*-container. A graph is w^*-connected if there is a w^*-container between any two nodes. A bipartite graph is w^*-laceable if there is a w^*-container between any two nodes from different parts. Two disjoint graphs, G"0 and G"1, with equal number of nodes, can be combined to form a new graph G. The set of n-dimensional hypercube-like graphs, H"n^', is defined recursively. Every graph in the set B"n^' and N"n^' (a combination of H"n^' and bipartite graphs) is w^*-laceable for w values between 1 and n. A constructed N"n^'-graph is not 4^*-connected and every graph in N"n^' is w^*-connected for w values between 1 and 3.
411674	41167468	The globally Bi-3*-connected property of the honeycomb rectangular torus	The honeycomb rectangular torus, represented by HReT(m,n), is a 3-regular bipartite graph that has gained attention as a cost-effective and well-connected network for parallel and distributed applications. In this network, with n=4 and m being a positive even integer, it has been proven that there are always three internally-disjoint paths connecting any two vertices belonging to different parts. Additionally, for vertices in the same part, there is always a third vertex in the opposite part that allows for three internally-disjoint paths to exist after removing it from the network. However, this property is only true for all three vertices being in the same part if n=6 or m=2. 
411674	41167480	Spider web networks: a family of optimal, fault tolerant, hamiltonian bipartite graphs	The paper introduces a new variation of a honeycomb mesh, called a spider web network. This network is a 3-regular bipartite planar graph with two parts, C and D. It is proven that the honeycomb rectangular mesh is a subgraph of the spider web network. Additionally, it is shown that removing one edge from the spider web network still results in a hamiltonian graph, and removing two specific vertices also maintains this property. These results are considered optimal. 
411675	41167529	Steering symbolic execution to less traveled paths	Symbolic execution is a powerful method for testing and analyzing programs by systematically exploring their execution space and generating high-coverage test cases. However, the huge number of possible program paths in real-world programs presents a challenge for symbolic execution. Existing heuristics for guiding symbolic execution are often inefficient and ad-hoc. In this paper, the authors propose a new approach that utilizes a specific type of path spectra, called length-n subpath program spectra, to prioritize less explored parts of a program. By using frequency distributions of explored length-n subpaths, their strategy improves test coverage and error detection. This approach has been implemented in the KLEE engine and evaluation results on GNU Coreutils programs show its effectiveness compared to traditional strategies.
411675	41167514	Systematically Ensuring the Confidence of Real-Time Home Automation IoT Systems.	Recently, there has been a rapid increase in the adoption of Internet of Things (IoT) systems in the real world, leading to the need for effective management of these connected devices. This has resulted in the development of popular home automation IoT frameworks like IFTTT, Apple HomeKit, and Google Brillo. However, the complexity of these systems and the lack of expertise among users can lead to bugs and safety concerns. To address this, the authors propose a hybrid automata-based framework called MenShen, which automatically checks the confidence properties of these systems and suggests fixes. Empirical studies have shown that MenShen is able to identify violations and generate solutions in just 10 seconds.
411676	41167627	Enabling multi-core based monitoring and fault tolerance in C++/Java	Monitoring and fault tolerance are important for ensuring the reliability of long-running online software systems. However, these approaches can be costly in terms of efficiency. Multi-core platforms can help mitigate this cost by taking advantage of parallel performance. To make it easier for ordinary software developers without knowledge of multi-core platforms to efficiently handle these tasks, the authors propose an approach for enabling multi-core based monitoring and fault tolerance in C++/Java. This involves presenting the principles of multi-core based monitoring and fault tolerance, designing and implementing tasks through multi-process/thread programming, and using special annotations for developers to specify a multi-core based design. A prototype tool, McC++/Java, is then used to automatically map these annotations to the runtime platform, making it an accessible and efficient approach for a wide range of software developers.
411676	41167648	A case study for monitoring-oriented programming in multi-core architecture	The development of multi-core architecture has led to the creation of high performance computing platforms to meet the demand for solving large and complex problems. In the field of verification, multi-core architecture has become a popular trend for improving performance. As software sizes continue to grow, Monitoring-Oriented Programming (MOP) is seen as a design trend to increase reliability. A case study was conducted to explore the integration of MOP with multi-core architecture, which showed that it can improve the efficiency of MOP. This lays the foundation for future discussions on how to design effective monitors.
411677	41167747	Probabilistic analysis of onion routing in a black-box model	The study analyzed the security of onion routing, a method of anonymous communication, using a black-box model in the Universally Composable framework. The model considered an active adversary who controls part of the network and knows user preferences for destinations. Results showed that the adversary can gain information about users by exploiting their probabilistic behavior. In larger networks, a user's anonymity is most at risk when other users always choose either the least likely or the same destination as the user. The study also found that the worst-case anonymity against an adversary controlling a fraction b of routers is similar to the best-case anonymity against an adversary controlling the square root of b.
411677	41167729	As-awareness in Tor path selection	Tor is a network that allows users to communicate anonymously through thousands of router nodes around the world. It is believed that as the network grows, it becomes more secure against potential observers because there is less of the network visible to them. However, no analysis has been done to confirm this belief and no changes have been made to Tor's path selection algorithm to address this potential threat. A recent study using improved data and modeling methods shows that the threat of a single autonomous system (AS) observing both ends of a Tor connection is higher than previously thought, despite the network's significant growth in the past five years. The study also proposes and evaluates new path selection algorithms that can better protect against AS-level observers.
411678	411678144	Reliability maps for probabilistic guarantees of task motion for robotic manipulators.	The use of reliable and safe robots is becoming increasingly important in various applications, such as assisting disabled or elderly individuals and performing surgical procedures. However, different tasks require varying levels of safety and reliability, making it crucial to understand the reliability of robots. Currently, fault-tolerant workspaces are used to ensure task completion, but they do not consider the reliability of different joints in the robotic manipulator. This paper seeks to address this issue by introducing the concept of reliable fault-tolerant workspaces, which takes into account the reliability of each joint. This is achieved through the use of conditional reliability maps, which can improve the performance of robots while ensuring their reliability for completing tasks. 
411678	411678226	Workspace analysis of two similar 3-DOF axis-symmetric parallel manipulators	This paper discusses the issue of limited workspace to footprint ratio in parallel manipulators and presents a workspace analysis of two axis-symmetric manipulators designed to overcome this problem. The manipulators are parametrized and their inverse kinematic models are derived, followed by a singularity analysis. Using a meta-heuristic algorithm, architectural parameters are generated to maximize the singularity-free workspace for one of the manipulators. Another parameter calculation is then conducted to explore the relationship between maximizing the global conditioning index (GCI) and the achievable workspace for the manipulators. This study provides valuable insights for improving the workspace of parallel manipulators.
411679	41167918	Circularly polarized spherical illumination reflectometry	We have developed a new technique for accurately measuring the surface reflectance of objects using a single uniform spherical light source. This method analyzes the Stokes reflectance field of circularly polarized light and can estimate the diffuse albedo, specular albedo, index of refraction, and specular roughness of isotropic BRDFs for each pixel in the scene. To gather this information, we take four photographs of the object with different polarizers and measure the Stokes parameters of the reflected light. Our method only requires knowledge of the surface orientation, which can be obtained with a few additional photometric measurements. We have tested our method with various lighting setups and it offers several advantages such as a more detailed model, minimal measurements, applicability to different materials, and independence from the viewpoint. 
411679	41167910	Polarization imaging reflectometry in the wild.	The authors propose a new method for accurately capturing surface reflectance and normal maps for planar, spatially varying, isotropic samples in uncontrolled outdoor environments. The method utilizes the linear polarization of incident and reflected light, and involves rotating a polarizing filter in front of a camera at three different orientations. This information is combined with multi-view analysis and inverse rendering to recover per-pixel, high resolution reflectance and surface normal maps. The method is designed to work in completely uncontrolled outdoor settings and can be implemented using an entry-level DSLR or mobile phone. The authors also analyze the method's performance under different outdoor conditions and provide practical guidelines for on-site acquisition.
411680	41168012	Providing QoS Guarantees in Large-Scale Operator Networks	This paper discusses the challenges of ensuring quality of service (QoS) guarantees in application areas such as global sensor networks and data stream processing, where large amounts of data are processed in an overlay network on top of the Internet. The paper proposes a formalized constrained optimization problem for placing operators in the network to meet user QoS constraints and minimize network load. To solve this NP-hard problem, the paper presents a two-step approach: first solving in a continuous latency space and then mapping to a discrete solution. Evaluations show that this algorithm effectively balances user requirements and network resource usage.
411680	41168079	Removing the memory limitations of sensor networks with flash-based virtual memory	Virtual memory has been successfully used in various fields to expand the available memory for applications. This concept has now been adapted for use in sensor networks, where RAM is typically limited. The paper presents ViMem, a virtual memory approach that uses compile-time optimizations to minimize overhead and make it practical for use in sensor networks. The system uses a memory layout based on access traces from simulation tools, optimized for the application's memory access patterns and the hardware of the sensor network. ViMem is implemented in TinyOS and includes a pre-compiler for nesC code. It uses flash memory as secondary storage and can be evaluated with existing applications, showing minimal runtime overhead even with large data sizes.
411681	41168149	Common due-window problem: Polynomial algorithms for a given processing sequence	The paper discusses the Common Due-Window (CDW) problem, where a single machine processes multiple jobs with different processing times and asymmetric penalties for early and late completion. The goal is to minimize the total penalty by determining the job sequence, completion times, and due-window position. The paper presents polynomial algorithms with a runtime complexity of O(n2) for optimizing a given job sequence and an O(n) algorithm for unit processing times. These algorithms take a sequence of jobs as input and return the optimal completion times. Additionally, the authors use Simulated Annealing to improve the results and compare them with previous research. 
411681	41168141	Global versus local search: the impact of population sizes on evolutionary algorithm performance.	In the field of Evolutionary Computation, there is a common belief that an Evolutionary Algorithm (EA) will always outperform a local search algorithm if given enough time and a large enough population. However, this is not necessarily true and can be challenged with simple considerations. The population size parameter of EAs can be controlled and there is evidence that there is an optimal setting for it to perform best on a given problem instance and computational budget. A study was conducted on 68 instances of the Traveling Salesman Problem with both static and adaptive population sizes. The results support the idea that there are optimal finite population size settings for EAs.
411682	41168243	Empirical analysis of evolutionary algorithms with immigrants schemes for dynamic optimization	The study of evolutionary algorithms (EAs) for dynamic optimization problems (DOPs) has become increasingly popular in recent years. One approach that has shown to be beneficial is the use of immigrants schemes, which aim to maintain diversity in the population by introducing new individuals. This study examines the mechanism of generating immigrants, dividing existing schemes into two types: direct and indirect. Experiments are conducted to compare their performance in dynamic environments, and a new scheme that combines the merits of both types is proposed. Results show that this combination improves the performance of EAs in dynamic environments. Overall, immigrants schemes have proven to be effective in addressing DOPs in EAs.
411682	41168235	A large population size can be unhelpful in evolutionary algorithms	Evolutionary algorithms (EAs) use populations to determine the best solution to a problem. Many studies have looked at the effect of different population sizes on the performance of EAs, but most of these studies have been based on computational experiments. The general belief is that a larger population will increase diversity and improve the performance of an EA. However, a study by He and Yao (2002) showed that for certain problem instances, a large population can actually decrease the runtime of an EA from exponential to polynomial time. This paper further explores the role of population in EAs and demonstrates that large populations may not always be beneficial. The analysis also identifies problem characteristics that can lead to the disadvantages of large population sizes. The findings have implications for other problems and the analytical approach used can be applied to analyze EAs on different problems. 
411683	41168326	An algorithm for shifted continued fraction expansions in parallel linear time	The Berlekamp-Massey algorithm (BMA) can efficiently determine the linear complexity profile of a sequence of length n in O(n2) steps. However, to ensure that the profile is acceptable for all shifted sequences, Piper requires the BMA to be repeated, resulting in O(n3) steps. This paper introduces a transducer that can compute the continued fraction expansions of all n shifted sequences in only Cq·n+12Fq operations, where C2=7 and Cq=6.5 for q⩾3. This eliminates the need for additional computational effort to check Piper's demand. When n transducers are used in parallel, the output can be obtained in parallel linear time, with an overall time-space product of O(n2·log n). Keywords include continued fraction expansion, linear complexity profile, Berlekamp-Massey algorithm, and transducer.
411683	41168337	Simultaneous Shifted Continued Fraction Expansions in Quadratic Time	This paper discusses the use of continued fraction expansion (c.f.e.) in assessing the randomness of a bit sequence in stream ciphers. The linear and jump complexity are important measures obtained from the c.f.e. of the generating function. The paper presents a method to compute all c.f.e.s and linear complexity profiles (l.c.p.'s) of shifted sequences simultaneously, requiring a maximum of bit operations on bit space. If the sequence is not fixed, the computation starts from the initial sequence. This results in bit operations on space or bit operations on linear space. In comparison, the Berlekamp-Massey algorithm requires O steps, while a recent algorithm by Stephens and Lunnon works in O integer operations but only gives the l.c.p. and not the complete c.f.e.
411684	41168431	Optimal rate list decoding of folded algebraic-geometric codes over constant-sized alphabets	We have developed a new type of algebraic-geometric (AG) codes that can be list-decoded up to an error fraction approaching 1-R, where R is the rate. These codes are constructed using class field theory and Drinfeld modules of rank 1, which allows us to use a large-order automorphism to "fold" the AG code. This is a generalization of previous work on folded AG codes based on cyclotomic function fields. The Chebotarev density theorem is used to establish a polynomial upper bound on the list-size for decoding, and this can be done in polynomial time with pre-processed information about the function field. Our codes can be list-decoded up to the Singleton bound, with an alphabet size of (1/ε)O(1/ε2) and close-by messages confined to a subspace with NO(1/ε2) elements. This is an improvement compared to previous methods that either used concatenation or involved subcodes of AG codes. Our approach also has potential for independent algebraic applications.
411684	41168439	List decoding reed-solomon, algebraic-geometric, and gabidulin subcodes up to the singleton bound	The authors propose a linear-algebraic list decoding algorithm for Reed-Solomon (RS) codes with evaluation points in a subfield. This algorithm can correct a high fraction of errors close to the code distance, while limiting the possible solutions to a smaller affine space. By pre-coding message polynomials into a subspace-evasive set, they construct a subcode of RS codes that can be list decoded from a fraction (1-R-ε) of errors in polynomial time. This method also extends to algebraic-geometric (AG) codes, and can achieve similar results over constant-sized alphabets. The approach is also applicable to Gabidulin's rank-metric codes, providing the first construction of positive rate codes list decodable beyond half the distance. The authors also introduce a new concept called subspace designs, which allows for deterministic constructions of polynomial time list decodable subcodes for RS codes and AG codes. This extends to a deterministic construction of an algebraic code family for AG codes that is list decodable from a large fraction of errors.
411685	41168531	Can We Interpret the Depth?: Evaluating Variation in Stereoscopic Depth for Encoding Aspects of Non-Spatial Data.	The article discusses the use of a 2D node-link representation on a stereoscopic platform to enhance focus and context interactions. This approach utilizes stereoscopic depth to highlight structural relations and allows for interactive operations such as expanding or contracting nodes in compound graphs. Different visual cues, such as color and shape, can also be used to encode data aspects. A user study with 30 participants was conducted to evaluate the effectiveness of this approach, considering factors such as graph size and transparency. The results show that stereoscopic depth can be used to encode data in compound graphs, but there are limitations based on certain circumstances. Overall, the study found that participants were generally accepting of using stereoscopic depth as a cue for compound graphs.
411685	41168536	Utilization of variation in stereoscopic depth for encoding aspects of non-spatial data	The paper discusses the use of stereoscopic depth to highlight structural relations in compound graphs. This is achieved by encoding different levels-of-details and using the ExpanD technique to expand or contract nodes in order to reduce occlusion. A study with 30 participants was conducted to evaluate the approach, using various graph sizes and visual clues. The results showed that stereoscopic depth can be effective in encoding data aspects of graphs in certain situations.
411686	41168653	Spring-based manipulation tools for virtual environments	This paper introduces new tools for user interaction with virtual worlds, aiming to make the manipulation of objects in these environments more natural. The authors propose principles for realistic behavior of virtual objects and present a set of input techniques suitable for semi-immersive virtual reality devices. These tools, including springs, spring-forks, and spring-probes, aid in direct manipulation of objects in virtual environments, with a focus on controlling rotational motions. These tools are designed to enhance the user's experience and make interactions more intuitive and realistic in virtual worlds. 
411686	41168610	Dynamics in interaction on the responsive workbench	This paper proposes a new perspective on user interaction with virtual worlds by focusing on incorporating natural object behavior into VR environments. The current behavior of objects in VR is often unnatural, and the paper suggests using physical laws and monitoring user actions to improve this. The authors introduce principles for more realistic object behavior and suggest using springs as a tool for direct manipulation in VR. These ideas are specifically aimed at semi-immersive VR devices like the Responsive Workbench. By implementing these techniques, the paper argues that user interaction in virtual environments can be made more intuitive and realistic.
411687	41168727	Geometric Spanners for Weighted Point Sets	We are studying spanners for a finite metric space (S,d) where each element p∈S has a non-negative weight w(p). The weighted distance function dω is defined as 0 if p=q and w(p)+d(p,q)+w(q) if p≠q. We present a method for turning spanners with respect to the d-metric into spanners with respect to the dω-metric. This method can be applied to obtain (5+ε)-spanners with a linear number of edges for points in Euclidean space, points in spaces of bounded doubling dimension, and points on the boundary of a convex body in ℝd. We also describe an alternative method for obtaining (2+ε)-spanners for weighted points in ℝd and points on the boundary of a convex body in ℝd with O(nlog n) edges. This bound on the stretch factor is almost optimal, as it is possible to assign weights that result in a stretch factor larger than 2−ε for any non-complete graph in any finite metric space.
411687	41168763	Data structures for halfplane proximity queries and incremental voronoi diagrams	The article discusses the problem of preprocessing a set of points in convex position in the plane to support queries for finding the farthest or nearest point to a given point q, to the left of a directed line ℓ. Two data structures are presented - one with O(n1+ε) space and preprocessing time and O(21/ε log n) query time, and the other with O(n log3n) space and polynomial preprocessing time and O(log n) query time. These are the first solutions to have O(log n) query time and o(n2) space. A new representation of nearest-point and farthest-point Voronoi diagrams is also developed, which supports insertion of new points in counterclockwise order with O(log n) amortized pointer changes and O(log n)-time point-location queries, while maintaining o(n) pointer changes per operation. This is the first demonstration of maintaining Voronoi diagrams with such properties. 
411688	41168814	Implementation of the ANSYS® commercial suite on the EGI grid platform	This paper discusses the implementation of the ANSYS® commercial software suite in a high-throughput computing environment. ANSYS® has been designed to efficiently utilize parallel architectures, making it a good fit for use on the Grid. To make ANSYS® accessible to a wider community, the User Support Unit of the Italian Grid Initiative (IGI) and the INFN-Legnaro National Laboratories (INFN-LNL) collaborated to create a Grid-enabled version of the code using the IGI Portal. This collaboration focused on adapting the code to run on the EGI Grid, which allows for easier access to distributed computing and storage resources. The goal of this project was to make ANSYS® more accessible and beneficial for both the involved community and other groups interested in using production Grid infrastructures.
411688	41168818	Porting of GROMACS package into the grid environment: testing of a new distribution strategy	The paper discusses the process of porting the GROMACS package onto the EGEE grid using the P-GRADE Grid Portal tool in COMPCHEM. A new strategy for accessing local and distributed resources was developed and a set of visualization tools were implemented to aid in chemical analysis. This allowed for easier and more efficient use of the GROMACS package on the grid, providing valuable insights for chemical research.
411689	41168931	Analysis of link failures in an IP backbone	This paper examines the impact of failures in Sprint's IP backbone on emerging services like Voice-over-IP (VoIP). The study analyzes the frequency and duration of failure events in the backbone and finds that link failures occur frequently but are typically short-lived. The authors also discuss various statistics related to link failure, such as inter-failure time and duration, which are important for creating a realistic failure model. They then look at the reconvergence time for routing and services during a controlled link failure and find that it is influenced by both routing protocol dynamics and router architecture. The paper concludes by suggesting that network-wide availability is a more suitable metric for service-level agreements to support emerging applications. 
411689	41168973	Fast local rerouting for handling transient link failures	Link failures are a common occurrence in networks and can be caused by maintenance, faulty interfaces, or accidental fiber cuts. Traditional link state routing protocols like OSPF react to failures by recomputing routing tables and causing delays in forwarding. Tuning parameters for faster convergence can lead to instability in the event of multiple transient failures. To address this, a new approach called failure insensitive routing is proposed, which uses interface-specific forwarding and a backwarding table to reroute traffic in the event of a failure. This approach guarantees loop-free paths for packets and has been shown to be feasible, reliable, and stable.
411690	41169040	Improved myocardial scar characterization by super-resolution reconstruction in late gadolinium enhanced MRI.	Image resolution plays a crucial role in accurately assessing myocardial scar using late gadolinium enhanced (LGE) MRI. However, the commonly used short-axis (SA) acquisition with anisotropic resolution can lead to overestimation of scar size due to partial volume effect, affecting the reliability of LGE MRI in critical clinical applications. To address this issue, a new method is proposed that combines three anisotropic orthogonal LGE sequences into a single isotropic volume using super-resolution reconstruction and motion compensation techniques. This approach was validated on 15 post-infarction patients using electroanatomical voltage mapping (EAVM) as the gold standard, and results showed improved agreement with EAVM in the clinically significant gray zone region compared to the conventional SA image.
411690	41169014	3D model-based approach to lung registration and prediction of respiratory cardiac motion	This paper introduces a new method for lung registration and cardiac motion prediction using a 3D geometric model of the left lung. The model is created by extracting feature points from tomographic images and an iterative process is used to achieve spatial alignment with the subject's data. The method was tested on thoracic MRI images of 5 healthy volunteers at different respiratory phases, and was also used to predict the displacement of the heart during respiration. Results showed promising registration performance. 
411691	4116915	On fixed-parameter algorithms for Split Vertex Deletion	The Split Vertex Deletion problem involves deleting k vertices from a graph G to create a split graph, which has two sets of vertices: one that forms a clique and one that forms an independent set. This paper explores fixed-parameter algorithms for this problem, showing that it can be solved in the same time as the Vertex Cover problem, with a quasipolynomial factor in k and polynomial factor in n. By using an existing algorithm for Vertex Cover, the problem can be solved in O(1.2738^k * k^O(log k) + n^O(1)) time. The authors also prove a structural result that can be useful for other problems: for any graph G, a family of partitions can be computed in size n^O(log n) that contains all possible partitions of G into two sets, one forming a clique and the other an independent set. 
411691	4116911	Directed subset feedback vertex set is fixed-parameter tractable	The Feedback Vertex Set (FVS) problem asks if there is a vertex set of size at most k that hits all cycles in a given graph G. Bodlaender (WG '91) gave the first fixed-parameter algorithm for this problem in undirected graphs. The problem's fixed-parameter tractability in directed graphs was a long-standing open problem until Chen et al. (STOC '08) showed that it is tractable with an algorithm that runs in 4kk!nO(1) time. This problem also has a subset version, where an additional subset of vertices or edges is given and the goal is to hit all cycles passing through a vertex or edge in the subset. Recent research has shown that the Subset Feedback Vertex Set problem in undirected graphs is fixed-parameter tractable, and in this paper, the authors extend this result to directed graphs with a time complexity of $2^{2^{O(k)}}n^{O(1)}$. This completes the picture for FVS problems and their subset versions in both undirected and directed graphs. The authors also introduce a general approach using random sampling of important separators to show the fixed-parameter tractability of other problems in directed graphs, such as Directed Multiway Cut and Directed Subset Feedback Vertex Set.
411692	41169235	Kernelization hardness of connectivity problems in d-degenerate graphs	A d-degenerate graph is one in which every subgraph has at least one vertex with a degree of at most d. This property is seen in planar graphs, which are 5-degenerate. Recent research has shown that problems in this class of graphs, such as DOMINATING SET, have polynomial kernels. However, CONNECTED DOMINATING SET does not have a polynomial kernel in d-degenerate graphs for d ≥ 2 unless the polynomial hierarchy collapses up to the third level. This is proven using a problem from bioinformatics, called COLOURFUL GRAPH MOTIF, which encapsulates the difficulty of connectivity in kernelization. It is also shown that other problems, including STEINER TREE, CONNECTED FEEDBACK VERTEX SET, and CONNECTED ODD CYCLE TRANSVERSAL, do not have polynomial kernels in d-degenerate graphs for d ≥ 2 unless PH = Σp3. However, a polynomial kernel is found for CONNECTED VERTEX COVER in graphs that do not contain the biclique Ki,j as a subgraph.
411692	41169211	On cutwidth parameterized by vertex cover	The Cutwidth problem involves finding a linear layout of the vertices in a given graph that minimizes the maximum number of edges intersected by a vertical line inserted between two consecutive vertices. A new algorithm has been developed with a running time of O(2knO(1)), where k is the size of a minimum vertex cover and n is the number of vertices in the graph. This algorithm also applies to bipartite graphs, and is the first exact exponential time algorithm for Cutwidth on a graph class that remains NP-complete. It has also been shown that Cutwidth parameterized by the size of the minimum vertex cover does not have a polynomial kernel unless NP ⊆ coNP/poly. This is in contrast to the recent result for Treewidth parameterized by vertex cover.
411693	41169378	Active exploration for robot parameter selection in episodic reinforcement learning	As robots and autonomous systems become more complex, it is crucial for them to actively adapt and optimize their settings. However, this optimization process is not always straightforward and can be costly in terms of time and resources. The parameter space is often continuous and multi-dimensional, which presents a challenge for effective optimization. To address this, a reinforcement learning framework is proposed, where the policy is defined by the system's parameters and rewards are based on system performance. This framework employs a method based on Gaussian process regression, which can handle non-linear mappings. The proposed method was tested on a real robot, where it successfully adapted its grasping parameters for different objects. Overall, the method shows promise for efficiently optimizing autonomous systems in continuous multidimensional spaces.
411693	4116938	Learning Operational Space Control	In this paper, the authors suggest a novel approach to learning operational space control for complex robots, such as humanoid robots, which are prone to modeling errors. Traditional analytical control methods can be difficult to implement in these cases, so the authors propose using learning control methods instead. However, the learning problem is challenging due to the non-convexity of the solution space. The authors present a solution to this problem by using a piecewise linear approach and formulating the control problem as a constraint optimal control problem. This allows them to develop a reinforcement learning algorithm that maximizes an immediate reward and employs an expectation-maximization policy search algorithm. The effectiveness of this approach is demonstrated through simulations on a three degree-of-freedom robot arm.
411694	41169459	Individual-based stability in hedonic games depending on the best or worst players	The article discusses different types of hedonic games where players' preferences are based on the best or worst player in their coalition. The focus is on analyzing the computational complexity of finding stable coalition partitions in these games, which can be represented concisely. The key difficulty is identified as extending preferences over players to preferences over coalitions. The article looks at various stability concepts, such as individually stable and Nash stable, and examines the difficulty of determining the existence of and computing these types of stable coalition partitions. 
411694	41169448	Complexity of comparison of influence of players in simple games	The complexity of comparison of influence between players in coalitional voting games is explored in this paper. Different representations of simple games, such as winning coalitions, minimal winning coalitions, and weighted voting games, are considered. The influence of players is measured using basic player types, desirability relations, and classical power indices. The results show that for a simple game represented by minimal winning coalitions, determining the Banzhaf value of a player is difficult. It is also shown that multiple weighted voting games are the only representations for which it is hard to determine if the game is linear. An algorithm is presented for determining if a game is non-linear and the complexity of transforming simple games into compact representations is examined.
411695	4116959	How to Protect DES Against Exhaustive Key Search	The DESX block cipher was developed as a way to protect the DES cipher from exhaustive key-search attacks. It was suggested by Ron Rivest and has been proven to be sound in a formal model. This means that it is a reliable and secure method of encryption. The construction of DESX, which involves bitwise exclusive-or operations, is more resistant to key search attacks than the original DES cipher. This is due to the use of an idealized block cipher, which has a higher effective key length. The analysis of DESX shows that it has an effective key length of ϰ+n - 1 - lg m bits, where ϰ is the key length of the idealized cipher, n is the block length, and m is the maximum number of (x, FXk(x)) pairs an adversary can obtain. Overall, DESX is a cost-effective and secure solution for protecting data using the DES cipher.
411695	4116953	The security of the cipher block chaining message authentication code	The cipher block chaining message authentication code (CBC MAC) is a method used to authenticate messages between parties who share a secret key for a block cipher. This method involves adding a prefix to the message using a function that is based on the block cipher. This method has been established as a widely used international and U.S. standard. A recent study has provided formal justification for this method, proving that using cipher block chaining with a pseudorandom function will result in a pseudorandom function. This is based on a technical lemma that bounds the success probability of an attacker in distinguishing between a random function and the CBC MAC.
411696	41169646	A bounded search tree algorithm for parameterized face cover	The face cover problem asks whether a given graph can be covered by a set of faces, with a specified limit on the number of faces. Previous approaches used a reduction to a special form of dominating set, resulting in run times of O^*(12^k) or worse. This paper presents a direct approach using a surgical operation on the graph at each branching decision. By building on previous work, the authors were able to develop an algorithm with a run time of O(4.6056^k+n^2), significantly improving upon previous results. This algorithm also has implications for solving red-blue dominating set on planar graphs and producing a linear kernel for annotated face cover.
411696	411696102	Fixed Parameter Algorithms for PLANAR DOMINATING SET and Related Problems	The article introduces an algorithm for determining the domination number of a planar graph. The algorithm has a time complexity of O(c√kn), where c = 36√34 and k is the domination number of the graph. This result is achieved by demonstrating that the treewidth of a planar graph with domination number k is O(√k), and that a tree decomposition can be found in O(√kn) time. The same approach can also be applied to solve the DISK DIMENSION problem in O(c1√kn) time, where c1 = 26√34. Similar results can be obtained for other versions of the DOMINATING SET problem, such as INDEPENDENT DOMINATING SET.
411697	4116979	Fast Shape from Shading for Phong-Type Surfaces	Shape from Shading (SfS) is a longstanding problem in image analysis that aims to reconstruct a 3-D scene from a 2-D image using brightness variations and illumination conditions. While the quality of models for SfS has improved, there is still a need for efficient numerical methods for large images. In this paper, the authors address this issue by proposing the use of a Fast Marching (FM) scheme, which is known to be one of the most efficient approaches. However, the FM scheme is not easily applicable to modern non-linear SfS models. The authors demonstrate how this can be achieved for a recent SfS model that incorporates the non-Lambertian reflectance model of Phong. Results from numerical experiments show that this FM scheme is significantly faster without compromising quality, making it two orders of magnitude faster than standard methods. 
411697	41169732	How to Choose Interpolation Data in Images	This article discusses shape-based models for finding the best interpolation data in image reconstruction using the Laplace equation. The shape analysis is done through Gamma-convergence, using both continuous PDE and finite dimensional approaches. The continuous model provides pointwise information on the importance of each pixel, while the finite dimensional model incorporates fat pixels and studies the optimal distribution of interpolation pixels. The results from these models are consistent with those obtained through level set techniques and approximation theory. Numerical experiments demonstrate the practical applicability of these models for image compression.
411698	411698139	Energy-efficient physically tagged caches for embedded processors with virtual memory	This paper presents a low-power tag organization for physically tagged caches in embedded processors with virtual memory support. By identifying a small subset of tag bits for each application hot-spot, the cache can access data without sacrificing performance. The minimal subset of physical tag bits, known as the compressed tag, is dynamically updated as the application's physical address space changes. The operating system plays a role in maintaining the compressed tag and updates it to match the current set of physical memory pages allocated to the application. The paper also proposes efficient algorithms for dynamic updates of compressed tags in cases such as memory allocation and swapping of physical pages. The only hardware support required is the ability to disable tag arrays' bitlines. Extensive experiments demonstrate the effectiveness of this approach.
411698	411698141	Data cache energy minimizations through programmable tag size matching to the applications	This paper presents a methodology for customizing data cache operations in embedded processors to minimize energy consumption. Specifically, the focus is on reducing the energy consumed by data cache tag operations, which are a major contributor to power consumption in embedded processors. The proposed approach suggests using a small number of tag bits or even eliminating them altogether for most load/store instructions within application loops. This leads to a significant reduction in energy consumption for tag reads and comparisons. The paper also presents an efficient implementation that can be reprogrammed using application-specific information to further reduce energy dissipation. Experimental results demonstrate the effectiveness of this approach in reducing energy usage for important numerical kernels. 
411699	41169914	Dual regularized multi-view non-negative matrix factorization for clustering.	Multi-view datasets are becoming increasingly prevalent in real-world applications, as they provide complementary information about the data. To more effectively utilize this information, a novel algorithm called DMvNMF has been developed for multi-view data clustering. The algorithm incorporates a parameter-free strategy for constructing a data graph that preserves the geometric structures of the data in both the data space and feature space. This is achieved by first learning an affinity graph for each view using self-expressiveness and sparsity principles, and then combining them to generate a global data graph. Similarly, feature graphs are constructed for each view. The algorithm also includes an iterative updating scheme for model optimization and has been shown to outperform other baseline methods in experiments on real-world datasets.
411699	411699206	Graph-regularized multi-view semantic subspace learning	The paper discusses the use of multi-view subspace learning algorithms to obtain a good data representation from multiple features in real-world datasets. Previous approaches did not effectively capture the underlying semantic structure of the data. The authors propose a new algorithm, called multi-view semantic learning (MvSL), which uses a novel graph embedding framework to capture the semantic structure. This is achieved by defining an affinity graph to represent intra-class compactness and a penalty graph to represent inter-class separability. The paper also introduces a multiple kernel learning method and sparseness constraints to improve the effectiveness of MvSL. Experiments on three datasets demonstrate the success of MvSL in capturing flexible conceptual patterns in multi-view features.
411700	41170051	Relational Similarity-Based Model Of Data Part 1: Foundations And Query Systems	The article presents a general rank-aware model of data for relational databases, where equalities on values are replaced by similarity relations. This model introduces a scale of truth degrees and intermediate truth degrees to represent similarity and partial match of queries. The model also includes truth functions for many-valued logical connectives to aggregate degrees of similarity. This approach is conceptually clean, logically sound, and retains most properties of the classical relational model while allowing for new types of queries and data dependencies. The article discusses the fundamentals of the formal model and presents two equivalent query systems. In future work, the authors plan to address similarity-based dependencies.
411700	4117003	Sensitivity Analysis for Declarative Relational Query Languages with Ordinal Ranks	This article discusses sensitivity analysis for query results in a relational data model that includes ordinal ranks. The model is based on Codd's data model, but also considers the ordinal ranks of tuples in data tables to measure how well they match queries. The study reveals that the assigned ranks are not significantly affected by small changes in the input data, indicating that the results of queries are relatively stable. This finding has important implications for the reliability and consistency of query results in this extended data model.
411701	41170141	Interpolation in Hierarchical Rule-Bases with Normal Conclusions	The combination of fuzzy rule interpolation and hierarchical structures in fuzzy rule bases, as suggested by Sugeno, can simplify the complexity of fuzzy algorithms. This paper focuses on using the KH method and its variations for interpolation, but notes that it can sometimes produce incorrect results, making it unsuitable for hierarchical structures. To address this issue, the paper introduces a modified version of the KH method called the MACI algorithm, which avoids these abnormal conclusions and allows for the use of hierarchical structures.
411701	41170110	Fuzzy rule interpolation for multidimensional input spaces with applications: a case study	Fuzzy rule based systems are commonly used in engineering applications, but they can result in a sparse rule base. To address this issue, fuzzy rule interpolation techniques have been developed. However, most of these techniques only consider one input variable, despite the fact that multiple input variables are often used in engineering applications. This paper explores two existing fuzzy rule interpolation techniques and proposes a new method that can handle multidimensional input spaces. Through application examples in petroleum engineering and mineral processing, the new method is shown to be effective in engineering applications.
411702	4117023	Predicting Locations Using Map Similarity(PLUMS): A Framework for Spatial Data Mining	Spatial data mining is a method used to uncover valuable patterns within spatial databases. This is particularly useful for organizations with large spatial datasets. Currently, the most common approach for spatial data mining is to first establish spatial relationships and then use traditional data mining tools. However, the unique property of spatial data is that it is inherently linked to its location, making it crucial to consider spatial relationships during the data mining process. Efficient tools for extracting information from spatial datasets are essential for organizations that handle these types of data. 
411702	41170257	Dimensionality reduction for long duration and complex spatio-temporal queries	This paper discusses a method for mining and querying spatio-temporal data in order to identify interesting patterns and understand the data generation process. One type of query focuses on identifying flocks, which are large groups of objects moving closely together for a specific period of time. A common approach for processing these queries is to map the data into a high dimensional space and use spatial indexing structures, but this can lead to poor performance. To address this issue, the paper proposes a preprocessing strategy using random projection to reduce the dimensionality of the data. Experimental results show that this method successfully overcomes the challenges of high dimensionality in spatio-temporal data analysis. 
411703	41170334	CUR decomposition for compression and compressed sensing of large-scale traffic data	Intelligent Transportation Systems (ITS) often deal with large amounts of data from traffic networks and need efficient methods to handle it. Subspace methods like Principal Component Analysis (PCA) can create accurate low-dimensional models, but they are not easily interpretable. The CUR matrix decomposition, on the other hand, creates low-dimensional models where each component corresponds to an individual link in the network, making it more interpretable. This approach can be used for both data compression and compressed sensing in traffic networks. Results from a case study in Singapore show that the CUR matrix decomposition is a feasible and effective method for handling massive traffic data.
411703	41170324	Spatiotemporal Patterns in Large-Scale Traffic Speed Prediction	Predicting traffic speed accurately in a large and diverse road network has many practical uses, such as route guidance and avoiding congestion. Data-driven methods, like support vector regression (SVR), have the potential to achieve high accuracy as traffic patterns tend to be consistent over time. However, the performance of these methods can vary significantly across the network and over different time periods. To improve the performance of intelligent transportation systems, it is important to understand these spatiotemporal trends. Traditional error measures only provide information about individual links in the network and do not capture overall trends. To address this, we propose using unsupervised learning methods to analyze performance trends at both the network and individual link levels. We apply these methods to an SVR-based algorithm and demonstrate their effectiveness in predicting traffic in a large interconnected road network for various time periods. 
411704	41170480	Provably good performance-driven global routing	The authors propose a performance-driven global routing algorithm for cell-based and building-block design. It uses a bounded-radius minimum routing tree formulation and is based on heuristics similar to Prim's minimum spanning tree construction. The algorithm aims to minimize both routing cost and the longest interconnection path, with the bounds being only slightly away from optimal. Geometry is shown to play a role in routing, with improved wire length for Steiner routing in the Manhattan and Euclidean planes. The algorithm can also handle varying wire length bounds for different source-sink paths. Extensive simulations demonstrate the effectiveness of this approach.
411704	411704248	Self-Compensating Design for Reduction of Timing and Leakage Sensitivity to Systematic Pattern-Dependent Variation	This paper discusses the issue of critical dimension (CD) variation caused by defocus in semiconductor design. The authors propose a new design methodology that allows for explicit compensation of focus-dependent CD variation. This is achieved by creating iso and dense variants for each library cell, resulting in more robust designs. The paper compares different optimization methods for this approach and finds that using a mixed iso/dense library yields better trade-offs between area, timing, and leakage power compared to using a self-compensated library alone. The results show that self-compensated designs have 25% less leakage power on average at the worst defocus condition compared to traditional designs using a conventional library.
411705	41170538	Logged virtual memory	Logged virtual memory (LVM) is a feature that allows for logging of writes to specific regions of the virtual address space. It is useful for applications that require rollback and persistence, such as parallel simulations and memory-mapped databases. LVM can also be used for output, debugging, and maintaining distributed consistency. This paper discusses the benefits of LVM and its implementation as an extension of the standard virtual memory system. The prototype implementation includes extensions to the virtual memory system software, hardware support, and a user-level library. Results and experience with the prototype show that LVM can be efficiently implemented and provides significant benefits for applications and servers. It also reduces the risk of error compared to manually generating log updates. LVM can be used for various purposes, such as supporting atomic transactions in object-oriented databases, debugging programs, and obtaining detailed address traces for performance analysis. Overall, incorporating LVM into virtual memory systems can greatly enhance the functionality and efficiency of computer systems.
411705	41170539	UIO: a uniform I/O system interface for distributed systems	A uniform I/O interface allows programs to work with various I/O services without being tied to a specific one. This simplifies the interface and maintains good performance. A uniform interface requires explicit definition and cannot be achieved solely through careful design of individual system interfaces. The UIO system interface used in the V distributed operating system for the past five years is described, including its key design features such as support for record I/O, locking, atomic transactions, and replication. The interface has been successfully implemented with different I/O services and has not shown any significant performance issues. Overall, the UIO interface provides a versatile and efficient solution for a uniform I/O system interface. 
411706	41170661	Thirty years is long enough: getting beyond C	After 30 years, C remains a popular systems programming language, but its power has become a hindrance for large projects focused on security and reliability. To address this issue, a new language called Ivy is proposed, which has an evolutionary path from C. This path involves using extensions and refactorings to add new features and assist programmers in updating their code. These tools have potential applications such as enforcing memory safety and detecting user/kernel pointer errors. A tool called Macroscope has also been developed to enable refactoring of existing C code. These developments aim to improve the quality of systems software and facilitate the adoption of modern languages and static analyses.
411706	41170610	SafeDrive: safe and recoverable extensions using language-based techniques	SafeDrive is a system that detects and fixes type safety issues in software extensions with minimal changes to existing code. It uses a unique type system to isolate extensions written in C and tracks invariants using simple wrappers for the host system API. This approach is more efficient and cost-effective than existing solutions like hardware-enforced domains (Nooks, L4, Xen) or software-enforced domains (SFI). SafeDrive can be applied to any large system with loadable and error-prone extension modules. In testing on Linux device drivers, only 4% of source code needed to be changed, and SafeDrive successfully recovered from all 44 crashes caused by injected faults. It resulted in a 4-23% increase in kernel CPU utilization but did not affect overall performance.
411707	4117070	Using the multi-living agent concept to investigate complex information systems.	The paper introduces the concept of the multi-living agent (MLA) in order to analyze complex information systems that operate under strict countermeasure environments. The authors first examine the system's livelihood under these conditions and then propose the concept of living self-organization based on the system's profile. They also present a Two-Set model for this self-organization mechanism. Using this framework, they develop a construction model for MLA-based information systems in the field of information security and countermeasures, including a three-level negotiation-coordination mechanism. Two practical examples are provided to demonstrate the application of the MLA concept in analyzing real complex information systems. This proposal aims to bridge the gap between research and practical application in the field, providing a theoretical basis for constructing and analyzing complex information systems in the context of information security and countermeasures.
411707	41170760	Image Encryption With Multiorders of Fractional Fourier Transforms	In this paper, the authors propose a new method for image encryption using multiple orders of the fractional Fourier transform (FRFT). The encrypted image is obtained by summing the inverse discrete FRFT of interpolated subimages, and the original image can be perfectly recovered using a linear system constructed in the FRFT domain. This method can be used for encrypting multiple images and has a larger key space compared to existing FRFT-based systems. The encryption can be achieved using a fast-Fourier-transform-based algorithm, with a computation burden that increases linearly with the key space. Experimental results show that the decryption process is highly sensitive to deviations in the transform orders.
411708	41170871	An Orientation Model for Hierarchical Phrase-Based Translation	The hierarchical phrase-based (HPB) translation method utilizes grammar to perform long distance reordering without specifying nonterminal orientations or considering lexical information. This paper introduces an orientation model, borrowed from phrase-based systems, to improve the reordering ability of HPB translation. The model identifies three orientations (monotone, swap, discontinuous) for a nonterminal based on grammar alignment and selects the appropriate one using lexical information. By incorporating this model, the approach outperforms standard HPB systems by up to 1.02 Bleu on NIST Chinese-English and 0.51 Bleu on WMT German-English translation tasks. 
411708	411708134	A topic similarity model for hierarchical phrase-based translation	Previous research on using topic models for statistical machine translation focused on incorporating topic information at the word level. However, as SMT has shifted towards a phrase/rule-based paradigm, a new approach is needed to utilize topic information at the rule level. In response, the authors propose a topic similarity model that associates each synchronous rule with a topic distribution and selects rules based on their similarity to given documents. This model is shown to significantly improve translation performance on Chinese-to-English experiments and outperforms previous word-level approaches in both accuracy and speed.
411709	41170973	Interpolation Techniques for MIMO OFDM with Interference Cancellation	.This paper discusses the challenges of implementing a MIMO OFDM receiver with interference cancellation due to its high computational requirements. To address this issue, the authors propose new interpolation algorithms that reduce the need for matrix inversion on each OFDM sub-carrier. They also introduce algorithms to determine the optimal order for interference cancellation. The proposed receiver shows a significant reduction in computation requirements while maintaining acceptable performance. For an interpolation factor of 6.25%, the performance degradation compared to the optimal solution is only 2dB. The results also demonstrate that the sub-optimal interference cancellation detection order has a minimal impact on performance compared to the coefficient matrix interpolation error. Overall, using interference cancellation in a MIMO OFDM system with interpolation greatly improves performance.
411709	41170928	Overhead Optimization In A Mimo-Ofdm Testbed Based On Mmse Mimo Decoding	This paper discusses the optimization of overhead in a MIMO-OFDM testbed using MMSE MIMO decoding. The testbed operates at 5.25GHz with a bandwidth of 25MHz and uses a 2x2 multi-antenna configuration. Modulation and demodulation are done offline, and an adaptive RLS based MMSE solution is used for decoding. The system's performance is evaluated for different numbers of training symbols and continuous pilots in the presence of analog impairments. Results show that 10 training symbols and 5 continuous pilots are optimal for this system. The analysis is presented in terms of slicer SNR, achievable capacity, and throughput. 
411710	41171022	A theoretical framework for interaction measure and sensitivity analysis in cross-layer design	Cross-layer design is a popular method for achieving Quality of Service (QoS) in communication networks, particularly in wireless multimedia networks. However, current research on cross-layer design lacks a comprehensive approach and understanding of complex cross-layer behavior. This article proposes a theoretical framework for measuring interactions between different design variables, which can be further used for sensitivity analysis to determine the impact of each variable on the overall design objective. This framework can greatly improve our understanding of cross-layer behavior and provide valuable insights for future designs. The article includes a case study on cross-layer optimized wireless multimedia communications to demonstrate the effectiveness of the proposed framework. Both analytical and experimental results support the validity of the framework.
411710	41171029	A theoretical framework for quality-aware cross-layer optimized wireless multimedia communications	Research has shown that cross-layer design is an effective way to improve multimedia communications over wireless networks. However, there is a lack of a comprehensive mathematical model to fully understand the tradeoffs involved in cross-layer design, from the application layer to the physical layer. This has led to existing cross-layer designs that may improve individual layers, but at the expense of overall system performance or violating the layered network architecture. This is due to the reliance on heuristic approaches, rather than a rigorous theoretical study. In this paper, the authors propose a new delay-distortion-driven cross-layer optimization framework and introduce new methods for high-dimensional nonlinear cross-layer optimization. This paper is a significant contribution to the development of cross-layer design in wireless multimedia communications, providing insights for current and future wireless networks. 
411711	4117115	Type checking annotation-based product lines	Software product line engineering involves creating a range of program variations for a specific domain from a single code base. However, testing all of these variants can be challenging, especially when it comes to ensuring type safety for the entire product line. To address this issue, a product-line-aware type system has been developed that can check the types of all program variants without generating each one separately. This type system is based on the Featherweight Java calculus and uses feature annotations for product-line development. It has been formally proven that all variants generated from a well-typed product line will also be well-typed. Additionally, a solution has been presented for handling mutually exclusive features. This approach has been implemented in the product-line tool CIDE for full Java, and has successfully detected type errors in existing product line implementations.
411711	41171170	Type-Checking Software Product Lines - A Formal Approach	A software product line (SPL) allows for the creation of multiple program variants from a single code base, making it an efficient tool for domain-specific software. However, testing all the possible variants and ensuring key properties, such as type-safety, can be challenging due to the large number of possible combinations. While some attempts have been made to address this issue, they are not comprehensive. This paper proposes an extension to the Featherweight Java (FJ) calculus that incorporates feature annotations to support SPLs. This formalization guarantees that all program variants generated from a well-typed SPL will also be well-typed. The authors demonstrate how this approach can be applied in their own SPL tool, CIDE.
411712	41171261	Domain Types: Selecting Abstractions Based on Variable Usage	Software model checking is a crucial process in verifying the correctness of a program. However, its success depends greatly on finding the right abstraction of the program. Currently, the choice of abstract domain and analysis configuration is left to the user, who may not have enough knowledge on the available options. To address this issue, the concept of domain types is introduced, which classifies program variables into more specific types to guide the selection of an appropriate abstract domain. This is done through a pre-processing step that assigns each variable to a domain type and then an abstract domain. By specifying a separate analysis precision for each domain, the impact of the choice of abstract domain per variable is shown in experiments. It is found that a combination of different abstract domains based on domain types can greatly improve the performance of software model checking. 
411712	4117120	Feature cohesion in software product lines: an exploratory study	Software product lines are becoming increasingly popular in both research and industry. These approaches use features as a central abstraction mechanism, with the goal of encapsulating them into cohesive units to support program comprehension, variability, and reuse. However, there is a lack of understanding about the characteristics of cohesion in feature-oriented product lines. To address this gap, a study was conducted on forty different software product lines, using both traditional software measures and novel measures based on clustering layouts. The results of the study revealed interesting correlations, such as between development process and feature cohesion. Further investigation into feature cohesion could provide insights and perspectives on feature interfaces and programming style. 
411713	41171350	Efficient revised simplex method for SVM training.	Existing active set methods for training support vector machines (SVM) often encounter singularities when finding the search direction. While an infinite descent direction can be chosen to avoid cycling, it can make the algorithm more complex and less efficient. The revised simplex method introduced by Rusin avoids this issue by providing a guarantee of nonsingularity. This allows for a simpler and more efficient implementation, without the need to test for rank degeneracies or modify factorizations. By utilizing this guarantee, our algorithm is shown to be competitive with SVM-QP and effective when dealing with a large number of nonbound support vectors. It also performs competitively against popular SVM training algorithms, SVMLight and LIBSVM.
411713	41171318	On Extending the SMO Algorithm Sub-Problem	The Support Vector Machine (SVM) is a popular machine learning model known for its superior generalization performance. The Sequential Minimal Optimization (SMO) algorithm is a commonly used approach for training SVMs due to its speed and ease of implementation. However, it has a limited working set size of only 2 points. This paper proposes a 4-point SMO formulation and addresses the theoretical challenges of increasing the working set size. The results show that increasing the working set size can lead to faster training times and fewer iterations for convergence. The SVM algorithm is based on structural risk minimization, seeking a balance between function complexity and training error. It works by finding a hyperplane that maximizes the margin between two classes of data, which is equivalent to structural risk minimization.
411714	41171419	A Metalanguage for the Formal Requirement Specification of Reactive Systems	The presented approach offers a new way of specifying both the static structure and dynamic activity of a system using a single formalism. Unlike other formalisms that use temporal logics, this approach allows for dealing with different entities and their subcomponents without sacrificing abstraction. It also includes specifications for abstract data types and integrates static and dynamic features of a system. The formalism is supported mathematically through the definition of appropriate institutions, with models being defined as entity algebras over extended signatures. The relationship between abstract requirement specifications and design level specifications can be explored using an algebraic approach. This approach is currently being used in industrial case studies to link requirements to concrete design specifications.
411714	41171434	An Qutline of the SMoLCS Approach	The SMoLCS methodology is a way to specify concurrent systems and languages in a highly modular and parameterized manner. It combines functions, data types, and concurrency in a single framework, allowing for a more comprehensive approach. This is achieved by using a parameterized schema to define and combine process specifications, resulting in an abstract data type. The methodology also employs a two-step approach to give semantics to languages, using an algebraic specification. A metalanguage schema is included in the methodology, consisting of an algebraic and an applicative kernel. Tools have been developed to aid in the rapid prototyping of specifications using this methodology.
411715	4117157	Analysing UML Active Classes and Associated State Machines - A Lightweight Formal Approach	The authors propose a precise definition of UML active classes using labelled transition systems in the formal specification language Casl. They believe that a formal model is necessary to fully understand UML and improve its practical use. They plan to use animation and verification tools for algebraic specifications with UML in the future. To simplify the application of their semantics, they only consider a subset of state machine constructs, but this does not limit the overall UML subset as the restricted constructs can be replaced by equivalent combinations. Due to some ambiguities in the official UML semantics, the authors discuss different options and choose the most logical or simplified approach for each case.
411715	41171518	Invited Talk: Plugging Data Constructs into Paradigm-Specific Languages: Towards an Application to UML	The author discusses the composition of languages, specifically a data description language and a paradigm-specific language, from a pragmatic perspective. The goal is to describe languages in a component-based style, with a focus on the data definition component. The proposed approach involves substituting the data constructs from the "data" language with constructs that describe data in a way that preserves syntax, semantics, and methodologies of both components. The author uses a toy example to illustrate this approach, using CASL as the data language and a "pre-post" condition logic as the paradigm-specific language. The author also mentions a more detailed application of this technique in a separate publication and briefly mentions its potential application to UML.
411716	41171618	Effective use of prosody in parsing conversational speech	In this study, researchers have identified a set of prosodic cues that can be used to improve the accuracy of parsing conversational speech. These cues were incorporated into a statistical parsing model and tested on the Switchboard corpus, resulting in improved accuracy compared to a state-of-the-art system that only used lexical and syntactic features. Additionally, the researchers looked into alternative methods for detecting and removing edit regions, which are known to improve parse accuracy. They found that specialized techniques were more effective than traditional methods such as PCFGs. This study highlights the importance of considering prosodic cues and specialized techniques for improved parsing of conversational speech.
411716	41171670	PCFG models of linguistic tree representations	This paper discusses how the type of tree representation used in a treebank corpus can greatly affect the performance of a parser based on the probabilistic context-free grammar (PCFG) created from that corpus. The Penn II treebank representations are found to have a significant impact on the parser's accuracy, and a simple node relabeling transformation is proposed to improve its precision and recall by 8%. This improvement is attributed to the independence assumptions inherent in a PCFG, which can be studied and tested through tree transformations. This research sheds light on the importance of choosing an appropriate tree representation for treebank corpora in order to improve parser performance.
411717	4117179	P2p Multi-Agent Data Transfer And Aggregation In Wireless Sensor Networks	Wireless Sensor Networks (WSNs) allow for seamless communication with the physical world. This paper presents an architecture for a P2P multi-agent data transfer and aggregation system in WSNs. The architecture includes four types of agents: interface, query, routing, and data acquisition agents. The interface agent interacts with users, while the routing agent ensures energy-efficient data transfer. The query agent facilitates collaboration between the interface and routing agents and creates optimized plans. These agents are placed at the resource-enriched base station due to their computation-intensive operations. Proxy agents are created for the query and routing agents to conserve energy and computing resources. The paper outlines the design and architecture of these agents, enabling them to coordinate and communicate efficiently in WSNs. 
411717	41171750	Sandstorm monitoring system architecture using agents and sensor networks	The use of wireless networks in Saudi Arabian highways, particularly in the eastern region, has seen a significant increase due to the frequent sandstorms and sand dunes that cause reduced visibility, unsafe roads, and even life-threatening situations. To address this issue, wireless sensor networks (WSNs) are being implemented as a natural solution to ensure safe roads. This paper proposes an agent-based system architecture for data dissemination in a WSN specifically designed for sandstorm monitoring. The architecture consists of three layers of agents - interface, routing, and data collection - that work together to provide efficient and optimized data gathering and dissemination. The paper also discusses the design and implementation of these agents, highlighting their communication and coordination in WSNs.
411718	41171884	A Divide-and-conquer Strategy in Shape from Shading Problem	The proposed strategy for information recovery of book surfaces under perspective conditions involves three steps: preprocessing, apparent shape recovery, and ortho-image generation. The preprocessing step uses a phenomenological model to extract pure shade images and remove pigment parts. Using existing invariances, equations for slope and ortho-image are derived from the equation of shading and observation. A recurrence relation is also derived to determine the mean slope in discrete images. Theoretically, unique shape can be recovered without iteration for Lambertian cylinders, but a practical algorithm is implemented to overcome self-shadows. Simulations and real experiments demonstrate the effectiveness of the strategy and algorithms. 
411718	41171883	Recovery of shape and surface reflectance of specular object from rotation of light source	The paper discusses a method for recovering the shape and surface reflectance of specular objects from color images taken with a rotating light source. The images are captured with a fixed camera while the light source rotates around the camera's optical axis. The paper presents a relationship between the object's surface reflectance and the rotating angle of the object, and develops an algorithm to extract the body reflectance component for estimating surface normals and body reflectance. The proposed method also estimates the reflectance parameters of specularity by minimizing the fitting error of a specular reflectance model. The paper demonstrates the effectiveness of the method by successfully recovering the shape, body reflectance, and specular reflectance of specular objects.
411719	41171915	A visual language compiler for information retrieval by visual reasoning	The article discusses how expanding databases can create challenges in retrieving data, and introduces a solution using data visualization and visual reasoning. The proposed approach involves transforming data objects and representing them in a visual space, allowing users to use a visual language to formulate information retrieval requests. A prototype system is described, which includes features such as customization for different applications, a fuzzy icon grammar for visual sentences, and a semantic model for translating fuzzy visual queries. Additionally, the system has a VisualNet component that stores learned knowledge for the VisualReasoner to adapt its behavior. 
411719	41171938	Modeling visual interactive systems through dynamic visual languages	In this paper, the authors propose a formal model for visual interaction processes, where each process is represented as a sequence of visual sentences. These visual sentences have a limited set of possible user actions and are part of a "dynamic visual language" that contains common elements. The authors present a formal model for deriving these visual sentences, including the possible actions and transformations that can be performed on them. This formal model allows for a precise specification of the interaction process and can be used to derive a user interface that incorporates context elements to prevent user confusion. The model is demonstrated using a prototype of a digital library from the University of Bari.
411720	4117207	Distributed Welfare Games	Game-theoretic tools are being increasingly used in the design of distributed resource allocation algorithms. The main aspect of this design approach is assigning utility functions to individual agents. The aim is to assign each agent a suitable utility function that ensures desirable properties such as scalability, tractability, and the existence and efficiency of pure Nash equilibria in the resulting game. This paper focuses on studying the design of utility functions for a specific type of game called distributed welfare games. The paper presents various methodologies for utility design that guarantee desirable game properties regardless of the application domain. The results are illustrated in two commonly studied resource allocation problems: "coverage" and "coloring" problems. 
411720	41172011	Potential Games Are Necessary to Ensure Pure Nash Equilibria in Cost Sharing Games.	In this study, the authors investigate the problem of designing distribution rules for sharing welfare among strategically behaving agents. While there are known distribution rules that ensure the existence of a Nash equilibrium, the full space of such rules is unknown. The authors provide a complete characterization of this space for a specific type of game, which has applications in various scenarios such as facility location, routing, and network formation. They prove that a distribution rule must be equivalent to a generalized weighted Shapley value on certain welfare functions to guarantee equilibrium existence in all games. Additionally, if budget-balance is required, the welfare functions must be the same. The authors also offer an alternate characterization of this space that is more computationally feasible. Overall, this study highlights the importance of considering potential games when designing distribution rules for ensuring equilibrium existence.
411721	41172165	An exploratory study of the impact of antipatterns on class change- and fault-proneness	Antipatterns are design choices that make object-oriented systems difficult to maintain. A study was conducted to examine the impact of antipatterns on classes in these systems, specifically looking at how the presence of antipatterns affects the likelihood of a class to experience changes or faults. The study analyzed 13 antipatterns in 54 releases of four popular systems and found that classes with antipatterns were more likely to experience changes and faults than other classes. This was not solely due to the size of the class, as even smaller classes with antipatterns had a higher likelihood of changes and faults. The study also found that structural changes were more likely to affect classes with antipatterns. These findings support previous research on antipatterns and can help inform quality assurance and testing efforts in object-oriented systems.
411721	41172193	Identification of design motifs with pattern matching algorithms	Design patterns are essential in maintaining software as they offer solutions to recurring design problems. These patterns suggest design motifs that help in understanding and re-engineering systems. Identifying these motifs in large systems involves finding classes that match the suggested structure and organization. To efficiently identify these occurrences, two classical approximate string matching algorithms were adapted. Two case studies were conducted to evaluate the performance, precision, and recall of these algorithms. The first study evaluated the algorithms on seven medium-to-large systems, while the second study compared them with three existing approaches on three small-to-medium size systems. The results showed that using bit-vector processing for approximate string matching provides efficient algorithms for identifying design motifs.
411722	41172211	Fuzzy efficiency measures in data envelopment analysis	This paper presents a new approach for measuring the efficiencies of decision making units (DMUs) using data envelopment analysis (DEA). The existing DEA models are limited to crisp data, but this new approach incorporates fuzzy data. The method involves transforming a fuzzy DEA model into a family of crisp DEA models using the a-cut approach. The result is a pair of parametric programs that describe the efficiency measures, which are expressed as membership functions rather than crisp values. This allows for more detailed information to be provided to management. By incorporating fuzzy data, the DEA approach becomes more powerful and applicable in a wider range of situations.
411722	41172227	Efficiencies of two-stage systems with fuzzy data	The article discusses the use of two-stage DEA models in decision-making, which are more informative than traditional one-stage models as they show the performance of individual processes. The authors extend this approach to uncertain situations by using fuzzy numbers to represent observations. They develop two-level mathematical programs to calculate the lower and upper bounds of fuzzy efficiency and construct membership functions for different values. The study finds that the property of system efficiency being equal to the product of process efficiencies also holds in fuzzy situations, and this can be applied to series systems with multiple processes. An example of non-life insurance companies in Taiwan is used to demonstrate the calculation of system and process efficiencies and their relationship when dealing with fuzzy data. 
411723	41172323	Towards large scale argumentation support on the semantic web	This paper introduces the concept of a World Wide Argument Web (WWAW), which is a large-scale Web of interconnected arguments posted by individuals to express their opinions in a structured manner. The authors propose an extension of the Argument Interchange Format (AIF) to express arguments with a structure based on Walton's argumentation schemes. They also describe an implementation of this ontology using the RDF Schema language, and showcase how it can be used to represent networks of arguments on the Semantic Web. Additionally, they present a pilot system, ArgDF, which allows users to create and query arguments using different argumentation schemes. This tool is intended to serve as a foundation for future applications related to authoring, linking, navigating, searching, and evaluating arguments on the Web.
411723	41172333	Representing and classifying arguments on the Semantic Web.	Recently, there has been little focus on representing and exchanging informal, semi-structured arguments commonly found in natural language. To address this gap, the research community has begun developing an Argument Interchange Format (AIF) to facilitate the exchange of these arguments between different analysis and support tools. This paper presents a Description Logic ontology, based on the AIF and Walton's theory of argumentation schemes, for annotating arguments. This ontology enables automated reasoning over argument structures, enhancing classical reasoning about argument acceptability. The use of Web Ontology Language allows for improved querying of arguments through automatic scheme classifications, instance classification, inference of indirect support in chained argument structures, and inference of critical questions. A pilot Web-based system has been implemented to demonstrate the use of this ontology for authoring and querying argument structures. 
411724	41172447	Extracting predicates from mining models for efficient query evaluation	This article discusses how modern relational databases are now able to handle ad hoc queries on mining models. The authors propose techniques for optimizing these types of queries by using the internal structure of the mining model to automatically derive traditional database predicates. They present algorithms for deriving these predicates for popular discrete mining models such as decision trees, naive Bayes, clustering, and linear support vector machines. The experiments conducted on Microsoft SQL Server show that using these derived predicates can greatly reduce the cost of evaluating these types of queries.
411724	411724112	Optimized stratified sampling for approximate query processing	Efficient and accurate answering of aggregation queries is important for decision support and data mining tools. Unlike previous studies that use sampling methods, this research treats the problem as an optimization one. It aims to minimize the error in answering a workload of queries by selecting a stratified random sample of the original data. One key innovation is the ability to adapt the sample choice to be robust for workloads that are similar but not exactly the same. The approach also takes into account the variance in the data distribution in a systematic way. Implementation on a database system and experiments on Microsoft SQL Server show the superiority of this method compared to previous approaches.
411725	41172542	Untraceable mobility or how to travel incognito	User mobility is becoming increasingly important in networks, particularly in wireless and cellular networks. However, this also raises concerns about security, specifically the ability to track a user's movements and location. Ideally, only the user and a responsible authority should know this information. Currently, many networks do not address this issue or use solutions that are specific to cellular networks. This paper discusses the problem of anonymity and location privacy in mobile networks and reviews existing approaches. It also proposes several low-cost solutions with varying levels of protection and assumptions about the network environment.
411725	41172513	Optimal Clock Synchronization Under Different Delay Assumptions	The study focuses on achieving optimal clock synchronization in a communication network with perfect clocks and various delay assumptions. The proposed algorithms have three components and can be adapted for different types of delay assumptions. The model also accounts for broadcast networks where messages arrive simultaneously at all processors. A composition theorem allows for a combination of different assumptions, resulting in the best possible precision. This concept of optimality is stronger than traditional worst-case optimality and applies to systems where the worst-case behavior is unbounded. Several types of delay assumptions are addressed, including lower and upper bounds and differences in delay between opposite directions. The results achieve the best possible precision in each execution.
411726	41172613	Defeasible Inheritance Through Specialization	The article discusses how typed substitution can capture inheritance in logic deduction systems, but this becomes problematic when dealing with method overriding and multiple inheritance. To address this issue, the article proposes a framework based on Dun's argumentation theory for developing a natural semantics for programs with dynamic nonmonotonic inheritance. The relationship between this proposed semantics and the perfect-model semantics proposed by Dobbie and Topor is explored, and it is found that they coincide for inheritance-stratified programs. The proposed semantics also provides accurate interpretations for programs that are not inheritance-stratified. 
411726	41172641	Declarative programs with implicit implication	The article discusses the importance of taxonomic information in determining the meaning of declarative programs. It proposes a general framework for understanding the implicit implication between atoms in an interpretation domain. This framework assumes that the implicit implication can be predetermined and represented by a preorder on the interpretation domain. Under the constraint that every interpretation must conform to the implicit implication, a model-theoretic and fixpoint semantics for declarative programs is described. The article also introduces the concept of an immediate-consequence operator which can efficiently determine the meaning of a program when the implicit-implication relation is a partial order. This approach is based on previous work by Kostler et al. and uses reduced representations of subsets of the interpretation domain.
411727	41172724	Prioritised fuzzy constraint satisfaction problems: axioms, instantiation and validation	This paper presents a framework for solving prioritized fuzzy constraint satisfaction problems (PFCSPs) and proposes methods for constructing specific schemes that follow this framework. The framework includes five methods for creating priority operators to calculate the local satisfaction degree of a prioritized fuzzy constraint, as well as identifying priority T-norm operators to calculate the global satisfaction degree. Numerical and real-life examples are used to demonstrate the effectiveness of the system and provide insights. The paper also discusses the relationship between weight schemes and prioritized FCSP schemes, showing that weighted FCSP schemes are the dual of prioritized FCSP schemes, also known as posterioritised FCSP schemes.
411727	41172711	An Atanassov intuitionistic fuzzy constraint based method for offer evaluation and trade-off making in automated negotiation.	Automated negotiation faces the challenge of accurately evaluating the acceptability of offers. Most existing methods use linear or nonlinear utility functions, but these can be difficult for humans to input accurately. This paper proposes a new approach where users can input fuzzy and indeterminate information. It introduces a framework for prioritized Atanassov intuitionistic fuzzy constraint satisfaction problems and a method for evaluating offers based on users' fuzzy negotiation goals. The paper also discusses how to make counter-offers by considering the opponent's previous offer's similarity and the satisfaction and dissatisfaction degrees towards the agent's most important goal. This approach allows for more human-like evaluation and trade-off in automated negotiation.
411728	41172844	Feature based robust watermarking using image normalization	A new watermarking scheme using feature point detection and image normalization is proposed in this paper. The scheme first detects stable feature points in the original image using a multiresolution filter. These points are then used to normalize the image and embed the watermark in the subband coefficients of each disk. The watermark detection process relies on the correlation between the embedding coefficients and the original watermark, eliminating the need for the original image. The combination of feature point detection and image normalization results in a robust scheme that can withstand signal processing and geometrical distortions. The experimental results show the effectiveness of this approach.
411728	41172878	Subsampling-based robust watermarking using neural network detector	This paper introduces a strong digital image watermarking method that utilizes a neural network detector. The process involves dividing the original image into four parts and embedding a random binary watermark sequence into the Discrete Cosine Transform (DCT) domain of these parts. Additionally, a fixed binary sequence is added to the beginning of the watermark to train the neural network detector. The detector's adaptive and learning capabilities allow it to effectively extract the watermark. Results from experiments demonstrate the effectiveness of this method in resisting common signal processing attacks.
411729	41172917	Strict sensitivity analysis in fuzzy quadratic programming	Quadratic programming is a mathematical approach that can be applied to both linear and non-linear programming problems. It is commonly used in practical fields such as regression, production efficiency, and portfolio selection. In real-life situations, where uncertainty and imprecision are common, fuzzy quadratic programming can be used to handle these complexities. This paper focuses on studying the sensitivity analysis of fuzzy quadratic programming when there are simultaneous and independent variations in the constraints and objective function coefficients. The authors propose a method that uses auxiliary problems to identify the intervals of invariance and provide a fuzzy representation of the optimal value function. Numerical examples are also provided to demonstrate the effectiveness of this approach.
411729	41172949	Fuzzy costs in quadratic programming problems	Quadratic programming problems, although a type of nonlinear programming, can also be viewed as general linear programming problems. These types of problems have become increasingly important in various practical fields. Due to the presence of ambiguity and vagueness in real-life situations, it is beneficial to address them using fuzzy concepts, specifically as Fuzzy Quadratic Programming problems with uncertainty. This paper presents two new methods based on fuzzy sets to solve a specific type of Fuzzy Quadratic Programming with uncertain coefficients in the objective function. Additionally, two linear approaches are extended to solve quadratic problems, and it is demonstrated that their solutions can be obtained from proposed parametric multiobjective approaches.
411730	411730196	Information acquisition through an integrated paradigm: agent + peer-to-peer	Agent computing and P2P development are two distinct paradigms that have their own strengths. While agent computing focuses on defining abstract problem-solving strategies, P2P development excels in resource gathering and efficient resource discovery. However, in order to create self-evolving and scalable systems, the integration of these two paradigms is necessary. This paper delves into the potential benefits of incorporating P2P facilities into agent development and explores different ways of integration. A prototype system, BestPeer, is introduced as an example of this integration. Additionally, the challenge of managing data in P2P environments where the schema is not predetermined is addressed with the introduction of PeerDB, a system that enables data sharing without a global schema.
411730	4117308	PeerDB: a P2P-based system for distributed data sharing	PeerDB is a peer-to-peer (P2P) distributed data sharing system that supports fine-grain content-based searching and facilitates sharing without a shared schema. It utilizes mobile agents to perform operations at peers' sites and has a self-configurable network that optimizes communication between nodes. The system was evaluated on a cluster of 32 PCs and showed effective utilization of P2P technologies for data sharing. 
411731	41173130	Using a pilot study to derive a GUI model for automated testing	Graphical user interfaces (GUIs) are widely used in software but testing them for functional correctness is an area that has not been extensively studied. GUIs have many different input options for users, making it challenging to test all possible interactions. Previous research has used informal examination and personal intuition to create an event-interaction graph (EIG) to help generate and execute test cases. This article describes a pilot study that empirically derives the EIG model and validates the intuition used in previous work. The EIG helps testers understand how events interact and why certain events detect faults, allowing them to better traverse the event space. New test adequacy criteria and algorithms are proposed and shown to be effective on four open-source applications.
411731	4117316	GUITAR: an innovative tool for automated testing of GUI-driven software	GUI testing is an important process in software testing, and it involves generating and executing test cases that are modeled as sequences of GUI events. While it is agreed that a variety of techniques should be used for effective GUI testing, currently available tools are limited in their support. In this paper, the tool GUITAR is introduced, which offers an innovative architecture using plug-ins to support flexibility and extensibility. This allows for the creation of new toolchains, workflows, and integration with different measurement tools for GUI testing. Several case studies demonstrate the capabilities of GUITAR.
411732	411732284	Graph indexing based on discriminative frequent structure analysis	This article discusses the importance of graphs in modelling complex structures and schemaless data. It explores the challenges of indexing graphs and proposes a new indexing model based on discriminative frequent structures, which are identified through graph mining. This compact index improves performance in retrieving graphs from a large database and is stable to database updates. The approach also shows how database indexing and query processing can benefit from frequent pattern mining. It can be applied to other structures such as sequences and trees.
411732	411732756	Graph Indexing: Tree + Delta >= Graph	In recent years, there has been a rise in the use of graphs to model structural patterns in scientific and technological advancements. This has led to a need for efficient processing of graph containment queries on large graph databases. These queries involve retrieving all graphs in a database that contain a specific query graph as a subgraph. Due to the complexity of subgraph isomorphism testing and the large number of graphs in a database, it is important to use effective graph indexing techniques to reduce query processing time. This paper proposes a new indexing method based on frequent tree-features of the graph database, which has been shown to be more efficient and effective than existing graph-based indexing methods. The proposed indexing method, called (Tree+¢), selects discriminative graphs on demand without a costly graph mining process, leading to a more efficient index construction and better query processing performance. Experimental studies show that (Tree+¢) outperforms other methods such as gIndex and C-Tree in terms of index structure, construction time, and query processing.
411733	41173338	Adaptable Mobile Applications: Exploiting Logical Mobility in Mobile Computing	The article discusses the challenges that come with the increasing use of mobile devices for applications. These applications are often limited in interoperability and context-awareness, making deployment and updates difficult. The dynamic environments that these applications are used in require a shift from design-time to run-time effort in software development. To address these challenges, the authors have created SATIN, a middleware system that allows for the use of logical mobility techniques on mobile devices connected to diverse networks. Through SATIN, applications can be deployed and updated easily and efficiently. The article outlines the approach and benefits of using SATIN for building adaptable applications on mobile devices.
411733	41173321	CARISMA: context-aware reflective middleware system for mobile applications	Mobile devices, such as phones and PDAs, are becoming increasingly popular and will soon be connected to networks. This allows for the creation of distributed applications that can adapt to changes in context, such as network bandwidth, battery power, connectivity, and reachability of services and hosts. The CARISMA middleware is designed to enhance the development of adaptive and context-aware mobile applications by using reflection and policies. These policies may conflict, so the middleware uses a microeconomic approach, specifically a sealed-bid auction, to handle conflicts at execution time. A distributed context-aware application is used to demonstrate this method, and performance evaluations show that it does not add significant overhead. This research is compared to related work and the paper concludes. 
411734	4117345	Supplier selection using fuzzy AHP and fuzzy multi-objective linear programming for developing low carbon supply chain	The sustainability of a supply chain is heavily influenced by the purchasing strategy of its members. However, traditional models have focused on factors such as cost, quality, and lead time, neglecting the importance of carbon emissions in supplier evaluation. With increasing pressure to reduce carbon emissions, this study proposes an integrated approach that utilizes fuzzy-AHP and fuzzy multi-objective linear programming to select the most suitable supplier in a supply chain. The method involves analyzing the weights of various factors, including cost, quality, delivery performance, and carbon emissions, using fuzzy AHP. These weights are then used in a fuzzy multi-objective linear programming model for supplier selection and quota allocation. An example is provided to demonstrate the effectiveness of the approach in handling uncertain information.
411734	41173449	Analyzing alternatives in reverse logistics for end-of-life computers: ANP and balanced scorecard approach	Reverse logistics activities, particularly in the computer hardware industry, are complex and challenging for top management. In order to evaluate various options for end-of-life (EOL) computers, a decision model using Analytic Network Process (ANP) has been proposed. This model structures the problem in a hierarchical form, linking the determinants, dimensions, and enablers of reverse logistics with potential alternatives. The dimensions of reverse logistics have been derived from a balanced scorecard approach, including customer, internal business, innovation and learning, and finance perspectives. This approach considers both financial and non-financial, tangible and intangible, and internal and external factors, providing a comprehensive framework for selecting the best reverse logistics option. The ANP's ability to consider interdependencies between decision attributes makes it an effective multi-criteria decision-making tool. The combination of balanced scorecard and ANP in this approach offers a realistic and accurate representation of the problem, making it useful in conducting reverse logistics operations for EOL computers.
411735	41173531	Development of a module based service family design for mass customization of airline sector using the coalition game.	This paper aims to address the challenges faced by the airline industry through the introduction of new and simplified methodologies. The use of strategic sharing of modules in service family design, modeled through coalition and Cournot game theories, is proposed as a cost-effective way to achieve mass customization and improve individual products. Through the analysis of empirical evidence, it is found that convenience, safety, and service quality play a significant role in passengers' choice of airline. The Cournot model is used to predict the impact of new services on an airline's profits and its competitors' response. This research can assist airline companies in selecting services and understanding the importance of service quality in the industry, giving them a competitive advantage.
411735	4117358	An integrated approach for logistic and vendor managed inventory in supply chain	This paper proposes a quaternary policy system for managing logistics and inventory in the supply chain. The system involves multiple retailers and distributors, each following a unique policy. These policies include continuous time replenishment, vendor managed inventory, and an order-up-to policy. The amount of inventory needed is determined by the total demand from customers and is updated through a delivery vehicle system. The path of the delivery vehicle is optimized through dynamic ant colony optimization. A framework is also developed to measure end-customer satisfaction and total supply chain cost, taking into account inventory holding, ordering, and transportation costs. Numerical simulations are used to compare the performance of the four policies. 
411736	4117364	The Propositional Mu-Calculus is Elementary	The propositional mu-calculus is a type of logic used to reason about computer programs. It includes a least fixpoint operator and is more powerful than other logics such as Propositional Dynamic Logic, Streett's infinite looping construct, and Parikh's Game Logic. A decision procedure for the mu-calculus based on automata on infinite trees is presented, and a small model theorem is also derived.
411736	41173631	Modalities for model checking: branching time logic strikes back	This paper discusses automatic verification of finite state concurrent programs using model checking, which involves determining if a given structure is a model of a specification expressed in a propositional temporal logic. The authors propose a unified approach for efficient model checking under a broad class of generalized fairness constraints, which applies to various types of fairness expressed in a certain canonical form. They argue that branching time logic is always better than linear time logic for model checking, as it subsumes the linear time logic in expressive power. They also discuss an application of their work to the theory of finite automata on infinite strings.
411737	41173755	Some Results on Fixed and Best Proximity Points of Precyclic Self-Mappings.	This paper discusses the limit properties of distances and the existence and uniqueness of fixed points, best proximity points, and limit cycles for single-valued and contractive precyclic self-mappings on a finite set of subsets of uniformly convex Banach spaces. These self-mappings are defined with a contractive condition that allows for the mapping of points within a subset or to an adjacent subset. The contractive condition is point dependent and is applied to the complete set rather than each individual subset. The distances between adjacent subsets can also vary, including the possibility of intersection between some pairs of subsets.
411737	41173735	On a General Contractive Condition for Cyclic Self-Mappings.	This paper discusses p-cyclic self-mappings in a metric space, with the mapping satisfying a general contractive condition. The paper examines the existence and uniqueness of fixed points and best proximity points, as well as the convergence of iterates generated by the mapping from initial points. The contractive condition includes various existing conditions in the literature as special cases.
411738	41173847	Experimental evaluation of query processing on encrypted Telemedicine data	Telemedicine technology has greatly improved the quality of medical care while reducing healthcare costs. However, it also brings challenges to data security and privacy. To address these concerns, various security strategies can be used, such as encrypting user-related data. With the increasing use of XML in data interchange, efficient querying over encrypted XML data is important. In a previous study, a hash-based approach was developed to enable efficient querying over encrypted XML data by augmenting it with encodings and filtering out candidate data for decryption. This paper presents an experimental evaluation of this approach for querying over encrypted XML-formatted Telemedicine data. The proposed search strategy consists of three phases: query preparation, pre-processing, and execution. 
411738	41173853	Schemata Transformation Of Object-Oriented Conceptual Models To Xml	XML is a popular data format used for sharing self-describing text on the internet. It offers a formalism called Document Type Definition (DTD) for defining the syntax and structure of XML documents. However, for effective use in large-scale e-commerce environments, XML lacks the ability to describe and model real-world data and their complex relationships. To overcome this, other data modeling methods are often used to develop a conceptual model, which is then translated into an XML format. This paper presents a method for modeling XML using Object-Oriented (OO) modeling techniques and translating them into XML Schema. The paper discusses the need for this transformation and presents several generic rules for transforming aggregation, association, and generalization relationships from OO models to XML Schema. Different perspectives on these relationships and their transformations are also discussed. 
411739	41173977	Spatial and temporal locality of content in BitTorrent: A measurement study	The study examines content locality on The Pirate Bay, one of the largest BitTorrent portals. The researchers analyze factors such as content category, publisher, and popularity to understand how and why content is consumed in different locations and times. They find that content consumption is heavily skewed in both spatial and temporal domains, with cultural factors and publishing purpose playing a role. The study also reveals that while spatial locality remains consistent on a daily basis, there is a noticeable spread of content consumption over the years. The researchers suggest that bundling and caching can be effective strategies to take advantage of content locality.
411739	41173964	Evaluation of network impact of content distribution mechanisms	The paper discusses the ongoing challenge of efficiently distributing large amounts of multimedia content due to the popularity of this type of content and the widespread use of peer-to-peer file sharing. The authors evaluate the impact of different distribution mechanisms on network traffic, including traditional unicast, content distribution networks (CDN), BitTorrent, and multicast. They develop a model for measuring the network traffic generated by these mechanisms and conduct experiments to obtain results. The findings show that BitTorrent can be inefficient in terms of network resources, while traditional mechanisms have a lower cost. The authors propose modifications to BitTorrent that can improve its efficiency and make it comparable to multicast-based distribution. 
411740	41174024	StubDroid: automatic inference of precise data-flow summaries for the android framework.	Smartphone users often lack information about how commercial and malicious apps handle their sensitive data. Automated taint analyses offer a solution by allowing users to detect and investigate how apps access and handle this data. However, most of these analyses rely on explicit models of the Android runtime library, which are difficult to obtain due to the size and rapid evolution of smartphone operating systems. In response, StubDroid was developed as the first fully automated approach for inferring accurate and efficient library models for taint analysis. These models significantly improve the efficiency and precision of analyses, as demonstrated in experiments where StubDroid-inferred models allowed for app analysis in seconds rather than the usual 30 minutes. Additionally, StubDroid's models outperformed manually crafted ones in terms of precision, time, and memory usage. 
411740	41174012	Hardening Java’s Access Control by Abolishing Implicit Privilege Elevation	The Java runtime, used on billions of devices and servers globally, is a popular target for cyber attacks due to incorrect or insufficient access-control checks. This paper delves into the issue, revealing that shortcuts in Java's stack-based access control principle are being exploited. These shortcuts, originally meant for convenience and performance, elevate code privileges without proper checks. To address this problem, the authors propose a tool-assisted adaptation of the Java Class Library that eliminates most shortcuts, resulting in a more secure system. Through experiments, they demonstrate that this approach does not significantly impact performance. This solution aims to prevent future attacks and limit the capabilities of attackers exploiting existing vulnerabilities. 
411741	41174135	Model Transformation Reuse Across Metamodels - A Classification and Comparison of Approaches.	Model transformations (MTs) are crucial in model-driven engineering (MDE) solutions. However, MDE often relies on creating new domain-specific metamodels, which can result in the need to create new MTs for each one. This paper examines different approaches to reusing MTs across different metamodels, including model types, concepts, a-posteriori typing, multilevel modeling, and design patterns. A feature model was developed to compare these approaches, and a common example was used to demonstrate their strengths and weaknesses. The paper also identifies gaps in current reuse approaches and provides a reading grid for comparing their features. This research highlights the importance of reuse mechanisms in MDE and offers insight into the most effective approaches for reusing MTs.
411741	41174154	Foundations of multi-paradigm modeling and simulation: computer automated multi-paradigm modelling: meta-modelling and graph transformation	CAMPaM is a model-driven development approach that uses meta-modelling and graph transformation to automatically generate interactive visual modelling environments. The abstract syntax of models is represented in a graph-like structure, allowing for model transformation using graph rewriting. A tool called AToM3 is introduced, which supports multiple formalisms and meta-modelling. A simple example of a reactive system, a traffic light model, is used to demonstrate the concepts of meta-modelling and graph transformation in AToM3. Model transformations such as simulation, conversion to Timed Transition Petri Nets, and code generation can be performed in this environment. 
411742	41174213	A Framework for Model Transformation By-Example: Concepts and Tool Support	Model-Driven Engineering (MDE) is a new approach to software development that uses model transformations to bridge the gap between specifications and implementations. Many different model transformation methods and languages have been developed, but most of them require knowledge of the implementation details of modeling languages. This paper presents a new approach called Model Transformation By-Example (MTBE) that allows users to define mappings between example models without needing to know the implementation details. The paper describes the MTBE concepts, including a model mapping language and a metamodel mapping language, as well as algorithms for inferring metamodel mappings from model mappings. The authors also discuss how these concepts have been integrated into existing graphical modeling and model transformation frameworks, and provide a case study to demonstrate their effectiveness. 
411742	41174283	Full contract verification for ATL using symbolic execution.	ATL is a widely used model transformation language in model-driven engineering, but the lack of methods for exhaustively verifying transformations has hindered its adoption in industry. Several techniques for verifying ATL transformations have been proposed, but most rely on non-exhaustive testing or human assistance. In this paper, the authors describe a method for statically verifying the declarative subset of ATL transformations by translating them into a model transformation language called DSLTrans. This allows for symbolic execution and verification of pre-/post-condition contracts on all possible input models, making the technique exhaustive for the declarative subset of ATL. The authors demonstrate the applicability of their technique on several complex ATL transformations and introduce a 'slicing' technique to reduce proving time.
411743	41174356	Rsa-Oaep Is Rka Secure	This paper presents a proof that the RSA-OAEP encryption scheme is secure against related key attacks (RKA) in the random oracle model, assuming the strong RSA assumption. The related key functions used can be affine functions. Unlike previous chosen ciphertext security proofs for OAEP, this proof overcomes two major obstacles: answering decryption queries under related keys, and preventing the adversary from promoting queries corresponding to the same message as the challenge ciphertext. These obstacles also exist in the RSA-OAEP+ and RSA-SAEP(+) RKA security proofs, but can be overcome by combining this technique with chosen ciphertext security proofs. The proof relies heavily on the algebraic property of the sRSA function.
411743	4117431	Constructions Secure against Receiver Selective Opening and Chosen Ciphertext Attacks.	This paper discusses the security of public key encryption schemes against IND-RSO attacks, where an attacker can corrupt some receivers and obtain their secret keys in a multi-party setting. The paper proposes a general construction for RSO security against chosen ciphertext attacks (RSO-CCA) by combining RSO security against chosen plaintext attacks (RSO-CPA) with a regular CCA secure scheme and a non-interactive zero-knowledge proof. The paper also shows that a previous construction using a weak hash proof system (wHPS) is RSO-CPA secure and that a construction based on the universal HPS is RSO-CCA secure, providing a more efficient approach for RSO-CCA security.
411744	4117443	Symbolic Trajectories.	This article discusses the management of mobility data, specifically trajectories recorded by GPS-enabled devices. The focus is on discovering meaningful patterns and representing them in a systematic way. The authors propose a simple model called symbolic trajectory, which captures a wide range of meanings derived from a geometric trajectory. This model is integrated into a data types and operations framework for moving objects. Additionally, the article presents a language for pattern matching and rewriting of symbolic trajectories, which can be used in query operations within a DBMS environment. The implementation of the model using finite state machines is described and an experimental evaluation demonstrates its efficiency in terms of storage space and response time. 
411744	4117440	Index-supported pattern matching on symbolic trajectories	The use of GPS-enabled devices, such as smartphones and vehicles, has led to an increase in the collection of mobility data. This has also resulted in the need for effective management of these large datasets, leading to the development of a research field known as trajectory management. To make querying of these datasets more efficient, a symbolic representation of the geometric data has been introduced. This paper presents a comprehensive framework for describing and querying these symbolic trajectories, which can contain information such as road names, speed profiles, and transportation modes. The main contribution of this paper is an improvement to the existing approach, utilizing two indexes (a trie and a one-dimensional R-tree) to optimize the matching of symbolic trajectories for any type of pattern. This results in a significant reduction in computation cost and improves the efficiency of tasks such as analyzing transportation systems, targeted advertising, and animal research. The paper outlines the concept and implementation of this new approach, followed by an experimental evaluation. 
411745	41174555	An Ultra-Fast User-Steered Image Segmentation Paradigm: Live Wire On The Fly	The authors have developed a new, faster method for user-guided image segmentation called "live wire on the fly." This method builds on previous segmentation paradigms and uses known properties of graphs to avoid unnecessary computations, resulting in faster segmentation even on low-powered computers. The user simply selects a point on the boundary and the live-wire segment is displayed in real time as they move the cursor, making the process more efficient. This method has been tested on medical images and found to be 1.3-31 times faster than previous methods, with the computational part alone being 120 times faster.
411745	41174579	Fuzzy-connected 3D image segmentation at interactive speeds	In recent years, fuzzy connectedness principles have been successfully used in image segmentation techniques for a variety of objects. However, these algorithms can be computationally intensive. This paper explores 18 'optimal' graph search algorithms in an effort to speed up the process. Extensive testing on medical images has shown that a combination of algorithms and modern PCs can achieve a 20-1000-fold improvement in speed. By using efficient algorithms and carefully selecting implementations, the computation of fuzzy connectedness values can be sped up by a factor of 16-29 compared to previous implementations. The running time can also be significantly reduced by utilizing pre-determined thresholds for fuzzy objects, making the process more accurate and efficient. The incorporation of human operators and computer delineation in an interactive process is also shown to be effective. Furthermore, if tissue-specific images are used, most parameters can be fixed and computed before user interaction is needed, providing more information during the process. 
411746	41174660	Supporting multimodality in service-oriented model-based development environments	This article discusses the challenges of developing multimodal interfaces and the lack of authoring tools available for this purpose. The author proposes using model-based languages to specify multimodality and demonstrates how this can be applied to graphical and vocal interactions. The article also introduces the use of CARE properties and a model-based language to support service-based applications and Web 2.0 interactions. This approach is supported by an authoring environment and can generate implementations of multimodal interfaces for Web environments. An example of this process is provided, highlighting the potential for designers to modify the solutions to meet their specific needs.
411746	411746154	Intelligent Support for End-User Web Interface Customization	As the use of interactive software continues to grow, new and more complex applications and systems are emerging. One example of this is multi-device applications, where users can interact with the system through different platforms. However, allowing end-users to have the ability to create and modify user interfaces is still a challenge that most users struggle with. This paper proposes a solution that uses implicit user intents, learned from the user's previous interface modifications, to make it easier for users to edit web interfaces. The design and implementation of this approach are discussed, along with the results of a user test.
411747	41174746	End-user visualizations	In recent years, computer visualization has greatly improved, largely due to the popularity of video games. Technology has advanced to the point where even handheld devices can efficiently render and animate complex 3D graphics. However, there is a lack of accessible tools and frameworks for end-user developers to utilize these advancements and create their own interactive 2D and 3D applications. The Agent Warp Engine (AWE) is a shape-warping framework that aims to address this issue and allow for the creation of rich visualizations, animations, and simulations by end-users.
411747	4117473	Collaborative end-user development on handheld devices	Web 2.0 has allowed for collaboration among end users through self-created content, such as Wikipedia and blogs. This has expanded to include images and videos, changing the role of end users from consumers to producers. However, there is currently no support for collaboration through interactive user-created content on emerging handheld devices, which are becoming the next collaborative platform. Ristretto Mobile is a Web-compliant framework that allows for the development and running of end-user created applications on handheld devices. This includes a compiler and runtime components for efficient and user-friendly access on devices like the iPhone and Nokia N800. The paper discusses the challenges in creating interactive content that is suitable for use on handheld devices.
411748	41174881	Experience with user-centred requirements engineering	This paper outlines a case study where human-computer interaction (HCI) principles and methods were used to develop a visualization tool, ADVISES, for epidemiological research. The development process involved scenario-based design, prototyping, and storyboarding to understand user tasks and mental models. HCI functional allocation heuristics were also applied to guide system requirements decisions. The prototype was evaluated to determine its effectiveness in supporting public health professionals. The study results and implications of using scenario-based design, functional allocation, and software architecture are discussed. 
411748	41174829	Personal and Contextual Requirements Engineering	The proposed framework for requirements analysis takes into consideration individual and personal goals, as well as the influence of time and context on personal requirements. It has implications for system architecture, which can be implemented through functional specifications, customisable features, or automatic adaptation. This means that system architecture requirements must be analyzed. The different implementation pathways have cost-benefit implications, so cost-benefit analysis techniques are suggested to evaluate trade-offs between goals and implementation strategies. Two case studies in assistive technology domains (e-mail and personalized navigation) demonstrate the use of the framework.
411749	41174944	Simulating a Shared Register in a System that Never Stops Changing.	Emulating a shared register is a way to simplify the design of algorithms for asynchronous message-passing systems with crash failures. It involves replicating the register's value in multiple servers and requiring communication with a majority of servers. While this approach has been successful for static systems, there are challenges when it comes to dynamic systems where nodes can enter and leave. Existing emulations have limitations such as assuming the system will eventually stop changing or imposing a bound on the system size. This paper presents a new emulation that can support any number of readers and writers in a crash-prone system with continually changing nodes and without a bound on system size. The algorithm has certain requirements for the number of entering and leaving nodes and crashed nodes, and the paper also includes a lower bound for the necessary number of correct nodes.
411749	41174922	Time-space tradeoffs for implementations of snapshots	A snapshot object is used in distributed systems to obtain a consistent view of shared memory while other processes may make changes. It stores an array of components and can be updated or scanned. This paper discusses time-space tradeoffs for implementing a fault-tolerant snapshot object using only Read and Write operations on registers. For anonymous implementations, the time required for a SCAN operation is at least n/r, where n is the number of processes and r is the number of registers used. For non-anonymous implementations, the time grows without bound as n increases. This is the first proof of a lower bound for the tradeoff between time and registers in asynchronous shared-memory systems. A new tool called "shrinkable execution" is introduced for proving distributed lower bounds.
411750	41175046	Effective Lossy Queue Languages	The paper discusses the computation of reachable states in a lossy channel system (LCS). While the set of reachable states is known to be regular, it cannot be effectively constructed. The paper characterizes certain classes of LCS for which the set of reachable states can be computed. It also introduces rewriting systems that capture the behavior of LCS and shows that for context-free rewriting systems, the corresponding language can be computed. The paper also presents a characterization of classes of meta-transitions with nested loops in the control graph of the system, rather than just single loops. The same technique is also used to establish a result for 0L-systems, where the downward closure of the language generated by any system is effectively regular.
411750	41175074	Computing Simulations over Tree Automata	The article discusses the issue of computing simulation relations on tree automata. It focuses specifically on downward and upward simulations, which are similar to forward and backward relations on word automata. The authors propose efficient algorithms for computing these relations by reducing the problem to simulations on labelled transition systems. They also demonstrate how combining downward and upward relations can lead to efficient size reduction of nondeterministic tree automata, which is important for methods like regular model checking. The article includes experimental results showing the effectiveness of their algorithms on examples from regular model checking.
411751	41175123	ADELFE: a methodology for adaptive multi-agent systems engineering	Adaptive software is necessary when the environment is unpredictable or the system is open. The ADELFE methodology, guided by the Rational Unified Process (RUP), is specifically designed for software engineering of adaptive multi-agent systems. By following ADELFE, the software is developed in accordance with the AMAS theory. This paper focuses on the additions that ADELFE brings to the first three core workflows of RUP. During the requirements phase, the system's environment is defined and characterized. In the analysis phase, the engineer is guided to use adaptive multi-agent technology and identify the agents through system and environment models. The design workflow provides a cooperative agent model and helps define the behavior of local agents. The methodology is demonstrated through a case study of timetable design.
411751	41175115	Enhancing self-organising emergent systems design with simulation	The challenge in designing complex systems that can adapt to changing environments has led to the use of multi-agent systems (MAS) and various agent-oriented methodologies. Self-organisation is a promising approach for making these systems adaptive, where the global function emerges from local interactions. However, the difficulty lies in finding the right behaviours at the agent-level to achieve the desired global function. This paper explores how simulation can aid designers in identifying and improving these behaviours during the design stage. A model of cooperative agents was implemented and integrated into the agent-oriented methodology ADELFE. This model was then applied to improve the behaviour of a simple ecosystem.
411752	41175210	Monitoring the monitor: an approach towards trustworthiness in service oriented architecture	Service-oriented architecture focuses on separating clients and providers of a service through an abstract service description. This allows a service broker to match clients with suitable service implementations, and clients can then directly send requests to the implementation. However, the current architecture lacks a reliable way for clients to specify and verify non-functional properties, such as data access and persistence restrictions. In response, the authors propose an extended architecture that addresses this issue. A prototype implementation and preliminary results show the potential usefulness of this architecture in real-world software applications. 
411752	41175243	Tisa: Toward Trustworthy Services in a Service-Oriented Architecture	Ensuring that service implementations adhere to their service-level agreements (SLAs) is crucial for building trust in a service-oriented architecture (SoA). While functional agreements can be checked by examining the service's published interface, non-functional agreements often require a monitor to observe the service's execution. However, these monitors must operate in an untrusted environment, making the integrity of their results questionable. To address this issue, the authors propose an extension of the traditional SoA that utilizes hardware-based root of trust. This allows clients, brokers, and providers to negotiate and validate the integrity of a requirements monitor. The authors claim that this approach is feasible and has a relatively small overhead, as demonstrated through a case study using Web service implementations.
411753	41175394	Power-aware multi-objective evolutionary optimization for application mapping on NoC platforms	Network-on-chip (NoC) is the next generation communication infrastructure that will be present in various environments. In platform-based design, an application is implemented using a set of collaborating intellectual property (IP) blocks. However, selecting the most appropriate IPs and mapping them onto the NoC efficiently is a challenging task. To address this, the paper proposes a power-aware multi-objective evolutionary algorithm for the assignment and mapping stages of NoC design synthesis. This algorithm uses either NSGA-II or microGA as a kernel and optimizes for area and execution time while also considering the decision maker's power consumption preferences. 
411753	41175348	Preference-based multi-objective evolutionary algorithms for power-aware application mapping on NoC platforms	Network-on-chip (NoC) is a new communication infrastructure for embedded systems. In platform-based design, an application is implemented with a set of intellectual property (IP) blocks. Two difficult problems in NoC-based implementation are selecting the appropriate IPs and mapping them onto the NoC infrastructure. This paper proposes a novel preference-based multi-objective evolutionary method for solving these problems. The well-known algorithms NSGA-II and microGA are used as a basis for the optimization processes, which aim to minimize silicon area and execution time while considering the decision maker's preference for overall power consumption. 
411754	41175479	Efficient concise deterministic pattern-matching automata for ambiguous patterns	Pattern-matching is a crucial aspect of various applications like functional programming, logic programming, and theorem proving. In this process, patterns are converted into a deterministic finite automaton for efficient matching. However, when dealing with ambiguous patterns, it is possible for a subject term to match multiple patterns, resulting in the need for a priority rule to select the appropriate one. To optimize this process, a new pre-processing technique has been developed that identifies and eliminates irrelevant instances of ambiguous patterns. This helps to reduce the space and time requirements of the matching automaton, making the process more efficient.
411754	4117546	More efficient left-to-right pattern matching in non-sequential equational programs	Pattern matching is a crucial aspect of various applications, such as functional programming, logic programming, and expert systems. This involves pre-processing patterns into a deterministic finite automaton, which allows for efficient identification of matching patterns in a single scan of input data. The construction of this automaton typically follows a left-to-right traversal approach. In this paper, the authors propose a method for building such an automaton and also suggest an incremental approach for non-sequential rewriting systems. In cases where patterns are ambiguous, a priority rule is used to select the correct pattern. However, pre-processing can lead to irrelevant instances being added, resulting in unnecessary space and time requirements. To address this, the authors introduce a new pre-processing operation that identifies and avoids such instances, leading to improved efficiency of the matching automaton. 
411755	411755139	A learning based training and skill assessment platform with haptic guidance for endovascular catheterization.	The demand for endovascular intervention has increased and there is a need for technical skill training and performance measures. However, there are no established online metrics for assessing technical skills and limited research on operator behavior during procedures. This paper introduces a platform for online training and objective assessment of endovascular skills, using multiple demonstrations to learn optimal catheter motions. An ungrounded hand-held haptic device is also proposed to provide haptic guidance to inexperienced users, based on the learned information. Statistical models are used to extract catheter motion patterns and improve performance and haptic guidance. Results show improved navigation and finer catheter motions with the proposed platform, suggesting its potential for integration into clinical training and further improvement of endovascular training with more realistic features.
411755	4117558	Learning-based modeling of endovascular navigation for collaborative robotic catheterization.	Robot assisted catheterization has seen rapid growth, but most current platforms have limited operator-robot collaborative control and automation. This means that information about subject behavior and context is not used for future interventions. Additionally, the design of the robot itself does not take into account the skills and motion patterns of the operator. To address these issues, a learning-based approach is proposed for generating optimal motion trajectories for catheterization tasks. This approach uses multiple demonstrations of the task to create motion models that can be replicated by a robotic catheter driver to assist inexperienced operators. The results show significant improvements in the quality of catheterization, allowing for the design of collaborative robots that utilize the natural skills of the operators.
411756	411756143	Bag Of Forests For Modelling Of Tissue Energy Interaction In Optical Coherence Tomography For Atherosclerotic Plaque Susceptibility Assessment	The use of intravascular optical coherence tomography (IV-OCT) allows for high resolution imaging of tissue pools that are at risk of rupturing in atherosclerosis. However, the interpretation can be subjective due to the stochastic nature of OCT speckles. To address this issue, a framework is proposed that utilizes OCT tissue energy interaction to identify susceptible tissues. This is achieved by combining local speckle statistics, optical attenuation estimates, and signal confidence measures in a multiscale approach. A bag of random forests is then trained on a bank of 22 OCT cross-sections and evaluated on 22 other diverse cross-sections to demonstrate the robustness of the framework. This framework has the potential to provide more objective assessment and aid in decision making during interventions for atherosclerosis.
411756	411756142	Deep learning of tissue specific speckle representations in optical coherence tomography and deeper exploration for in situ histology	Optical coherence tomography (OCT) uses coherent sensing of photons to create speckle images of tissues, allowing for non-invasive, high-resolution imaging of organs and tissue. However, the stochastic nature of speckles can lead to variability in reporting by observers. To address this issue, a deep neural network (DNN) based architecture is proposed for unsupervised learning of speckle representations and supervised learning of tissue specifics. This allows for more accurate characterization of healthy skin and healing wounds, reducing clinical reporting variability. The performance of this method is compared to conventional histology and shows promising results for in situ histology of live tissues.
411757	41175790	EDAS: energy and distance aware protocol based on SPIN for wireless sensor networks	The challenge of designing energy-efficient dissemination protocols for Wireless Sensor Networks (WSNs) has been addressed by multiple recent studies. One notable protocol, SPMS, has been shown to outperform the well-known SPIN protocol due to its use of the shortest path to minimize energy consumption. However, this approach also leads to a reduction in network lifetime as the same shortest path is repeatedly used. To address this issue, a new protocol called EDAS (Energy and Distance Aware protocol based on SPIN) is proposed. EDAS takes into account both residual energy and the most efficient distance between nodes to determine a path for data dissemination, resulting in both energy efficiency and a 69% increase in network lifetime compared to SPMS. 
411757	41175780	A Dissemination Protocol to Guarantee Data Accessibility within N-Hops for Wireless Sensor Networks	Designing a dissemination protocol for Wireless Sensor Networks (WSNs) is challenging due to the need for energy efficiency. Flooding and SPIN are commonly used proactive methods that disseminate data without a request from a sink node, but they can be energy inefficient by transmitting data to unnecessary nodes. To address this issue, a semi-proactive protocol is proposed to only disseminate data to relevant nodes within a certain number of hops. The simulation results show that this protocol has higher energy efficiency than SPIN, with up to 83.7% improvement if a 1-hop burden is used. This approach could help improve the energy efficiency of WSNs compared to existing methods.
411758	4117582	Junction detection for linear structures based on Hessian, correlation and shape information	This paper introduces a method for detecting junctions in 2D images with linear structures and determining the number of branches and branch orientations. The method combines Hessian information and correlation matrix to select candidate junction points, which are then refined and the branches are identified using intensity information from a stick-shaped window and a Gaussian-shaped multi-scale stick template. The proposed approach is compared to three other methods and results show that it is more accurate in detecting junctions. The multi-scale template is effective in detecting structures with varying widths. The algorithm was tested on various types of images and showed promising results.
411758	4117580	Fast Stereo Matching Using Rectangular Subregioning and 3D Maximum-Surface Techniques	The paper introduces a quick and dependable method for creating a detailed disparity map in stereo matching. This is achieved by using fast cross correlation, rectangular subregioning (RSR), and 3D maximum-surface techniques in a coarse-to-fine process. The algorithm utilizes the box-filtering technique and segments the images into rectangular subimages at different levels, leading to faster processing and reduced memory usage. The disparity map is obtained by finding the global 3D maximum-surface in the correlation coefficient volume, rather than just selecting the local maximum for each pixel. The paper also presents two new techniques, RSR and TSDP, for faster similarity measurement and efficient computation of 3D maximum-surface. The algorithm has been tested on various images and has shown good results, with a running time of just a few seconds on a 500 MHz PC. 
411759	41175966	A scalable and highly available system for serving dynamic data at frequently accessed web sites	This paper discusses the system and techniques used to achieve high performance and availability for the official website of the 1998 Olympic Winter Games. The website utilized thirteen SP2 systems with a total of 143 processors, and a key feature was the constant updating of data presented to users. To efficiently serve dynamic data, the website cached pages and implemented a new algorithm called Data Update Propagation (DUP) which updated stale pages directly in the cache. This resulted in a cache hit rate of close to 100%. The website was able to serve pages quickly and was available 100% of the time. The paper also describes the key features used for high availability and the website's structure to provide useful information with minimal page visits.
411759	41175952	A scalable system for consistently caching dynamic Web data	This paper introduces a new method called Data Update Propagation (DUP) for caching dynamic Web data. DUP maintains data dependence information between cached objects and underlying data, allowing for efficient updates when changes occur. This approach was successfully used at the official website for the 1998 Olympic Winter Games, resulting in cache hit rates close to 100% compared to 80% with an earlier version of the system. This allowed for quick data retrieval even during times of high demand.
411760	41176040	A DEVS component library for simulation-based design of automated container terminals	Modeling and simulation are commonly used during the analysis phase of design processes, but simulation-based design allows for continuous use throughout the synthesis phase. This allows for quick comparison of alternative designs in areas such as layouts, terminal operating systems, and equipment. Container terminals are complex systems with many interacting entities, so a component-based approach simplifies the model construction process by allowing designers to focus on relevant constructs rather than low-level details. However, achieving compatibility and modularity between components requires significant effort. DEVS offers higher level constructs to conceptualize complex systems independently from their implementation. A DEVS component library has been developed for container terminal design, with a focus on distinguishing between control and mechanics. This library supports the design process for container terminals.
411760	4117607	Experimenting with the multiple worlds concept to support the design of automated container terminals	The paper discusses the need for iterative and interactive design approaches to handle the complexity of systems. It introduces the multiple worlds approach, which allows multiple actors to participate in the design process. The approach utilizes a simulation and 3D visualization to explore, analyze, and compare design alternatives. The environment facilitates shared understanding and transparency in decision-making among stakeholders. An experiment was conducted to evaluate the effectiveness of this approach.
411761	41176143	Detection and description of moving objects by stochastic modelling and analysis of complex scenes	This paper discusses a novel approach for detecting and describing moving objects in natural scenes using statistical analysis of video sequences. Traditional methods rely on change detection algorithms, but these can lead to false alarms or missed events due to factors like camera noise or varying illumination. To address this, the proposed technique incorporates additional features such as texture and motion to better distinguish between true moving objects and other sources of temporal changes in the image signal. The algorithm also adapts to fluctuations in the scene by estimating the probability distributions of the different features over time. Significant deviations from these distributions are identified as potential moving objects.
411761	4117616	Statistically Optimal Averaging for Image Restoration and Optical Flow Estimation	This paper introduces a Bayesian best linear unbiased estimator (Bayesian BLUE) and applies it to generate optimal averaging filters for linear filtering of signals in low level vision tasks. Traditional methods of filter selection are often based on ad hoc assumptions and lack theoretical justification. The proposed approach uses statistical estimation theory to derive optimal filter masks based on the characteristics of the signal and noise. This allows for the determination of the filter's shape and size, as well as the estimation of the variance of the estimate. Experimental results on image reconstruction and optical flow estimation demonstrate the superiority of the proposed approach over traditional methods.
411762	41176214	A formal model of user-defined resources in resource-restricted deployment scenarios	The paper discusses the importance of considering deployment choices in software development and proposes a method for modeling and analyzing these choices using a combination of a generic cost model and deployment components in the Timed ABS language. The cost model can be customized by the user to capture specific resource requirements, while architectural variations are represented through resource-restricted deployment scenarios. The approach is demonstrated using an example of multimedia processing servers and a simulation tool is used to analyze deadline misses for different usage and deployment scenarios. This integration of low-level deployment concerns into high-level models allows for early evaluation of the consequences of deployment choices on software performance.
411762	4117620	Dynamic resource reallocation between deployment components	Software systems today are highly configurable and can be deployed on various architectures, including sequential machines, multicore systems, and the cloud. Examples of such systems can be found in software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. To effectively model and analyze these systems, it is important to capture and consider different deployment scenarios. This paper proposes an extension to the object-oriented modeling language Creol, which includes a notion of dynamic deployment components with parametric processing resources. This allows for explicit reallocation of processor resources and enables a compositional approach where functional models and reallocation strategies can be expressed in Creol and executed on Maude. This allows for simulations and testing of models with varying resources and reallocation strategies.
411763	4117636	A large user pool for accessibility research with representative users	Accessibility research is crucial for ensuring that technology is usable and accessible for all individuals. One key aspect of this research is involving representative users in the exploration and evaluation of ideas. However, it can be challenging to recruit these users, especially in a timely manner. This paper discusses the creation of a large user pool specifically for older adults who are interested in participating in technology research studies. The process of establishing and maintaining this pool is also discussed, along with lessons learned. This user pool will help facilitate accessibility research by providing a large and diverse group of users for studies.
411763	4117639	What's on your mind?: investigating recommendations for inclusive social networking and older adults	Social networking sites (SNSs) are gaining popularity as a means of social interaction. However, there is a digital divide between younger and older users. While SNSs can be beneficial for maintaining connections with family and feeling digitally included, many older adults choose not to use them. To understand the barriers and challenges faced by older users, a series of user studies were conducted. Based on the findings, recommendations were developed to address these barriers. These recommendations were then evaluated through a comparative study involving 25 older adults completing tasks on two versions of a simulated SNS interface. The recommendations and their creation and evaluation methods are presented, along with implications for SNS developers.
411764	4117643	CaptainTeach: multi-stage, in-flow peer review for programming assignments	In this paper, the authors discuss the use of peer review in computing education, specifically in programming courses. They introduce a new tool called CaptainTeach, which allows for peer review at multiple stages within ongoing assignments. This is different from traditional peer review methods that only focus on completed assignments. The authors describe their experience using CaptainTeach in two undergraduate courses and highlight the logistical and pedagogical challenges that arise from this type of multi-stage, in-flow peer review. They focus on the unique issues that come with combining multiple stages and in-flow reviewing, rather than peer review in general.
411764	41176449	The Impact Of A Single Lecture On Program Plans In First-Year Cs	Programming problems often have multiple solutions that vary in how they organize the tasks. The choice of solution often depends on the student's previous exposure and the features of their programming language. To determine how much exposure students need to appreciate and produce different plans, a study was conducted with students in introductory programming courses at two universities. After a single lecture on planning, many students were able to produce multiple plans and discuss their tradeoffs. This suggests that planning can be effectively taught with minimal effort once students have a basic understanding of programming.
411765	41176547	Coding partitions: regularity, maximality and global ambiguity	The canonical coding partition is a way of dividing a set of words into classes based on their factorizations in a sequence. It ensures that words with multiple factorizations are grouped together in the same class. If the set is not uniquely decipherable, the partition separates a unique unambiguous class from the remaining parts that reveal the ambiguities in finite sequences. The partition for a regular set has a finite number of regular classes, and an algorithm is provided for its calculation. The article also explores maximality conditions in coding partitions, proving the equivalence between two notions in the regular case. This has applications in deriving new properties of maximal uniquely decipherable codes.
411765	41176513	Words and Patterns	The paper discusses new ideas and results related to patterns. It compares the set of binary patterns in an infinite binary word with the set of factors of the word, leading to a classification of infinite words based on the difference between these two sets. The study also explores the set of patterns in a finite word, and the relationship between the structure of this set and the combinatorial properties of the word. It concludes by showing that the set of patterns in a regular language is also a regular language. 
411766	41176678	Adapting the Fitness Function in GP for Data Mining	The Stepwise Adaptation of Weights (saw) technique, initially designed for constraint satisfaction problems, can also be effectively utilized in genetic programming. This mechanism is suitable for data mining tasks where the fitness of a solution is determined by 'local scores' on data records. By incorporating saw-ing into genetic programming, it was found to improve performance on various benchmark classification data sets. However, the results also revealed that saw-ing may have a negative impact if different types of misclassifications are weighted differently. Overall, saw-ing shows promise in enhancing the capabilities of genetic programming for certain types of problems.
411766	41176668	Peer-to-peer evolutionary algorithms with adaptive autonomous selection	The paper discusses a fully distributed peer-to-peer evolutionary algorithm that incorporates adaptive autonomous selection. This means that individuals within the algorithm make their own decisions regarding survival and reproduction, without any central control. The algorithm also includes a gossiping mechanism that allows individuals to regulate their own selection pressure, addressing the issue of uncontrolled population size. The algorithm is tested on a problem and the results show that a fully decentralized approach is feasible and can maintain a stable population size. These findings provide insight into the dynamics of such an algorithm and demonstrate its potential effectiveness.
411767	411767101	Intelligent Software Agent Design Tool Using Goal Net Methodology	Intelligent agent technology is rapidly expanding and has many practical uses. However, there are limited tools available to assist in converting paper-based designs into effective representations that can be used by agent management systems. The Goal Net Designer, an Integrated Development Environment (IDE), is proposed as a solution. It is based on the Goal Net model, a goal-oriented approach, and simplifies the design process while automatically generating design data for the Multi-Agent Development Environment (MADE). This reduces the required level of expertise, allowing even those with little knowledge of intelligent agent technology to easily incorporate intelligent agents into their applications and save time and resources during development.
411767	41176730	Design of fuzzy cognitive maps using neural networks for predicting chaotic time series	Fuzzy Cognitive Maps (FCMs) are a popular and powerful method for representing knowledge and simulating complex systems. However, traditional FCMs lack efficient methods for determining system states and quantifying causal relationships, leading to a heavy reliance on expert knowledge. This can result in subjective and unreliable models. To address this issue, a fuzzy neural network is proposed to enhance the learning and inference abilities of FCMs. This allows for automatic construction of FCM models from data, making them independent from experts. In addition, mutual subsethood is used to define and describe causalities in FCMs, providing a clearer understanding of the inference process. The proposed approach is tested on predicting chaotic time series and shows promising results.
411768	411768154	Implementation of fuzzy cognitive maps based on fuzzy neural network and application in prediction of time series	The fuzzy cognitive map (FCM) is a popular tool for representing knowledge and simulating different systems. However, it is limited by the lack of efficient methods for determining system states and quantifying causal relationships. This means that FCMs are often constructed using expert knowledge, leading to subjective models with reliability issues. To address this, a fuzzy neural network approach was proposed that combines the inference mechanism of FCMs with the determination of membership functions and quantification of causalities. This allows for automatic construction of FCM models from data, reducing the need for human intervention. The approach was tested through numerical simulations and shown to be effective in predicting time series.
411768	41176846	Temporal fuzzy cognitive maps	This paper presents a new extension of Fuzzy Cognitive Maps (FCMs) called temporal FCMs (tFCMs), which address the limitation of FCMs in modeling long-term inference and time-related knowledge. The proposed method includes a temporalized FCM that adds a time dimension to the traditional FCM, a design approach that simplifies the construction of the map, and a discussion on how errors in fuzzy knowledge affect the results. The paper also compares two different causality models and their error effects during inference, providing guidance for users to balance error accumulation and map sensitivities. The results demonstrate a significant difference in error accumulation between the two models.
411769	41176923	Shoot & copy: phonecam-based information transfer from public displays onto mobile phones	Public displays are a common sight in our daily lives, but they usually only provide information without any interaction options. As a result, people often forget what they saw after leaving the display. This paper introduces a new interaction technique that allows users to transfer information from a public display onto their personal mobile phone by taking a picture with the phone's camera. This technique, called Shoot & Copy, eliminates the need for visual codes and allows users to capture a specific region of the screen, such as icons representing data. The captured image is analyzed and a reference to the corresponding data is sent to the phone. This allows users to retrieve the data at a later time, making it easier to remember and use. The paper also includes an evaluation of the prototype and its potential applications.
411769	41176963	Touch projector: mobile interaction through video	Tani et al. proposed in 1992 the remote operation of machines in a factory through manipulating a live video image on a computer screen. Recently, the Touch Projector system has been developed to allow users to interact with remote screens using their mobile device. This system tracks the handheld device with respect to surrounding displays and projects touch on the video image onto the target display. However, this adaptation of Tani's idea has limitations due to the lack of stability and control in handheld video. To address this, improvements such as zooming and freezing the video image have been implemented. A user study showed that participants performed best with automatic zooming and temporary image freezing. 
411770	4117706	Developing a Dataset for Technology Structure Mining	The paper discusses the creation of a development dataset for the task of Technology Structure Mining, which involves mapping a scientific corpus into a labeled graph that expresses interdependencies between technologies. The dataset consists of sentences from the ACL Anthology Corpus, with each sentence annotated with at least two technologies and their interdependence. The annotations are done at two layers: lexical and termino-conceptual. The lexical layer represents varying lexicalizations of a technology, while the termino-conceptual layer groups these variations under a single concept. The dataset also includes five groups of contexts, classified based on linguistic and syntactic criteria. The initial dataset contains 482 sentences and is intended to serve as a benchmark for the technology structure mining task.
411770	41177064	Query Expansion Using Wikipedia and Dbpedia.	This paper discusses an approach for query expansion in the Semantic Enrichment task of Cultural Heritage in CLEF (CHiC) 2012. The approach utilizes external knowledge bases like Wikipedia and DBpedia, and involves two main steps: generating concept candidates from the knowledge bases, and selecting the top K related concepts based on their semantic relatedness with the query. To calculate the semantic relatedness scores, Wikipedia-based Explicit Semantic Analysis is used. The approach is evaluated on 25 queries from the CHiC Semantic Enrichment dataset.
411771	41177128	Can fault-exposure-potential estimates improve the fault detection abilities of test suites?	Code-coverage-based test data adequacy criteria treat all coverable code elements equally, but in practice, some faults are more easily revealed than others. Researchers have suggested estimating the probability that a fault in a code element will cause a failure and using this to determine the required number of executions for a certain level of confidence. This could improve fault-detection effectiveness and help distribute testing resources. However, the empirical evidence did not support this conjecture and the results showed only small increases in fault-detection effectiveness. This suggests that this approach may not be worth the cost of implementation. Further research is needed to explore methodologies for obtaining fault-exposure-potential estimates and incorporating them into test data adequacy criteria. 
411771	41177167	Incorporating varying test costs and fault severities into test case prioritization	Test case prioritization techniques aim to schedule test cases for regression testing in a way that maximizes their ability to meet a performance goal, specifically the rate of fault detection. Previous research has developed a metric, APFD, and techniques for prioritizing test cases to improve this rate. However, this metric and techniques were only applicable in cases where test costs and fault severity were uniform. This paper introduces a new metric that considers varying costs and severity, and presents the results of a case study demonstrating its application. The study also raises practical questions for practitioners and provides suggestions on how to address them.
411772	411772143	The u-SIG System; A Connectionist Driven Simulation of Socially Interactive Agents	The popularity of games such as The Sims and Black & White has highlighted the demand for Non Player Characters (NPCs) to have well-developed personalities, moods, and relationships. To achieve this, agent architectures used to create NPCs must be enhanced with models of these aspects of a character's persona. This paper introduces the μ-SIC system, which utilizes psychology-based models and Artificial Neural Networks (ANNs) to drive character behavior in social interactions. The system considers a character's personality, mood, and relationships with other characters in a virtual environment to determine which interaction is most suitable. The paper provides an overview of the project, describes the psychological models used, and explains the implementation and simulation of the μ-SIC system. It also discusses the strengths and weaknesses of the system and suggests potential improvements.
411772	41177222	ECUE: A Spam Filter that Uses Machine Learning to Track Concept Drift	Text classification is a promising area for Artificial Intelligence, but there have been few successful applications so far. In this paper, a spam filtering system is introduced that uses example-based machine learning techniques to train a classifier. This allows for personalization to the user's filtering preferences and the ability to adapt to changing spam and legitimate email patterns over time. The system also had to be designed to easily integrate with existing email systems for efficient management of training data. The system has been deployed and evaluated over a long period, with the results being presented in this paper. 
411773	41177356	An approach to complex agent-based negotiations via effectively modeling unknown opponents.	This article proposes a novel approach to complex agent-based negotiations. The approach focuses on learning the strategy of an unknown opponent and suggests making concessions in an adaptive manner. Through extensive experiments, the approach, called OMAC¿, has shown to improve negotiation performance compared to top agents from the International Automated Negotiating Agents Competition. OMAC¿ uses discrete wavelet transformation and non-linear regression with Gaussian processes to model opponents in real-time and adaptively adjust its utility expectations and negotiation moves. This approach has potential applications in various areas, such as e-commerce and e-business, and addresses limitations in current negotiation algorithms and techniques.
411773	41177328	Indicators for Self-Diagnosis: Communication-Based Performance Measures	Multiagent systems (MAS) have become increasingly popular in industrial applications and are seen as a promising technology originating from AI research. However, there is a lack of evaluation standards commonly used in other fields such as scheduling or database systems. This paper suggests a new method called communication-based performance measurement (CBPM) that is well-suited for open and communication-heavy MAS. The goal is to use CBPM as a way for MAS to self-diagnose and improve, in line with the concept of autonomic computing. CBPM focuses on measuring the communication processes within the MAS, and different levels of analysis are presented. Examples using FIPA-ACL and the contract-net protocol demonstrate the effectiveness of this approach, and it is believed that this method can lead to improvements in MAS performance and self-improvement methods.
411774	41177423	The Dimensional Fact Model: A Conceptual Model For Data Warehouses	Data warehousing systems allow enterprise managers to gather and combine data from various sources and effectively query large databases. Building a data warehouse requires a different approach from operational information systems, with a strong emphasis on conceptual design. This paper presents a formal graphical model, the Dimensional Fact model, and a semi-automated methodology for building it from existing schemes. The model includes fact schemes with elements such as facts, measures, attributes, dimensions, and hierarchies, as well as features like additivity and optionality. Compatible fact schemes can be overlapped to support drill-across queries. The proposed language for denoting data warehouse queries, based on fact instances, can assist in the logical and physical design phases when coupled with information about expected workloads. 
411774	41177436	A Survey On Temporal Data Warehousing	Data warehouses are databases designed to support decision making by storing and analyzing historical data. The concept of time is crucial in this process, as it is necessary to accurately track changes in information over time. This paper focuses on temporal data warehousing, which involves managing data and schema changes in data warehouses and data marts, querying temporal data, and designing temporal data warehouses. The authors provide an overview of the main concepts and terminology related to temporal databases, and then discuss each of these topics separately. They also address the current research challenges and their potential implementation in commercial tools. Overall, the paper highlights the importance of properly managing time in data warehousing systems for effective decision making.
411775	41177518	Data Warehouse Testing	Testing plays a crucial role in the development of software products, including data warehouses. While various aspects of data warehouse design have been extensively studied, there is a lack of research on data warehouse testing. In this paper, the authors propose a set of testing activities specifically for data marts and categorize them based on what is being tested and how. They also demonstrate how these activities can be incorporated into a reference design method to create a comprehensive and adaptable approach. The authors further discuss the practical application of their proposed approach through a real-life case study.
411775	41177540	WAND: A CASE Tool for Data Warehouse Design	 Data warehouse project failures are often attributed to a lack of structured design methodology. To address this issue, researchers have developed WAND, a prototype CASE tool that supports the data warehouse design process. WAND guides designers through the steps of structuring a data mart, automates parts of the conceptual design process, and allows for the definition of a core workload based on the conceptual scheme. It also assists with the logical design of the data mart scheme. This demonstration aims to showcase the capabilities of WAND and its role in promoting a correct and efficient data warehouse design process.
411776	41177612	Time-Frequency Image Descriptors-Based Features For Eeg Epileptic Seizure Activities Detection And Classification	This paper introduces a new set of features for automatically detecting and classifying epileptic seizure activity in EEG signals. Previous methods focused on analyzing signal features, but this new approach uses image descriptors extracted from the time-frequency representation of the signals. These descriptors include shape and texture-based features and are processed using time-frequency image processing techniques. The results from real EEG data showed an overall classification accuracy of up to 98% using a one-against-one SVM classifier. This outperforms other methods that only use signal features or a combination of signal and image features by 3%. These findings suggest that the proposed method is effective for detecting and classifying epileptic seizures in EEG signals.
411776	41177667	Human gait recognition based on Haralick features.	This paper presents a supervised feature extraction method for recognizing human gait under different clothing and carrying conditions. The approach uses Haralick features extracted from gait energy images and applies the RELIEF feature selection algorithm to choose the most relevant features with minimal redundancy. The method is evaluated on a gait database and achieves a high identification rate of 80% at rank-1 using a k-NN classifier. This outperforms existing methods and demonstrates the effectiveness of the proposed approach for improving recognition performances.
411777	41177781	Problem-Based Learning Supported By Semantic Techniques	Problem-based learning has been utilized for the past thirty years in various learning settings. This approach involves presenting learners with different problems in a specific domain, allowing them to develop solutions while gaining knowledge about the subject. In conceptual modeling, specifically in Qualitative Reasoning, these solutions take the form of models that depict the behavior of a dynamic system. The learner's objective is to connect their initial model, which is their first attempt at representing the system, with the target models that provide solutions to the problem. To aid in this process, we suggest the use of semantic technologies and resources that can provide links to relevant terminology, formal definitions, and matching techniques, making use of existing models to enhance the learning experience.
411777	41177751	Models to represent linguistic linked data.	This article discusses the growing interest in linguistic linked data (LLD) among the Semantic Web and computational linguistics communities. With an increasing number of contributions focused on LLD, it can be difficult for linguists to determine which mechanisms are most suitable for their needs and which challenges have already been addressed. The article reviews the current state of models, ontologies, and their extensions for representing language resources as LLD, with a focus on the type of linguistic content they aim to encode. Four main groups of models are identified, and their reuse and remaining challenges are discussed. This review aims to provide a comprehensive overview for those interested in developing LLD resources.
411778	41177810	A scheduling policy with maximal stability region for ring networks with spatial reuse	A ring network with spatial reuse allows multiple users to transmit messages simultaneously, resulting in higher throughput compared to standard token rings. However, this also raises fairness concerns as some nodes may be blocked from accessing the ring for extended periods. To address this, policies have been proposed that guarantee a certain number of packets will be transmitted by each node in a cycle. This paper focuses on designing a stable policy for a ring with spatial reuse, even when the arrival rates are within the stability region. The proposed policy is adaptive and does not require knowledge of arrival rates, and can be implemented using a distributed mechanism. The main results are derived using the Lyapunov test function technique and methods from regenerative processes. 
411778	41177820	Throughput-Optimal Link-Layer Design in Power Constrained Hybrid OW/RF Systems	This paper focuses on developing efficient transmission schemes for hybrid optical wireless/radio frequency systems with power consumption constraints. The system is modeled using a timeslot structure and a queue for storing data packets. The hybrid channel is treated as an erasure channel with varying parameters according to a Markov chain. A stochastic optimization problem is formulated to maximize throughput while adhering to power constraints, and a solution is proposed using the Lyapunov optimization framework. This approach does not require knowledge of the channel or packet arrival statistics. The paper also introduces reduced feedback coding schemes to reduce the need for full feedback at the transmitter, while still achieving the throughput objective and managing power consumption.
411779	4117792	A Simple PromiseBQP-complete Matrix Problem	The article discusses the problem of estimating a diagonal entry of a real symmetric matrix using an efficiently computable function. The goal is to approximate the entry with an error of e bm, where b is an upper bound on the norm of the matrix and m and e are polylogarithmic and inverse polylogarithmic in N, respectively. It is shown that this problem is PromiseBQP-complete, meaning it can be efficiently solved on a quantum computer by repeatedly applying measurements to the jth basis vector and raising the outcome to the mth power. The article also discusses how every uniform quantum circuit can be encoded into a sparse matrix, and how estimating off-diagonal entries is also PromiseBQP-complete.
411779	41177944	Regression by dependence minimization and its application to causal inference in additive noise models	The authors propose a new regression method that minimizes the statistical dependence between regressors and residuals, making it suitable for causal inference problems. Unlike traditional methods, it does not assume a specific noise distribution, making it non-parametric. However, solving the resulting optimization problem can be challenging due to its non-convex nature. The method performs well on the NIPS 2008 Causality Challenge, where it accurately distinguishes causes from effects in pairs of dependent variables. The authors also present an algorithm for efficiently inferring causal models from observational data with multiple variables, which is a significant improvement over existing methods. 
411780	41178063	Reachability by paths of bounded curvature in a convex polygon	B is a point robot that can only move forward with a maximum curvature of 1 in a two-dimensional plane. P is a convex polygon with n vertices. The starting configuration of B is inside P, and we want to determine the region of P that B can reach. This region has a complexity of O(n), and we have an algorithm that can compute it in O(n^2) time. We also prove that a point in P is only reachable if it can be reached by a specific path pattern, which consists of unit circle arcs and line segments.
411780	41178024	Cutting hyperplane arrangements	The article discusses the concept of an &egr;-cutting for a set of n hyperplanes in a fixed dimension d. An &egr;-cutting is a collection of d-dimensional simplices that cover all of Ed and are intersected by at most &egr;n hyperplanes. The article presents a deterministic algorithm for finding a (1/r)-cutting with &Ogr;(rd(log r)C) simplices in time &Ogr;(n(log n)Ard-1 (log r)B). It also discusses a similar algorithm for finding a (1/r)-net for a range space (X, H(X)), where X is a set of n points in Ed. The algorithm has an additional &Ogr;(r&Ogr;(1)) overhead and the (1/r)-net has size &Ogr;(r log r). In the plane, the algorithm can find a (1/r)-cutting of size &Ogr;(r2) in time &Ogr;(nr), which is optimal. This method is an improvement over previous results and is conceptually simpler.
411781	41178132	The Limits of Composable Crypto with Transferable Setup Devices	The paper discusses the use of setup devices in realizing secure communication in the UC framework. It points out that in stronger versions of the UC framework, such as EUC or JUC, multiple instances of these setups are allowed. The concept of "transferable setups" is introduced, which are setup devices that do not publicly disclose if they have been maliciously passed on. The paper proves that using transferable setups, one cannot realize certain protocols, such as oblivious transfer, in the EUC model. It also shows that physically unclonable functions (PUFs) can be considered as transferable devices, making them unsuitable for use as global setups. The paper further shows that if setups are transferable, they cannot intrinsically leak in the case of a relay attack. The concept of authenticated channels and their importance in the UC model is also discussed, and the paper presents ways to strengthen existing protocols using PUFs to make them not only UC-secure but also JUC-secure. 
411781	41178195	An experiment on DES statistical cryptanalysis	Linear cryptanalysis and differential cryptanalysis are two commonly used methods of attacking block ciphers, particularly the Data Encryption Standard (DES). These methods can be combined and improved upon within a statistical framework, leading to similar results as those obtained through linear analysis. Another statistical attack, known as the x2cryptanalysis, has been found to be just as effective as differential and linear cryptanalysis. A new heuristic method has also been proposed for finding good characteristics in attacks, which has resulted in a successful attack against DES that is equivalent to Matsui's. Differential cryptanalysis involves analyzing the internal structure of a function and controlling correlated information on specific plaintexts, while linear cryptanalysis works by tracing a correlation between a plaintext bit and a ciphertext bit. Both methods have proven to be successful in attacking block ciphers, with differential cryptanalysis requiring 247 chosen plaintexts and linear cryptanalysis requiring 243 known plaintexts.
411782	41178239	Uncovering the Small Community Structure in Large Networks: A Local Spectral Approach	Large graphs are commonly encountered in various contexts, and studying their structure and extracting information from them is an important area of research. Previous algorithms for mining communities have focused on the global structure of the graph and have been time-consuming. However, with the increasing size of networks, it is crucial to shift the focus to the microscopic structure when dealing with large graphs. A new approach called LEMON (Local Expansion via Minimum One Norm) has been proposed, which identifies communities by finding a sparse vector that includes the seed members. LEMON has shown to be highly efficient and effective in finding communities, with a running time dependent on the community size rather than the entire graph. This approach has been evaluated on various datasets and has shown promising results. Further, the impact of different seed sets on the performance of the algorithm has also been analyzed. 
411782	41178216	Community structure in large complex networks	This paper presents a new definition of community that differs from previous studies. Instead of assuming communities are densely connected internally but sparsely connected to the rest of the network, the new definition considers a community as a densely connected subset that is also well connected to the rest of the network. The authors introduce the terms "whiskers" and "core" to identify and extract the desired community structure. They prove that detecting whiskers is a difficult problem and propose three heuristic algorithms for finding an approximate core. The results of their experiments show that this core structure exists in both random and real-world networks, supporting the idea of well-defined communities in complex networks. 
411783	41178369	A trusted subject architecture for multilevel secure object-oriented databases	This paper discusses security concerns in object-oriented database systems for multilevel secure environments, where users have varying levels of security clearance and access information with different classifications. The authors propose a solution that incorporates security into the object model and can be implemented using a cost-effective and popular security architecture. They also provide a formal proof to demonstrate the confidentiality of their approach and highlight the support for secure synchronous write-up operations, which can be achieved through write-up messages sent by low level users. The authors also address the issue of covert channels and present a concurrent computation model to prevent information leaks while maintaining confidentiality, integrity, and performance. A confidentiality proof for a trusted subject architecture and implementation is also provided. 
411783	41178363	A conceptual framework for Group-Centric secure information sharing	This paper proposes a conceptual framework for developing models for Group-Centric information sharing. The traditional approach, Dissemination-Centric, focuses on attaching attributes and policies to an object as it is disseminated. In contrast, Group-Centric sharing brings subjects and objects together in a secure meeting room or subscription model. The authors argue that authorizations in this context are influenced by the temporal ordering of group membership and the nature of membership operations. They develop a lattice of models based on these variations and discuss usage scenarios. The principles guiding Group-Centric models are "share but differentiate" and "groups within groups". The paper focuses on read accesses in a single group.
411784	411784103	Encryption policies for regulating access to outsourced data	Access control models currently assume that a trusted party manages access requests to ensure compliance with the access control policy. However, with the rise of Web technology and the need for third party services, this approach is no longer sufficient. To address this issue, data owners can encrypt their sensitive information before outsourcing it, allowing only authorized users with the key to decrypt it. This article presents a solution that combines cryptography with authorizations to enforce selective access on outsourced data without involving the owner. The approach includes a formal model for access control management and a two-layer encryption method for efficient policy updates. Experimental results show the effectiveness of this approach.
411784	411784263	A data outsourcing architecture combining cryptography and access control	Data outsourcing is a popular solution for utilizing external servers to distribute resources. However, challenges arise in enforcing authorization policies and updating them. Encryption is a common method for protecting outsourced data, but a promising solution is combining access control with cryptography. This approach presents challenges in an outsourced architecture. The paper discusses the principles of this architecture and proposes a solution for enforcing authorization policies and supporting dynamic authorizations, allowing for policy changes and data updates with minimal impact on bandwidth and computational power.
411785	4117850	Relational Dependency Networks	Graphical models for relational data have shown promise in improving classification and inference by representing dependencies among instances. However, the assumption of instance independence, common in conventional statistical models, does not hold in most relational data sets. For instance, citation data has dependencies among paper references and genomic data has dependencies among protein functions. To address this, relational dependency networks (RDNs) are proposed as graphical models capable of expressing and reasoning with such dependencies. RDNs have strengths in representing cyclic dependencies, parameter estimation, and structure learning using pseudolikelihood techniques. Real-world data sets and synthetic data demonstrate the effectiveness of RDNs in prediction and accuracy under various relational data characteristics.
411785	41178525	Dependency Networks for Relational Data	Traditional machine learning methods assume that each instance in a dataset is independent of the others. However, this assumption is often violated in relational datasets, such as in scientific literature where there are dependencies among the references of a paper. To address this issue, recent research has focused on graphical models for relational data, which take into account these dependencies and have shown improved performance. This paper introduces relational dependency networks (RDNs), a new type of graphical model designed to handle relational dependencies. The authors explain the details and strengths of RDN models, including their ability to handle cyclic dependencies. They demonstrate the effectiveness of RDNs on real-world datasets and synthetic data, showing significant improvements in classification performance.
411786	41178636	Improving the Discovery and Characterization of Hidden Variables by Regularizing the LO-net	The paper introduces a new neural network architecture, called LO-net, which is able to infer hidden variables from streaming multivariate time series data. It consists of two networks, O-net and L-net, where O-net makes initial predictions based on a time delay embedding and L-net learns to approximate the value of a single hidden variable. To prevent L-net from learning the target output of O-net, a penalty term is introduced. The paper proposes a new penalty, called the decor relation penalty, which has shown to improve prediction performance for data with periodic patterns. This extension of the regularized neural network architecture has potential applications in various domains.
411786	4117860	Discovering And Characterizing Hidden Variables Using A Novel Neural Network Architecture: Lo-Net	Theoretical entities, or hidden variables, are important for understanding the world even though they cannot be directly sensed. Scientific inquiry has uncovered many of these entities, such as black holes and dark matter. To better understand and utilize these entities, a new neural network architecture has been developed that can discover their existence, determine their number, and compute their values. Experiments have shown that this approach is useful for solving problems involving hidden variables in discrete time dynamical systems and in visual data from a mobile robot. Two different regularization methods have also been explored to improve the network's performance, and it has been compared to the capabilities of Hidden Markov Models. 
411787	41178715	Pride: A Public Repository Of Protein And Peptide Identifications For The Proteomics Community	PRIDE is a database that contains protein and peptide identifications from scientific literature, often related to specific species, tissues, and disease conditions. It also includes information about post-translational modifications and supporting mass spectra. Currently, it contains data from the HUPO Plasma Proteome Project and the human platelet proteome submitted by the University of Ghent. The HUPO Brain Proteome Project's data is expected to be added by late 2005. PRIDE encourages proteomics laboratories to submit their identifications and spectra to support their manuscript submissions. Data can be submitted in PRIDE XML format or mzData format. The database can be accessed through a web application and searched by experiment or protein accession number, literature reference, and sample parameters. Data can be retrieved in PRIDE or mzData XML format, or as HTML.
411787	41178732	Accurate estimation of isoelectric point of protein and peptide based on amino acid sequences.	The isoelectric point (pI) is an important characteristic of macromolecular polyprotic systems such as proteins, DNA, and RNA. It is defined as the point in a titration curve where the overall surface charge of the molecule is zero. Many modern analytical biochemistry and proteomics methods rely on pI for protein and peptide characterization. Accurate prediction of pI can aid in protein separation and identification by techniques such as 2-D gel electrophoresis and LC-MS/MS. However, pI prediction methods vary in accuracy and are sensitive to the choice of basis set. Machine-learning algorithms show superior performance but require a large training dataset. Benchmarking of pI prediction methods is needed to improve their accuracy.
411788	411788175	Task scheduling for heterogeneous reconfigurable computers	This article discusses the problem of executing a constantly changing set of tasks on a system consisting of a processor and a reconfigurable device. A scheduler is responsible for allocating tasks to either the processor or the device. Each task has a corresponding implementation that is stored as a rectangular block in a database and must be placed on the device at run-time. A placer module is responsible for this task placement, but if there is not enough space on the device, the scheduler may choose to preempt a running task or execute the task on the processor. The article presents an implementation of the placer module and investigates task preemption, both of which are part of an operating system for reconfigurable systems currently in development.
411788	411788209	Online Placement For Dynamically Reconfigurable Devices	The use of reconfigurable hardware components, such as FPGAs, is becoming increasingly popular in embedded systems due to their capacity to support complete systems on a chip or network on a chip. However, efficient utilization of their dynamic and partial reconfiguration capabilities requires support in the form of operating systems. One aspect of this management is online placement, which is addressed in this paper. A new approach for online placement of modules on reconfigurable devices is presented, along with an optimization of communication between running modules and outside of the chip. Experimental results demonstrate a significant decrease in communication and routing costs. The paper also explores task scheduling between hardware and software, proposing a communication infrastructure for online placement that has been implemented.
411789	41178915	An Appealing Computational Mechanism Drawn from Bacterial Quorum Sensing	Quorum Sensing is a major discovery in microbiology that has had a significant impact in the last decade. It involves bacterial colonies using synchronized gene expression and phenotype changes to protect their environment, coordinate host invasion, and form biofilms. This process has been modeled in various ways and has been shown to be computationally complete. This means that it has the potential to impact other areas of computer science. This article provides a brief overview of the microbiological background and discusses some of the modeling approaches used. It also suggests potential applications for this computational method.
411789	41178926	Automatic selection of verification tools for efficient analysis of biochemical models.	Formal verification is a computational method used to check the correctness of systems, commonly used in engineering applications. Model checking, an algorithmic approach, is often applied in systems and synthetic biology, but is limited by its computational cost and scalability. To address this, statistical model checking (SMC) was introduced, but choosing the most efficient tool for a specific model and requirement can be challenging for biologists without computational expertise. This article introduces a method and tool for automatically selecting the most appropriate model checker for a given biological system, with over 90% accuracy. The SMC Predictor tool is available online and can significantly reduce verification time, making this powerful technology more accessible for biologists.
411790	41179057	Generating Test Sets from Non-Deterministic Stream X-Machines	Holcombe proposed X-machines as a specification language, and subsequent investigations have shown that it is intuitive, easy to use, and versatile for various applications. Stream X-machines, in particular, have been found to be highly effective for specification and most of the theory developed has focused on this type. A testing method has also been developed for systems specified by stream X-machines, which can detect all implementation faults if certain initial requirements are met. However, this method only works for deterministic X-machines. This paper presents a theoretical basis for a method to generate test sets from non-deterministic generalised stream X-machines.
411790	41179030	Testing against a non-controllable stream X-machine using state counting	Stream X-machines are a type of extended finite state machines that have been extensively researched in recent years. They consist of a finite set of states, an internal store or memory, and transitions between states labelled with function names. One advantage of using stream X-machines is their testing method, which can determine the correctness of an implementation if the processing functions are accurately implemented. However, this method is often hindered by strict design for test conditions that require additional functionality to be disabled after testing. A new method is proposed in this paper that is more flexible and can be applied to a wider range of systems specified by stream X-machines without the need for extra functionality. This makes it a more practical and efficient testing method.
411791	41179166	The Complexity of the Hajos Calculus	The Hajos calculus is a method for creating non-3-colorable graphs. The question of whether there exist graphs that require exponential-sized constructions has been posed, but little progress has been made despite efforts. This paper proves that the Hajos calculus can generate polynomial-sized constructions for non-3-colorable graphs if extended Frege systems are polynomially bounded. These systems are used to prove tautologies and finding lower bounds for them is a difficult problem in logic and complexity theory. The paper also establishes a relationship between the Hajos calculus and bounded-depth Frege systems, allowing for exponential lower bounds to be proven for a subsystem of the Hajos calculus.
411791	41179135	Exponential Lower Bounds for Monotone Span Programs	Monotone span programs were introduced in 1993 and are equivalent to linear secret sharing schemes. They have various applications in complexity theory and cryptography but obtaining lower bounds for them has been challenging. The best known lower bounds are quasipolynomial for a function in P. The open problem of proving exponential lower bounds for any explicit function in monotone P has been resolved, along with the first exponential lower bounds for linear secret sharing schemes. This was achieved using Razborov's rank method, which is strong enough to prove lower bounds for many monotone models. As a result, new proofs of exponential lower bounds for monotone formula size, monotone switching network size, and the first lower bounds for monotone comparator circuit size have been obtained. Additionally, new polynomial degree lower bounds for Nullstellensatz refutations have been found using an interpolation theorem. Finally, quasipolynomial lower bounds for the st-connectivity function have been obtained, providing tight bounds for st-connectivity in all of the mentioned computational models.
411792	41179219	Shvil: collaborative augmented reality land navigation	Shvil is an Augmented Reality (AR) system designed for collaborative land navigation. It enables an overseer (indoor user) and an explorer (outdoor user) to work together by using AR and 3D printing techniques. The overseer is provided with a physical representation of the mission's topography, while the explorer's actions and physical presence are merged with dynamic AR visualization. The system allows for collaboration by overlaying relevant visual information for both users. The prototype and initial results of Shvil are discussed, along with the future vision for the system.
411792	41179226	Flying Frustum: A Spatial Interface for Enhancing Human-UAV Awareness	Flying Frustum is a 3D spatial interface that allows for control of semi-autonomous UAVs using pen interaction on a physical model of the terrain. The interface also displays the information from the UAVs onto the physical model, providing a better understanding of the UAV's position and orientation in relation to the terrain. This interface is based on a 3D printout of the terrain, allowing for the operator to draw goals and paths directly onto the physical model. The design aims to improve human-UAV awareness and situational awareness. The prototype currently uses handheld and headset augmented reality interfaces and future plans include evaluating and improving the prototype. Previous related work in CSCW and HRI is also discussed. 
411793	41179336	Evaluating system integrity	The traditional approach to ensuring system integrity focuses on implementing specific controls, such as separation of duties and well-formed transactions. However, this paper suggests a new definition of integrity based on dependability that is not tied to a specific implementation. The authors provide examples to demonstrate how techniques like separation of duties, assured pipelines, fault-tolerance, and cryptography can be seen as ways to achieve integrity, rather than being the definition of integrity itself. This new perspective allows for a more flexible and adaptable approach to maintaining system integrity.
411793	4117932	Believing the Integrity of a System: (Invited Talk)	An integrity policy outlines when information can be changed and is enforced by a system's protection mechanisms. Traditional protection models rely on experience and "best practices" rather than theoretical foundations to define integrity. However, in complex systems, errors in the integrity policy or insufficient protection mechanisms can lead to unexpected system breaches. This paper discusses the concept of integrity and proposes a belief logic approach for evaluating the integrity of a system's configuration. This approach aims to address potential vulnerabilities and improve the overall integrity of the system.
411794	41179445	Autonomic management of multimodal interaction: DynaMo in action	Multimodal interaction is essential in pervasive environments because it offers a natural and flexible way to interact with various digitally controlled equipment. The DynaMo framework is specifically designed to support the development and management of multimodal interaction in these environments. The autonomic approach of DynaMo uses partial interaction models to dynamically create and manage multimodal interaction according to the current conditions and predicted models. This allows for efficient and adaptive interaction with the equipment. Several examples and partial interaction models are used to demonstrate the effectiveness of this autonomic solution.
411794	41179457	AutoHome: An Autonomic Management Framework for Pervasive Home Applications	The article introduces the AutoHome service-oriented framework, which is designed to simplify the development and runtime support of autonomic pervasive applications. It combines the areas of autonomics and service orientation to create a component-based platform with features such as monitoring and touchpoints. This unique infrastructure allows for distributed autonomic control and global conflict management in a management hierarchy. The platform is specifically tailored for pervasive home systems and is demonstrated through the development of two automated home applications: intruder detection and medical support. These applications were evaluated and showed minimal overhead while providing numerous benefits. The article concludes by discussing the contributions of AutoHome and potential future research directions.
411795	41179573	Meta-Argumentation Modelling I: Methodology and Techniques.	This paper discusses the use of meta-argumentation in modeling argumentation. It combines Dung's abstract argumentation theory with an extended argumentation theory, using both instantiation and extension methods. The technique of meta-argumentation applies Dung's theory to itself through a process called flattening. The domain of instantiation is represented using a technique based on soundness and completeness. The use of specification languages helps to differentiate between different instantiations. Overall, this approach allows for a more comprehensive and flexible modeling of argumentation.
411795	411795128	A logic of abstract argumentation	This paper presents a logic of abstract argumentation that is inspired by Dung's theory. It utilizes connectives for attack and defend to capture the essence of Dung's theory. The logic is then expanded into a modal logic, allowing for the generalization of Dung's theory and the creation of different versions of it. The authors also demonstrate how this logic can be used to connect Dung's theory to other traditional formalisms, such as conditional and comparative logic. Additionally, the paper showcases the application of this logic in meta-argumentation, providing a framework for reasoning about arguments. 
411796	41179645	A construction of peer-to-peer streaming system based on flexible locality-aware overlay networks	The peer-to-peer multicast system uses participants as peers to create an overlay topology over physical infrastructures. This allows for easy data dissemination and gathering through a multicast application. However, the topology of the overlay can negatively impact the system's performance, such as by affecting delivery efficiency and quality due to factors like heterogeneous and dynamic peers. To address this, the authors propose a flexible locality-aware overlay system that allows peers to establish streaming sessions and act as sources without dedicated servers. This overlay has a 2-layered structure that matches the underlying topology and reduces delivery paths. Simulation results show that this system has better transmission efficiency, shorter delivery delays, and higher reliability compared to other systems.
411796	411796111	M-Ring: A Distributed, Self-Organized, Load-Balanced Communication Method on Super Peer Network	Peer-to-peer file sharing systems often use a two-layered architecture, where powerful peers act as super peers and are responsible for connecting with other super peers in neighboring or different groups. However, this can lead to a single-point-of-failure issue if the super peers are used as gateway-like peers. To address this problem, the M-Ring method is proposed, which is a distributed, self-organized, and load-balanced communication approach. In this method, each super peer maintains a link table to connect with other super peers in its neighboring group, and each super peer has a unique identity within the same identity space. By utilizing the "overlap handle scope" feature, our method can efficiently reduce storage space and bandwidth requirements, resulting in lower overhead for connecting with neighboring super peers. Simulation results demonstrate the effectiveness of this approach.
411797	41179710	Optimal location-multi-allocation-routing in capacitated transportation networks under population-dependent travel times	The article presents a model for optimizing transportation in a network with varying travel times between nodes. The model accounts for the possibility of allocating population in demand nodes to multiple server nodes. It also considers the impact of population flow on travel times and limitations on server node and arc capacities. The goal is to minimize total transportation time by determining optimal server node locations, allocation of population to servers, and allocation of population to routes. The problem is formulated as a mixed-integer non-linear programming model and then transformed into a mixed-integer linear programming problem. A genetic algorithm (GA) and a heuristic algorithm (GALS) are proposed to solve the problem. Numerical experiments show that GALS outperforms the standard GA and the CPLEX solver.
411797	41179711	Hybridizations of genetic algorithms and neighborhood search metaheuristics for fuzzy bus terminal location problems.	The authors present a graphical abstract of their research on improving the mathematical model for a real-world problem, specifically the fuzzy bus terminal location problem. They propose modified hybrid genetic algorithms with neighborhood search based metaheuristics and test them on randomly generated large scale fuzzy bus terminal location problems. The algorithms are also compared to other optimization methods, and their effectiveness is demonstrated through computational experiments. The authors also compare their algorithms on crisp facility location and quadratic assignment test problems. Their findings show the effectiveness of their proposed hybrid algorithms for solving large scale fuzzy bus terminal location problems.
411798	41179852	Cognitive Adaptive Computer Help (Coach): A Case Study	COACH is an intelligent agent that aims to improve the user experience with computer interfaces. It uses a cognitive interface to anticipate the user's needs and provide proactive help. By analyzing user actions, COACH adapts its responses and provides useful information before the user even requests it. The system uses dynamic models to teach and guide the user, and has been shown to improve productivity and comfort in interface usage. COACH has been successfully implemented in various problem domains, including writing Lisp programs, teaching UNIX commands, and formatting text in GML. It has also been adapted for use in modern graphical user interfaces, using techniques such as masking and animation to guide users through procedures. A tool is available to easily create adaptive help systems for different problem domains. Future development is focused on incorporating 3D animation and adaptive tutoring into the system. 
411798	41179831	Style and function of graphic tools	In the future, our computer interfaces should be designed to make using computers easier and more intuitive. This can be achieved by creating a library of graphical presentation and interface techniques that are tailored to their specific use. It is important for these interfaces to take into account the physical and psychological needs of users, and to accurately represent the function of the device. However, when these design goals become too restrictive, users may need assistance in understanding the designer's vision. It is also important to consider when the interface should be unobtrusive and when it can be more visually appealing. Just like the color of a room can affect our mood, interfaces can also have an impact on our experience. It is important for designers to have the freedom to create interfaces that reflect their brand and style, while also making informed decisions about the interface techniques they use.
411799	41179918	Synthesis of Mapping Logic for Generating Transformed Pseudo-Random Patterns for BIST	The paper discusses a new technique for improving the effectiveness of built-in self-test (BIST) by synthesizing combinational mapping logic. This logic transforms the set of patterns generated by a pseudo-random pattern generator in order to achieve better fault coverage while minimizing area overhead. By formulating the problem as finding a minimum rectangle cover in a binate matrix, the paper presents a procedure for selecting a mapping function and synthesizing the corresponding mapping logic. Experimental results show that this method requires less hardware overhead compared to other techniques. This is achieved through iterative global operations, resulting in improved fault coverage for a given test length. 
411799	41179924	MINVDD Testing for Weak CMOS ICs	In this paper, the concept of a weak chip is introduced as one that contains flaws that may not affect normal circuit operation but can lead to failures. It is explained that MINVDD testing, which measures the minimum supply voltage a chip can function correctly at, can be used to identify weak chips. The paper presents experimental results showing the effectiveness of MINVDD testing for screening out burn-in rejects. It also proposes test conditions for low voltage testing and provides experimental evidence to support this proposal.
411800	41180030	Socioscope: Human Relationship and Behavior Analysis in Social Networks	This paper proposes a new socioscope model that uses mobile-phone call-detail records for social-network and human-behavior analysis. The model utilizes various probability and statistical methods to quantify social groups, relationships, and communication patterns and detect changes in human behavior. It also introduces a reciprocity index that can be applied in areas such as homeland security, spam detection, and product marketing. The effectiveness of the model is validated using real-life call logs of 81 users from MIT and 20 users from UNT, containing approximately 5000 callers. The results show that the proposed model is effective in analyzing social networks and human behavior.
411800	41180050	Quantifying Reciprocity in Social Networks	This paper presents a new reciprocity index that can measure social relationships using mobile phone call detail records and Twitter blogs. The index is helpful in identifying spam calls and for product marketing purposes. The researchers tested the index using call logs of 100 users from MIT and Twitter blogs of 460 users from UNT, collected over 8 and 12 months respectively. The results showed high accuracy and demonstrate the effectiveness of the index. 
411801	41180125	Iteratively Extending Time Horizon Reinforcement Learning	Reinforcement learning is a method used to find the best control policy for a system by interacting with it over an infinite time horizon. This is achieved by approximating the Q-function, which represents the optimal control, using a sample of four-tuples (system state, control action, reward, successor state). Classical algorithms use a stochastic approximation to iterate over these four-tuples. In this paper, the problem is reformulated as a sequence of batch mode supervised learning problems, which converges to an approximation of the Q-function. This approach allows for the use of standard supervised learning algorithms and has been tested successfully on the "Car on the Hill" control problem using regression trees. This technique is also efficient for solving larger reinforcement learning problems.
411801	41180142	Statistical interpretation of machine learning-based feature importance scores for biomarker discovery	Univariate statistical tests are commonly used in bioinformatics for identifying biomarkers. However, these tests are limited in their ability to detect biomarkers that interact with other variables. Machine learning techniques can provide relevance scores that take into account these interactions, but their lack of statistical interpretability makes it difficult to determine a relevant threshold for feature selection. In this study, several methods were tested for extracting relevant features from machine learning rankings and were evaluated on artificial and real datasets. The results showed that using measures such as p-values or false discovery rates can greatly improve the extraction of relevant biomarkers. The source codes for these methods are available for use.
411802	4118028	Marketing of Vice Goods: A Strategic Analysis of the Package Size Decision	Consumers often struggle to resist the temptation of overconsuming products like cookies, crackers, and soft drinks. Some try to control their consumption by buying small packages or avoiding the product altogether, while others continue to buy large packages and overconsume. From a strategic perspective, companies can choose to offer small or large packages. Research on hyperbolic discounting shows that small packages can help consumers with self-control issues, but may also affect prices, profits, and welfare. Results suggest that small packages can increase profits if only a small portion of consumers have self-control issues or if they attract new customers. Competition can reduce incentives for small packages, especially when many consumers are attracted to them. Producing healthier alternatives may also decrease profits for companies. Despite this, small packages are found to enhance consumer and social welfare, even if they increase consumption of unhealthy goods.
411802	41180279	Self-Control and Optimal Goals: A Theoretical Analysis	Setting goals is a common practice among consumers to achieve various objectives, such as losing weight, saving for retirement, and improving health. Research in psychology and consumer behavior has shown that goals can be effective in helping consumers achieve these objectives. However, there is little research on how to set optimal goals. This paper aims to develop a simple framework for understanding how goals can impact performance and how to set the best goals. By using the concept of hyperbolic discounting, the study finds that while goals can increase performance, they can also lead to procrastination and myopic behavior. The results suggest that the most challenging goals should be given to consumers with moderate motivation and self-control issues, and sometimes, it may be beneficial to set goals that are never achieved.
411803	41180330	Unknown solution length problems with no asymptotically optimal run time.	This study focuses on optimizing a fitness function with unknown dimensions. The function is defined over bit-strings of length N, but only a small subset of n bits influences the fitness. The position and number of these relevant bits are also unknown. Previous research has developed variants of the (1+1) evolutionary algorithm (EA) that can solve instances of OneMax and LeadingOnes simultaneously for all n values in expected time. This study proves lower bounds for this problem, showing that the (1+1) EA with certain mutation operators has an expected run time of ω(n2 log(n) log log(n) ... log(s)(n)). Additionally, it is shown that there is no algorithm with the best asymptotic performance for this problem. Similar results are also shown for OneMax.
411803	4118036	Static and Self-Adjusting Mutation Strengths for Multi-valued Decision Variables.	Evolutionary computation often uses bit strings as a representation, but there is minimal research on using this approach for decision variables with more than two values. To address this, the study examines the performance of simple evolutionary algorithms on OneMax-like functions over multi-valued domains. The study finds that there are different ways to change selected positions in the solution vector, with some methods resulting in faster optimization times. For example, using a random mutation strength based on previous iterations can yield an expected optimization time of , which is the best possible among all dynamic mutation strengths. A new multiplicative drift theorem is also introduced for computing lower bounds in these processes.
411804	41180418	Asynchronous Veri.able Information Dispersal	Information dispersal is a method of storing files by distributing them among multiple servers in an efficient manner. The problem of verifiable information dispersal in an asynchronous network, where some servers and clients may have faults, is introduced. Verifiability ensures data consistency despite these faults. A scheme for efficient verifiable information dispersal in an asynchronous setting is presented, achieving optimal storage blow-up. The method also guarantees confidentiality against adaptive attacks. This approach also improves the communication complexity of asynchronous reliable broadcast for large inputs.
411804	41180446	Secure key-updating for lazy revocation	This article discusses the challenge of efficient key management and user revocation in cryptographic file systems that allow shared access to files. A method called lazy revocation, which delays the re-encryption of a file until the next write, is proposed as a performance-efficient solution. The concept of key-updating schemes for lazy revocation is introduced, along with a security definition for such schemes. Two methods for combining secure key-updating schemes are presented, and the security of existing constructions is proven. A new binary tree construction is also proposed and proven secure. The article concludes with a comparison of the computational and communication complexities of the three constructions, showing that the novel construction is an improvement.
411805	41180532	Parameter tuning of evolutionary reactions systems	Reaction systems are a formalism based on chemical reactions, introduced by Rozenberg and Ehrenfeucht. A new evolutionary algorithm, called Evolutionary Reaction Systems, has been developed and shown to have comparable performance to other machine learning methods. However, further analysis is needed to fully establish this algorithm, specifically regarding its unique parameters. This study focuses on the effect of one specific parameter, the number of symbols used to represent reactions, on the algorithm's performance. Results show that this parameter greatly impacts the algorithm's success and suggests the need for a set of default parameter values to aid beginners or inexperienced practitioners.
411805	4118054	A quantitative study of learning and generalization in genetic programming	This paper discusses the relationship between generalization and functional complexity in genetic programming (GP). It introduces a new measure of functional complexity, called Graph Based Complexity (GBC), which has a higher correlation with GP performance on out-of-sample data compared to a previous measure. The paper also presents a new measure, called Graph Based Learning Ability (GBLA), which quantifies GP's ability to learn difficult training points and is negatively correlated with out-of-sample performance. Additionally, a new fitness function is proposed based on the ideas behind GBC and GBLA, and its effectiveness is demonstrated through experiments on three real-life multidimensional regression problems.
411806	4118062	A quantitative study of learning and generalization in genetic programming	This paper explores the relationship between generalization and functional complexity in genetic programming (GP). It introduces a new measure of functional complexity, called Graph Based Complexity (GBC), which has a strong correlation with GP performance on out-of-sample data. Another measure, called Graph Based Learning Ability (GBLA), is also introduced to quantify the ability of GP to learn difficult training points. The results show that GBLA is negatively correlated with out-of-sample performance. The paper also proposes a new fitness function based on GBC and GBLA, which is empirically shown to be effective. The experiments were conducted on three real-life multidimensional regression problems.
411806	41180657	Unsure When to Stop? Ask Your Semantic Neighbors.	In supervised learning algorithms, there is a risk of overfitting when the search for optimal solutions continues beyond a certain point. To address this issue, this paper explores using information from the semantic neighborhood to determine when to stop the search. Two semantic stopping criteria are proposed and tested on real-world regression datasets using Geometric Semantic Genetic Programming and the Semantic Learning Machine. The results show that these criteria can accurately detect stopping points that lead to competitive generalization, and also result in computationally efficient algorithms. Additionally, using these criteria with optimal mutation/learning steps leads to smaller trees and neural networks. 
411807	411807114	A Purely Evolutionary Memetic Algorithm As A First Step Towards Symbiotic Coevolution	The paper introduces a new memetic algorithm that combines a local search step with a standard evolutionary algorithm. This approach was tested on symbolic regression problems and compared to other methods that incorporated non-evolutionary local search optimization. The results show that the memetic algorithm outperforms these other methods. The paper also presents preliminary results from a more complex coevolutionary scheme, where the fitness of individuals in the evolutionary algorithm is based on their contribution to the performance of individuals in the genetic programming component. This coevolutionary approach allows for the evolution of heterogeneous populations with a fitness function that reflects their contribution to solving the main problem. 
411807	41180772	Self-tuning geometric semantic Genetic Programming	This paper introduces a self-tuning algorithm for genetic programming that dynamically updates crossover and mutation probabilities during the algorithm's execution. Unlike other self-tuning algorithms, this one assigns different probabilities to each individual in the population. The results of experiments on seven test problems show that this algorithm produces solutions of comparable or better quality than those achieved using known values for crossover and mutation rates. The algorithm also reveals that mutation is primarily used for exploration, while crossover is used for exploitation, improving already good solutions. This approach has potential for improving the efficiency and effectiveness of evolutionary algorithms.
411808	41180844	Higher-Dimensional Box Integrals	This article discusses the use of symbolic computation to solve previously unsolved problems in the theory of n-dimensional box integrals. The authors were able to find solutions for n = 1, 2, 3, 4, and 5, including an elusive integral called kappa(5). They also discovered a general residue at the pole at s = n, leading to new relations and definite integrals. Additionally, the role of the Clausen function and its generalizations in these higher-dimensional evaluations is highlighted. These results serve as a benchmark for testing symbolic-algebra simplification methods.
411808	41180848	Advances in the theory of box integrals	Box integrals over the unit n-dimensional cube have been studied for over three decades, with some closed forms being discovered for isolated n and s values. However, using experimental mathematics and a new analytic approach, it has now been proven that for each of the first four dimensions, the box integrals are "hyperclosed" in a specific manner. Only for n = 5 is there a single integral, called kappa(5), that remains unresolved, but it has been shown that the majority of n = 5 cases also exhibit hyperclosure. The theory has also led to a collection of closed forms that demonstrate the concept in practice.
411809	4118097	Evolutionary Support Vector Regression Machines	Evolutionary support vector machines (ESVMs) are a new approach that combines the learning engine of support vector machines (SVMs) with evolutionary algorithms (EAs) to evolve the coefficients of the decision function. This simplifies the optimization process in training and makes ESVMs a viable alternative to traditional SVMs. While SVMs are primarily used for classification, this paper introduces \in-evolutionary support regression (\in-ESVR) which applies the same approach to regression problems. The hybridization with the classical \in-support vector regression (\in-SVR) and subsequent evolution of the regression hyperplane coefficients is demonstrated on the Boston housing benchmark problem, showing the potential of ESVMs for regression tasks.
411809	41180940	Support vector machine learning with an evolutionary engine	The paper introduces an evolutionary technique that can be used as an alternative to the traditional support vector machines architecture. This approach follows the same learning strategy as the standard method but aims to simplify and generalize its training process. Unlike the traditional technique, the evolutionary method explicitly obtains the coefficients of the decision function without any additional constraints and does not require positive (semi-)definition properties for kernels in nonlinear learning. The method has been tested and confirmed on various benchmarking test problems, showing promising results in terms of runtime, prediction accuracy, and flexibility. The full study was published in the Journal of the Operational Research Society in 2009. 
411810	41181076	Shape from Texture: Homogeneity Revisited	This paper aims to estimate the orientation of a scene plane using an uncalibrated perspective image with an unknown texture. The main contributions of the paper are: 1) showing that the problem is equivalent to estimating the vanishing line of the plane, 2) decomposing the estimation into two searches for each parameter of the line, and 3) providing an algorithm that works for regular and irregular textures without explicitly identifying texels. Once the vanishing line is determined, the locations of texels can be found and the geometry of the scene plane can be computed up to an affine transformation. Examples of this approach on real images are also demonstrated.
411810	41181060	Refining Architectures Of Deep Convolutional Neural Networks	Deep Convolutional Neural Networks (CNNs) have shown great success in image recognition tasks. However, it is still unclear whether the chosen CNN is optimal for a specific dataset in terms of accuracy and model size. In this paper, a novel strategy is proposed to adjust the architecture of a given CNN for a particular dataset, potentially improving accuracy while reducing model size. This is achieved through two operations, stretching and symmetrical splitting. The effectiveness of this approach is demonstrated through experiments on two natural scenes attributes datasets, showing the potential of the proposed method for improving CNN architectures.
411811	41181135	An information-theoretic framework to aggregate a Markov chain	This paper proposes a framework for aggregating a large-scale Markov chain to obtain a reduced order Markov model using the Kullback-Leibler divergence rate as a metric for measuring the distance between two stationary Markov chains. The process involves solving an optimization problem to find the optimal aggregated model, which is shown to be the solution to an eigenvalue problem. A recursive algorithm is also presented for constructing a reduced order model with a specified number of super-states. This approach is demonstrated with examples, showing its effectiveness in reducing the complexity of large Markov chains.
411811	41181129	A simulation-based method for aggregating Markov chains	This paper discusses a method for reducing the complexity of a Markov chain with a large number of states. The method involves using a simulation-based approach to group similar states together based on observations of a single sample path. The Kullback-Leibler divergence rate is used as a measure of distance between two Markov chains and is used to determine the optimal partition of the state space. The process is simplified using an approximate dynamic programming framework, which involves relaxing the policy space and using a parameterized set of optimal policies. The algorithm can be implemented using a single sample path and has been shown to have fast convergence and low variance in theoretical examples. 
411812	41181251	Domain-specific sentiment analysis using contextual feature generation	This paper introduces a new framework for sentiment analysis that utilizes sentiment topic information to generate context-driven features. The authors observed the difficulty of sentiment classification in specific domains and propose considering more contextual information, such as topic or domain. Their system automatically extracts sentiment clues in different domains and identifies their relationship with sentiment topics. Through a bootstrapping process, they generate new clues and train domain-specific sentiment classifiers. Experiments showed that these newly aggregated clues were more effective than commonly used features in sentiment analysis. 
411812	41181248	Terminological paraphrase extraction from scientific literature based on predicate argument tuples	Terminological paraphrases (TPs) are sentences or phrases that express the same concept as a terminology but in a different form. A new method has been proposed to effectively identify and extract TPs from large scientific literature databases. This method uses semantic units called predicate-argument tuples to retrieve sentences containing a specific terminological concept. It also includes six TP ranking models to minimize errors in similarity computations. An evaluation collection was created to assess the effectiveness of this method, and the results showed that there are many TPs in scientific literature that have not been previously identified. This method has the potential to accurately extract TPs and can be expanded to other databases.
411813	41181338	Dynamic Partitioned Sampling For Tracking With Discriminative Features	The article presents a new method for tracking objects using particle filters. This method, called Dynamic Partitioned Sampling (DPS), divides the state space into partitions based on different cues and samples from them in a hierarchical manner. Unlike previous approaches, the order of partitions changes dynamically based on the reliability of each cue. The reliability is measured by the cue's ability to distinguish the object from the background, which is represented by a set of informative "background particles". The effectiveness of this method is demonstrated in head tracking experiments using three different cues. Results show the robustness of the algorithm in various challenging video sequences. 
411813	41181328	Tracking the multi person wandering visual focus of attention	This paper discusses the problem of estimating the wandering visual focus of attention (WVFOA) for multiple people, which has many applications in understanding human behavior. The specific application addressed is monitoring the attention of passers-by to outdoor advertisements. To solve this problem, the paper proposes a multi-person tracking approach using a hybrid Dynamic Bayesian Network that infers the number of people, their body and head locations, and their head pose simultaneously. A trans-dimensional Markov Chain Monte Carlo sampling scheme is used for efficient inference in the high-dimensional state-space. The model was evaluated for its ability to track and recognize when people look at outdoor advertisements using a realistic dataset.
411814	411814142	ESearch: Incorporating Text Corpus and Structured Knowledge for Open Domain Entity Search.	The paper presents ESearch, an open domain entity search system that uses a combination of a Wikipedia text corpus and DBPedia knowledge base to find relevant entities to a natural language query. The system ranks entities based on context matching and category matching, with a re-ranking component that takes into account user or blind feedback. The paper highlights the importance of category matching for search performance and addresses the challenge of users not being familiar with entity types in the knowledge base. ESearch has an effective ranking model for entity types, allowing users to perform entity search without explicitly specifying query entity types.
411814	411814125	A game-based framework for crowdsourced data labeling	Data labeling is crucial for many applications, but current solutions are either expensive or produce inaccurate results. This paper presents a new approach that aims to generate high-quality labeling rules while minimizing cost. The proposed method, called CrowdGame, utilizes a game-based crowdsourcing approach that involves two groups of workers: rule generators and rule refuters. The two groups play a game where the rule generator tries to find high-quality rules with a large coverage, while the rule refuter tries to refute these rules with the help of tuple checking tasks. The paper addresses challenges such as balancing coverage and accuracy, estimating rule accuracy, and selecting tasks efficiently. Experiments on entity matching and relation extraction show that the proposed method outperforms existing solutions.
411815	41181567	Evaluating the Influence of User Searches on Neighbors	The rise of Big Data has rendered traditional data management techniques insufficient in many real-life situations. The abundance of user data, particularly in the form of suggestions and searches, has created the need for advanced analysis methods to effectively utilize this information. Additionally, the heterogeneity and fast pace of this data require new data storage and management tools to be developed. This paper presents a solution for analyzing the impact of user searches and suggestions on their social network, with the goal of identifying influential users. This information is valuable in various scenarios such as viral marketing, tourism promotion, and food education. Understanding user preferences is crucial in these contexts.
411815	41181579	Discovering User Behavioral Features to Enhance Information Search on Big Data	The rise of Big Data and the availability of intelligent services have made traditional data management techniques inadequate. This has led to the development of a framework specifically designed for analyzing user interactions with intelligent systems in order to gather domain-specific information, such as finding a good restaurant in a visited area. The framework takes into consideration a user's social environment, the extent of their influence, and the influence they have on others. It also dynamically calculates user influence across the network and provides tailored suggestions for the user's search strategy. This approach has been successfully tested in tourist recommendation scenarios and has potential for use in other contexts such as viral marketing and food education.
411816	41181673	A general language for evolution and reactivity in the semantic web	This paper presents a UML model for a language that can handle evolution and reactivity in the Semantic Web. The language is based on Event-Condition-Action rules and allows for the combination of different languages for events, conditions, and actions. This accommodates language heterogeneity, which is important for dealing with the complexity of evolution and reactivity in the Semantic Web. The model specifies an ontology for the language, providing a foundation for its use and development. 
411816	41181619	An ontology- and resources-based approach to evolution and reactivity in the semantic web	The Web today is made up of various independent systems that constantly exchange information and evolve. Just as the diversity of data drove the development of XML and the Semantic Web, the diverse ways of expressing behavior also require a semantic approach. This is achieved through an ontology-based approach using Event-Condition-Action (ECA) rules, which model the components of rules as well as their events, conditions, and actions. These rules are associated with language resources, utilizing URI data integration to seamlessly integrate information and services from different sources. This approach also promotes sharing and reuse of resources throughout the Semantic Web.
411817	411817112	Behaviour-Aware Discovery Of Web Service Compositions	Service-oriented computing faces the challenge of efficiently discovering and combining (Web) services to create complex applications. To address this issue, a matchmaking system has been developed that utilizes both semantics and behavioral data to identify suitable service compositions that can fulfill a client's request. This system aims to improve the discovery process and make it more efficient by taking into account the specific requirements and capabilities of individual services. By leveraging both semantic and behavioral information, this system enables the creation of more effective and tailored service compositions to meet the needs of clients.
411817	411817114	Ontology- And Behavior-Aware Discovery Of Web Service Compositions	This paper proposes a methodology for discovering semantic Web services, which addresses the issue of web service discovery in Service-oriented Computing. The approach is based on using functional and behavioral properties in OWL-S service advertisements to match functional and behavioral client queries. The authors build upon previous research and introduce a data structure called dependency hypergraph to collect functional information and a notion of behavioral equivalence for Web services. They also discuss the architecture and implementation of their matchmaking system.
411818	4118188	On Semantic Update Operators for Answer-Set Programs	Logic programs under the stable models semantics, also known as answer-set programs, are a popular and well-understood knowledge representation framework. However, managing changes to these rule bases is still a challenge. The traditional framework for belief change, AGM, does not work well with nonmonotonic semantics like stable models. Various approaches have been proposed to address this issue, but they mostly focus on syntactic rule rejection. Recently, AGM revision has been applied to a more expressive semantic characterization of logic programs based on SE models. In this paper, the authors use results from belief update to tackle the problem of updating logic programs. They prove a representation theorem and define a specific update operator, which they compare to existing syntactic operators. Surprisingly, they uncover a major drawback in many semantic operators.
411818	41181855	A unifying perspective on knowledge updates	The paper introduces an abstract update framework that uses additional interpretations, called exceptions, to update a knowledge base. This framework has been shown to accurately represent syntax-based rule updates in a previous study. The authors demonstrate that the framework can also encompass a variety of belief update operators, both model- and formula-based, which are used in existing approaches to ontology updates. This unifies the perspective on both ontology and rule updates, providing a new way to address updates for knowledge bases containing both an ontology and a rule component.
411819	411819162	Exact formulations and algorithm for the train timetabling problem with dynamic demand	This paper examines the optimization of train timetabling in a dynamic demand environment, specifically for rapid train services in major cities. Three formulations are presented, with the goal of minimizing passenger average waiting time. While a binary variable model is most intuitive, it results in a non-linear objective function. To address this, flow variables are introduced to allow for a linear representation. Incremental improvements are made to these formulations and a branch-and-cut algorithm is developed. Computational experiments using real data from the Madrid Metropolitan Railway demonstrate the benefits of adapting timetables to demand patterns. Comparison of the linear formulations also reveals their differences in size, quality of solution, and running time.
411819	411819117	A general rapid network design, line planning and fleet investment integrated model	Traditionally, public transportation planning has been divided into two separate phases: network design and line planning. The network design phase focuses on minimizing travel time or maximizing trip coverage, while the line planning phase aims to minimize cost or the number of transfers. This paper introduces a new approach by integrating these two phases into one strategic and tactical planning process for rapid transit systems. A mathematical model is proposed that considers factors such as infrastructure network, line planning, train capacity, fleet investment, and personnel planning. The demand for transportation is also taken into account and split between the rapid transit network and other modes of transportation. The goal of this integrated approach is to maximize the total profit of the network by balancing trip coverage and cost. The paper includes a detailed analysis and presents numerical results based on real-world data.
411820	41182043	Verifying Programs with Unreliable Channels	This article discusses the verification of a specific type of infinite-state systems, composed of finite-state processes communicating through unlimited lossy FIFO channels. This class can model link protocols such as Alternating Bit Protocol and HDLC. The authors present algorithms for solving various verification problems for this class, including reachability, safety properties, and eventuality properties. These algorithms have been successfully applied to verify idealized sliding-window protocols. This is significant because these problems are typically undecidable for systems with perfect FIFO channels. 
411820	411820111	Undecidable verification problems for programs with unreliable channels	This paper discusses the class of finite-state systems that communicate through lossy FIFO channels, known as lossy channel systems. These systems have infinite state spaces due to the unboundedness of the channels. In a previous paper, the authors proved that certain problems, such as reachability and safety properties, are decidable for these systems. However, the paper also shows that other problems, like model checking in propositional temporal logics and deciding eventuality properties with fair channels, are undecidable. These results are obtained through a reduction from a variant of the Post correspondence problem.
411821	41182134	Dual Laplacian Editing for Meshes	Recent developments in mesh editing have utilized differential information as a local intrinsic feature descriptor. By minimizing changes in this information, a deformed mesh can be reconstructed based on user input constraints. However, since this information is in a global coordinate system, it must be transformed to fit the orientations of details in the deformed surface, or distortion will occur. To address this, a new method has been proposed that encodes both local parameterization and geometry details in the dual mesh domain. This approach allows for simpler neighborhood structures and better preservation of information. With this framework, both primal vertex positions and dual Laplacian coordinates are iteratively updated, resulting in visually pleasing deformations with minimal user input.
411821	41182177	Mesh saliency via ranking unsalient patches in a descriptor space.	This paper introduces a new method for detecting saliency in mesh models. The approach involves computing a descriptor vector for each patch of the model, based on Zernike coefficients and a center-surround operator. Patches with low or high local distinctness are identified as background or foreground patches, respectively. The saliency of each patch is then determined by its relevance to the least salient background patches, using manifold ranking. This approach is more robust and less sensitive to the initial queries compared to existing methods. The ranking is performed in the descriptor space, taking into account the manifold structure of the shape descriptors. A Laplacian smoothing procedure is used to spread the saliency values to each vertex. Experiments on various models demonstrate the effectiveness and robustness of this approach. 
411822	41182219	Generating, Maintaining, and Exploiting Diversity in a Memetic Algorithm for Protein Structure Prediction.	Computational methods for predicting the three-dimensional structure of proteins have been limited in their ability to accurately predict the structures of larger proteins. This is due to several factors, such as a lack of exploration of diverse protein folds and an energy function that can be deceiving. To address these issues, a multistage memetic algorithm was developed, incorporating a successful local search method called Rosetta. By incorporating specialized genetic operators and a stochastic ranking procedure for selection, this algorithm was able to increase the diversity of protein folds and traverse deep energy wells that were previously considered deceptive. These improvements have the potential to significantly enhance the performance of protein structure prediction algorithms in blind experiments and advance the field in predicting the structures of larger proteins.
411822	41182231	Multiobjectivization by Decomposition of Scalar Cost Functions	Multiobjectivization is the process of transforming a single-objective optimization problem into a multiobjective one by adding supplementary objectives or decomposing the original objective function. This paper examines how decomposition affects the fitness landscape and search in multiobjective optimization. The authors find that decomposition introduces plateaus of incomparable solutions, reducing the number of local optima that can be found by hillclimbing algorithms. However, this effect can be partly reversed with the use of an archive. The paper also analyzes the impact of multiobjectivization and the use of an archive on the performance of hillclimbing algorithms, revealing an exponential/polynomial divide in running time. 
411823	4118235	Manifold Regularized Multitask Learning for Semi-Supervised Multilabel Image Classification	Classifying images with multiple labels using a small number of labeled samples is a significant challenge. One approach is to use manifold regularization, which explores the geometric structure of the data distribution, to improve classification performance. However, this method is not effective when dealing with high-dimensional visual features. To address this issue, a new algorithm called manifold regularized multitask learning (MRMTL) is proposed. MRMTL learns a shared discriminative subspace for multiple classification tasks, effectively controlling model complexity. The algorithm also ensures smoothness along the data manifold through manifold regularization. Experiments on two datasets show that MRMTL outperforms other image classification algorithms.
411823	41182322	Large-margin multi-view Gaussian process	The goal of image classification is to determine if an image belongs to a certain category. Multiple features are used to understand the contents of images, but this can lead to challenges in effectively combining them and handling high-dimensional features with a small training set. This paper proposes a method that integrates the large-margin principle with the Gaussian process to find a shared subspace among multiple features. This approach combines the advantages of both methods and provides a probabilistic explanation for embedding features into a low-dimensional subspace, resulting in improved classification accuracy. The proposed algorithm is demonstrated to be effective in discovering discriminative latent subspace and improving performance on real-world image datasets.
411824	41182444	Ranking Model Adaptation for Domain-Specific Search	The emergence of vertical search domains has made it difficult to apply a broad-based ranking model to different domains. Building a unique ranking model for each domain is time-consuming and labor-intensive. To address this issue, a new algorithm called ranking adaptation SVM (RA-SVM) is proposed. This algorithm adapts an existing ranking model to a new domain, reducing the need for labeled data and training time while maintaining performance. The algorithm only requires predictions from existing models, and assumes that documents with similar features should have consistent rankings. Constraints are added to control the margin and slack variables of RA-SVM, and a ranking adaptability measurement is introduced to quantify the adaptability of an existing ranking model. Experiments show the effectiveness of this approach on two large datasets. 
411824	41182429	Learning from search engine and human supervision for web image search	Visual reranking is a technique used to improve the accuracy of text-based Web image searches. This paper suggests combining two learning strategies to create a reranking model: learning from search engine results and learning from human supervision. The first strategy involves using search engine results as a form of pseudo-relevance, while the second strategy manually labels the relevance of search results for a few representative queries. This combination is expected to improve the reranking process by utilizing the strengths of both strategies and mitigating their weaknesses. The proposed method involves a two-stage learning approach, where a pseudo-supervised approach is used to learn from search engine results and human supervision is used in the offline stage to create a final reranking function. Experimental results show that this method outperforms existing reranking approaches.
411825	41182514	Principles of Constructing a Performance Evaluation Protocol for Graphics Recognition Algorithms	Graphics recognition is a process that identifies and groups graphic objects, such as text and lines, in digital images. However, there is a lack of objective evaluation protocols and methods for comparing the performance of these recognition algorithms. To address this, a methodology for performance evaluation has been proposed, which includes a matching degree and comprehensive performance metrics. While some research has been done on evaluating specific types of graphic objects, there is a need for a more general methodology that can be applied to different graphics recognition algorithms. This methodology aims to improve the understanding and comparison of graphics recognition algorithms and inform the design of new systems. 
411825	41182582	Object-process based graphics recognition class library: principles and applications	The Graphics Class Library (GCL) is a library developed using the object-process methodology and object oriented implementation for graphics recognition. Its purpose is to provide generic code for graphics recognition algorithms that can be easily used and extended in future systems. The library contains reusable classes for graphic objects commonly found in engineering drawings and other line drawings. Its foundation is a generic integrated graphics recognition algorithm that offers standard operations necessary for graphics recognition applications. This library was copyrighted in 1999 by John Wiley & Sons, Ltd.
411826	4118262	Goal Satisfaction in Large Scale Agent-Systems: A Transportation Example	This study showcases the effectiveness of using a simple physics-based approach to solve a complex transportation problem on a large scale. The approach involves modeling a cooperative Multi-Agent System (MAS) using physical principles, where the agents inherit physical properties and behave similarly to physical systems. The researchers provide a detailed algorithm for a single agent and use simulations to demonstrate its successful use in allocating and executing tasks in a dynamic MAS with thousands of agents and tasks. This approach proves to be efficient and effective in managing such a complex system.
411826	4118267	Methods for task allocation via agent coalition formation	In multi-agent environments, agents may need to cooperate to complete tasks. This involves assigning tasks to groups of agents, which can improve efficiency. This paper presents solutions for task allocation among autonomous agents, proposing that agents form coalitions to complete tasks or improve performance. Efficient distributed algorithms with low computational complexities are presented and supported by simulations and implementation in an agent system. The paper discusses different scenarios, including agents being part of only one coalition, overlapping coalitions, and tasks with a precedence order. The algorithms are any-time, simple, efficient, and easy to implement.
411827	41182740	Topographical maps of orientation specificity	The article presents a new framework for understanding how orientation is encoded in the primate striate visual cortex. This framework is based on the representation of straight lines and can explain the organization of orientation specificity in the cortex, as well as other observed phenomena such as the lack of orientation specificity in singularities, increased neural activity in these areas, and the distribution of singularities along ocular dominance columns. The framework can also be applied to other cortical regions with topographical maps of sensory spaces. A neural model of the proposed framework is presented, along with simulation results.
411827	41182744	Realistic neuromorphic models and their application to neural reorganization simulations	This article describes a method for creating realistic 2D and 3D neural cells by using statistical data from biological cells. Image analysis techniques are used to extract key morphological parameters, which are then used to define probability functions. These functions are then used in Monte Carlo simulations and a graphic language to produce neural structures. The approach is also applied to simulating neurogenesis and neural reorganization processes with attractive and repulsive interactions. This method allows for the generation of accurate and realistic neural cells, which could have applications in various fields of neuroscience. 
411828	41182818	Exact Max-SAT solvers for over-constrained problems	This article introduces a new method for solving over-constrained problems using Max-SAT. A formalism called soft CNF formulas is defined, allowing for the declaration of blocks of clauses as either hard or soft. Two Max-SAT solvers are presented, utilizing branch and bound algorithms, lazy data structures, and variable selection heuristics. An experimental study on various problem instances shows that this approach is competitive with existing methods from the CSP and SAT communities.
411828	41182833	Exploiting Unit Propagation to Compute Lower Bounds in Branch and Bound Max-SAT Solvers	This paper discusses the differences between complete SAT solvers and exact Max-SAT solvers, with a focus on their use of unit propagation. A new branch and bound MaxSAT solver is described, which applies unit propagation at each node of the proof tree to compute a lower bound. This lower bound is based on inconsistency counts and is of higher quality than those used by other state-of-the-art solvers. The solver also incorporates various heuristic and preprocessing techniques. Experimental results show that this solver is highly competitive when compared to other modern Max-SAT solvers.
411829	41182912	Auction Equilibrium Strategies for Task Allocation in Uncertain Environments	This paper presents a model of self-interested information agents competing for tasks in an uncertain environment. These agents have different capabilities and are motivated to deviate from cooperative strategies in order to increase their own benefits. The paper focuses on a protocol where a central manager conducts a reverse auction among the agents to allocate tasks. The concept of a stable solution, where no agent can benefit from changing its strategy, is introduced. The paper suggests an efficient algorithm for calculating equilibrium strategies and compares this protocol to a central allocation mechanism. The effect of environmental settings on the equilibrium is also explored through sample environments.
411829	41182924	Solving the auction-based task allocation problem in an open environment	This paper examines the task allocation process in uncertain and changing open environments, where agents are self-interested and aim to maximize their own revenue. The allocator uses a second price reverse auction to allocate tasks in a dynamically arriving scenario. Unlike previous models for cooperative agents, this model poses a challenge in identifying equilibrium strategies for the agents. The paper proposes an algorithm to accurately approximate these strategies and compares the system performance in closed and open environments, showing the advantage of the allocator operating in the latter. The results approach those achieved by a central enforceable allocation. 
411830	41183054	The metabolic algorithm for P systems: Principles and applications	Metabolic P systems, or MP systems, are a type of P systems used to model biological metabolism. Their evolution is governed by a metabolic algorithm, which replaces the traditional approach of using ordinary differential equations with chemical principles. The basic principles of MP systems are outlined and demonstrated through examples of biological modeling. A novel regulation mechanism is also discussed, which could potentially improve the construction of computational models from experimental data of specific metabolic processes. Overall, MP systems offer a unique and potentially more efficient way to express and understand biological metabolism. 
411830	41183058	Psim: A Simulator For Biomolecular Dynamics Based On P Systems	Metabolic P systems, or MP systems, are a type of P systems designed to represent biological metabolism. These systems use metabolic algorithms to transform populations of objects based on a mass partition principle, which is inspired by chemical laws. The main principles of MP systems are explained and Psim, a simulation tool developed for calculating their dynamics, is introduced. An example is provided to illustrate how Psim works, including a simulation experiment that was conducted using real data. Overall, MP systems and Psim offer a powerful way to model and simulate biological processes, particularly in the context of metabolism. 
411831	41183114	Enterprise Modelling for Value Based Service Analysis	Service oriented architectures (SOAs) are essential for facilitating communication and collaboration between organizations and individuals. However, managing and designing services poses challenges due to their abstract nature. In this paper, the authors analyze the concept of services and propose a conceptual model based on the REA ontology. This model connects services to resources and demonstrates how services can be represented using encapsulation. An example of using this model is provided in an application for oil marketing, showcasing how it can aid in the representation and design of services. The proposed model offers a framework for understanding and managing services in a more structured and effective manner.
411831	41183150	Accounting for service value - An ontological approach	The growing importance of services in modern enterprises has led to a need for innovation in traditional management accounting practices. While there has been some conceptual work on service accounting in the Service Science literature, there is a lack of practical frameworks. To address this, the authors have developed an integrated service accounting framework using the REA business ontology. This framework can be combined with the e3value model to provide a comprehensive view of service networks. The framework has been evaluated and applied to an online gaming example. This is the first service accounting framework in Service Science that provides a precise definition for commonly used concepts such as value-in-use.
411832	411832101	Economic Viability of Paris Metro Pricing for Digital Services	Digital services, such as cloud computing and network access, are becoming increasingly popular, allowing for dynamic resource allocation and virtual resource isolation. This has led to the development of flexible pricing schemes, such as Paris Metro Pricing (PMP), which involves allocating multiple isolated service classes with different prices. PMP is advantageous due to its simplicity and applicability to a wide range of digital services. However, the viability of PMP in terms of profit for the service provider and social welfare has been a central issue, with previous studies providing conflicting conclusions. In this article, the authors identify general principles and conditions that ensure the success of PMP, and apply them to various digital service examples.
411832	41183276	A unifying model and analysis of P2P VoD replication and scheduling.	The paper explores a peer-to-peer video-on-demand system where each peer can store a limited number of movies to reduce server load. The challenge is to determine the best content replication strategy to minimize server load. Previous research on this topic has yielded varying results, which can be attributed to different assumptions about how peers are scheduled to serve user requests. The authors propose a unified model, known as Fair Sharing with Bounded Degree (FSBD), which takes into account the maximum number of peers that can serve a single request. Using this model, they compare different replication strategies and demonstrate how different strategies are more effective for different scheduling degrees. They also propose a simple, distributed replication algorithm that can adapt to different degrees of scheduling.
411833	41183353	The Impact of Network Structure on the Stability of Greedy Protocols	This article discusses the stability of packet-switching networks, focusing on how network structure affects this property. The authors use Adversarial Queueing Theory, which assumes that packets are adversarially injected into the network. They explore the impact of various structural parameters, such as size, diameter, maximum vertex degree, and minimum number of edge-disjoint paths, on the stability and instability bounds of different greedy protocols. They show that increasing the size of a network can lead to a drop in its instability bound, and that maintaining a smaller network size can already result in a low instability bound. They also demonstrate how network subgraphs can affect stability and contribute to a better understanding of the relationship between network structure and stability properties.
411833	41183363	Optimal, distributed decision-making: the case of no communication	The article presents a framework for studying distributed optimization problems involving decision-making by a group of agents in the presence of incomplete information. This framework is based on a combinatorial approach and can be applied to both oblivious and non-oblivious algorithms. The framework allows for the calculation of the winning probability, which is the probability that no "overflow" occurs, using a simple combinatorial polytope. The authors also provide optimality conditions in the form of combinatorial polynomial equations and show that optimal non-oblivious algorithms must be non-uniform. This highlights a trade-off between the amount of knowledge used by agents and uniformity in achieving optimal decision-making without communication.
411834	41183473	Congestion games with player-specific constants	This article discusses a specific type of congestion game where players have individual delay functions and constants for each resource. The delay functions and constants are combined using a group operation, resulting in a player-specific latency function. The group used is a totally ordered abelian group. This class of games is a subset of a new intuitive class called dominance weighted congestion games. Results for games on parallel links, network congestion games, and arbitrary congestion games are presented, including the existence of a weighted potential for games with linear delay functions and player-specific additive constants.
411834	4118342	On the power of nodes of degree four in the local max-cut problem	The study focuses on the complexity of local search in the max-cut problem with a FLIP neighborhood, where only one node changes the partition. The authors introduce a technique for constructing instances that enforce specific sequences of improving steps. They demonstrate that even graphs with a maximum degree of four exhibit properties that make it difficult to find a local optimum, including instances where local search takes exponential time to converge and a PSPACE-complete problem of computing a local optimum reachable by improving steps. This is an improvement on previous results for graphs with higher degrees. The paper also explores the implications of these findings for other related problems and raises questions about the difficulty of computing a local optimum on graphs with maximum degree four.
411835	411835100	Porting social media contributions with SIOC	Social media sites have become extremely popular, with millions of users and billions of dollars invested in them. To make it easier for users to access multiple sites, there is a need for portability of personal profiles, friend networks, and content objects. The Semantic Web provides the necessary representation mechanisms for this portability by linking people and objects through agreed-upon formats like FOAF and SIOC. These formats help describe people, content objects, and their connections in a way that allows for interoperability between social media sites. This paper discusses how the SIOC project can address the issue of portability between sites by using Semantic Web technology. It also explores how SIOC data can be used to represent and transfer user contributions on various social media sites.
411835	411835102	The Future of Social Web Sites: Sharing Data and Trusted Applications with Semantics	The rise of Social Web sites has led to a need for better ways to represent and navigate the growing number of connections between people and their interests. This has prompted the use of Semantic Web technologies, which allow for interoperability among sites and the representation of data and applications in a unified and extensible manner. This chapter will discuss methods for using Semantic Web formats to describe people, content objects, and connections on Social Web sites, as well as how the Semantic Web can enhance data sharing and application development on these sites.
411836	411836105	A decentralised public key infrastructure for customer-to-customer e-commerce.	The success of eBay has highlighted the demand for customer-to-customer (C2C) e-commerce. However, eBay's centralised infrastructure has scalability issues that can lead to network bandwidth, server load, and availability problems. This paper suggests that C2C e-commerce is a natural fit for peer-to-peer (P2P) systems, which can expand beyond file sharing into new application areas. The ultimate goal is to create a decentralised P2P system that functions similarly to eBay without relying on its centralised infrastructure. Due to the importance of security in e-commerce, the paper proposes a decentralised P2P public key infrastructure (PKI) as a first step and presents an analytical model to quantify its effectiveness and resilience against potential threats and attacks.
411836	41183642	The chatty web: emergent semantics through gossiping	This paper introduces a new method for achieving semantic interoperability between data sources without relying on pre-existing global semantic models. The approach allows for incremental development of global agreement through local interactions between participants, using translations between local schemas. The system also includes a formal framework for assessing the quality of these agreements, with ratings that are adjusted as the system operates. This approach can be applied to any system with a communication infrastructure and offers the opportunity to study semantic interoperability in a network of information sharing parties. The paper provides results from a case study to support the effectiveness of this approach. 
411837	41183768	Maintaining Consistency Of Ontologies Registry With Description Logic	The article discusses the use of ontology registry in Semantic Web Services (SWS) to achieve semantic interoperability. Ontology models in the registry must meet certain representation and consistency requirements for use by both software agents and humans. The article proposes an approach for maintaining consistency in ontology models using Description Logic representation. This is done through an evolution algorithm that transforms and combines ontology models to improve the ontology registry's metamodel framework. The effectiveness of this approach is demonstrated through testing with the knowledge representation system PowerLoom. The study of ontology registry consistency has both theoretical and practical importance in achieving semantic interoperability in SWS.
411837	41183731	An approach for business process model registration based on ISO/IEC 19763-5.	This article discusses the challenge of integrating business processes modeled in different languages and stored in different repositories. To address this issue, the authors propose using a semantic interoperability technique to transform heterogeneous process models into uniform registered items. They also introduce a process model registration framework, based on a general and unambiguous metamodel, which allows for semantic discovery of business processes and promotes collaboration across enterprises. The article specifically focuses on mapping rules and transformation algorithms for Event-driven Process Chain (EPC) models, and presents an automatic registration tool for EPC. The effectiveness of the framework is demonstrated through experiments using real data from SAP.
411838	41183889	Towards Optimal and Scalable Non-functional Service Matchmaking Techniques	Service-orientation lays the foundation for the Internet of Services (IoS), which will offer millions of services for creating innovative applications. Therefore, the non-functional aspect of services must be taken into account when selecting from the vast number of functionally-equivalent options for a specific user task. Current approaches to non-functional service discovery use constraint solving techniques to optimize the matchmaking time between a service offer and demand. However, this method becomes less efficient as the number of service offers increases, making it unsuitable for the IoS. To address this issue, two alternative techniques have been proposed and evaluated. These techniques were found to improve matchmaking time without compromising accuracy, with the second technique proving to be highly scalable.
411838	41183846	Novel Optimal and Scalable Nonfunctional Service Matchmaking Techniques	Service-orientation is a key factor in the development of the Internet of Services (IoS), where millions of services will be available for everyday user tasks. With a vast number of similar services, nonfunctional aspects must be considered to filter and select the most suitable ones. Existing approaches for nonfunctional service discovery use constraint solving techniques, but these do not scale well and are not yet suitable for the IoS. This article proposes three new techniques that organize the service offer space to improve matchmaking time. Evaluations show that all techniques optimize matchmaking time without compromising accuracy, with each being better in different situations. 
411839	41183921	Visual Modeling Of Defeasible Logic Rules With Dr-Vismo	Standardization of the Semantic Web has focused on ontologies and ontology languages, but reasoning over available information is crucial for its success. Defeasible reasoning, which handles incomplete and conflicting data, is one approach, but it can be confusing for users. To address this issue, a new representation schema based on directed graphs has been proposed. This paper introduces DR-VisMo, a system that implements this schema for editing and visualizing defeasible logic rule bases. It also includes a stratification algorithm for organizing the elements in the graph. DR-VisMo is part of VDR-DEVICE, a system for modeling and deploying defeasible logic rule bases on RDF ontologies.
411839	41183920	A visual environment for developing defeasible rule bases for the semantic web	Defeasible reasoning is a rule-based approach that allows for efficient reasoning with incomplete and inconsistent information. It has many applications in the Semantic Web, but its RuleML syntax can be complex for some users. Additionally, the use of multiple technologies and languages, such as defeasible reasoning, RuleML, and RDF, can require the use of multiple tools to build rule-based applications for the Semantic Web. To address this issue, VDR-Device is presented as a visual integrated development environment that combines a visual RuleML-compliant rule editor and a defeasible reasoning system within a user-friendly graphical interface. This allows for the creation and use of defeasible logic rule bases on top of RDF ontologies, with the vocabulary constrained by analyzing the input RDF ontologies.
411840	4118406	Taming Displayed Tense Logics Using Nested Sequents with Deep Inference	This article discusses two sequent calculi for tense logic, where the syntactic judgements are nested sequents. The first calculus, SKt, is a variant of Kashima's calculus and uses "shallow" inference, where rules are applied only to the top-level nodes. It includes "display postulates" to bring a node to the top level and allow for inference at any point. The second calculus, DKt, is more natural and uses deep-inference, allowing for inference rules at any node without structural rules. The two calculi are shown to be equivalent, and this equivalence also holds for extensions of tense logic. The deep-sequent system has easier proof search compared to the shallow system and a procedure is outlined for its use. 
411840	41184030	Induction and Co-induction in Sequent Calculus	Proof search is a common method for specifying computation systems, and a framework for reasoning about these specifications can be built using a sequent calculus involving induction and co-induction. This approach is based on a proof-theoretic notion of definitions, which are similar to logic programs and allow for reasoning about syntax. The use of definitions also enables reasoning about properties of computational systems through induction and co-induction, using higher-order abstract syntax. This logic, called Linc-, addresses the compatibility issues between HOAS and inductive reasoning, and guarantees consistency through cut-elimination. It is a valuable addition to the field of logical frameworks and formal verification.
411841	4118417	Faster and more robust point symmetry-based K-means algorithm	This paper introduces a new point symmetry distance (PSD) measure called symmetry similarity level (SSL) operator for K-means algorithm. The authors' modified point symmetry-based K-means (MPSK) algorithm is more robust than the previous PSK algorithm. The MPSK algorithm is suitable for both intra-clusters and inter-clusters that exhibit symmetry. Additionally, the authors present two speedup strategies to reduce the execution time of the MPSK algorithm. Experimental results show that the MPSK algorithm outperforms the previous PSK algorithm in terms of execution time and its ability to handle symmetrical inter-clusters.
411841	41184130	Novel efficient two-pass algorithm for closed polygonal approximation based on LISE and curvature constraint criteria	This paper presents a new algorithm for solving the closed polygonal approximation problem, which involves finding the best polygonal approximation for a closed curve with n points. The algorithm uses local integral square error and curvature constraint criteria to find a solution in O(Fn+mn^2) time, where m is the minimal number of covering feasible segments for one point and F is the number of feasible approximate segments. Experimental results show that the proposed algorithm outperforms previous algorithms in terms of quality and execution time, but may have some degradation in execution time when compared to other recently published algorithms.
411842	41184242	More efficient periodic traversal in anonymous undirected graphs	The article discusses the problem of periodic graph exploration, where a mobile agent with limited memory needs to visit all nodes of a connected graph in a periodic manner. The graphs are anonymous, meaning the nodes are unlabeled, but the agent can distinguish between edges through unique port numbers. Previous research has shown that arbitrary port number assignments make the problem unsolvable, but recent algorithms have achieved short traversal periods by carefully assigning port numbers. The article presents new algorithms that improve upon previous bounds, with a period length of no more than (4+13)n-4 for oblivious agents and 3.5n-2 for agents with constant memory. The article also introduces a new graph decomposition technique and presents a lower bound of 2.8n-2 for the oblivious case. 
411842	4118428	Improved analysis of the online set cover problem with advice.	The study focuses on the advice complexity of an online version of the set cover problem, which aims to determine the amount of information needed for online algorithms to efficiently solve the problem without knowing future requests. This concept has been successfully applied to various online problems in the past to understand their difficulty. The research introduces a linear number of advice bits as both sufficient and necessary for optimal performance in the online set cover problem. It also shows that a certain number of advice bits is sufficient for designing a c-competitive algorithm, with upper and lower bounds for achieving this level of competitiveness. The advice complexity is also analyzed in relation to measurable properties of the inputs.
411843	41184368	Sparse polynomial interpolation and Berlekamp/Massey algorithms that correct outlier errors in input values	This article proposes algorithms for sparse interpolation with errors, using a combination of Prony's--Ben-Or's & Tiwari's algorithm and a Berlekamp/Massey algorithm with early termination. The first algorithm can recover a t-sparse polynomial from a sequence of values, even if some values are incorrect due to random or misleading errors. It requires bounds on the number of terms and errors, and interpolates values at specific field elements. The article also addresses the problem of recovering a minimal linear generator from a sequence of linearly generated values with errors. The proposed Majority Rule Berlekamp/Massey algorithm can recover the generator and correct the errors. These algorithms have potential applications in sparse interpolation with numerical noise and outlier errors.
411843	41184351	Interactive certificate for the verification of Wiedemann's Krylov sequence: application to the certification of the determinant, the minimal and the characteristic polynomials of sparse matrices	Certificates for linear algebra computations are extra data structures that can be used to prove the correctness of outputs using a verification algorithm. Wiedemann's algorithm transforms a Krylov sequence, generated by multiplying a vector by a matrix, into a linearly recurrent sequence. The minimal polynomial of this sequence is a divisor of the minimal polynomial of the original matrix. This paper presents algorithms for computing certificates for Krylov sequences of sparse or structured matrices over a field, with a verification complexity that is essentially linear. This can be used to obtain certificates for the determinant, minimal polynomial, and characteristic polynomial of sparse or structured matrices at a low computational cost.
411844	41184442	Uncovering the unarchived web	Web archiving is an important practice for preserving cultural heritage, and is typically done by either harvesting a national domain or crawling a predetermined list of websites. However, this often results in more information being collected than just the intended websites, which can provide valuable insights into the past. To capture this additional information, a method called "web collection aura" has been developed, which uses crawl dates, anchor text, and link structure to reconstruct pages that were not included in the archived collection but are known to have existed. This method has been shown to uncover a substantial number of additional pages, highlighting the importance of considering the "aura" in web archiving.
411844	41184418	Linking wikipedia to the web	This study focuses on finding links from Wikipedia pages to external web pages, which can enhance the information available on Wikipedia. The researchers used a language modeling approach and experimented with different document priors. They also explored the use of social bookmarking site Delicious to improve performance. A test collection of 53 topics was created for this purpose. The findings showed that using anchor text indexing was the most effective method for retrieving home pages. Combining the best anchor text run with the Delicious run resulted in even better results. However, using Delicious alone did not yield significant improvements. Overall, this study highlights the potential of using external links to enhance information on Wikipedia.
411845	411845114	Providing consistent and exhaustive relevance assessments for XML retrieval evaluation	Retrieval approaches are methods used to find relevant information within a set of documents. In order to accurately compare these approaches, test collections are necessary. These collections contain documents, queries, and relevance assessments. It is important for these assessments to be consistent and comprehensive in order to properly compare retrieval methods. While the evaluation methodology for traditional text retrieval is well-established, evaluating XML retrieval is still a research topic. This is because XML documents are made up of nested components that cannot be considered independently in terms of relevance. The INEX initiative has developed a methodology to ensure consistent and thorough relevance assessments for XML retrieval.
411845	41184580	Investigating the exhaustivity dimension in content-oriented XML element retrieval evaluation	INEX is an evaluation initiative for content-oriented XML retrieval that uses two dimensions, exhaustivity and specificity, to measure the relevance of an element. This approach was chosen to provide a more stable measure of relevance compared to a single scale. However, obtaining relevance assessments is costly, as each document and its elements must be assessed by a human. There has been ongoing debate within INEX about whether this complex definition of relevance is necessary. This paper aims to answer this question by conducting statistical tests to compare system performance under different assessment scenarios.
411846	41184629	Probabilistic construction and manipulation of free Boolean diagrams	The content is not available in abstract form.
411846	411846122	A Formal Foundation for Secure Remote Execution of Enclaves.	Recent developments in trusted hardware platforms, like Intel SGX and MIT Sanctum, have been praised for their enhanced security features, but they lack formal guarantees. To address this issue, a team has introduced a verification method that uses a trusted abstract platform (TAP), which is a formalization of the idealized enclave platform, along with a parameterized adversary. The researchers have also defined the concept of secure remote execution and have provided machine-checked proofs that demonstrate how the TAP satisfies the three main security properties for secure remote execution: integrity, confidentiality, and secure measurement. Furthermore, they have also verified that both SGX and Sanctum are refinements of the TAP under specific adversary models, proving that these systems are able to implement secure enclaves for the designated adversaries.
411847	41184714	Optimising long-latency-load-aware fetch policies for SMT processors	Simultaneous multithreading (SMT) processors are designed to increase instruction level parallelism by fetching instructions from multiple threads. The fetch engine plays a crucial role in determining which threads have priority and access to shared resources. However, when a thread experiences a cache miss, it can monopolize critical resources and slow down the execution of other threads. To address this problem, several approaches have been proposed. This paper evaluates and compares the three best published policies for handling long latency loads. Additionally, the paper proposes improved versions of these policies, which have been shown to significantly enhance both throughput and fairness in SMT processors. 
411847	41184722	Dynamically Controlled Resource Allocation in SMT Processors	SMT processors improve performance by executing instructions from multiple threads simultaneously. However, this can lead to competition for critical resources, which affects overall performance. Existing fetch policies use indirect indicators to distribute resources, resulting in resource monopolization or wastage. To address this issue, a new concept of dynamic resource control is introduced for SMT processors. This allows for a novel resource allocation policy that directly monitors thread resource usage, ensuring fair distribution and reducing resource under-use. Simulation results show that this dynamic policy outperforms both static and existing dynamic policies, achieving a better balance of throughput and fairness.
411848	41184820	Pedestrian detection via classification on Riemannian manifolds.	This paper introduces a new algorithm for detecting pedestrians in still images. It utilizes covariance matrices as object descriptors, which cannot be represented as a vector space. Therefore, traditional machine learning techniques are not effective for learning classifiers. To address this, the paper proposes a novel approach for classifying points on a connected Riemannian manifold, which is used to represent the space of d-dimensional nonsingular covariance matrices. The algorithm is tested on two popular pedestrian datasets and shows better detection rates compared to previous approaches. 
411848	41184832	Region covariance: a fast descriptor for detection and classification	A new region descriptor has been introduced and applied to object detection and texture classification. This descriptor is based on the covariance of d-features, such as color vector, derivatives of intensity, etc. A fast method for computing covariances using integral images has been described. This method is more general and efficient compared to other methods like image sums or histograms. The distance metric used for feature matching involves generalized eigenvalues, which is derived from the Lie group structure of positive definite matrices. The performance of this method is better than others, and it can handle large rotations and illumination changes. 
411849	4118494	A Group Theoretical Toolbox For Color Image Operators	This paper discusses the use of the direct product of two mathematical groups, the dihedral group D(4) and the symmetric group S(3), to automatically derive low-level image processing filters for RGB images. These filters lead to a block-diagonalization of the correlation matrix for certain stochastic processes. The group theoretical derivation allows for a fast implementation using mainly additions and subtractions, making them suitable for applications such as video processing and content-based image database search. The authors demonstrate the effectiveness of these filters in an experiment using a large image database. They also discuss the generalizability of this approach for other types of image data, such as multiband images and images with non-rectangular sampling grids. 
411849	41184929	Lie methods for color robot vision	Lie-theoretical methods can be applied to analyze color-related problems in machine vision by recognizing that spectral color signals are restricted to a conical section within the larger Hilbert space of square-integrable functions. This allows for the use of a coordinate system consisting of a half-axis and a unit ball with the Lorentz groups as a natural transformation group. The Lorentz group SU(1, 1) is introduced as a useful tool for analyzing color image processing problems, and algorithms are derived for investigating dynamical color changes. These methods can be used to compress, interpolate, extrapolate, and compensate image sequences affected by such color changes. 
411850	4118505	Enhancing IPTV personalization	This paper presents an improved IPTV personalization system that utilizes viewer attributes instead of solely relying on set-top box identifiers. It eliminates the need for user logins, which may not accurately identify all viewers present. The system includes a lighting normalization module to improve attribute recognition in situations where lighting may vary, such as when watching TV at home. The proposed architecture aims to enhance user experience and provide more accurate personalization for multiple viewers.
411850	41185063	Multichannel Pulse-Coupled-Neural-Network-Based Color Image Segmentation for Object Detection	The authors of this paper propose a pulse-coupled neural network (PCNN) with multichannel (MPCNN) linking and feeding fields for color image segmentation. This model incorporates pulse-based radial basis function units to determine fast links among neurons based on spectral and spatial features. The segmentation process can be performed in parallel using a field-programmable-gate-array chip. The results can then be used for object detection. Experimental results show that the performance of the MPCNN is similar to other popular image segmentation methods for noisy images, while its parallel neural circuits greatly improve processing speed compared to sequential-code-based methods.
411851	41185155	Adaptive Multiple Resources Consumption Control for an Autonomous Rover	In the context of autonomous rovers, controlling resources consumption is crucial. This consumption is often unpredictable and the rover must adapt during execution to prioritize important tasks and avoid failure. The Progressive processing model allows for tasks to be performed in multiple ways, giving the agent the ability to control resource consumption during the mission. This is achieved through an off-line solved Markov decision process. Initially, Progressive processing could only control time as a resource, but an extension has been developed for multiple resources. The main contribution is a compact and efficient state space representation for multiple consumable resources.
411851	41185123	Multi-Robot Exploration Of Unknown Environments With Identification Of Exploration Completion And Post-Exploration Rendez-Vous Using Ant Algorithms	The paper introduces a new ant algorithm for navigating multiple robots to autonomously explore an unknown environment. The robots build a shared representation of the environment, treating it as a graph with local memory in each cell. Unlike other ant-based approaches, this algorithm allows the robots to determine when the coverage is complete and move to a designated meeting point. The proposed algorithm includes improvements to the existing Brick&Mortar algorithm, such as using local strategies like LRTA*. Benchmark tests against other ant algorithms on different graph topologies are also presented. The key novelty of this approach is that the algorithm leads the robots to a predetermined evacuation point as a result of its emerging property. 
411852	41185246	Homomorphic Computation of Edit Distance.	Genomic sequence analysis is a crucial tool for understanding an organism's biology, but it can also pose a risk to privacy due to the sensitive information contained in the sequences. To address this issue, a method has been developed to perform the edit distance algorithm on encrypted data, allowing for sequence analysis to be done in a public setting without revealing the raw data. This approach involves the data owner providing only the encrypted sequence, which can be decrypted by designated individuals with the decryption key. The efficiency of this method has been analyzed and shown to be feasible, with the potential for further optimizations. A proof of concept using short DNA sequences has also been presented.
411852	41185235	Can homomorphic encryption be practical?	The growing trend of outsourcing data storage and management to cloud services raises privacy concerns. Encrypting data before sending it to the cloud can address these concerns. Fully homomorphic encryption schemes allow for computations on encrypted data, but they are not yet efficient enough for practical use. This article presents a new approach using "somewhat" homomorphic encryption, which supports a limited number of operations and is faster and more compact. The authors also demonstrate a proof-of-concept implementation of a recent somewhat homomorphic encryption scheme, which is efficient and has short ciphertexts. This approach has potential applications in the medical, financial, and advertising industries. The authors also discuss optimizations for the encryption scheme, including the ability to convert between different message encodings in a ciphertext.
411853	4118534	Model-based analysis of configuration vulnerabilities	Vulnerability analysis is the process of identifying weaknesses in computer systems that could be exploited to compromise their security. This paper presents a new approach to vulnerability analysis using model checking. This involves formal specification of security properties, creating an abstract model of the system, and using a verification procedure to check if the model satisfies the properties. If not, it can identify exploit scenarios. This approach is beneficial as it can detect known and unknown vulnerabilities. The paper demonstrates this approach on a simplified UNIX system and shows that it is feasible despite the complexity of the system. The authors believe that with continued advancements in model checking techniques, automated vulnerability analysis of realistic systems will be possible in the future.
411853	41185318	A process calculus for Mobile Ad Hoc Networks	The @w-calculus is a process calculus designed for modeling and analyzing Mobile Ad Hoc Wireless Networks (MANETs) and their protocols. It captures important features of MANETs, such as node mobility and broadcast messaging within a limited range. The calculus separates a node's communication and computational behavior from its physical transmission range. Formal operational semantics are provided, and it is shown to be a conservative extension of the @p-calculus. Congruence results are established for a weak version of late bisimulation equivalence. A symbolic semantics is defined for the @w-calculus with the mismatch operator, and its practical usefulness is demonstrated through formal models of a leader election protocol and AODV routing protocol for MANETs. 
411854	41185457	Extensions of the Queueing Relations L = λW and H = λG	This paper discusses the extension of fundamental queueing relations, L = λW and H = λG, which connect customer averages (waiting time and cost) to time averages (queue length and cost) for an arrival process with rate λ. The relations can be established using a two-dimensional cumulative input process and can include continuous versions and higher dimensions. The paper also establishes inequalities when conditions for equality are not met and introduces central limit theorem versions of H = λG, expanding on previous results.
411854	41185455	Ordinary CLT and WLLN versions of L=λW	The equation L = λW, commonly known as Little's law, can be seen as a connection between strong laws of large numbers (SLLNs). In this article, the authors prove both central-limit-theorem (CLT) and weak-law-of-large-numbers (WLLN) versions of L = λW. This means that if the sequence of interarrival and waiting times follows certain criteria, the queue-length process will also follow a CLT with a related distribution. In a previous paper, the authors also proved a functional-central-limit-theorem version of L = λW, showcasing the differences between establishing ordinary limit theorems and their functional equivalents.
411855	41185566	Special Issue: Learning and creativity Part 1	The special issues of AIEDAM (Artificial Intelligence in Engineering Design, Analysis and Manufacturing) in Issue 3 and 4 were the result of a workshop on Learning and Creativity at the 2002 AID conference on Artificial Intelligence in Design. This workshop was the sixth in a series, with the previous five focusing on Machine Learning in Design and being held at AIDs '92, '94, '96, '98, and '00. The first three workshops also resulted in special issues of AIEDAM. The workshop aimed to explore the intersection of learning and creativity in design and its implications for artificial intelligence.
411855	4118551	On the use of shared task models in knowledge acquisition, strategic user interaction and clarification agents	This paper discusses the various roles of a shared task model as an intermediate representation of a task, using examples from applications developed in collaboration with industry. The first role is in knowledge acquisition, where a generic task model was used to structure knowledge in a soil sanitation decision support system. In a chemical process diagnosis application, a shared task model was developed for Nylon-6 production. The second role is in designing user interaction, where a user interface based on the shared task model was designed for an environmental decision making system. Finally, the role of shared task models in a multi-agent system with a clarification agent is explored, with examples from a chemical process diagnosis application. The multi-agent architecture allows for support at both the diagnostic and clarification levels.
411856	41185668	Improved Distributed Algorithms for SCC Decomposition	The OBF technique, developed by Barnat and Moravec, is used in distributed algorithms to break down a partitioned graph into its strongly connected components. In this study, a recursive version of OBF is introduced and different implementations of it are experimentally evaluated, varying in their level of parallelism. Synthetic graphs with a few large components and graphs with many small components were used for the evaluation, as well as real model checking applications. The results were compared to other successful SCC decomposition techniques, such as those developed by Orzan and Fleischer, Hendrickson, and Pinar. 
411856	41185661	Multi-core on-the-fly SCC decomposition.	Tarjan's strongly connected component (SCC) algorithm has the advantages of being able to return SCCs while traversing or generating a graph, and having a linear time complexity. However, previous parallel SCC algorithms have sacrificed these advantages by either running in quadratic time or requiring the full graph beforehand. This paper presents a new parallel, on-the-fly SCC algorithm that maintains the linear-time property by allowing workers to explore the graph randomly and communicate partially completed SCCs. A concurrent, iterable disjoint set structure is used for efficient communication of partial SCCs. The algorithm is shown to be 10-30 times faster than Tarjan's algorithm for graphs with a large SCC, and comparable in performance to previous state-of-the-art for graphs with many small SCCs.
411857	41185761	The Degree Of Squares Is An Atom	The article discusses the theory of degrees of infinite sequences and their transducibility by finite-state transducers. Previous studies have left many basic questions unanswered, including the existence of atom degrees. An atom degree is one that is only below the bottom degree consisting of ultimately periodic sequences. The article focuses on the 'squares sequence' and shows that its degree is an atom. The main tool used to prove this is the characterization of transducts of 'spiralling' sequences and their degrees. It is also shown that every transduct of a 'polynomial sequence' can either be transduced back to a polynomial sequence or is in the bottom degree.
411857	41185726	Generalized Innermost Rewriting	The article discusses two generalizations of innermost rewriting, a term rewriting strategy, and proves that their termination is equivalent to innermost rewriting. This means that in certain cases, allowing non-innermost steps can improve the termination behavior and efficiency of the rewriting process. The article also explores the concept of different reduction strategies, such as innermost and outermost, and how they can lead to different outcomes when applied to the same term. It is shown that for orthogonal and non-overlapping TRSs, innermost rewriting is not worse than any other strategy. However, for practical purposes, innermost rewriting is often preferred due to its simplicity and efficiency. The article also discusses the importance of non-innermost rewriting in implementing lazy evaluation for functional programming and presents two generalizations of innermost rewriting that are always at least as good as innermost rewriting, while also allowing for non-innermost steps. These generalizations are particularly useful in cases where the TRSs may have overlaps or lack confluence.
411858	41185848	Amigo: proximity-based authentication of mobile devices	Amigo is a method for authenticating devices that are physically close to each other, without needing any prior knowledge or additional hardware. It uses the shared radio environment to verify the devices' proximity and is effective against various types of attacks. Its main benefits include not requiring extra hardware or user involvement, and being resistant to eavesdropping.
411858	41185825	Enabling Secure and Spontaneous Communication between Mobile Devices using Common Radio Environment.	The widespread use of mobile devices has made it common for them to spontaneously interact with other nearby devices that they have no prior knowledge of. However, ensuring the security of these interactions against eavesdropping and attacks is a difficult task. This paper suggests that mobile devices in close proximity can use the signal strength of existing radio sources in their environment (such as GSM cell towers or WiFi access points) to derive a shared secret and secure their communication. Two approaches for using this location-based secret are discussed, but it is a challenging task since the radio environment is not identical for all devices in close proximity.
411859	41185967	Managing very large distributed data sets on a data grid	This work focuses on managing large data sets that need to be stored and processed across multiple computing sites. The motivation for this research comes from the ATLAS experiment for the Large Hadron Collider (LHC), where the authors have been involved in developing the data management middleware called DQ2. This middleware has been successfully used for several years to transport petabytes of data to research centers and universities around the world. The authors share their experience in deploying DQ2 on the Worldwide LHC computing Grid, a production Grid infrastructure with hundreds of computing sites. Through this experience, they have identified uncertainty in the behavior of large Grid infrastructures and have developed novel modelling and simulation techniques for Data Grids. They also discuss the practical limits of data distribution algorithms for Data Grids and suggest future research directions.
411859	41185971	Structuring research methods and data with the research object model: genomics workflows as a case study.	Biomedical research faces a major challenge in understanding complex molecular mechanisms through the integration of large and diverse data. To address this, it is important to preserve the materials and methods used in computational experiments with clear annotations. The bioinformatics community recognizes the importance of this and suggests that providing structured aggregation and annotation of experiment objects can aid in understanding and reproducing results. A model of a workflow-centric Research Object (RO) was explored, where an RO is a resource that brings together various resources such as datasets, software, spreadsheets, and text. This model was applied to a case study on human metabolite variation using workflows.
411860	411860121	Mobile Image Search via Local Crowd: A User Study	This paper examines the effectiveness of a mobile social search application in Japan that utilizes crowd sourcing to assist foreign visitors with image-based questions. A controlled field experiment was conducted over 6 weeks with 55 participants, and it was found that the application had a reliable response speed and quantity. On average, half of the requests were answered within 10 minutes, and 75% were answered within 30 minutes, with an average of 4.2 answers per request. The top active crowd workers were primarily motivated by intrinsic factors rather than extrinsic incentives. The application had the most success in the afternoon, evening, and night, with an average response time of 10 minutes and over 4 answers per request.
411860	411860132	Extracting intermediate-level design knowledge for speculating digital-physical hybrid alternate reality experiences.	This paper outlines a process for creating a service design and analysis framework to design digital services that offer alternate reality experiences. Alternate reality experiences involve modifying our senses to perceive our surroundings differently, and can influence our attitudes and behaviors. The framework is derived through discussions in exploration workshops, where three digital services are chosen to examine how they create a sense of value in alternate reality experiences. The paper then explains how the framework is derived and presents a feasibility analysis through a case study. This process is important for developing advanced digital services that offer alternate reality experiences in the future.
411861	41186131	Action Recognition from One Example	The authors propose a new method for recognizing actions in videos using space-time locally adaptive regression kernels and the matrix cosine similarity measure. This method only requires a single example of an action as a query and does not rely on prior knowledge or complex motion estimation. The proposed method computes space-time descriptors from the query video and compares them to features from the target video using a matrix generalization of the cosine similarity measure. The resulting scalar resemblance volume indicates the likelihood of similarity between the two videos. The method is shown to have high performance on challenging datasets and outperforms other methods in action categorization. 
411861	41186139	Training-Free, Generic Object Detection Using Locally Adaptive Regression Kernels	Our study proposes a detection and localization algorithm that can search for a specific visual object without needing any prior training. This method uses a single example of the object to find similar matches and does not require any preprocessing or segmentation of the target image. The algorithm is based on computing local regression kernels and extracting salient features to compare with features from the target image using a cosine similarity measure. We demonstrate the effectiveness of the algorithm using a naive-Bayes framework and it produces a map indicating the likelihood of similarity between the query and the target image. By using nonparametric significance tests and nonmaxima suppression, we are able to detect and locate objects with high accuracy, even when there are large variations in scale and rotation. Our approach performs well on various challenging data sets, showing its ability to detect objects in different contexts and imaging conditions. 
411862	41186252	A web-based novel term similarity framework for ontology learning	The article discusses the importance of pairwise similarity computations in ontology learning and data mining. The authors propose a similarity framework that uses a conventional Web search engine to obtain the most up-to-date knowledge on terms. This is particularly useful for dynamic ontology management as ontologies must evolve over time. The method is also less sensitive to data sparseness compared to other approaches that use a fixed amount of web documents. The proposed methodology utilizes two measures for similarity computation and can be used for modifying existing ontologies. Experimental results show that the method can extract topical relations between terms that are not present in traditional concept-based ontologies.
411862	41186250	Dynamic pattern mining: an incremental data clustering approach	The proposed framework aims to efficiently mine and analyze news articles from Web news services by utilizing incremental data clustering. This involves using data mining tools to extract useful knowledge from the articles and store it in a database. Instead of directly interacting with the news service, an information delivery agent can use the knowledge in the database to respond to user requests. To manage the high rate of document insertion, a sophisticated incremental hierarchical document clustering algorithm is introduced. This algorithm can identify meaningful patterns and reduce computation by maintaining cluster structure incrementally. Additionally, a topic ontology learning framework is proposed to overcome the lack of topical relations in conceptual ontologies. Experimental results show that the proposed methods produce high-quality clusters and provide interpretations of news topics at different levels of abstraction.
411863	41186355	Multi-modal pharmacokinetic modelling for DCE-MRI: using diffusion weighted imaging to constrain the local arterial input function	In oncology, magnetic resonance imaging (MRI) is routinely used to gather data on tissue structure and function. It is now possible to combine different models, such as diffusion weighted imaging and dynamic contrast enhanced (DCE) MRI, to gain a more comprehensive understanding of tissue characteristics. In this study, the researchers use data from 8 subjects, including 4 with head and neck tumors, to investigate the potential benefits of incorporating parameters from the intra-voxel incoherent motion (IVIM) model in the DCE modeling process. While the results were not significant in this study, the researchers believe this technique could be useful in other applications.
411863	41186316	A hybrid fem-based method for aligning prone and supine images for image guided breast surgery	The study discusses the use of preoperative images, specifically high quality dynamic contrast enhanced magnetic resonant images obtained in the prone position, in breast conserving surgery. However, due to the significant deformation between prone and supine positions, traditional image registration methods have limited success. To address this, the paper proposes a hybrid method that combines biomechanical models and intensity-based image registration. The biomechanical models are used to estimate breast deformation, while the intensity-based registration methods recover any discrepancies between experimental and predicted results. The proposed method is shown to have good performance in registering prone and supine MR breast images, as demonstrated by an experimental example.
411864	41186471	An evolutionary algorithm to model structural excursions of a protein.	Proteins need to be able to move between different structures in order to perform their biological functions effectively. This movement is determined by the energy landscape, which categorizes available structures based on their energies. Computational research aims to understand and simulate these structural changes, in addition to traditional laboratory studies. One popular method is to use robot motion planning or construct structured representations of the energy landscape. However, a new approach using evolutionary computation has been proposed. This method uses an evolutionary algorithm to evolve paths of structural excursions without prior knowledge of the energy landscape. Initial results on different protein variants show potential for this method and warrant further exploration.
411864	41186462	Evolutionary search for paths on protein energy landscapes.	Proteins are constantly moving and changing their structures to interact with other molecules. This movement is guided by an energy landscape that determines the available structures based on their potential energies. To study these structural changes, researchers have developed an evolutionary algorithm that can calculate the motions of a protein without needing to know its energy landscape beforehand. The initial findings of this algorithm are encouraging and point towards potential future research opportunities.
411865	41186516	Partial and Constrained Level Planarity.	A level planar drawing of a directed graph G is where each vertex is mapped to a unique point on a horizontal line based on its assigned level, and edges are drawn without crossing in a y-monotone curve. In the problem CLP, we are given a partial ordering of vertices on each level and seek a level planar drawing where the order follows this ordering. A simpler version, PLP, involves extending a given level-planar drawing of a subgraph to a complete drawing without changing it. A polynomial-time algorithm with running time O(n5) has been developed for CLP, using a modified data structure that can handle the constraints efficiently. However, PLP remains NP-complete, even in restricted cases.
411865	41186535	Testing the simultaneous embeddability of two graphs whose intersection is a biconnected graph or a tree	This paper focuses on the time complexity of the SEFE problem, which involves determining if two planar graphs have a common planar embedding with certain constraints. The paper presents a polynomial-time algorithm for SEFE when the intersection graph of the two input graphs is biconnected. It also shows that when the intersection graph is a tree, the SEFE problem is equivalent to a book embedding problem and presents a linear-time algorithm for the SEFE problem when the intersection graph is a star. These results are based on recent findings by Hong and Nagamochi.
411866	41186698	Online Dynamic Power Management with Hard Real-Time Guarantees.	This paper discusses the problem of online dynamic power management in multi-processor systems. The goal is to minimize energy consumption while meeting all job deadlines. The paper examines the complexity of the problem and presents efficient online strategies for real-time systems. It is shown that even when the set of jobs can be feasibly scheduled on a single processor, the competitive ratio of any online algorithm is at least 2.06. A 4-competitive algorithm is also presented for using at most two processors. The paper also considers multiple streams of jobs and presents a trade-off between energy-efficiency and the number of processors used. Finally, it is shown that even for arbitrary input sets of jobs, the competitive ratio is at least 2.28, with a 1-competitive algorithm for jobs with unit execution times.
411866	41186686	Efficient Scheduling of Data-Harvesting Trees	This work focuses on finding efficient ways to route data in sensor networks towards a sink while minimizing energy and time usage. The goal is to create TDMA schedules that are energy and time optimal, even with strict limitations such as nodes having limited memory and only being able to communicate with a single message to each neighbor. The proposed solutions are analyzed in two interference models and it is shown that energy and time optimal schedules can still be achieved. Additionally, an algorithm is presented for the k-local interference model. The mechanisms can also be extended to situations with packet loss while maintaining energy consumption bounds.
411867	41186762	Information processing in complex networks: Graph entropy and information functionals	This paper presents a new framework for defining the entropy of a graph. The definition is based on a local information graph and information functionals that measure the topological structure of the graph. These functionals are used to calculate a probability distribution, which is then used to determine the entropy of the graph. The paper also explores the relationship between different graph entropies and provides numerical results to demonstrate the usefulness and practicality of the method. The proposed method has polynomial time complexity and can be applied to understanding information processing in complex networks. 
411867	41186752	A NOVEL METHOD FOR MEASURING THE STRUCTURAL INFORMATION CONTENT OF NETWORKS	This paper introduces a new method for measuring the structural information of a network represented by an undirected and connected graph, called graph entropy. This approach utilizes local vertex functionals obtained through the Dijkstra algorithm to calculate j-spheres. The paper also presents lower and upper bounds for the graph entropy, which can be used to estimate the structural information of different types of graphs. A specific graph class is used as an example to demonstrate the calculation of graph entropies. This novel method can be valuable for analyzing and comparing complex systems represented by networks. 
411868	41186845	Assessing the health information needs of the emergency preparedness and management community	 A group of 34 professionals working in health-related emergency response management shared their sources of information, unmet information needs, and desired information systems tools. They heavily rely on the internet, but struggle with information overload and difficulty finding specific information among the vast amount available. They believe a system with social tagging and recommender features would be beneficial for accessing relevant documents in the "gray literature." This highlights the importance of such services for professional communities in general.
411868	41186817	Software Innovations To Support The Use Of Social Media By Emergency Managers	A study based on 477 responses from U.S. county-level emergency managers found that although social media can be useful for providing situational awareness during disasters, many EMs do not use it. The main barriers to its use are lack of staff and fears of information overload. However, potential software enhancements to overcome these limitations were deemed highly useful. Factor analyses were conducted on the barriers and software enhancement questions, revealing that trustworthiness of social media data was a significant predictor of usefulness. Differences in responses based on respondent and agency characteristics were explored, and open-ended comments were used to explain the findings.
411869	41186963	D-AHP method with different credibility of information.	Multi-criteria decision making (MCDM) is a widely used method in various practical applications. The D-AHP method, which extends the traditional AHP method by incorporating D numbers preference relation, was previously proposed for MCDM problems. This method provides a solution for obtaining weights and rankings of alternatives based on decision data. However, the credibility of information used in the D-AHP method has not been thoroughly studied, leading to unsolved issues. This paper investigates the impact of information credibility on the results of MCDM problems using the D-AHP method. The study shows that while the ranking of alternatives is slightly affected by information credibility, the weights assigned to alternatives are significantly influenced. 
411869	4118698	A new method in failure mode and effects analysis based on evidential reasoning.	The traditional failure mode and effects analysis (FMEA) uses the risk priority number (RPN), which is the product of occurrence, severity, and detection, to determine risk factors. However, there are issues with accurately determining and combining these factors. The traditional FMEA has been criticized for various reasons. To address this, a new method using fuzzy FMEA, evidential reasoning (ER), and the technique for order preference by similarity to ideal solution (TOPSIS) is proposed. This approach takes into account imprecise and uncertain expert assessments and uses weighted average and TOPSIS to rank risk factors. The proposed method is efficient and provides a cardinal ranking of alternatives without assuming independent attribute preferences. A numerical example demonstrates its effectiveness. This method is developed by The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden in 2014.
411870	4118706	Unifying the proper cores and dominance cores of cooperative fuzzy games	This paper aims to examine the equalities between various types of proper cores and dominance cores for a general worth function, without assuming it to be nonnegative. It presents different sufficient conditions that ensure the equalities between these cores, thus providing a comprehensive understanding of the relationships between them in this particular scenario. By investigating these equalities, the paper contributes to the understanding of core solutions for worth functions that may not satisfy the nonnegative property.
411870	4118700	Cores and dominance cores of cooperative games endowed with fuzzy payoffs	This paper investigates the cores and dominance cores in cooperative games with fuzzy payoffs. Fuzzy numbers are used to represent the imprecise payoffs of coalitions. Different ordering schemes of fuzzy numbers are used to define various concepts of cores and dominance cores. The paper also establishes the inclusion and equality relations of these concepts to provide a better understanding of fuzzy games. Additionally, the relations of pre-cores and dominance pre-cores are examined. Overall, this study contributes to the understanding of cooperative games with fuzzy payoffs and provides insights into the various concepts of cores and dominance cores in this context.
411871	41187167	Triangle factors of graphs without large independent sets and of weighted graphs.	The classical Corradi-Hajnal theorem states that a graph with at least 2/3 of its vertices having a degree of at least n will contain a triangle factor if the number of vertices is divisible by 3. In this paper, the authors present two related results that use the absorbing technique of Rodl, Rucinski, and Szemeredi. The main result determines the minimum degree condition needed to guarantee a triangle factor in graphs with a sublinear independence number. They also propose several open problems related to this result. Additionally, a fractional variant of the Corradi-Hajnal theorem is considered and a conjecture by Balogh, Kemkes, Lee, and Young is settled. Specifically, if the sum of weights on edges incident to a vertex is at least (1+2t/3 + o(1))n for every vertex, there will be n/3 vertex disjoint heavy triangles in the graph. 
411871	41187115	Transitive triangle tilings in oriented graphs	The paper presents a proof of a theorem similar to the classical theorem of Corrádi and Hajnal. It states that for any n≥n0, where n0 is a fixed number, if an oriented graph on n vertices has every vertex with an indegree and outdegree of at least 7n/18, then it contains a perfect transitive triangle tiling. This means that the graph can be covered by vertex-disjoint transitive triangles. The result is proven to be optimal, as there exists an oriented graph without a perfect transitive triangle tiling for every n≥n0 with indegree and outdegree at least ⌈7n/18⌉−1 for each vertex.
411872	41187242	Continuous matrix approximation on distributed data	This paper addresses the challenge of tracking and approximating data matrices in a streaming fashion, particularly in the case of multiple distributed sites receiving a stream of data. The goal is to track an ε-approximation to the norm of the matrix along any direction. The paper presents algorithms that maintain a smaller matrix B as an approximation to the distributed streaming matrix A, with the property that for any unit vector x, the difference between the norms of Ax and Bx is within the limit of ε||A||2F. These algorithms work in a streaming fashion and have low communication requirements, making them suitable for distributed computation. The best method presented in the paper is deterministic and uses only O((m/ε) log(βN)) communication, where N is the size of the stream and β is an upper bound on the squared norm of any row of the matrix. Real-world experiments with large datasets demonstrate the effectiveness of these algorithms.
411872	41187239	Small synopses for group-by query verification on outsourced data streams	Data outsourcing is a popular solution for businesses dealing with large amounts of data in data stream applications. This involves the data owner outsourcing streaming data to third-party servers, which handle queries from clients on their behalf. However, trust issues arise in this model, especially when it comes to ensuring the accuracy of query results. Previous solutions relied on cryptographic techniques, but they were limited to specific types of queries. In this article, the focus is on “GROUP BY, SUM” queries, for which new solutions have been developed using algebraic and probabilistic techniques. These solutions produce a small synopsis of the true query result, which can be used to verify the correctness of the server’s response with a low probability of failure. The synopsis can also be adjusted to allow for some errors, enabling semantic load shedding on the server. The article concludes with an empirical evaluation of these techniques using real network traffic.
411873	411873110	A Fast Reduced Kernel Extreme Learning Machine.	This paper introduces a new algorithm, RKELM, which is a fast and accurate kernel-based supervised method. Unlike traditional approaches, which use iterative methods to identify support or weight vectors, RKELM randomly selects a subset of data samples as support vectors. This leads to significant cost savings in the training process, particularly for large datasets. The algorithm is based on rigorous proof of universal learning using a reduced kernel-based SLFN. The authors demonstrate through experiments on various real-world applications that RKELM performs competitively with traditional methods, such as SVM and LS-SVM, while requiring much less computational effort. 
411873	411873134	Learning word dependencies in text by means of a deep recurrent belief network.	The authors propose a deep recurrent belief network with distributed time delays for learning multivariate Gaussians. They address the challenge of learning long time delays in deep belief networks by using Gaussian networks with time-delays to initialize the weights of hidden neurons. This allows for the hierarchical learning of long delays from shorter delays. Unlike previous works, they use Markov Chain Monte Carlo to determine the initial weights of each hidden layer, resulting in a deep model that can hierarchically model time-delayed network motifs. They validate this framework by modeling word dependencies in text and the dynamic movements of basketball players, achieving significant improvements in accuracy compared to existing methods.
411874	41187496	DynamicMR: A Dynamic Slot Allocation Optimization Framework for MapReduce Clusters	MapReduce is a popular computing method used for large-scale data processing in cloud computing. However, the traditional slot-based MapReduce system, such as Hadoop MRv1, can have performance issues due to its inefficient allocation of resources. To address this, a paper proposes a new technique called Dynamic Hadoop SlotAllocation, which allows for reallocation of slots between map and reduce tasks. Additionally, the paper suggests Speculative Execution Performance Balancing to balance performance and efficiency, and Slot PreScheduling to improve data locality without affecting fairness. Combining these techniques forms a new slot allocation system called DynamicMR, which has been shown to improve performance by up to 46-115% for single jobs and 49-112% for multiple jobs. Experimental results also show that DynamicMR outperforms YARN by 2-9% for multiple jobs.
411874	41187442	Long-term resource fairness: towards economic fairness on pay-as-you-use computing systems	Fair resource allocation is crucial for shared computing systems, but existing frameworks like MLRF are not suitable for pay-as-you-use computing. To solve this issue, LTRF is proposed as a novel resource allocation mechanism. LTRF has several desirable properties, including incentivizing clients to share resources through group-buying, submit non-trivial workloads, and yield unneeded resources to others. It also ensures resource-as-you-pay fairness and is strategy-proof. LTRF has been implemented in YARN as LTYARN and has shown to have better resource fairness compared to other fair schedulers.
411875	41187556	Boosting Face Retrieval by using Relevant Set Correlation Clustering	The article presents a method for improving the performance of face retrieval in news videos by using a relevant-set correlation (RSC) clustering model. This involves retrieving faces of a specific person by searching for their name in associated transcripts, organizing the faces into clusters using RSC, and only presenting representative faces to the user. This approach results in a significant increase in retrieval performance, as only a small number of relevant faces are returned. The RSC model has two main contributions: it can automatically determine the number of clusters and discard irrelevant faces, and high precision clusters can be used for feature selection through linear discriminant analysis (LDA). Experiments on a dataset showed that this method outperforms other existing methods for face retrieval.
411875	41187553	ciForager: Incrementally discovering regions of correlated change in evolving graphs	Data mining techniques for understanding evolving graphs have become increasingly important in various applications such as computer networks, multiplayer games, and medical imaging. The discovery of compact subgraphs that change in a similar manner, known as regions of correlated change, is a key problem in analyzing evolving graphs. However, previous methods for identifying these regions have limited scalability. In this paper, a new algorithm called ciForager is introduced, which uses incremental techniques and Voronoi representations to efficiently detect change and determine distance. Experimental results show that ciForager can achieve significant speedups over previous approaches, making it possible to analyze extremely large graphs such as the Internet's BGP routing topology. 
411876	41187645	Unsupervised change analysis using supervised learning	The authors propose a new problem called change analysis and a method for solving it. Unlike existing methods, which only detect changes, the goal of change analysis is to find the explanation for the changes. The proposed approach uses supervised learning, where a classifier is trained using a hypothetical label to interpret the changes. This approach has shown promising results in both change analysis and concept drift analysis when tested on real data. 
411876	41187660	Finding itemset-sharing patterns in a large itemset-associated graph	Itemset mining and graph mining are important areas in data mining with applications in biology, marketing, and social network analysis. However, most existing studies focus on only one of these areas and few have combined both. This paper introduces a new problem called itemset-sharing subgraph (ISS) set enumeration, which aims to find sets of subgraphs with common itemsets in a large graph where each vertex has an associated itemset. This problem has potential applications in drug discovery and marketing analysis. The paper proposes an efficient algorithm, ROBIN, for finding ISS sets in graphs, which can handle networks with over one million edges and find biologically interesting patterns. The algorithm is also applied to a citation network and successfully identifies collaborative research works.
411877	4118770	Characterizing causal action theories and their implementations in answer set programming.	In this article, the authors introduce a simple language for writing causal action theories and outline certain properties that the state transition models of these theories should possess. They then explore how these theories can be embedded into other action formalisms and implemented using logic programs with answer set semantics. The authors propose the concept of "permissible translations" from the causal action theories to logic programs, and identify two sets of properties that these translations must satisfy. They prove that these sets of conditions are minimal and that removing any condition would result in multiple possible mappings. The authors also show that these two sets of conditions correspond to two well-known action languages, B and C, and provide a new way of comparing and evaluating different action languages. They suggest that this approach could also be used to define new action formalisms based on different sets of properties. 
411877	4118776	Characterizing Causal Action Theories and Their Implementations in Answer Set Programming: Action Languages B, C, and Beyond.	This article discusses a simple language for writing causal action theories and outlines certain properties for their state transition models. It then explores how these theories can be incorporated into other action formalisms and implemented using logic programs with answer set semantics. The concept of "permissible translations" is introduced, which involves converting the causal action theories into logic programs. The authors identify two sets of properties and prove that there is only one permissible translation, under strong equivalence, that can satisfy all properties in each set. This approach offers a new way to understand and compare action languages, and could potentially lead to the development of new action formalisms based on different sets of properties.
411878	41187834	Interactive cross and multimodal biomedical image retrieval based on automatic region-of-interest (ROI) identification and classification.	Biomedical articles often use annotation markers, such as arrows, letters, or symbols, to highlight important regions within figures and illustrations. These annotations are then correlated with concepts in the caption text or figure citations, creating a bridge between visual characteristics and their semantic interpretation. This allows for the extraction of relevant regions and aids in semantic search without knowing specific keywords or visual patterns. To improve biomedical image retrieval, a method using a combination of rule-based and statistical techniques localizes and recognizes annotations, which are then annotated for classification using biomedical concepts. This allows for both perceptual and conceptual search, validated through experiments on a dataset of thoracic CT scans.
411878	41187819	Classification of CT Figures in Biomedical Articles Based on Body Segments	Figure categorization plays a crucial role in enhancing the user experience in retrieving biomedical articles. One effective approach to improve retrieval performance is by categorizing figures into different modalities. In this study, the authors expanded upon their previous hierarchical classification method by adding body segment classification for CT images, such as head, abdomen, pelvis, or thorax. They extracted a wide range of features and used a multi-class SVM classifier, with feature selection to reduce the vector length. The proposed method was evaluated on a dataset of 2465 figures from open access biomedical articles, achieving an accuracy of over 90%. This demonstrates its potential to be a valuable component in biomedical document retrieval systems like NLM's OpenI.
411879	41187947	BitDew: A programmable environment for large-scale data management and distribution	Desktop Grids are used to efficiently run resource-demanding distributed applications by utilizing idle desktop PC's over multiple networks. However, data management in such large-scale, dynamic, and distributed Grids has been overlooked, resulting in ad-hoc solutions. The BitDew framework addresses this problem by providing a programmable environment for automatic and transparent data management on computational Desktop Grids. BitDew uses meta-data to drive key data management operations and its runtime environment integrates P2P components for data catalog and distribution. Through examples, the paper explains how BitDew can be utilized by application programmers and users. The performance evaluation shows that BitDew offers scalability, performance, and fault tolerance with minimal programming effort.
411879	41187930	BitDew: A data management and distribution service with multi-protocol file transfer and metadata abstraction	 Desktop Grids utilize unused resources from multiple desktop PCs connected through LANs or the Internet to run complex distributed applications. However, there has been little focus on managing the large amounts of data required for these applications in such a dynamic and distributed environment. The BitDew framework proposes a solution for this issue by providing a programmable interface for automatic data management. This framework integrates various protocols and services, such as Distributed Hash Tables and P2P components, to handle data lifecycle, distribution, placement, replication, and fault tolerance. The BitDew runtime environment is flexible and can support different file transfer protocols, including HTTP, FTP, and BitTorrent. Performance evaluations show that BitDew offers scalability, fault tolerance, and performance while maintaining a high level of abstraction and transparency for the user. 
411880	41188077	Fault Tolerance for PetaScale Systems: Current Knowledge, Challenges and Opportunities	The emergence of PetaScale systems has sparked renewed interest in effectively managing failures and ensuring the successful completion of large applications. This talk will present existing results for key mechanisms of fault tolerance in HPC platforms, which are largely derived from distributed system theory and have received significant attention in the past decade. However, these approaches may not be suitable for the evolving challenges of large scale systems. Therefore, there is a need for new approaches, such as incorporating dedicated fault tolerance hardware or relaxing constraints from traditional distributed system theory. The talk will discuss these opportunities and their limitations.
411880	41188016	Toward Exascale Resilience	Resilience has become a major concern for high-performance computing (HPC) systems, especially for large petascale and future exascale systems. These systems use millions of CPU cores and are expected to experience frequent faults. The current approach of automatic or application-level checkpointing and restarting will not be effective due to the large system size and long restart times. This presents a challenge for the HPC community to find new approaches to ensure applications can run until their normal termination despite the unstable nature of exascale systems. This white paper discusses the motivations, observations, and research issues in various areas of HPC, such as applications, programming models, distributed systems, and system management, in order to find solutions to this problem within the next five to six years. 
411881	41188133	An Evaluation of Dynamic Mobility Anchoring	The article discusses the limitations of current centralized mobility support approaches in IP based mobility schemes and cellular networks. To address these issues, the authors propose a new distributed and dynamic approach called Dynamic Mobility Anchoring (DMA) for managing mobility at the access node level. The effectiveness of DMA is evaluated through simulations and compared to traditional centralized schemes such as Mobile IP. The results show that DMA has promising benefits in terms of handover and traffic delays for real-time traffic delivery. Future work will focus on adapting DMA to different networking environments.
411881	41188136	Host-based distributed mobility management support protocol for IPv6 mobile networks	The rapid increase in mobile Internet usage has led to changes in mobile network architectures. To handle this growth, mobile networks are being flattened, requiring the adoption of existing IP mobility support protocols. A new host-based distributed mobility management protocol has been developed to support the evolution of mobile network architectures. This protocol distributes mobility anchors at the access network level, allowing a mobile node to maintain previous IP addresses while changing its point of attachment to the Internet. The paper explains the protocol's operation and compares its performance with Mobile IPv6 in terms of handover latency and throughput.
411882	41188215	Fast nonlinear autocorrelation algorithm for source separation	ICA and BSS have been commonly used for pattern recognition, but they rely on the statistical properties of the original sources or components. In this paper, a new algorithm is proposed that maximizes the nonlinear autocorrelation of source signals, resulting in a faster and more efficient fixed-point algorithm for BSS. The convergence property of this algorithm is studied and it is shown to have a quadratic convergence speed. Simulations using artificial signals and real-world applications demonstrate the effectiveness of this approach. Overall, the proposed method offers an efficient solution for BSS in pattern recognition problems.
411882	4118828	Blind Source Separation Using Quadratic form Innovation	Blind source separation (BSS) is a widely used data analysis technique that has various applications. One of the well-known methods for BSS is independent component analysis (ICA), which utilizes the non-Gaussianity of original sources. In this paper, a new BSS method is proposed based on a statistical property called the quadratic form innovation of original sources. This property includes linear and energy predictability as specific cases. The proposed method uses a gradient learning algorithm to minimize a loss function of the quadratic form innovation and its stability is also analyzed. Simulation results show the effectiveness of this method.
411883	41188346	Requirements and compliance in legal systems: a logic approach	The article discusses the similarities between the concepts of requirements and implementation in normative systems, such as law, and those in software engineering. It also explores the similarities between the concepts of compliance and conformance in these two areas. The use of a logic analyzer, specifically Alloy, is proposed as a tool for verifying legal compliance by checking consistency between legal and enterprise requirements. This approach is demonstrated through examples from privacy law and financial reporting law. Overall, the article highlights the potential for applying techniques from software engineering to improve the understanding and implementation of legal requirements.
411883	41188310	Secrecy UML method for model transformations	This paper discusses the development of secrecy models using transformation and formal validation in an enterprise setting. The process involves collaboration between policy makers and implementers, with policy makers providing governance requirements and implementers translating them into computer-executable terms. However, this process is prone to errors and can compromise the security of the enterprise. The paper proposes a formal transformation method using UML and the Alloy analyzer to ensure property preservation and detect potential threats. The security officer plays a crucial role in guaranteeing business continuity and enforcing the security policy through secrecy models. The paper demonstrates how this method can be used to construct and validate different types of secrecy models and prevent unpreserved properties. 
411884	41188423	On the value of coordination in network design	The article discusses network design games where self-interested agents must purchase links to form a network. Shapley cost sharing mechanisms are used to fairly split the cost among agents. The price of anarchy, which measures the worst-case performance in terms of cost, is high in these games. Research has shifted to evaluating the price of stability, which measures the cost of the best Nash equilibrium compared to the optimal solution. The concept of strong Nash equilibria, introduced by Aumann, is studied and shown to improve the quality of solutions through coordination among agents. Bounds for unweighted and weighted games in both directed and undirected graphs are developed, with tight or nearly tight results for many scenarios. The first super-constant lower bound for the price of stability is also proven for standard equilibria without coordination. This shows that coordinated equilibria have similar performance to the best equilibria without coordination in most settings.
411884	4118846	Dynamic TCP Acknowledgment: Penalizing Long Delays	The paper discusses the problem of acknowledging data packets sent through a TCP connection. Previous research has focused on minimizing the number of acknowledgments sent and the delays incurred for all packets. A deterministic algorithm with a competitive ratio of 2 was presented, while a randomized algorithm with a competitive ratio of 1.58 was developed. The paper introduces a new objective function that minimizes the sum of acknowledgments and the maximum delay for any packet, which is useful for interactive data transfer. A deterministic algorithm with a competitive ratio of 1.644 is proposed, and it is proven that no deterministic algorithm can have a smaller competitiveness. The paper also explores a generalized objective function with tight upper and lower bounds on the competitiveness for deterministic algorithms. For randomized algorithms, a lower bound of 1.324 is shown for the first objective function and 1.225 for the generalized function.
411885	41188572	Delay estimation of a user-preferred content distribution scheme in disruption tolerant networks	This article discusses disruption tolerant networks (DTN) that involve mobile nodes being carried by humans. These nodes subscribe to a content distribution service and share their content with others who have similar interests. The probability of a node subscribing to a content and the distribution of inter-contact times are known. The article examines both closed-form expressions and asymptotic approximations to determine the expected message delay and number of message copies at the time of delivery. These results are validated through simulations using various mobility models.
411885	41188589	From popularity prediction to ranking online news.	The rise of online news articles has captured the attention of many Internet users, especially on mobile devices, and are widely shared on social media. This has led to a growing interest in predicting which articles will become popular. This falls under the field of content popularity prediction and has implications for online advertising and content distribution. In this paper, the authors focus on predicting the popularity of news articles based on user comments, using a ranking approach. They test their method on data from two major news sites in France and the Netherlands and find that predicting popularity can effectively rank articles, making it a useful tool for automatic online news ranking.
411886	41188625	Lightness, heaviness and gravity	The gravity g(H,H) of a graph H is the largest number of vertices that can be found in every subgraph of a larger graph G, where each subgraph is isomorphic to H and has at least a given degree m. This concept is studied in relation to different families of plane graphs, and new types of graphs called almost-light and absolutely heavy are introduced and examined. The paper ends with some unresolved questions for further research.
411886	41188666	Construction of Large Graphs with No Optimal Surjective L(2,1)-Labelings	An L(2,1)-labeling is a way of assigning labels to the vertices of a graph G, such that adjacent vertices have labels that differ by at least two and vertices at a distance of two have labels that differ by at least one. The smallest possible value of K for which an L(2,1)-labeling exists is called lambda(G), and the minimum number of unused labels (holes) in an optimal labeling is denoted by rho(G). A previous conjecture stated that if rho(G) is equal to the maximum degree of G, and G is connected, then the order (number of vertices) of G must be at most Delta(Delta + 1). However, this has been disproven by the construction of graphs with rho(G) = Delta and order approximately Delta3/4.
411887	41188730	Poster session: ASETS: A self-managing transaction scheduler	The success of Web-database applications relies heavily on user satisfaction, which can be measured through expected response time or delay. Due to the unpredictable nature of web user behavior, user requests are modeled as transactions with soft-deadlines. In such scenarios, the average tardiness is a more effective metric than the hit ratio. To address this, a new adaptive self-managing algorithm called ASETS is proposed and evaluated. ASETS prioritizes resources based on varying workloads to maintain user satisfaction. Results show that ASETS outperforms other optimal algorithms for under and over utilization system conditions. 
411887	41188736	Freshness-Aware Scheduling of Continuous Queries in the Dynamic Web	The increasing demand for new, active services on the Web has led to the need for improved performance and efficiency in Web servers. One such service is the processing of continuous queries, which can help personalize individual user's web pages. To address this, a new scheduling policy called Freshness-Aware Scheduling of Multiple Continuous Queries (FAS-MCQ) has been proposed. This policy aims to maximize the freshness of output data streams and improve the quality of service for these new services. It prioritizes continuous queries based on their properties and the variability of input update streams, resulting in a 50% increase in freshness compared to existing scheduling policies. 
411888	41188870	Active Database Rules with Transaction-Conscious Stable-Model Semantics	Active databases face a major challenge in defining the semantics of their rules due to the lack of a formal framework and the use of ad-hoc operational semantics in different systems. This makes it difficult to predict the behavior of rule sets and ensure their termination, hindering the application of this technology in critical areas. This paper presents a durable change semantics for active database rules, combining elements from Starburst, Postgres, and Heraclitus to create a formal logic-based model that can be efficiently implemented and solves the non-termination problem. This approach aims to improve the applicability of active databases in important application areas.
411888	41188878	The Dynamics of Active Database Rules: Models and Refinements	Active databases face a significant challenge when it comes to semantics, as there is currently no formal framework for defining the abstract meaning of active rules. Additionally, existing systems have different ad-hoc operational semantics, making it difficult to predict how a set of rules will behave at run-time. This limits the applicability of active databases in critical applications, as ensuring rule termination is a major research issue. However, a new durable change semantics has been introduced that improves upon existing notions of deferred activation and incorporates concepts from other databases. This transaction-oriented semantics has a formal logic-based model, efficient implementation, and solves the problem of non-termination. 
411889	41188949	Quantifying the Impact of Input Data Sets on Program Behavior and its Applications	Workload composition is crucial in the design of a microprocessor. Selecting appropriate benchmarks and input data sets is a challenge due to limitations on simulation time and instruction counts. To efficiently explore the workload space, statistical data analysis techniques like principal components analysis (PCA) and cluster analysis are used. This allows for the identification of different input data sets for a given benchmark, measuring the distance between program-input pairs to understand their behavioral differences, and selecting representative input data sets. This approach is validated by showing that program-input pairs with a close distance exhibit similar behavior. The ultimate goal is to select a limited set of representative benchmark-input pairs that cover the entire workload space. This methodology can also be used to analyze the impact of input data sets on program behavior and evaluate the representativeness of sampled traces.
411889	411889156	Using cycle stacks to understand scaling bottlenecks in multi-threaded workloads	This paper introduces a method for evaluating parallel performance by creating cycle stacks. These stacks provide insight into potential optimization opportunities by showing how different components of the cycles scale with increasing core counts and input data set sizes. The authors present case studies demonstrating the application of this method and then extend it to analyze sets of parallel workloads using statistical data analysis. They perform a workload characterization on three benchmark suites and find that they cover similar areas in the workload space, but the scaling behavior is highly dependent on the benchmark, input scaling, and machine configuration. 
411890	41189070	Compilation and virtualization in the HiPEAC vision	This paper presents the HiPEAC vision of embedded virtualization, which has been developed over two years of discussions among HiPEAC cluster members. The vision integrates system and process virtualization into a unified layer for embedded systems, providing solutions for consolidation, performance optimization, software engineering, and legacy hardware components. Four adoption requirements are identified: real-time execution support, low performance overhead, virtualization of accelerator cores, and trustworthiness. Four research challenges are also defined: full virtualization of heterogeneous multi-core platforms, portable performance for heterogeneous multi-cores, virtual machine management interfaces, and standards for embedded virtualization. Overall, this vision aims to improve the efficiency and flexibility of virtualization in embedded systems.
411890	411890106	Towards an Extensible Context Ontology for Ambient Intelligence	The creation of an Ambient Intelligence environment relies on applications having access to context information in a broad and comprehensive manner. This requires the assembly of various types of information to form a representation of the device's context. To ensure interoperability in this environment, a common understanding of context terminology is essential across all devices. The proposed solution is an adaptable and extensible context ontology that can be used in various computing infrastructures, from small embedded devices to high-end service platforms. This ontology addresses several challenges in Ambient Intelligence, such as application adaptation, automatic code generation and code mobility, and customized user interfaces for specific devices. 
411891	41189138	LMIRA: Large Margin Instance Reduction Algorithm.	Instance-based learning involves using a training set to classify new instances. However, not all information in the training set is useful and irrelevant instances can be discarded through instance reduction. This reduces the time for classification and training. The proposed Large Margin Instance Reduction Algorithm (LMIRA) removes non-border instances and keeps border ones. The instance reduction process is formulated as a constrained binary optimization problem and solved using a filled function algorithm. Storing too many instances can be problematic and LMIRA aims to select the most representative instances while maintaining high accuracy and reduction rates. The performance of LMIRA has been evaluated on real world data sets and compared to other methods, showing its superiority in terms of classification accuracy and reduction percentage.
411891	41189143	Unsupervised kernel least mean square algorithm for solving ordinary differential equations	This paper introduces a new method for solving ordinary differential equations (ODEs) using an unsupervised version of the kernel least mean square (KLMS) algorithm. Unlike other approaches, this method does not require a desired signal to be determined by the user and the output is generated through iterative processes. The method has advantages such as simple implementation, fast convergence, and low error. It utilizes the KLMS algorithm to estimate the answer to the ODE by writing a trial solution as a sum of two parts and training the second part using the KLMS algorithm. The accuracy of the method is demonstrated through solving various problems and its convergence is analyzed by changing parameters and kernel functions. The method is also compared to a neuro-fuzzy approach. 
411892	41189210	TapList: testing the effectiveness of tap access to the n-best list in pen-based text entry	Handwriting recognition is prone to errors, so managing these errors is important for user interfaces. In an experiment, a technique called TapList was tested, where a stylus tap can switch a recognized character to the next item on a list provided by a character recognizer. The goal was to see if this technique affects text entry performance under time constraints. The results showed that TapList did not significantly decrease performance compared to traditional error correction methods. This suggests that TapList could improve performance if the cost of re-entering misrecognized text is higher than with the fast unistroke alphabet used in the experiment. More research is needed to compare TapList to other techniques. 
411892	41189213	Motor efficiency of text entry in a combination of a soft keyboard and unistrokes	UniKeyb is a text entry system that allows users to enter characters by landing the stylus on a key on a soft keyboard. It also offers the option to enter a second character by performing a unistroke gesture before lifting the stylus. A simulation showed a 20% speed advantage for expert users, and an experiment confirmed that the necessary learning is possible. While UniKeyb may outperform traditional soft keyboards in expert use, becoming an expert requires learning to chunk writing into two-character chunks. However, the use of Unistrokes is not mandatory, making for a smooth transition to this writing style.
411893	4118935	Truthful Online Auction For Cloud Instance Subletting	This paper discusses the issue of instance underutilization in the Infrastructure as a Service (IaaS) market, where users are constantly scaling up their cloud instances to meet growing demands. The limited billing options offered by major cloud providers and the constantly changing demands have led to instances being underutilized in both time and space. To address this issue, the paper proposes an instance subletting service where users can lease their underutilized instances to others for a specific period of time. The paper highlights the challenges of implementing such a service and presents an online auction mechanism that ensures truthful and rational decision-making while maximizing cost and social welfare. Simulations show that this mechanism can significantly improve performance in terms of cost and social welfare.
411893	41189369	Measurement and utilization of customer-provided resources for cloud computing	Cloud computing has become a popular way to provide resources efficiently. Major companies like Amazon, Google, and Microsoft offer powerful and cost-effective cloud platforms. However, these platforms primarily use data centers and do not take advantage of the resources available to customers. A new platform called SpotCloud allows customers to sell their private resources to offer cloud services. While not as large or reliable as data centers, SpotCloud offers flexible services at lower costs for short-term and customized tasks. The challenge with SpotCloud is the diverse range of instances available, making it difficult for customers to select the best option. To address this, a model has been proposed to recommend instances based on performance and cost, which has been shown to be effective in both analysis and real-world experiments.
411894	41189417	An analysis of reduced error pruning	Top-down decision tree learning can be hindered by the pruning phase, resulting in larger trees without improved accuracy. Reduced Error Pruning is a commonly used algorithm to address this issue, but its effectiveness in different settings has not been fully analyzed. This paper examines the properties of Reduced Error Pruning in different scenarios, including situations where the pruning examples are independent of the decision tree. The analysis shows that the probability of pruning a node due to pure noise decreases exponentially as the tree grows. The algorithm is also analyzed under the assumption of uniform distribution of examples, providing insights into the number of subtrees that are pruned. This paper offers a better understanding of Reduced Error Pruning and includes previously overlooked empty subtrees in the analysis.
411894	4118946	Multi-target regression with rule ensembles	Decision rules are becoming increasingly popular in solving various problems, especially when interpretability is important. However, when tackling multiple related target attributes, the traditional covering algorithm falls short in regression problems. A better solution is the rule ensemble approach, which uses a collection of decision trees that are then optimized to select the best subset of rules with respective weights. The FIRE algorithm is introduced for multi-target regression problems, which incorporates the rule ensemble approach and further improves accuracy by adding simple linear functions. Results show that rule ensembles are highly accurate, even outperforming multi-target regression trees, and are significantly more concise than random forests. In fact, it is possible to create compact rule sets that are smaller than a single regression tree but still maintain high accuracy.
411895	41189538	On the well-behavedness of important attribute evaluation functions	 Well-behaved evaluation functions simplify and improve the handling of numerical attributes by focusing on boundary points when searching for the optimal partition. This applies to both binary and multisplits, as long as the function is cumulative and well-behaved. This class includes convex evaluation functions, making it a widely applicable superclass. However, some commonly used functions, such as C4.5's default gain ratio, have limitations when it comes to numerical attributes. While not convex, gain ratio is still well-behaved for binary partitioning but struggles with higher arity partitioning. Through experiments, a simple cumulative rectification method is shown to outperform gain ratio in handling numerical attributes.
411895	4118959	Linear-Time Preprocessing in Optimal Numerical Range Partitioning	This paper discusses the efficient optimization of numerical value ranges using segment borders. It is shown that for commonly used attribute evaluation functions in classification learning, only a subset of the boundary points need to be considered. These segment borders can be found in a linear-time preprocessing step. The paper also explores the use of segment borders for optimizing any convex evaluation function, and finds that for strictly convex functions, examining all segment borders is necessary. The evaluation function Training Set Error is also studied, and it is found that preprocessing the data into even smaller cut point candidates, known as alternations, can reduce the number of candidates needed for optimal partition. Empirical testing on real-world data confirms the effectiveness of this method.
411896	41189658	Anomalous loss performance for mixed real-time and TCP traffic in routers with very small buffers	Recent discussions have focused on the appropriate size of buffers at core Internet routers. Studies have shown that smaller buffer sizes can still achieve acceptable TCP throughputs, which is a step towards implementing optical packet switched networks. However, these studies have not taken into account the increasing use of real-time traffic, which is a significant source of revenue for Internet service providers. In this paper, the authors investigate the interaction between real-time and TCP traffic in small buffers and discover that in a certain range, larger buffer sizes actually result in increased losses for real-time traffic. They demonstrate this through simulations and models, and also highlight factors that affect this anomaly. This is the first study to consider the effects of real-time traffic in small buffers, providing important insights for router manufacturers and network operators.
411896	41189643	Routers With Very Small Buffers: Anomalous Loss Performance for Mixed Real-Time and TCP Traffic	Researchers have been discussing the necessary buffer size for core Internet routers, with a focus on TCP throughput. Recent arguments, based on theory and experimentation, suggest that a small number of packets in the buffer is sufficient for acceptable link utilization. However, when real-time traffic is introduced, an anomaly occurs: larger buffers actually lead to higher losses for real-time traffic. The paper presents simulations and an analytical model to explain this phenomenon, and discusses how various factors such as traffic characteristics and link rates can affect it. This study has implications for all-optical packet router designs and network service providers, warning against investing in larger buffers in the regime of a few tens of packets.
411897	41189762	Unifying Byzantine Consensus Algorithms with Weak Interactive Consistency	The paper discusses the problem of achieving consensus in a partially synchronous system with Byzantine processes. There are two types of Byzantine faults - authenticated (where messages can be signed) and non-authenticated (where the sender's identity is known). The paper introduces a new abstraction called weak interactive consistency (WIC) that can be implemented with or without signatures. This abstraction is applied to two well-known Byzantine consensus algorithms, Castro-Liskov PBFT (without signatures) and Martin-Alvisi FaB Paxos (with signatures), showcasing the power of WIC in providing a concise representation of these algorithms.
411897	41189734	Bounded Delay in Byzantine-Tolerant State Machine Replication	 The paper introduces a new state machine replication protocol, BFT-Mencius, designed for partially synchronous systems with Byzantine faults. This algorithm ensures that updates made by correct processes have a limited delay, even in the presence of faulty processes. BFT-Mencius relies on a new communication method, ATAB, and does not utilize signatures. The protocol is evaluated in cluster environments and demonstrates promising results in terms of latency and throughput, performing similarly to other established algorithms like PBFT and Spinning in normal conditions and surpassing them in the presence of Byzantine attacks.
411898	411898302	A Decentralized and Cooperative Workflow Scheduling Algorithm	In current workflow scheduling methods, there is no collaboration among distributed brokers, causing conflicts in schedules. To address this issue, a decentralized and cooperative algorithm is proposed in this paper. It uses a Peer-to-Peer (P2P) coordination space to coordinate schedules among brokers in the Grid. This approach is completely decentralized, with no central point of contact. Key functionalities such as resource discovery and scheduling coordination are delegated to the P2P coordination space. This approach aims to improve performance, scalability, and autonomy for users. Extensive simulations show the feasibility of this approach.
411898	411898374	Coordinated load management in Peer-to-Peer coupled federated grid systems	The paper proposes a coordinated load management protocol for Peer-to-Peer coupled federated Grid systems. The participants, such as resource providers and consumers, work together to create a coordinated federation. The protocol uses a Distributed Hash Table (DHT) space with a logical spatial index for effective management of coordination objects. This DHT-based space acts as a decentralized blackboard system. The protocol has a message complexity that is logarithmic to the number of nodes in the system, which is better than existing broadcast-based protocols. It can be applied to efficiently coordinate resource brokering services in distributed computing systems like grids and PlanetLab. The lack of coordination in existing systems can lead to inefficient application schedules and resource utilization, which can be improved by implementing the proposed protocol. The feasibility of the protocol is demonstrated through simulations.
411899	41189925	Solving a real-world train-unit assignment problem	The problem at hand is assigning train units to timetabled train trips in a regional area while fulfilling the required number of passenger seats. This is made difficult by the large number of distinct train-unit types and the high number of trips that need to be covered. The existing solution used by practitioners can be improved upon, but even finding a solution of the same value is challenging. To address this, the authors propose a heuristic approach based on an ILP formulation with strong seat requirement constraints. Real-world instances show the effectiveness of this approach.
411899	41189917	Modeling and Solving the Train Timetabling Problem	The train timetabling problem involves creating a timetable for trains that follows operational constraints and does not exceed track capacities. The problem focuses on a single, one-way track with intermediate stations and trains must stop for a minimum time at certain stations. Trains can only overtake each other at intermediate stations and a minimum time interval between trains is specified. The problem is formulated using a graph with nodes representing departures and arrivals at stations. An integer linear programming model is used to solve the problem, with a novel feature being that variables are only associated with nodes rather than arcs. A heuristic algorithm is used to solve the relaxed model, using dual information from Lagrangian multipliers. The results from real-world instances provided by Italian railway companies are reported.
411900	411900299	Lamarckian clonal selection algorithm with application	The paper introduces a new algorithm called Lamarckian Clonal Selection Algorithm (LCSA), which integrates Lamarckism and Immune Clonal Selection Theory. This algorithm incorporates the idea of Lamarckian evolution, which describes how organisms can evolve through learning, into the Standard Clonal Selection Algorithm (SCSA). Ten benchmark functions are used to test the performance of LCSA, and the impact of parameters is carefully studied. The results show that LCSA outperforms SCSA and other evolutionary algorithms in terms of robustness and convergence. This suggests that the incorporation of Lamarckian principles into the immune-inspired algorithm can improve its performance.
411900	41190038	A hybrid memetic algorithm for global optimization	MADM is a hybrid memetic algorithm designed for global optimization problems. It combines two meta-learning systems to improve both global and local exploration. The algorithm uses double mutation operators to guide the local learning process towards the global optimum, while also incorporating crossover and elitism selection operators for further enhancement. In experiments, MADM outperforms other improved evolutionary algorithms for numerical optimization problems and also shows success in clustering complex and non-separable datasets. Overall, MADM is an efficient and effective approach for tackling global optimization problems.
411901	41190112	Online Transfer Of Human Motion To Humanoids	Transferring motion from a human to a humanoid robot is crucial for creating robots that can easily be programmed and mimic human actions. This is known as the motion retargeting problem, which has been extensively studied and has existing solutions that use optimization methods and motion capture data. However, there is a growing interest in online motion transfer without using markers, which presents challenges for existing methods. To address this, a new approach using task space control theory is proposed, taking into account joint limits, velocity, and self-collision constraints. This method uses low dimensional human motion descriptors obtained through a vision-based key-point detection and tracking algorithm, without the need for markers or special equipment. The effectiveness of this approach is demonstrated through experiments on the Honda humanoid robot ASIMO.
411901	4119012	Task maps in humanoid robot manipulation	The paper proposes an integrative approach for a humanoid robot to solve the problem of reaching and grasping objects in a cluttered environment. This approach uses task maps, which represent the feasible grasps for an object, instead of a single end-effector goal position. The task maps are efficiently learned using the rapidly exploring random tree algorithm. The previously developed motion optimization scheme is also generalized to work with task maps and incorporates the robot's whole body controller. This approach results in a preference for grasps that are easy to reach, as it considers various criteria such as collision and joint limit avoidance, energy efficiency, etc. The effectiveness of this approach is demonstrated in two simulation scenarios with the humanoid robot ASIMO.
411902	4119022	Incorporating Weather Updates for Public Transportation Users of Recommendation Systems	This work introduces a system that enhances the capabilities of Yelp-like recommendation sites by allowing users to search for places based on travel-time via public transportation. It also takes into account weather conditions and adjusts recommendations accordingly. The use of public transport may require walking to and from specific locations, and the preferences for certain types of services may change based on weather. The system uses a model to predict a user's preferred mode of transport and incorporates weather data into the recommendation process. It also utilizes event-based modeling and existing tools like Google Maps Direction API and Open Weather Map API. A web application has been developed for both desktop and mobile platforms.
411902	41190249	Trajectory Data Reduction In Wireless Sensor Networks	This work focuses on finding a balance between energy cost and accuracy in tracking-based trajectory detection and representation in Wireless Sensor Networks (WSNs). The authors explore approaches used in Moving Objects Databases (MOD) and Computational Geometry (CG) communities and show that these can be adapted to save energy and increase the lifetime of WSNs. They develop distributed versions of three spatio-temporal data reduction approaches (Dead-Reckoning, Douglas-Peuker algorithm, and a CG-based optimal algorithm for polyline reduction) and examine different buffer management policies for individual tracking nodes. The study also investigates the potential benefits of combining different data-reduction approaches into "hybrid" methods for tracking an object's trajectory. Results from experiments show that these methodologies can significantly reduce energy costs and increase the lifetime of WSNs.
411903	41190331	Motion Trends Detection in Wireless Sensor Networks	The problem of detecting motion trends in Wireless Sensor Networks (WSN) is addressed, where tracking is done collaboratively among sensor nodes. In addition to determining single locations, applications may also need to detect properties along entire trajectories. However, sending location and time data to a central sink for property detection creates high communication overhead. To address this, an in-network distributed algorithm is proposed for efficiently detecting the Continuously Moving Towards predicate for a given destination. This destination can be a point or a region with a polygonal boundary. Experiments show that this approach is more efficient compared to the traditional brute-force method.
411903	41190351	Trajectory Data Reduction In Wireless Sensor Networks	This work deals with the challenge of balancing energy consumption and tracking accuracy in Wireless Sensor Networks (WSNs). By adapting approaches from Moving Objects Databases (MOD) and Computational Geometry (CG), the study shows that significant energy savings and longer network lifetime can be achieved. The researchers developed distributed versions of three spatio-temporal data reduction techniques - two heuristics (Dead-Reckoning and Douglas-Peuker) and a CG-based optimal algorithm for polyline reduction. They also explored different buffer management strategies for storing partial trajectory data. Furthermore, the potential benefits of combining these approaches into "hybrid" ones during tracking were investigated. The experiments proved that these methods can effectively reduce energy costs and increase network lifetime.
411904	41190429	Fast two-party secure computation with minimal assumptions	This content discusses the limitations of existing protocols for secure two-party computation and proposes a new protocol that overcomes these limitations. The proposed protocol is secure against malicious adversaries, requires minimal hardness assumption (only oblivious transfers), has minimal overhead complexity, and can be parallelized for efficient processing. To achieve these properties, the protocol uses novel solutions to address obstacles such as establishing authenticity of the generator's output, ensuring input consistency, and handling selective failure attacks. The implementation of this protocol also shows significant improvement over previous work that relied on specific number-theoretic assumptions, demonstrating that strong performance does not necessarily require specific algebraic assumptions. 
411904	4119041	Scaling ORAM for Secure Computation.	We created a new data structure called Distributed Oblivious Random Access Memory (DORAM) specifically for use in secure computation between two parties. Our design greatly improves upon previous constructions in terms of access time (up to ten times faster), memory overhead (one hundred times less), and initialization time (thousands of times faster). We were able to successfully use DORAM to handle large amounts of data (234 bytes) and perform operations on them in seconds, which was not possible before. Our DORAM is based on a new Function Secret Sharing scheme, which reduces the need for secure computation but increases the amount of local memory operations required. Despite this, our construction still outperforms other known methods for datasets larger than 32 KiB, and is also more efficient for applications such as stable matching and binary search.
411905	41190522	An Experimental Study of Phase Transitions in Matching	The difficulty of finding models for a predicate logic formula is a well-known problem, with complexity increasing exponentially as the number of variables increases. However, this complexity can vary greatly across different instances of the problem, as seen in recent research. Many hard problems have a specific point, known as a phase transition, where the complexity suddenly becomes much higher. In this study, the focus is on the problem of matching a Horn clause to a universe, in search of a model or proof of its non-existence. The emergence of a phase transition in this problem is found to be influenced by two factors: the structure of the clause and the structure of the universe. These findings have important implications for understanding and solving hard problems in predicate logic.
411905	41190553	Relational learning as search in a critical region	Machine learning is a popular technique that relies on the covering test to determine if a hypothesis covers training examples. This paper focuses on learning relational concepts, also known as inductive logic programming, and examines the success and computational cost of this type of learning. The presence of a phase transition in the covering test greatly affects relational learning, and three modern relational learners were tested on a variety of relational learning problems. The results show that the phase transition acts as an attractor for these learners, and there is also a failure region where no accurate hypothesis can be learned. Surprisingly, the probability of failure does not necessarily increase with the size of the target concept. The paper provides some explanations for these findings.
411906	41190633	Phase Transitions in Relational Learning	Relational learning faces a major limitation in verifying hypotheses on examples due to the complexity of the task. Recent research has shown that many difficult problems have a "phase transition," where there is a sudden increase in computational complexity with respect to some order parameter. This paper examines this issue by investigating artificial and real-world learning problems. The authors find that learners tend to generate hypotheses that lie exactly on the phase transition, but not all problems within this region are intractable. To address this problem, a Monte Carlo algorithm is proposed to estimate the likelihood of a matching problem exceeding a certain amount of computational resources. The implications of these findings for relational learning are also discussed.
411906	41190626	An Experimental Study of Phase Transitions in Matching	The complexity of finding models for a predicate logic formula is known to be exponential in the number of variables. However, recent research has shown that even when the number of variables is constant, there are significant differences in complexity for different problem instances. This is due to a narrow phase transition, where the complexity drastically increases in a certain range of an order parameter. This paper presents a study on the phase transition in the matching problem of a Horn clause to a universe, where the complexity is affected by two order parameters: the clause structure and the universe structure.
411907	41190719	Analysing interactive voice services	IVR services are becoming more common in automated telephone systems and VoiceXML is a popular language for creating these services. However, VoiceXML descriptions are complex and difficult to understand, making it challenging to check for errors. CRESS is a graphical notation that allows for a more abstract and language-independent way of describing IVR services. These descriptions can then be translated into LOTOS for automated analysis. While it is not practical to formally verify these specifications due to the infinite state space, CRESS and MUSTARD (a language for defining tests) can help identify potential issues and interactions among IVR features. By using these tools, confidence in IVR applications can be increased.
411907	4119076	A rigorous approach to orchestrating grid services	BPEL (Business Process Execution Language) was originally designed for web services, but it has also been successfully used to orchestrate grid services. This is made possible by using CRESS (Communication Representation Employing Systematic Specification) to describe the composition of grid services. Cress descriptions are then automatically translated into LOTOS (Language Of Temporal Ordering Specification) for interoperability and logical error checks before implementation. The MUSTARD (Multiple-Use Scenario Test and Refusal Description) technique is used to validate the generated specification against use case scenarios. The same CRESS descriptions are then converted into BPEL/WSDL code for practical implementation of the composed services. The orchestration of grid services is supported by the ACTIVEBEPEL engine, while the execution is done by Globus Toolkit 4. This approach allows for rigorous development and automated creation of orchestrated grid services, with the use of MUSTARD scenarios for evaluation.
411908	41190826	Multifaceted Exploitation of Metadata for Attribute Match Discovery in Information Integration	Automating the process of matching attributes for information integration is a difficult task, especially given the constantly changing nature of the Web. To address this challenge, a framework for using multiple facets of metadata is proposed. By gathering information from different aspects of metadata and using machine learning, confidence values are assigned to potential attribute matches. Experiments have shown promising results, indicating that when the different facets align, the matches are highly reliable. This framework offers a solution for the complex task of attribute matching in a dynamic web environment.
411908	41190859	Multilingual ontologies for cross-language information extraction and semantic search	The internet contains a wealth of valuable local information, but it is often written in a foreign language that non-local users cannot understand. To solve this issue, a system is proposed that allows users to search for information in one language while retrieving results in another. This is achieved through a set of multilingual extraction ontologies, which are grounded in each language and mapped through a central ontology. An early prototype has shown promising results in cross-language information extraction and semantic search, with a high level of accuracy in ontology-based query translation and extraction. This system simplifies the process of adding new languages, making it a feasible solution for this complex problem.
411909	41190949	Bundling three convex polygons to minimize area or perimeter	The problem of finding optimal translations for a set of k convex polygons in the plane is considered. The translations must result in disjoint copies of the polygons and minimize the area or perimeter of the convex hull of their union. For k=2, this problem can be solved in linear time, but for larger k, it has been proven to be NP-hard. However, for k=3, a decomposition of the translation space into O(n2) cells has been discovered, allowing for an O(n2)-time algorithm to find the optimal translations and minimize the area or perimeter of the convex hull. This is the first algorithm of its kind for k=3. 
411909	41190911	Maximum overlap of convex polytopes under translation	We have developed an algorithm for maximizing the overlap of two convex polytopes in \mathbb R^d for d≥3. This algorithm can find an overlap that is very close to the optimal value, with a translation that achieves it. The running time of this algorithm is O(n^{{\lfloor d/2 \rfloor}+1} \log^d n) for any desired accuracy ε, with a probability of at least 1−n−O(1). In the case of d=3, the running time can be improved to O(n log^3.5 n). Our analysis assumes a bounded incidence condition, which we ensure by randomly perturbing the input polytopes. The perturbation causes a small error ε, which can be reduced by decreasing the perturbation magnitude. Our algorithm can also find the maximum overlap of the perturbed polytopes, and all time complexity bounds are independent of ε.
411910	41191046	Face Recognition For Homeland Security: A Computational Intelligence Approach	Morphological Shared-Weight Neural Networks (MSNN) can be used to improve common access restriction points by identifying specific individuals of interest. These networks are trained for face recognition and have the ability to learn and encode the necessary feature extraction and classification abilities for identifying a specific face. MSNN has been proven effective in analyzing images with the target face in a group of faces, even when the target face is at different angles, lighting, or partially covered. Recent experiments have demonstrated the potential use of MSNN for watch-list scanning in access screening areas, allowing for quick and accurate identification of individuals passing through.
411910	411910118	Clustering ellipses for anomaly detection	The article discusses three problems that often arise in various applications: comparing, clustering, and merging ellipsoids. These problems are relevant in fields such as wireless sensor networks and patterned fabrics. The authors propose a theory that uses three measures of similarity to identify groups of similar ellipsoids in p-space. The theory is based on a dissimilarity image, created using the recursive iVAT algorithm, which can visually estimate the number of clusters and compare the three similarity measures. The authors also apply two clustering algorithms to two synthetic data sets and a real data set from a wireless sensor network. They conclude that focal distance is the best measure of elliptical similarity and that single linkage can successfully extract clusters.
411911	41191147	Hand Posture Classification and Recognition using the Modified Census Transform	Developing new techniques for human-computer interaction is a difficult task, and vision-based techniques have become increasingly popular due to their non-intrusive nature. Hands, being a natural device, have great potential for creating more intuitive interfaces. However, in order to use hands for interaction, they must be accurately recognized in images. In this paper, the authors propose using a successful approach from face detection for hand posture classification and recognition. The approach uses illumination-invariant features based on Modified Census Transform, and a simple linear classifier is trained using a set of feature lookup-tables. The experiments are conducted on a benchmark database for posture recognition and show promising results. Two protocols are defined for the experiments, and the authors provide results for both classification and recognition tasks. 
411911	4119113	A scalable formulation of probabilistic linear discriminant analysis: applied to face recognition.	The paper introduces a new scalable and exact solution for probabilistic linear discriminant analysis (PLDA), a model commonly used in face and speaker recognition. PLDA has been proven to be effective, but has a major drawback of requiring the inversion and storage of a large matrix during training, which becomes impractical with a large number of samples. Previous solutions either used an exact method which is not scalable, or a variational approximation. The authors present a scalable method that is equivalent to the previous exact solution, eliminating the need for a variational approximation. Experiments show the effectiveness of the proposed method in both labeled faces and a large database, showing improved performance with a higher number of training samples.
411912	41191228	Recognition of Genuine Smiles	This paper discusses the importance of automatic distinction between genuine and posed expressions for visual analysis of social signals. The authors propose a system that uses facial landmarking and tracking to extract features describing the dynamics of eyelid, cheek, and lip corner movements. These features are then fused to create a highly accurate smile classifier. The system is evaluated on a large database and compared to previous methods, showing significant improvement in smile classification. The authors also investigate the effects of age and gender on the system's performance and demonstrate the usefulness of their system in smile psychophysics. 
411912	41191233	Combining facial dynamics with appearance for age estimation.	Age estimation from facial images is a challenging task that traditionally relies on appearance features. However, this paper proposes using dynamic features, specifically a person's smile, in addition to appearance information. The approach is tested on a large database with 400 subjects and a new database with 324 subjects, both with an age range of 8 to 76. The proposed method is compared to existing appearance-based age estimation methods and is found to significantly improve their performance. A novel hierarchical age estimation architecture is also introduced, which further improves the accuracy by utilizing spontaneity information and gender-specific age estimation. These advancements demonstrate a 21% reduction in mean absolute error, pushing the boundaries of facial age estimation.
411913	4119136	Model-driven sketch reconstruction with structure-oriented retrieval.	Our proposed system allows users to convert a 2D sketch into a 3D sketch by utilizing existing shape models. The system utilizes a sketch-based shape retrieval method and a novel matching algorithm that takes into account both shape features and structure. After selecting a model from the retrieved options, users can use it as a 3D proxy to provide abstract 3D information. The sketch is then transformed into 3D geometry through back-projection and optimized using the Laplacian mesh deformation framework. Preliminary evaluations have shown that our system outperforms a state-of-the-art method and allows users to create impressive 3D forms from simple sketches, without requiring advanced drawing skills. 
411913	41191326	Multitouch Gestures for Constrained Transformation of 3D Objects	This paper discusses the use of 3D transformation widgets in 3D applications for precise manipulations of objects. These widgets have traditionally been designed for mouse-based systems and are not user-friendly for multitouch screens. The paper proposes a set of multitouch gestures that allow for seamless control of constraints and modes of transformation. These gestures do not require complex widgets and instead use candidate axes for visualization. The paper also introduces intuitive gestures for relative manipulations, such as snapping and borrowing axes from other objects. A preliminary evaluation shows that this technique is more effective than directly adapting standard transformation widgets to the tactile paradigm. 
411914	4119147	Computer vision for music identification	This paper discusses the use of computer vision techniques to address tasks in the audio domain, specifically the problem of music identification. The approach involves treating the spectrogram of a music clip as a 2D image and using pairwise boosting on Viola-Jones features to learn compact and discriminative local descriptors. During the query phase, a set of song snippets that match the noisy sample are retrieved and geometric verification is used to identify the most consistent song. The system has been implemented and tested, showing significant improvements in identifying music from short and distorted audio samples.
411914	41191419	Computer Vision for Music Identification: Video Demonstration	 reliably identify songs from real-world environments.This paper outlines a demonstration video for a music identification system that aims to accurately recognize songs from small, noisy audio samples. The system works by sending a few seconds of audio from a mobile phone to a music identification server and receiving a text message with the song title. The system is challenging due to noise interference and limited audio sample matching. To address this, the system uses computer vision techniques to analyze 2-D time-frequency representations of the audio, treating the problem as a sub-image retrieval task. The system employs pairwise boosting and geometric verification to efficiently identify the song from a large database. The system has been implemented and can accurately identify songs in real-world environments.
411915	41191563	Score-informed syllable segmentation for a cappella singing voice with convolutional neural networks.	The 18th International Society for Music Information Retrieval Conference, known as ISMIR 2017, was held in Suzhou, China from October 23-27, 2017. This conference focused on the intersection of music and technology, bringing together experts and researchers from around the world to discuss the latest advancements and innovations in the field. Presentations and discussions covered topics such as music analysis, music recommendation systems, and music perception. The conference provided a platform for collaboration and exchange of ideas between professionals in the music and technology industries.
411915	41191539	Metrical-accent Aware Vocal Onset Detection in Polyphonic Audio.	The ISMIR 2017 conference was held from October 23 to 27, 2017 in Suzhou, China. The conference, organized by the International Society for Music Information Retrieval, featured various presentations on the latest research in the field. Attendees had the opportunity to learn about advancements in music information retrieval and exchange ideas with experts from around the world. The conference aimed to promote collaboration and innovation in the field of music information retrieval. 
411916	41191630	Vision-based finger detection, tracking, and event identification techniques for multi-touch sensing and display systems.	This study introduces a cost-effective and efficient approach for finger detection, tracking, and event recognition in multi-touch applications. It utilizes a fast bright-blob segmentation technique and connected-component analysis to extract touch blobs from infrared lights captured by a video camera. A blob tracking and event recognition process is then applied to determine touch events and actions performed by users. This system is robust and adaptable to different lighting conditions and infrared noise. The blob tracking and event recognition process includes two phases: blob tracking and touch event identification. This system has been tested and proven successful in various environments and conditions. Overall, the study shows that this vision-based approach is effective for multi-touch sensing applications.
411916	41191668	Personal Based Authentication by Face Recognition	Authentication is a crucial issue in controlling systems, but traditional account-based methods are not foolproof as they can be easily guessed. With the increasing popularity of cameras, personal-based authentication using facial recognition has become possible for mobile users. This paper proposes a system called PBAS, which combines face image and password authentication. By using password information, the face recognition rate can be improved, which is an important benchmark for identification systems. The system was simulated using the PCA method on a self-constructed database and a subset of FERET, and the results showed a significant improvement compared to the PCA method without password integration.
411917	41191775	Training a reciprocal-sigmoid classifier by feature scaling-space	The paper introduces a new classifier, the reciprocal-sigmoid model, for pattern classification. It combines the theoretical advantages of linear machines and logistic regression, while addressing the issues of local minima and recursive search. To prevent overfitting, the classifier is trained using multiple samples of uniformly scaled pattern features. Empirical evaluation on synthetic data and 10-fold cross validations on 40 datasets show comparable performance to top classifiers in the literature, thanks to its use of reciprocal sigmoid for nonlinearity and bundled feature sets for smoothing training error. This makes the reciprocal-sigmoid model an effective and efficient classifier for pattern classification.
411917	41191747	A Robust Change-Point Detection Method by Eliminating Sparse Noises from Time Series	Singular Spectrum Transform (SST) is a popular technique used for change-point detection (CPD) in information security applications. However, its performance is limited by its vulnerability to large noisy observations. To address this issue, a combination of Robust Principal Component Analysis (RPCA) and SST is proposed. This method eliminates sparse large noises from observation trajectory matrices using RPCA and then uses the noise-free matrices to extract SST subspaces for CPD. Experiments on synthetic and real-world datasets show that this approach outperforms competing methods in terms of detection accuracy for time series with sparse large noises. Overall, this method provides a more robust solution for CPD in practical scenarios.
411918	41191839	A privacy-preserving smart parking system using an IoT elliptic curve based security platform.	Wireless Sensor Networks (WSNs) have evolved significantly since the initial concept of SmartDust was introduced 15 years ago, to the point where the Internet of Things (IoT) is now a reality. There are now various hardware and software options available for easy setup and use, and real-world applications utilizing large deployments of IoT devices are increasing. However, the wireless nature of communication and low-end capabilities of the devices pose security and privacy concerns that have not been adequately addressed. This paper proposes using Elliptic Curve Cryptography (ECC) as a more efficient and secure alternative to traditional public key cryptography, especially for constrained devices. A generic implementation of ECC is presented, which can run on various operating systems and devices. The paper also presents a solution for protecting user privacy in the Smart Parking application domain, using zero knowledge proofs (ZKP) in conjunction with ECC. The performance of the system is evaluated in a real-world IoT testbed, and the code is available as open source for developers looking to enhance security and privacy in their applications.
411918	41191862	A Sleep-Awake Protocol for Information Propagation in Smart Dust Networks	Smart Dust is a collection of tiny, self-sufficient computing and communication devices that work together to efficiently perform large sensing tasks. These devices, with limited energy and computing abilities, can be used for practical purposes such as detecting important events and transmitting data about them. This article focuses on developing algorithms for Smart Dust and presents a new protocol called the "Sleep-Awake" protocol, which takes advantage of the energy-saving features of the devices. Through analysis and experiments, the success probability and efficiency of the protocol are evaluated in relation to the parameters of the Smart Dust network. This research allows for the programming of the network to suit specific characteristics.
411919	41191965	Power to the peers: authority of source effects for a voice-based agricultural information service in rural India	Online communities provide a platform for people to easily connect and share knowledge regardless of their geographical location. With the use of mobile phones, billions of new users in emerging countries are now able to participate in these online communities. However, in India where social hierarchy is important, users may give more value to information from institutionally-recognized authorities rather than peer-sourced content. To test this hypothesis, a controlled experiment was conducted on a voice-based agricultural information service for farmers in Gujarat, India. The results showed that farmers were more likely to follow up on tips recorded by peers rather than scientists. However, in interviews, most farmers stated that they preferred information from scientists, which could be due to social pressure. This highlights the demand for peer-based information services in rural communities in India and suggests design implications for such services.
411919	41191913	TeleTorchlight: remote pointing and annotation using a mobile camera projector	The article discusses the challenges of remote support for physical tasks and proposes a new system called TeleTorchlight to address them. Remote support often takes longer and results in more mistakes due to the difficulty of communicating and demonstrating tasks from a distance. TeleTorchlight uses a tablet and mobile camera projector unit to allow remote supporters to draw instructions directly on physical objects, making it easier for on-site workers to understand and learn tasks. This system aims to enhance traditional voice chat-based support by providing visual aids and specific instructions for where and how to operate. 
411920	41192021	A Completion-Based Method for Mixed Universal and Rigid E-Unification	Our method addresses a new version of E-unification called "mixed" E-unification, which combines universal and rigid E-unification. Rigid E-unification is useful for handling equality in first-order calculi. By using mixed E-unification instead of just rigid, we can improve the performance of provers using E-unification. We provide soundness and completeness results and share results from experiments with our method's implementation.
411920	4119209	Incremental Theory Reasoning Methods for Semantic Tableaux	Theory reasoning is a crucial tool for improving the efficiency of automated deduction systems. This paper introduces incremental theory reasoning, a technique that enhances the interaction between the foreground and background reasoners, resulting in a more efficient combined system. The paper also discusses the application of this method in free variable semantic tableaux and the potential cost reduction it can achieve. An example of completion-based equality reasoning is presented, along with experimental data from an implementation. Overall, incremental theory reasoning proves to be a valuable approach for increasing the efficiency of automated deduction systems.
411921	41192144	Pattern-based retrieval in a fuzzy object oriented data base	This paper introduces a fuzzy object database, called FOOD, which can handle both crisp and fuzzy information. The database uses a graph-based data model and provides pattern-based operations for selecting and browsing data. This allows for direct manipulation of both the database schema and instances. The defined operations make it easy to navigate through the database and retrieve desired information. The FOOD model is an efficient and effective way to manage fuzzy data and provides a user-friendly interface for interacting with the database.
411921	41192119	Graph-Based Interaction In A Fuzzy Object Oriented Database	The paper proposes a definition for graph-based operations to navigate and retrieve information from a fuzzy object oriented database that handles both precise and fuzzy data. The database is based on a fuzzy graph model (FOOD), developed by Bordogna et al., using fuzzy set and possibility theories. The evaluation of the graph-based operations is described using graph transformations and fuzzy pattern matching. This research was published in 2001 by John Wiley & Sons, Inc. 
411922	41192259	Evaluation of Task Assignment Policies for Supercomputing Servers: The Case for Load Unbalancing and Fairness	The MPP architecture is still the most commonly used in supercomputer centers, but a cheaper and simpler alternative has emerged. This alternative involves a collection of multiprocessors or a distributed server system, where jobs are sent to one of the machines for processing. The main challenge in this setup is determining a good rule for assigning jobs to host machines. Many policies have been proposed but have not been systematically evaluated under supercomputing workloads. In this paper, the performance of existing policies is compared using simulations and analytical proofs. A new policy is proposed that intentionally unbalances load among hosts but is fair and outperforms the other policies.
411922	41192245	Analysis of scheduling policies under correlated job sizes	Correlations in traffic patterns have a significant impact on the performance and optimization of real systems, but existing analytical work has only focused on First-Come First-Served scheduling. This raises questions about how different scheduling policies interact with correlated job sizes and if scheduling can be used to mitigate their negative effects. In this paper, the authors present the first asymptotic analysis of common size-independent scheduling policies under a simple model for job size correlations. The results show that the performance of these policies is greatly influenced by the degree of correlation, and that there is no single optimal policy for all scenarios. The analysis is supported by numerical algorithms and simulations, as well as real-world trace data.
411923	41192315	3d Morphable Face Models Revisited	This paper discusses the creation of a high resolution 3D morphable model of face shape variation using statistical tools such as thin-plate splines and Procrustes analysis. The new model is more efficient and accurate than previous ones, and has a reformulated probabilistic prior that can be used as a regularisation constraint in fitting the model to data. The authors also demonstrate the model's application in estimating 3D face shape from a sparse set of 2D feature points, with an average error of less than 3.6mm. Experimental results using ground truth data are presented, providing absolute reconstruction errors.
411923	41192350	Driving 3D morphable models using shading cues	This paper discusses the use of surface orientation information from shape-from-shading to improve the fitting of a 3D morphable model to a face image. The issue of model dominance is addressed and shading constraints are used to refine shape estimates, potentially surpassing the model's maximum accuracy. An optimization scheme based on surface normal error is proposed to fully utilize the shading information in the image. The framework also allows for estimation of per-vertex albedo and bump maps, expanding the model's capabilities beyond the training set. Results show successful reconstruction and synthesis, and the potential for illumination insensitive recognition using only one image.
411924	41192430	CENTAUR: realizing the full potential of centralized wlans through a hybrid data path	In recent years, enterprise WLANs have shifted towards centralized architectures due to the benefits of easier management and improved control and security functions. However, the data path of WLANs still uses a distributed, random-access model as defined by the 802.11 standard. While a centrally scheduled data path has been shown to be more efficient in theory, its complexity has hindered practical consideration. This paper presents a new perspective on data path choices in enterprise WLANs and conducts extensive measurements to evaluate the impact of design choices. The results show that centralization can be beneficial in mitigating hidden terminals and exploiting exposed terminals. To address these issues, the authors propose a hybrid data path, called CENTAUR, that combines the simplicity of DCF with limited centralized scheduling. This solution does not require client cooperation and can support legacy 802.11 clients.
411924	41192438	Interference mitigation in enterprise WLANs through speculative scheduling	Wireless LANs are commonly used in enterprise environments, but their management and security can be challenging. Centralization has been proposed as a solution, with centralized authentication and allocation of channels and power levels in the control plane, and time slotted medium access with centralized scheduling in the data plane to mitigate interference. While centralization in the control plane has been successful, it is more difficult in the data plane due to the variability of the wireless medium and the need to maintain bounds on delay and jitter. This study explores the challenges of centralization in the data plane for enterprise WLANs and suggests a pragmatic solution for dense deployments of access points.
411925	41192527	Engineering Wireless Mesh Networks: Joint Scheduling, Routing, Power Control, and Rate Adaptation	Researchers have identified key engineering insights for optimizing the configuration of medium- to large-size wireless mesh networks (WMNs) to maximize the minimum throughput of all flows. They have developed efficient computational tools using column generation with greedy pricing, allowing for exact solutions to be computed for larger networks than previously possible. Additionally, fast approximations have been created to find nearly optimal solutions for even larger networks. These insights also apply to the case of proportional fairness, showing their applicability in different scenarios. Overall, this research provides valuable information for improving the design of WMNs and maximizing their performance.
411925	41192513	Throughput optimization in wireless multihop networks with Successive Interference Cancellation	Successive Interference Cancellation (SIC) is a technique that can improve the performance of wireless multihop networks. This study presents a method to calculate the maximum achievable throughput of such networks, taking into account factors such as conflict-free scheduling and multi-rate and multi-power capabilities. Different levels of SIC are considered, and a flexible framework is formulated to measure the potential throughput improvement in a realistic sized multihop network. The joint optimization problem of routing, scheduling, and SIC is solved numerically for different scenarios, revealing significant throughput gains with SIC at each node. The study also shows that SIC can provide non-negligible gains even when performed only at the gateway. This information can be useful for network operators in managing mesh networks. 
411926	4119260	A proposal for news recommendation based on clustering techniques	The article discusses the use of clustering techniques in recommendation systems for electronic journals, specifically in a journalistic context where users have access to categorized news. The paper aims to present an approach for recommending news to readers based on their profile and previously accessed news. The Aspect Model and K-Means clustering algorithm are used and compared in this context.
411926	41192658	Use of textual and conceptual profiles for personalized retrieval of political documents.	The use of Information Retrieval Systems (IRSs) has become essential in our daily lives due to the increasing amount of information available. However, these systems often fail to provide relevant and personalized results for users. Personalization techniques are necessary to address this issue, but they are not widely used due to concerns about privacy. In order to improve the personalization process and address privacy concerns, six generic user profile representations have been developed and compared. A new weighting scheme and personalization technique have also been proposed to combine the advantages of previous profiles. A comprehensive evaluation study has shown promising results for these generic user profiles, particularly in a political context with official documents from the Andalusian Parliament.
411927	41192783	A Bayesian Mixed Effects Model Of Literary Character	This study focuses on automatically determining the hidden character types in a large collection of English novels published between 1700 and 1899. Previous research assumed that character types were solely responsible for generating text, but this study introduces a model that takes into account other factors, such as the author. The results of this model were compared to the judgments of a literary scholar and were found to be more accurate than alternative models.
411927	4119276	Adversarial Evaluation for Models of Natural Language	The paper discusses the limitations of current methods for inducing linguistic structure from text that is not fully annotated. A new framework for evaluating natural language processing models, specifically unsupervised ones, is proposed. This framework aims to clarify the roles of researchers and promote error analysis in order to better understand model successes and failures. It can be applied in various ways, including simulating existing evaluations and introducing new ones. This approach may lead to more effective and measurable contributions towards the larger goal of improving natural language processing.
411928	4119288	BML and Related Tools	The Bytecode Modeling Language (BML) is a language used for specifying properties in Java bytecode. It allows for a high level of abstraction without restricting the format of the bytecode. BML specifications can be stored in class files, making it useful for proof-carrying code frameworks. BML is also similar to the source code level language JML, allowing for easy translation of specifications and proofs. This paper explains the BML language and its binary representation, as well as the available tools for working with BML. These tools include BMLLib, Umbra, JML2BML, BML2BPL, and CCT, which allow for inspecting, editing, and generating proofs for BML specifications. 
411928	41192849	Extending JML Specifications with Temporal Logic	This paper introduces a proposed extension of the Java Modeling Language (JML) that includes temporal specifications. The inspiration for this extension comes from the Bandera project and is specifically designed for Java(Card) programs. This extension allows for the specification of exceptional behavior in methods and has been designed to be user-friendly for software engineers. An example is provided to demonstrate how the JML extension can be used to specify temporal aspects of the JavaCard API. The paper also discusses the semantics of the extension and provides a way to translate it into standard JML for the purpose of re-using existing verification techniques. A trace-based semantics is given for the new features of the language.
411929	41192963	Inapproximability Results for Guarding Polygons and Terrains	Past research on art gallery problems has focused on finding the minimum number of guards needed in various situations. However, there is limited research on the complexity of these problems. This paper examines the approximability of three types of art gallery problems - VERTEX GUARD, EDGE GUARD, and POINT GUARD - motivated by a practical problem. The authors prove that for polygons without holes, these problems cannot be approximated by a polynomial time algorithm with a ratio of 1 + δ. For polygons with holes, the problems cannot be approximated by a polynomial time algorithm with a ratio of ((1 − ε)/12) ln n, unless NP ⊆ TIME(n O(log log n)). The inapproximability of the POINT GUARD problem for polygons with holes also applies to the problem of covering a 2.5-dimensional terrain with guards at a fixed height above the terrain or on the terrain itself.
411929	41192923	Optimum Inapproximability Results for Finding Minimum Hidden Guard Sets in Polygons and Terrains	The Minimum Hidden Guard Set problem involves placing the fewest possible guards in a given polygon or terrain so that no two guards can see each other and every point in the area is visible from at least one guard. This problem has been shown to be difficult to approximate, as it cannot be approximated by a polynomial-time algorithm with an approximation ratio of n1-驴 for any 驴 0, unless NP = P. This result holds even for polygons without holes, which distinguishes it from other visibility problems. Additionally, an approximation algorithm with a ratio of n is the best possible for this problem.
411930	411930106	Inapproximability Results for Guarding Polygons and Terrains	Past research on art gallery problems has focused primarily on determining the minimum number of guards needed for worst-case scenarios in different settings. However, there is limited research on the complexity of these problems. This paper aims to fill this gap by studying the approximability of three types of art gallery problems: VERTEX GUARD, EDGE GUARD, and POINT GUARD. The authors show that for input polygons without holes, no polynomial time algorithm can achieve an approximation ratio of 1 + δ for these problems. They also prove that for polygons with holes, these problems cannot be approximated with a ratio of ((1-ε)/12) ln n, unless NP is a subset of TIME(n O(log log n)). These results are obtained through gap-preserving reductions from 5-OCCURRENCE-MAX-3-SAT and SET COVER problems. Furthermore, the inapproximability of the POINT GUARD problem for polygons with holes extends to the problem of covering a 2.5-dimensional terrain with a minimum number of guards, even when the guards can be placed on the terrain itself.
411930	41193063	Positioning Guards at Fixed Height Above a Terrain - An Optimum Inapproximability Result	The problem of minimizing the number of guards at a fixed height to ensure complete visibility of all triangles on a 2.5-dimensional terrain is studied. It is proven to be NP-hard and cannot be approximated within a certain ratio by a polynomial time algorithm, unless a specific condition is met. This condition depends on the number of triangles in the terrain. An approximation algorithm with an approximation ratio of ln n+1 exists, making this result close to the best possible hardness for this problem.
411931	41193186	Towards an integration of Golog and planning	Golog, an action language, has been successfully used for controlling robots. Its main advantage is the flexibility in writing programs that can constrain the search for an executable plan. However, when it comes to general planning, Golog falls short compared to modern planners. To address this issue, this paper proposes an integration of Golog and planning, where planning problems are solved by a modern planner during program execution. The focus is on the ADL subset of the plan language PDDL, which is shown to be correctly embedded within Golog. A comparison of the performance of Golog with an integrated ADL planner is also presented.
411931	41193139	On the Relative Expressiveness of ADL and Golog: The Last Piece in the Puzzle	Integrating agent programming languages and efficient action planning is a promising approach that combines the expressive power of languages like Golog with the ability to search for plans efficiently. To integrate a Golog interpreter with a planner, it is important to understand which aspects of Golog can be captured by the planning language. Using Nebel's compilation framework, a maximal fragment of basic action theories, the formalism underlying Golog, is identified as being equivalent to the ADL subset of PDDL. However, it is found that most features that allow for specifying incomplete information in basic action theories cannot be compiled to ADL.
411932	41193246	A Formal Framework for On-line Software Version Change	Traditional methods of updating software involve shutting down the current version and installing the new one, causing a delay in service for users. However, an on-line software replacement system allows for updates to be made while the program is running, avoiding the need for a shutdown. While there have been various examples of this system in practice, there has been little research on its theoretical aspects. This paper presents a formal framework for studying on-line software version change, including a definition of validity and conditions for ensuring it in a procedural language. It is found that determining validity is generally impossible, but certain conditions can guarantee it.
411932	41193236	On line software version change using state transfer between processes	Traditional software updates require the program to be shut down before the new version can be installed, causing inconvenience and service downtime for users. An on-line software replacement system allows for seamless updates by replacing parts of the software while it is still running. The system described in this paper specifically focuses on implementing this for software written in C. When the user initiates the update, a new process with the new version is created and the state is transferred from the old process at the appropriate time. This results in minimal delay for the user during the switch to the updated version.
411933	4119330	Topological map: an efficient tool to compute incrementally topological features on 3d images	This paper discusses the use of a three dimensional topological map to efficiently compute topological features on objects in a 3D image. These features have practical applications in image processing and computer vision. The topological map is a combinatorial model that represents both topological and geometrical information of a labeled 3D image. It can be constructed incrementally using only two operations, and in this study, the authors demonstrate how the Euler characteristic can also be computed incrementally during this process. This opens up possibilities for efficiently computing other features using this model.
411933	41193343	First results for 3D image segmentation with topological map	This paper introduces a new segmentation operation within the 3D topological map framework. The authors demonstrate how a traditional segmentation algorithm can be applied to a 3D image represented by a topological map, and the consistency of the results despite modifications to the algorithm. The complexity of the operation is also studied. Experimental results on 3D medical images are presented, showing the duration of the process and validating the use of 3D topological maps in image processing. Overall, this paper highlights the potential benefits of utilizing 3D topological maps in image segmentation.
411934	41193437	Implementation of Elliptic Curve Cryptosystems on a reconfigurable computer	In recent years, there has been a focus on developing reconfigurable computers that combine traditional microprocessors with Field Programmable Gate Arrays (FPGAs). These machines have shown promising speed improvements for computationally intensive tasks like codebreaking. However, effectively using and programming these machines remains a challenge. In this study, the researchers demonstrate an efficient implementation of an Elliptic Curve scalar multiplication over GF(2m) using SRC-6E, a leading reconfigurable computer. They discuss how the hardware architecture and programming model influenced their algorithm partitioning strategy. The paper also includes a detailed analysis of control, data transfer, and reconfiguration overheads, as well as a performance comparison with a microprocessor implementation. Overall, this study highlights the potential and challenges of using reconfigurable computers for complex computations.
411934	41193448	An Implementation Comparison of an IDEA Encryption Cryptosystem on Two General-Purpose Reconfigurable Computers	The combination of traditional microprocessors and Field Programmable Gate Arrays (FPGAs) is emerging as a promising platform for intensive computational computing. It combines the strengths of microprocessors for front-end development and the reconfigurability of FPGAs for solving complex problems. Prototype PC-FPGA machines have shown significant speedups compared to standalone PCs for computationally intensive tasks, making them ideal for cryptographic applications. In this paper, the authors showcase an efficient implementation of IDEA encryption on two leading reconfigurable computers, SRC Computers' SRC-6E and Star Bridge Systems' HC-36. They compare the hardware architecture, programming model, and implementation of IDEA encryption on both platforms, and provide a detailed analysis of FPGA resource utilization, data transfer and reconfiguration overheads for the SRC system, and a comparison with a software implementation.
411935	41193543	An n! lower bound on formula size	We have developed a new game, based on the Ehrenfeucht--Fraïssé method, for proving lower bounds on the size of first-order formulas. This game has been used to show that the CTL+ formula Occurn ≡ E[Fp1 ∧ Fp2 ∧ … ∧ Fpn], which states that all n predicates must occur on a path, requires a formula size of n! in CTL. This lower bound is optimal, meaning that no smaller formula can express this property. This result shows that the succinctness of CTL+ compared to CTL is exactly Θ(n)!. We have also used our game to prove an optimal Ω(n) lower bound on the number of boolean variables needed for RLf to polynomially embed CTL+. The number of booleans needed for RL and FO2(TC) is still unknown.
411935	41193545	Dyn-FO: a parallel, dynamic complexity class	Traditionally, computational complexity has focused on static problems, with complexity classes such as NC, P, and NP defined in terms of checking whether an input satisfies a certain property. However, for many computer applications, a dynamic approach is more appropriate. This involves working on a large object over time, with repeated modifications and computations. A new complexity class, dynamic first-order logic (Dyn-FO), has been developed to model this process. It includes problems such as multiplication, graph connectivity, and minimum spanning trees, which were previously thought to be beyond the capabilities of first-order database languages. The concept of "bounded-expansion reductions" is also introduced to honor dynamic complexity classes. While certain standard complete problems for static complexity classes remain complete under these reductions, others, such as REACH for NL and REACH d for L, are no longer complete. Finally, a version of REACH a , called REACH a + , is shown to not be in Dyn-FO unless all of P can be contained in parallel linear time.
411936	4119368	Framework for Analyzing Garbage Collection	The development of garbage collection algorithms has advanced significantly, but the analysis of these algorithms is still in its early stages. Current analyses only measure the costs of individual executions, and a theoretical foundation is needed to make conclusive statements about the efficiency of garbage collectors across entire programs. This theoretical framework also allows for abstract examination and new designs without concern for implementation details. The proposed framework can accurately compute the time cost of garbage collection over the entire program execution, and has resulted in a new and significantly faster trace generation algorithm called Merlin. The paper also presents a novel result, proving the optimality of Merlin's asymptotic running time for trace generation.
411936	41193612	Nested transactional memory: model and architecture sketches	The article presents a reference model for nested transactions at the memory access level and outlines potential hardware designs to implement it. The model covers both closed and open nesting and focuses on memory access conflicts and the effects of commits and aborts. The hardware designs involve bounded size caches in a processor with overflows to memory. The article also introduces a simpler model called linear nesting, which supports only one thread of execution but may be easier to implement. The model is intended as a target for compiling transactions from source languages, but the mapping process is not discussed in the paper.
411937	41193760	Scalable logo recognition based on compact sparse dictionary for mobile devices	This paper introduces a new logo recognition system that can be used on mobile devices. The system is unsupervised and does not require any supervised training, making it time efficient and low in memory usage. It is also robust against challenging conditions such as noise, different image scales, and rotation. The system uses a segmental quantization approach to generate a compact vocabulary of over one million visual words. This vocabulary is then used to efficiently retrieve logo instances in the dataset through a K-nearest neighbors (K-NN) algorithm. The system's vocabulary is significantly smaller than traditional methods, making it suitable for mobile devices. Additionally, a verification method is proposed to filter out false positives. Experiments on a dataset with 400 logo classes demonstrate the system's efficiency and effectiveness.
411937	41193742	Adult Image Detection Combining Bovw Based On Region Of Interest And Color Moments	The authors suggest a new approach to effectively prevent the spread of pornography on the Internet. This method combines bag-of-visual-words (BoVW) and color moments (CM) to detect adult images. BoVW identifies local patterns of adult content by clustering visual words from patches in the whole image, but this can be affected by background noise. The proposed method uses BoVW based on region of interest (ROT) to improve the representative power of visual words, and also adopts a soft-weighting scheme for better detection. CM is added to BoVW based on ROT for further improvement. Experiments show that this method outperforms other techniques currently in use. 
411938	41193851	Causal Behavioural Profiles - Efficient Computation, Applications, and Evaluation	Analysis of behavioural consistency is crucial in software engineering, particularly in process and service management. This involves verifying that behavioural models are consistent with each other, such as a business process model and a workflow model. Existing notions of behaviour equivalence, such as bisimulation and trace equivalence, can be computationally intensive and only provide a Boolean result. To address this, the article proposes the use of causal behavioural profiles, which capture important information about the behaviour of a process model. These profiles are weaker than trace equivalence but can be efficiently computed for a wide range of models. The article also introduces techniques for computing these profiles using structural decomposition techniques and presents findings from applying the technique to three industry model collections.
411938	411938140	Generalised computation of behavioural profiles based on petri-net unfoldings	Behavioural profiles are a way to assess the consistency of process models by describing the relationships between the activities in the model. They are less sensitive to model projections than traditional equivalence criteria. An algorithm has been developed to efficiently derive these profiles for unlabelled, sound free-choice workflow nets. This paper expands on this concept by introducing an algorithm that can derive behavioural profiles from the complete prefix unfolding of a bounded Petri net, making it applicable in a wider range of cases. The concept is also extended to labelled Petri nets, and the approach has been tested on a set of industry models.
411939	41193926	Arity bounds in first-order incremental evaluation and definition of polynomial time database queries	The paper discusses a first-order incremental evaluation system (foies) for database queries that uses a first-order query on the new and old databases, as well as stored auxiliary relations, to derive a new query answer after inserting or deleting a tuple. The paper measures the space needed for foies in terms of the maximal arity of the auxiliary relations and presents results on the existence and nonexistence of space-restricted foies for graph queries. The paper also constructs space efficient foies for these queries and shows that the arity bounds are tight using Ehrenfeucht–Fraı̈ssé games. The paper also considers a variation of foies where the cost of storing the answer is also taken into account and shows that the arity hierarchy remains strict.
411939	4119394	A technique for proving decidability of containment and equivalence of linear constraint queries	This article discusses a new technique that uses counter machines to analyze queries with linear constraints over different types of numbers. It shows that the problems can be solved in exponential space and that there is a lower bound for solving them in exponential time for conjunctive queries. However, for a specific subclass of queries (called SQL-like conjunctive queries), the problems can be solved in polynomial space. The article also uses the counter machine technique to prove that for a restricted subclass of first-order queries with linear constraints over integers and natural numbers, the containment and equivalence problems are undecidable for finite databases but can be solved for a specific type of finite databases.
411940	411940149	A workflow net similarity measure based on transition adjacency relations	In business process management, determining the similarity or distance between two processes is essential for activities like process retrieval, process mining, and process integration. While various approaches have been proposed for measuring this similarity, there is no widely accepted definition or method. This paper defines similarity and distance based on firing sequences in workflow nets (WF-nets) and introduces transition adjacency relations (TARs) as a practical way to compare processes. It is shown that this distance measure is a metric and can be efficiently calculated using model reduction techniques. Experiments on artificial and real-life processes confirm the effectiveness of this approach.
411940	41194085	Querying business process models based on semantics	Business process management technology has become increasingly popular in recent years, resulting in a larger number of business process models being created. Managing such a large quantity of models poses a challenge, and the ability to query these models is essential. This allows designers to find related models and make changes, saving time and reducing errors. To facilitate this, a language called BQL has been proposed for expressing requirements based on semantics. An efficient method for computing semantic features and the use of indexes for query processing has also been adopted. Furthermore, the approach takes into account the similarity between labels, making it applicable in various scenarios. The proposed approach has been implemented in the system BeehiveZ and has been proven effective through analysis and experiments.
411941	41194151	Monitoring Influenza Trends through Mining Social Media	This paper analyzes the correlation between Google search queries for influenza-like-illness (ILI) and Center for Disease Control and Prevention (CDC) seasonal ILI reporting data. It also evaluates trends in blog posts discussing influenza and identifies influential bloggers and communities that could play a role in disseminating information during a potential outbreak or epidemic. The study suggests that using web and social media (WSM) can aid in detecting increases in ILI and proposes a response strategy that utilizes WSM to reduce the impact of an infectious disease outbreak. The paper concludes with recommendations for expanding ILI-trend identification and creating an integrated Public Health and WSM community intervention campaign. 
411941	41194141	Activity Recognition Using Graphical Features from Smart Phone Sensor.	The authors have developed a graphical feature-based framework that combines data from various sensor networks and represents it as a graph. They then extract graphical features from the graph and combine them with non-graphical features specific to the application. This approach is applied to predict activities of smart phone users using GPS sensor data. The location categories are represented as nodes in the graph and user movement between locations is represented as edges. The addition of these graphical features to the basic sensor data features results in a 7.27% improvement in activity recognition accuracy. The best performing feature set includes the existence of nodes.
411942	41194225	Two-way coupling of rigid and deformable bodies	The proposed framework allows for the full two-way coupling of both rigid and deformable bodies through a unified time integration scheme and individual two-way coupled algorithms. This eliminates the need for ad hoc methods to address stability issues and allows for the treatment of key aspects of both types of bodies, such as contact, collision, stacking, and friction for rigid bodies, and arbitrary constitutive models, thin shells, and self-collisions for deformable bodies. Additionally, the framework supports advanced features such as proportional derivative controlled articulation between rigid bodies, which can lead to the simulation of new phenomena and the design of deformable creatures that interact realistically with their environment.
411942	41194252	Interpenetration free simulation of thin shell rigid bodies.	The proposed algorithm ensures that objects in a rigid body simulation do not overlap or intersect, improving accuracy and robustness while eliminating the need for additional methods to separate objects. Collision and contact resolution are handled separately, with the first step guaranteeing non-interfering geometry through an approximation of continuous collision detection. This allows for handling of thin and flat objects moving at high speeds. A fail-safe method is also introduced for efficient interpenetration resolution. In the second step, a contact model is used to handle thin shells in close proximity, taking into account their instantaneous positions at the end of the time step.
411943	41194362	Robust Gaze Estimation Via Normalized Iris Center-Eye Corner Vector	Gaze estimation is important in various practical scenarios, but accurately estimating gaze with just a single web-cam is still difficult. This paper presents a method called the normalized iris center-eye corner (NIC-EC) vector, which uses facial features and pupil centers to improve the accuracy of single web-cam based gaze estimation. An interpolation method is used to map the NIC-EC vector to points of regard. Experimental results showed that this method has significantly improved accuracy compared to previous methods, with an average accuracy of 1.66 degrees even with slight head movements.
411943	41194321	Accurately estimating rigid transformations in registration using a boosting-inspired mechanism.	Feature extraction and matching are important techniques used in various tasks such as object registration, modeling, retrieval, and recognition. However, these methods often introduce false matches due to various factors such as lack of features, noise, occlusion, and cluttered backgrounds. This can lead to inaccurate estimation of the underlying transformation needed to align overlapping shapes. To address this issue, a novel boosting-inspired method is proposed in this paper. It involves three steps: estimating the transformation using weighted least squares, estimating boosting parameters and regularizing them using Tsallis entropy, and re-estimating weights using Shannon entropy and a maximum fusion rule. This process is iterated and the final optimal transformation is estimated as a weighted average of the transformations from the latest iterations. Experimental results show that this method outperforms existing methods and can recover the transformation with an error as small as 5% from point matches.
411944	41194419	An algebraic semantics of event-based architectures	This article proposes a mathematical framework for event-based architectures, with the goal of characterizing the modularization properties and supporting the categorical approach to architectural modeling. The use of this formalization allows for the integration of both synchronous and asynchronous interactions in the same modeling approach. The proposed framework is based on transition systems with events and utilizes a family of logics to support different levels of abstraction. This is seen as a first step towards engineering architectural styles.
411944	41194426	New Insights on Architectural Connectors	This study aims to bridge the gap between two distinct approaches to system modeling, namely the categorical approach and the algebraic approach. By mapping the CommUnity program, which utilizes categorical colimits, to the Tile Model, which uses algebraic operators, the researchers were able to establish a connection between the two methods. The results of this work include a decomposition method for CommUnity programs and a strong link between the colimit computation and the abstract semantics of configurations. This study demonstrates that the encoding of a CommUnity diagram is equivalent to the encoding of its colimit, providing a promising first step towards reconciling these two approaches in system modeling.
411945	41194512	Tool-supported compression of UML class diagrams	Tool-supported compression techniques have been developed for UML class diagrams. These techniques involve hiding less important parts of the diagram, which can be revealed again at selected points. This allows users to gradually refine the diagram from an abstract view to more detailed ones, making it easier to manage large diagrams and understand the overall architecture of an object-oriented legacy system. The compressed form of the diagram can be created automatically or with human control. A management algorithm and prototype implementation are also presented.
411945	41194561	Active text for structuring and understanding source code	This paper discusses the use of non-linear active text as a medium for structuring and browsing source code. While traditional program editors are character-based, the authors argue that incorporating non-textual elements such as pictures, links, folds, and annotations can improve the readability of programs. They demonstrate this through examples and explain how their approach differs from syntax-oriented editors. The implementation was done in the Oberon system, which allowed for object-oriented extensions. This paper was published in SOFTWARE - Practice and Experience in July 1996.
411946	41194665	Maximizing Barrier Coverage Lifetime with Mobile Sensors	Sensor networks play a crucial role in detection and tracking tasks, and one of their main purposes is to provide coverage. The goal of this study is to maximize the coverage lifetime of a barrier using mobile sensors with limited battery power. Coverage lifetime is measured as the time until a sensor dies, leading to a breakdown in coverage. Energy is consumed both for mobility and coverage, with the latter being dependent on the radius of the sensor. The study focuses on two variants: fixed radii and variable radii, and proposes parametric search algorithms for both cases. The study also discusses the challenges and limitations of the variable radii problem, showing its complexity and providing approximation results for the fixed radii problem in certain scenarios.
411946	41194673	Application of halftoning algorithms to location dependent sensor placement	In this study, we examine the placement of sensors in a network with varying sensing ranges based on terrain features. This is done in order to maximize coverage. By using digital halftoning algorithms commonly used in image processing, we can effectively solve this sensor placement problem. The problem is reduced to an image halftoning problem and two known algorithms, dither mask halftoning and direct binary search, are applied. Experimental results demonstrate the effectiveness of this approach, which can also be applied to the problem of preferential coverage.
411947	41194745	Mobile multi-layered IPsec	To improve the efficiency of wireless networks, smart forwarding and processing of packets in access routers is crucial. However, when end-to-end encryption is used, access routers cannot provide these services as the necessary information is encrypted. The Multi-layered IPsec (ML-IPsec) protocol was developed to expose certain portions of the packet to intermediate network elements, allowing them to enhance performance. In this paper, ML-IPsec is extended to support mobility in wireless networks. A key distribution protocol and two mobility protocols compatible with Mobile IP are defined and implemented. Measurements show that integrated Mobile IP/ML-IPsec handoffs result in a pause of 53-100 milliseconds, with only 28-75 milliseconds due to ML-IPsec. The implementation of ML-IPsec, when combined with SNOOP, significantly increases throughput compared to standard TCP over IPsec. A dynamic version of ML-IPsec is also proposed, allowing for a balance between performance and security. 
411947	41194735	A Flexible Privacy-Enhanced Location-Based Services System Framework and Practice	The paper discusses the increasing importance of location based services (LBS) in next generation wireless systems. However, there is a conflict between the need for user privacy and the use of location information. To address this, the paper proposes a framework for privacy enhanced LBS. It classifies the services and suggests a hierarchical key distribution method, where location information is encrypted under different keys and only distributed to group members with permission. Four methods for delivering hierarchical location information without compromising privacy are proposed, along with a key tree rebalancing algorithm for efficient group key management. The paper also presents a practical LBS system implementation that offers flexible location information access and good efficiency and scalability.
411948	41194858	Exploring the structure of rule based systems	The article discusses the importance of understanding the internal structure of rule-based expert systems in order to effectively measure and assess their performance. While previous attempts have been made to formalize this structure through the concept of a rule base execution path, these definitions have been found to be inadequate. The paper presents a new formal definition for rule base execution paths, which has been integrated into a tool called Path Hunter. This tool was used to analyze a rule base of 442 CLIPS rules, and successfully controlled the issue of combinatorial explosion that can occur during path enumeration. The analysis revealed important considerations for the development of rule-based systems.
411948	41194868	Verifying ontological commitment in knowledge-based systems	An ontology is a set of concepts and their relationships within a specific field of knowledge. For knowledge-based systems to work together, they must agree on a common ontology. This article explains how existing verification techniques can be used to ensure that a knowledge-based system is committed to a given ontology. The method takes into account that different representation languages may be used for the ontology and knowledge base, and includes translation in the verification process. While the specific languages used may vary, the method can be applied to a wide range of projects. 
411949	41194928	On Clock-Based Fault Analysis Attack For An Aes Hardware Using Rsl	Random Switching Logic (RSL) is a technique proposed in 2004 as a countermeasure against Differential Power Analysis (DPA) attacks. It was applied to an AES hardware prototype and tested with a 0.13-mu m standard CMOS library. However, despite its purpose to resist DPA attacks, Clock-based Fault Analysis (CFA) was able to reveal secret information from the prototype chip. This paper explains the mechanism of the CFA attack and discusses its success against the RSL-AES implementation. It also proposes an ideal RSL-AES implementation that can resist CFA attacks.
411949	41194921	An Efficient Countermeasure against Fault Sensitivity Analysis Using Configurable Delay Blocks	This paper introduces a new countermeasure against Fault Sensitivity Analysis (FSA), a type of fault attack that exploits the relationship between fault sensitivity and secret information. Previous studies have shown that FSA can break conventional countermeasures against Differential Fault Analysis (DFA). The proposed countermeasure uses configurable delay blocks (CDBs) to detect both DFA and FSA attacks based on setup time violation faults. By post-manufacture configuring the delay blocks, the overhead in operating frequency can be minimized. The paper presents an implementation of the countermeasure for an AES module, describes its configuration method, and evaluates its hardware overhead through experiments on both an ASIC and a prototype FPGA implementation. 
411950	41195085	Practical Password Recovery Attacks On Md4 Based Prefix And Hybrid Authentication Protocols	This paper discusses practical password recovery attacks on two challenge and response authentication protocols that use MD4. The number of queries required for these attacks is a crucial factor, as the opportunity for an attacker to ask queries is limited in real protocols. Previous methods required 2(37) queries, which is almost impossible in real protocols. However, the proposed attack only needs 17 queries and 2(34) MD4 computations to recover up to 8-octet passwords, and 2(10) queries and 2(41) MD4 computations for 12-octet passwords. When using the hybrid approach, previous methods required 2(63) queries, while the proposed attack only needs 28 queries and 2(39) MD4 computations to recover up to 8-octet passwords. This is achieved by guessing part of the password and simulating intermediate chaining variables from observed hash values.
411950	41195075	Improved collision attack on MD4 with probability almost 1	At the EUROCRYPT2005 conference, a team proposed a collision attack on the MD4 hash function. They claimed a success probability of 2−6 to 2−2 and a complexity of less than 28 MD4 operations. However, their paper contained typos and oversights. This paper reevaluates the success probability and points out the errors in the original paper. It also proposes a new method for modifying messages in the third round of MD4. The reevaluation shows a success probability of 2−5.61, and the improved method can find collisions with almost 100% probability and a complexity of only 3 MD4 operations. This is 85 times faster than the original method. 
411951	41195148	All-pairs bottleneck paths for general graphs in truly sub-cubic time	The all-pairs bottleneck paths (APBP) problem involves finding the maximum capacity path between every pair of vertices in a directed graph with non-negative edge capacities. This problem was first studied in operations research and is now being addressed with a new sub-cubic algorithm for general dense graphs. The algorithm uses a procedure for computing the (max, min)-product of two matrices in O(n2+Ω/3) time, where n is the number of vertices and Ω is the exponent for matrix multiplication. This allows for the extraction of an explicit maximum bottleneck path in linear time.
411951	41195119	Finding heaviest H-subgraphs in real weighted graphs, with applications	The MAX H-SUBGRAPH problem involves finding a subgraph with the maximum total weight in a graph G with assigned weights to its vertices and edges. The article presents new strongly polynomial algorithms for this problem, some of which use fast matrix multiplication. A more general problem, the all pairs MAX H-SUBGRAPH problem, is also solved for vertex-weighted graphs with n vertices. The runtime for finding heaviest triangles for all pairs is O(n2+1/(4-ω)), which can be improved using rectangular matrix multiplication. The article also presents improved algorithms for the edge-weighted case, including an O(m2−1/k log n)-time algorithm for finding the heaviest cycle of length 2k or 2k−1 and an O(n3/log n)-time randomized algorithm for finding the heaviest cycle of any fixed length. The methods used in these algorithms can also be applied to other related problems, such as finding chromatic H-subgraphs in edge-colored graphs and computing the most significant bits of the distance product of two real matrices in subcubic time.
411952	41195252	Bounding Performance Loss in Approximate MDP Homomorphisms	The article discusses a metric for measuring behavior similarity between states in a Markov decision process (MDP) that takes action similarity into account. This metric corresponds to the classes of states defined by MDP homomorphisms and can be used to upper-bound the difference in optimal value function between different states. It is shown to be tighter than previous bounds provided by bisimulation metrics. The metric is applicable for both discrete and continuous actions and an algorithm is provided for constructing approximate homomorphisms using this metric. Previous research on this topic has primarily relied on heuristics. 
411952	41195244	On-the-Fly Algorithms for Bisimulation Metrics	This article discusses the problem of finding approximate equivalences in Markov Decision Processes (MDPs) with rewards, using bisimulation metrics. It builds upon previous work by Ferns et al. (2004) and introduces an extension that uses the Earth Mover's Distance (EMD) to measure the similarity between states. The authors address two limitations of the previous framework by incorporating "on-the-fly" methods that prioritize certain state comparisons and by using heuristics to obtain approximate state comparisons. They prove that this approach converges to the correct value of the bisimulation metric. Additionally, they demonstrate how this method can be applied to generate new algorithmic strategies for prediction and control in MDPs. 
411953	41195337	The TELAR mobile mashup platform for Nokia internet tablets	The rise of Web 2.0 has led to an increase in online data and information services such as websites, Wikis, and web services. This has also given rise to the development of Mashups, which are web applications that combine data from multiple sources into one service. This is made possible by the use of script languages and web development platforms. Additionally, mobile devices are becoming more powerful and have ubiquitous access to the web, allowing for integration of local sensors such as GPS. This enables mobile applications to adapt to the user's current situation, particularly their location. The Telar Mashup platform is a client-server solution that facilitates the creation of adaptive Mashups for mobile devices, using wrappers to integrate data from web-based services and the DCCI specification to incorporate context information from local sensors. The platform is demonstrated on the Nokia N810 Internet Tablet. 
411953	41195346	Federated Spatial Cursors	The increasing use of small mobile devices for data-heavy tasks requires consideration of their limitations in system design, such as memory and communication bandwidth. To address this, the application should have more control over the data delivery process, taking into account device status, communication costs, and user needs. This paper proposes a flexible and scalable approach through spatially federated cursor functionality, utilizing loosely coupled data sources with simple object retrieval interfaces. This allows for better adaptation of data delivery and retrieval based on the device and user's needs.
411954	41195437	Customization and provisioning of complex event processing using TOSCA.	In the world of Internet of Things, a large amount of sensor data is constantly being produced and shared among devices in smart environments. This poses a challenge as the data needs to be processed quickly and efficiently to conserve network resources. Complex event processing (CEP) is a commonly used method to handle this data. However, automatically provisioning CEP systems can be difficult as they require customization to be used in IoT scenarios. This can be a tedious task if done manually. To address this issue, a new approach based on the Topology and Orchestration Specification for Cloud Applications standard has been developed, allowing for the customization and provisioning of CEP systems with all necessary data sources, sinks, and queries.
411954	41195474	Building the data warehouse of frequent itemsets in the DWFIST approach	Data mining tasks can generate large amounts of data, leading to a new knowledge management problem. Frequent itemset mining is one such task, and various approaches have been proposed to address this issue. However, these approaches have limitations, especially when dealing with data streams. The DWFIST approach aims to overcome these limitations by providing a Data Warehouse of Frequent ItemSets. This allows for efficient and flexible access to frequent itemsets and related patterns, without needing the original data during the analysis phase. This paper outlines the construction of such a data warehouse, which provides a standardized view for developing analytical tools.
411955	4119554	New algorithms for finding approximate frequent item sets	Standard frequent item set mining requires all items in a set to be present in a transaction, which can make it difficult to find relevant groups of items. By relaxing the support definition, allowing for some missing items, approximate, fault-tolerant, or fuzzy item sets can be found. This paper presents two new algorithms for finding such item sets: one based on cover similarities and subset size occurrence distribution, and the other using a clustering approach with item cover distances and a one-dimensional Sammon projection. These algorithms are demonstrated to be effective in a concept detection task and neuron ensemble detection in simulated parallel spike trains.
411955	41195540	Fixed Parameter Algorithms For The Minimum Weight Triangulation Problem	This article discusses four fixed parameter algorithms for finding the minimum weight triangulation of a simple polygon with n-k vertices on the perimeter and k vertices in the interior. The algorithms use a divide-and-conquer approach and a variant of dynamic programming. They are based on two observations about triangulations: triangle splits and path splits. The first two algorithms use only one split type, while the last two combine them for improved time complexity. The goal is to simplify and better understand these approaches. The algorithms were implemented in Java and experiments were conducted to evaluate their performance.
411956	411956175	Playing Fast and Loose with Music Recognition.	In this study, the authors discuss their experience developing a music recognition system that allows musicians to incorporate digital interactions into their performances. They worked with 23 musicians of varying skill levels and instruments, and the system evolved quickly to meet their needs. The system allows musicians to compose music fragments that can trigger digital interactions with varying levels of embellishment, disguise, and looseness. The authors highlight the challenges of control, feedback, and attunement in incorporating technology into performance practice and suggest the use of written notations in other recognition-based systems. Overall, the study supports the idea of intentionally introducing looseness into interactive systems to enhance musical expression.
411956	41195633	Extending authoring tools for location-aware applications with an infrastructure visualization layer	Current authoring tools for location-aware applications typically involve placing trigger zones on a map and linking them to events and media assets. However, studies have shown that the limitations of the underlying ubiquitous computing infrastructure can greatly impact the user experience. To address this, we propose a new approach where designers work with three layers of information: physical world, digital media, and visualizations of the infrastructure. We have developed a prototype authoring tool that implements this approach and used it to create a location-based game called Tycoon. However, there are challenges in generalizing this approach to more advanced authoring tools, such as obtaining and visualizing infrastructure and map data, and specifying the relationship between digital content and these layers. 
411957	4119574	Simple Plans Or Sophisticated Habits? State, Transition And Learning Interactions In The Two-Step Task	The 'two-step' behavioural task has become popular for differentiating between model-based and model-free reinforcement learning and producing neurophysiologically-friendly decision data. However, the task's structure can affect the strategies used and the accuracy of conclusions drawn from behavioural performance. A balance must be struck between the need for stochasticity to distinguish strategies and the need for determinism to incentivize subjects to optimize their choices. Changes to the task structure can create correlations that make model-free strategies appear as model-based. A suggested correction to analysis can mitigate this issue, but understanding these complexities is crucial for fully utilizing the two-step task in behavioural neuroscience.
411957	41195762	Dopamine: generalization and bonuses.	The temporal difference model explains the activity of primate dopamine neurons as reporting a prediction error for future reward. This is supported by experimental evidence, but there are instances where the activity seems abnormal under the model. This paper discusses two types of anomalies - generalization and novelty responses - and proposes that dopamine cells also play a role in guiding exploration and responding to different types of reward bonuses. This additional function of dopamine is linked to its attentional and psychomotor effects, providing a computational role in guiding exploration.
411958	41195856	Honey Encryption Beyond Message Recovery Security.	Juels and Ristenpart developed honey encryption (HE), which ensures message recovery security despite attacks that try all possible keys. This is particularly useful for password-based encryption, where keys are low entropy. Other HE schemes based on the Juels-Ristenpart construction were later proposed for password management and long-term protection of genetic data. However, in this setting, message recovery security is a weak property and does not prevent an attacker from obtaining partial information about plaintexts or altering ciphertexts. To address this, the authors introduce target-distribution semantic-security and target-distribution non-malleability notions and prove that a variation of the JR HE construction can meet them. Their analysis is different from previous work and they also provide a formal proof that an unbounded attacker can always succeed in message recovery with a limited number of encryptions of known plaintexts.
411958	4119585	Honey Encryption: Encryption beyond the Brute-Force Barrier	Honey encryption (HE) is a method of encrypting messages using keys that are vulnerable to guessing attacks. It creates a ciphertext that appears valid but contains a fake message when decrypted with an incorrect key. This makes it difficult for attackers to determine if decryption was successful. HE allows weak passwords to be used for encryption, making it nearly impossible for even a strong attacker to decrypt the message. It can be used for encrypting passwords in a password manager or credentials used in SSH. HE combines the use of honey objects and decoys with cryptography principles to enhance system security.
411959	41195924	Delay-Insensitive On-Chip Communication Link using Low-Swing Simultaneous Bidirectional Signaling	This paper introduces a new asynchronous delay-insensitive on-chip link structure that allows for simultaneous data exchange between two modules. Unlike traditional dual-rail links, this design only requires N+1 interconnects instead of 2N+1, making it more cost-effective for future systems. The transceiver circuits are designed using multiplevalued current-mode logic and have a low power consumption of 8.32mW with a 689ps propagation delay and 5mm interconnect length. Potential applications for this link include use in GALS systems, NoC routers, and adaptive systems. The circuit was simulated using Cadence Analog Spectre with 0.13um CMOS technology.
411959	411959115	Full-Duplex Link Implementation Using Dual-Rail Encoding And Multiple-Valued Current-Mode Logic	This paper introduces a new asynchronous on-chip link structure that allows for simultaneous data exchange between two modules. The link uses a 2-color 1-phase communication protocol, which only requires one communication action per transfer, making it potentially faster than traditional handshake-based protocols. The transceiver circuits are designed using multiple-valued current-mode logic, and the linear summation is done through wiring without active devices, simplifying the circuitry. The link consumes 16mW of power with a 90mV voltage swing and has a propagation delay of 178ps for a 2mm interconnect length. The circuit was designed and simulated using Cadence Analog Spectre and a 0.13um CMOS technology.
411960	41196024	Task oriented weighting in multi-criteria analysis	This paper introduces a new method for determining criteria weights in decision making processes involving multiple conflicting factors. A fuzzy knowledge base is used to address imprecise decision making and criteria weights are determined by specifying the state of each task requirement. This approach is incorporated into a fuzzy multi-criteria analysis model and tested on a dredger dispatching problem in China. The results demonstrate the effectiveness of the model and its potential for general application in decision making. This task oriented weighting procedure and algorithm have the potential to improve the consistency and accuracy of decision making in various situations.
411960	41196047	Customer Order-Driven BOCR-Based Supplier Selection	This paper presents a new method for choosing the best supplier to fulfill a specific customer order based on considerations of benefits, opportunities, costs, and risks (BOCR). The impact of customer order attributes on these dimensions is analyzed and a customer order-driven weighting method is created to determine their relative importance. A multicriteria decision making (MCDM) model is then developed to select the supplier that best meets the BOCR requirements of the customer order. An empirical study is conducted using data from an auto parts company to demonstrate the effectiveness of this approach.
411961	41196116	Resolution Limits of Sparse Coding in High Dimensions	 Grants DMS-1007062 and DMS- 1209017, and a grant from the Simons FoundationThis paper discusses the issue of detecting sparsity patterns in unknown k-sparse n-dimensional signals observed through m noisy, random linear measurements. This problem arises in various situations, such as statistical model selection, pattern detection, and image acquisition. The main focus of this paper is to determine necessary and sufficient conditions for achieving reliable sparsity pattern recovery, taking into account the dimensions m, n, and k, as well as the signal-to-noise ratio (SNR) and the minimum-to-average ratio (MAR) of the nonzero entries of the signal. The paper presents a necessary condition of m > 2klog(n − k)/(SNRMAR) for any algorithm to succeed, and a sufficient condition for a computationally-trivial thresholding algorithm. This analysis can provide insight into the limitations of convex programming-based algorithms and can be applied in the context of visual cortex modeling to understand the resolvability of visual features from visual data. The abstract estimation problem considered in this paper involves an unknown sparse signal x, modeled as an n-dimensional real vector with k nonzero components, and the problem of detecting the sparsity pattern of x from an m-dimensional measurement vector y = Ax + d, where A is a known measurement matrix and d is an additive noise vector with a known distribution. 
411961	41196113	Message-Passing De-Quantization With Applications to Compressed Sensing	This paper introduces a new method, called message-passing de-quantization (MPDQ), for estimating a random vector from quantized linear measurements. It is based on a technique called generalized approximate message passing (GAMP) and allows for overcomplete or undercomplete linear expansions and regular or non-regular scalar quantization. MPDQ is computationally simple and can incorporate various priors on the input vector, such as sparsity-inducing priors used in compressed sensing. Under certain assumptions, the error performance of MPDQ can be accurately predicted using a set of scalar state evolution equations. The paper also presents experimental results demonstrating the effectiveness of MPDQ, particularly in cases where oversampling or undersampling combined with a sparsity-inducing prior is involved. 
411962	41196230	Capacity of Systems with Queue-Length Dependent Service Quality.	This study focuses on the maximum amount of information that can be reliably processed by a server with a varying level of service quality based on queue length. The capacity of the system is measured in terms of bits processed per unit time, and is influenced by arrival and service process distributions. For systems with one arrival per time slot, a memoryless distribution is optimal, while multiple arrivals can negatively impact the system. This research is driven by the need to incorporate reliability into queuing systems, and has practical applications in crowdsourcing, multimedia communication, and stream computing.
411962	41196216	The Non-Regular CEO Problem	This article discusses the CEO problem for non-regular source distributions, where a CEO needs to estimate data based on independently corrupted observations from a group of agents. The agents cannot communicate before transmitting their observations. This problem is motivated by the practical issue of a CEO needing to estimate beliefs about a sequence of events. The observations are jointly distributed with the data through a given probability function. The article examines the minimum achievable mean squared error distortion in the limit of infinite agents and sum rate. It establishes a convergence of distortion and compares it to previous studies on discrete and Gaussian CEO problems. The article proves achievability through a layered architecture using quantization, entropy coding, and estimation, and proves the converse using the Bayesian Chazan-Zakai-Ziv bound.
411963	41196329	Generic Incremental Algorithms for Local Search	Introducing a new constraint in local search requires defining measures for penalty and variable conflicts and implementing incremental algorithms to maintain these measures. This can be time-consuming and reduces productivity. A generic scheme is proposed that automatically generates these measures and algorithms for a constraint described in monadic existential second-order logic with counting. The variable-conflict measure is proven to be bounded by the maximum penalty decrease that can be achieved by changing the value of a variable, ensuring good local search performance. This approach is demonstrated by replacing a built-in global constraint with a modelled version, resulting in competitive runtime and robustness.
411963	4119637	Inferring variable conflicts for local search	To improve the efficiency of local search algorithms, we often only consider moves that change variables that impact the overall penalty. These variables are known as conflicting variables. In this study, we propose a new way to measure the conflict of a variable in a model expressed in existential second-order logic with counting. This measure can be automatically and incrementally evaluated and is bounded by an intuitive conflict measure and the model's penalty. We also show that this approach can successfully replace a built-in global constraint with an ∃SOL+ version while still achieving comparable results.
411964	41196433	Properties of Almost All Graphs and Generalized Quantifiers	The study examines 0-1 laws for first-order logic expanded with Lindström quantifiers, which express graph properties. The logic FO[Q] has a 0-1 law if certain conditions on the quantifier Q are met. Using these conditions, it is shown that FO[Rig], with the quantifier for rigidity, has a 0-1 law. However, extensions with quantifiers for Hamiltonicity, regularity, and self-complementarity do not have a 0-1 law. The question of whether there exists a logic that can express Hamiltonicity or rigidity and has a 0-1 law is considered. It is concluded that there is no such logic for Hamiltonicity, but there is one for rigidity. Additionally, the study examines sequences of vectorized quantifiers and shows that adding quantifiers closed under substructures results in 0-1 laws for extensions of first-order logic. These results also apply to infinitary logic with finitely many variables.
411964	41196410	A restricted second order logic for finite structures	The article discusses a modified version of second order logic called SO ω, where the second order quantifiers operate over relations that are closed under a specific equivalence relation called ≡ k. This restricted version is considered to be an effective fragment of the infinitary logic L ω ∞ ω, but it differs from other fragments as it is not based on a fixed point logic. The article explores the connections between SO ω and fixed point logics, demonstrating that their inclusion relations are equivalent to problems in complexity theory. The expressibility of NP-complete problems in this logic is also examined. 
411965	41196511	Fast $q$-gram Mining on SLP Compressed Strings	This article introduces efficient algorithms for calculating the frequency of $q$-grams in compressed strings represented by a straight line program (SLP). The proposed algorithm has a time and space complexity of $O(qn)$, where $n$ is the size of the SLP. Experiments demonstrate that the algorithm performs well on real string data and outperforms other algorithms designed for uncompressed text. The article also discusses potential applications in data mining and classification of string data. 
411965	4119654	A Fully Compressed Pattern Matching Algorithm For Simple Collage Systems	The FCPM problem involves finding patterns in a compressed text without decompressing it. This is difficult because the patterns are also in a compressed form. The paper introduces an algorithm for solving this problem specifically for simple collage systems, which are a type of dictionary-based compression. The algorithm, called FCPM, has a time complexity of O(|D|^2 + mn log |S|), where D is the dictionary, S is a sequence of variables, n is the length of the text, and m is the length of the pattern. This is an improvement from the previous best result of O(m^2n^2) time. 
411966	4119669	Connected Identifying Codes	The problem of generating a connected identifying code for a graph is discussed. It is shown that this problem is NP-complete and a polynomial-time approximation algorithm, called ${\\tt ConnectID}$, is proposed which produces a connected code of at most twice the size. When the input code is robust to graph distortions, the size of the output code is related to the best error-correcting code of a given minimum distance. The size of the input and output codes converge for increasing robustness, and simulations on various random graphs demonstrate the effectiveness of ${\\tt ConnectID}$. It is found that for Erdős–Rényi random graphs, the connected codes generated are only 25% larger than unconnected codes, and robustness often provides connectivity without increasing the size of the code.
411966	41196632	Joint Monitoring and Routing in Wireless Sensor Networks Using Robust Identifying Codes	Wireless Sensor Networks (WSNs) are used to monitor the physical world, but their limitations make it difficult to provide fundamental network services like routing. To address this issue, the theory of identifying codes is used as an abstraction for WSNs. This has been helpful in solving important problems such as localization and contamination detection. In this work, the authors propose an algorithm for generating robust identifying codes with a logarithmic performance guarantee, based on a novel reduction to the set k-multicover problem. This is the first such guarantee for the NP-hard problem of robust identifying codes. Additionally, the identifying-code infrastructure can also be used for efficient routing with small routing tables. Experimental results show the superiority of this approach over previous heuristics.
411967	41196723	Efficient re-resolution of SMT specifications for evolving software architectures	This study introduces a method for efficiently handling changes in a component-based software architecture. By using formal descriptions and satisfiability modulo theory (SMT) techniques, the method can identify and carry out the necessary steps to ensure compliance with constraints after each change. The approach is demonstrated on a constraint-satisfaction problem related to cloud-based software services, and is proven to be significantly faster than standard SMT resolution methods. The method streamlines the process of re-resolving system constraints, making it a valuable tool for managing changes in software architectures.
411967	41196733	Establishing a Framework for Dynamic Risk Management in `Intelligent' Aero-Engine Control	Safety critical software systems must have control functions that prevent known hazards from occurring. These bounds are determined through safety analyses and implemented through design features. However, unexpected real-world problems can lead to changes in the operating context, making these bounds invalid. This paper discusses the challenges of dealing with these complex and dynamic problems and proposes a self-management framework for on-line risk management. The framework is demonstrated in the context of using intelligent adaptive controllers for Gas-Turbine Aero Engine control. The framework also enables safety assurance arguments necessary for certification. In highly complex problems, it may not be possible to determine precise behavioural bounds prior to deployment, making on-line risk management essential. 
411968	41196818	A description of the diamond grid for topological and combinatorial analysis.	A new coordinate system has been developed for all cells in the diamond grid, allowing for easy retrieval of topological relations between cells. This system can be used in various applications, including image processing and shape analysis, as it simplifies operations such as boundary tracking and computation of the Euler characteristic. The system is based on simple integer operations, making it efficient and suitable for implementation in different contexts.
411968	41196810	A Combinatorial 4-Coordinate System for the Diamond Grid	The article introduces a novel coordinate system for cells in the diamond grid, using four dependent coordinates to address voxels, faces, edges, and corners. These coordinates effectively capture the incidence and adjacency relations of the cells, making them useful for morphological and topological operations. The system allows for a more comprehensive understanding of cell structure and can aid in various analytical and practical applications.
411969	41196952	Transactional Tasks: Parallelism in Software Transactions.	Many programming languages, including Clojure, Scala, and Haskell, offer different models for handling concurrency. However, when these models are combined, their semantics are not always clearly defined. In this paper, the authors explore the combination of futures and Software Transactional Memory (STM). Currently, when futures are created within a transaction, they cannot safely access the transactional state, which can lead to unexpected behavior and violate the serializability of transactions. To address this issue, the authors propose a new construct called transactional tasks, which allow futures to be created within transactions while still providing safe access to the state. Transactional tasks offer several advantages, such as coordination, maintaining serializability, and eliminating non-determinism. Overall, this approach allows for the full utilization of parallelism in a program while preserving the desirable properties of both futures and STM. 
411969	41196972	Enriching the Internet By Acting and Reacting	The rise of rich internet applications (RIAs) has led to a shift towards moving application logic to the client side. This can improve performance and responsiveness, but also presents challenges in terms of concurrency and integrating multiple services. To address these issues, a new programming model based on actors and reactors is proposed. These modular components can be easily coordinated through a unified communication mechanism, reducing the need for complex event-driven code. A collaborative code editor is used as an example to demonstrate the effectiveness of this approach. 
411970	41197046	Automated Derivation of Translators From Annotated Grammars	This paper introduces a method for automating the creation of translators between operations languages, which are specialized programming languages used for satellite operations procedures. The technique utilizes annotated grammars to identify similarities between languages and create a transformation schema. Additionally, the paper suggests using an intermediate representation shared by all operations languages to further simplify the translation process. To demonstrate the effectiveness of this approach, the authors semi-automatically generated translators between several operations languages using a prototype tool they developed.
411970	41197035	APPAREIL: A Tool for Building Automated Program Translators Using Annotated Grammars	Operations languages are essential for writing procedures for spacecraft operations. The APPAREIL tool simplifies the process of creating program translators between these languages by automatically generating them from annotated grammars. This partial translator covers most translations, but may require manual adjustments for more complex cases. To ensure the accuracy of the translation, the tool also includes a control-flow equivalence verification module. This helps to increase confidence in the correctness of the translation. Overall, the APPAREIL tool streamlines the process of creating program translators for operations languages, making it more efficient and accurate.
411971	41197134	Joint generative model for fMRI/DWI and its application to population studies.	The authors suggest a new approach to combine data from diffusion-weighted MRI tractography and resting-state functional MRI correlations using a probabilistic framework. This framework considers the relationship between anatomical and functional connectivity patterns in the brain and can be applied to group studies. They use a mean-field approximation to fit the model to the data and observe significant differences in connectivity between normal controls and patients with schizophrenia. The proposed method provides a promising tool for investigating brain connectivity in various populations and has potential for further research in this area.
411971	41197145	Directional functions for orientation distribution estimation.	Computing the orientation distribution function (ODF) from high angular resolution diffusion imaging (HARDI) signals allows for determining the orientation of fiber bundles in the brain. Previous techniques used spherical harmonics or spherical radial basis functions, but this work introduces three new directional functions that require fewer parameters to represent the measured signals. These functions can also be used in mixture models to represent multiple fiber orientations. The framework is compared to using spherical harmonics on real and synthetic data, and its potential applications in tractography and segmentation are discussed.
411972	41197266	Automatic Analysis of Programming Assignments	In a virtual university setting, there is a need for advanced support in handling assignments, especially homework assignments, due to the lack of face-to-face communication between teachers and students. In this paper, the AT(x) approach (analyze-and-test) is introduced as a way to automatically analyze and test programs. This approach is used to provide feedback to students working on programming exercises and is tailored to specific programming languages such as Prolog and Scheme. The AT(x) framework is further divided into AT(P) and AT(S) for programs written in Prolog and Scheme respectively. 
411972	41197233	Probabilistic knowledge representation using the principle of maximum entropy and Gröbner basis theory	The MaxEnt principle is a commonly used method for reasoning with probabilistic conditional knowledge bases. It aims to minimize the amount of assumed information and achieve unbiased results. This paper explores how MaxEnt distributions can be computed by solving nonlinear equation systems, using the theory of Gröbner bases. A three-phase compilation scheme is developed to extract crucial information from a knowledge base and transform it into a Gröbner basis. This allows for the derivation of necessary conditions for knowledge bases to be consistent and for answering MaxEnt queries. Computational methods for establishing general MaxEnt inference rules are also discussed. 
411973	41197340	Using answer set programming for a decision support system	ACMI is a decision support system used by a German health insurance company to check medical invoices. It is designed to streamline the invoice verification process and ensure accuracy. The system is implemented using DLV, a logic programming language, and utilizes various modules such as a knowledge base and a rules engine. ACMI helps identify potential errors or discrepancies in the invoices, leading to more efficient and effective processing. It also provides transparency and traceability in the decision-making process. Overall, ACMI serves as a valuable tool for the health insurance company in managing and verifying medical invoices.
411973	41197350	Angerona - A Flexible Multiagent Framework for Knowledge-Based Agents.	The Angerona framework is designed for creating knowledge-based agents that are flexible, extensible, and compatible with different knowledge representation methods. It introduces the concept of compound agents, which consist of hierarchies of interacting epistemic and functional components. Different knowledge representation formalisms can be used within one agent and different agents in the same system can have different architectures. This framework is implemented through a flexible JAVA plug-in architecture and utilizes the Tweety library for knowledge representation. It also includes an environment plug-in for communication between agents and a user-friendly GUI for monitoring the system. Angerona and Tweety are open source and well documented for easy use.
411974	4119749	Tutorial: an overview of UML 2	This tutorial provides an overview of the main changes in UML 2, the first major update to the Unified Modeling Language. It covers key features and concepts in a half-day session. The main topics include an introduction to UML 2, new diagram types, enhanced notation, and changes to the metamodel. Other important topics covered are the revised structure of UML 2, the new action language, and the integration of UML 2 with other modeling languages. This tutorial is a concise summary of the major updates and changes in UML 2.
411974	41197441	Tutorial H2: an overview of UML 2.0	This tutorial provides an overview of the significant changes in the first major update of the Unified Modeling Language, UML 2.0. It includes the reasons behind these changes and the perspective of one of the main designers. The structure of UML 2.0 is explained, along with a more in-depth look at the new modeling features and their applications through examples. Additionally, the tutorial discusses how UML 2.0 meets the requirements of model-driven development methods. 
411975	41197530	Application of paraconsistent logic in an intelligent tutoring system	It has been observed that testing students' understanding of new concepts immediately after they are introduced greatly enhances the learning process. The measure of understanding is based on the student's ability to provide accurate answers to questions that require the application of the concept. One solution to determining the level of understanding is to administer tests, which can provide information to intelligent agents and aid in evaluating tests. However, factors such as time and the formulation of questions can greatly affect a student's understanding. To address this, the use of paraconsistent logic is proposed as classical logic is unable to draw conclusions in the presence of inconsistencies.
411975	4119754	Knowledge Assessment Based on Many-Valued Logic	This paper discusses the use of six-valued logic in an assessment component of an intelligent tutoring system. This enables the system to handle situations where input is incomplete or inconsistent. The focus is on the decision making rules used by the intelligent agent to assess students' understanding of new terms. If the assessment is not connected to grading, the system does not use penalty rules to discourage misuse. Instead, rules are implemented to differentiate between students' hesitation in answering and lack of knowledge. This allows for a more accurate assessment of the student's understanding.
411976	41197652	A fractional approach for the motion planning of redundant and hyper-redundant manipulators	This paper introduces a new approach to address the issue of drift in joint space trajectory planning for redundant robots, using a fractional differential of order @a. Two performance measures, positional error index and repeatability performance index, are defined to evaluate the effectiveness of this method. The results of testing on redundant and hyper-redundant planar manipulators show that a wide range of values for @a can achieve repetitive joint trajectories. This technique offers a solution to the problem of drift in joint space trajectory planning for redundant robots.
411976	41197649	Maximin Spreading Algorithm	This paper introduces a genetic algorithm that can effectively optimize uni-objective problems with an infinite number of optimal solutions. The algorithm utilizes the maximin concept and epsilon-dominance to promote diversity in the solution space. It is tested on two well-known functions and produces results that closely match the optimal solutions. The algorithm is also applied to two real-world engineering optimization problems, resulting in a set of optimal solutions for each problem. This demonstrates the algorithm's ability to find multiple ways to solve a problem with equal maximum performance. Overall, the proposed optimization method shows promising results for tackling complex problems with a large number of optimal solutions.
411977	41197739	A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them	Optical flow estimation algorithms have improved significantly, but the basic formulation has remained unchanged since the work of Horn and Schunck. Researchers have analyzed the impact of the objective function, optimization method, and implementation practices on accuracy. Surprisingly, classical methods perform well when combined with modern techniques such as median filtering during optimization. However, this leads to higher energy solutions and does not optimize the original objective function. To address this, a new objective function is developed that includes a non-local smoothness term and information about flow and image boundaries. An asymmetric pyramid downsampling scheme is also introduced to better estimate long-range horizontal motions. The methods are evaluated on various datasets and show promising results.
411977	41197713	A Database and Evaluation Methodology for Optical Flow	In 1994, Barron et al. conducted a quantitative evaluation of optical flow algorithms, which greatly improved their performance. However, current challenges for these algorithms extend beyond the datasets and evaluation methods proposed in that study. These challenges include nonrigid motion, sensor noise, and motion discontinuities in complex natural scenes. To address these issues, the authors propose a new set of benchmarks and evaluation methods, including four types of data to test different aspects of optical flow algorithms. These include sequences with nonrigid motion, realistic synthetic sequences, high frame-rate video, and modified stereo sequences. In 2007, the authors published the performance of several methods on their preliminary data, and since then, more researchers have used the data and shown significant improvements in performance. In this paper, the authors analyze these results and draw conclusions.
411978	41197860	The impact of continuous integration on other software development practices: a large-scale empirical study.	Continuous Integration (CI) has brought about significant changes in the software development process. With the right tools and implementation, it has been proven to positively impact pull request throughput and the ability to handle larger projects. However, like any new innovation, adopting CI requires adapting existing practices and implementing "best practices" for maximum benefit. A study was conducted on the adoption and evolution of code writing, submission, issue and pull request closing, and testing practices as Travis CI was adopted by established projects on GitHub. A survey of GitHub developers was also conducted to gain insight into their experiences with adopting CI. The results suggest a more nuanced understanding of how GitHub teams are adapting to and utilizing CI technology compared to previous research.
411978	41197867	A Data Set for Social Diversity Studies of GitHub Teams	The software development process is influenced by social diversity within programming teams, just like any other team-oriented activity. The impact of diversity can be significant, but it is complex, especially in decentralized teams. To fully understand the effects of diversity on team effectiveness, quantitative studies using large data sets are necessary. This article introduces a data set of 23,493 GitHub projects that includes social diversity attributes of programmers, such as location and gender. The authors demonstrate how this data set can be used to study diversity in software teams through case studies, and they hope it will encourage further research in this area. 
411979	4119794	Fuzzy Classification of Web Reports with Linguistic Text Mining	This paper introduces a fuzzy system that is able to classify textual web reports. The system utilizes third party linguistic analyzers and builds upon previous work in web information extraction and fuzzy inductive logic programming. The main contributions of the paper include formal models, a prototype implementation of the system, and evaluation experiments. The system shows promise in accurately classifying web reports and has potential for practical applications. 
411979	41197936	Induction of Fuzzy and Annotated Logic Programs	The current focus of research in data mining is on developing methods to handle imperfection, such as uncertainty and imprecision. This involves a heavy emphasis on probability models, as well as studying imperfection in fuzzy logic. Specifically, the paper discusses fuzzy logic programs (FLP) and Generalized Annotated Programs (GAP) and proposes a formal model for fuzzy inductive logic programming (FILP) and induction of GAP programs (IGAP). The lack of a standardized proof-theoretic approach in FILP is addressed, and a new method for learning from entailment in IGAP is described. This approach is applied to detecting user preferences in a web search application, and is compared to other fuzzy ILP methods.
411980	41198061	Dynamical Systems and Stochastic Programming: To Ordinary Differential Equations and Back	This paper focuses on the relationship between models of biological systems using ordinary differential equations (ODE) and models using stochastic and concurrent constraint programming (sCCP). The authors propose a method to convert between the two types of models and investigate their properties. They show that the conversion from sCCP to ODE's preserves rate semantics for biochemical models and study the invertibility of the mappings. They also examine the preservation of dynamics in the models obtained through the mappings, providing a convergence theorem in the direction from ODE's to sCCP. Several examples are discussed in detail to show that dynamics may not be preserved in the inverse direction. 
411980	41198020	Hybrid dynamics of stochastic programs	Stochastic Concurrent Constraint Programming (sCCP) is a process algebra based on CCP that has a hybrid automaton semantics. Each sCCP program is associated with both a stochastic and a non-deterministic hybrid automaton. These automata are compared to the standard stochastic semantics given by a Continuous Time Markov Chain and the one based on ordinary differential equations, which is obtained through a fluid-flow approximation technique. Two case studies, Repressilator and the Circadian Clock, are discussed in detail, focusing on the robustness of different semantic models and the impact of discreteness on the system's dynamics.
411981	41198111	A Delay-Based Piggryback Scheme In Ieee 802.11	Data frames can be combined with control frames to improve channel efficiency in wireless communication. However, this piggyback scheme can lead to decreased efficiency and increased frame transmission delay for other stations with low transmission rates. This is similar to the anomaly phenomenon in multi-rate transmission networks. To address this issue, a delay-based piggyback algorithm is proposed, which considers the delay efficiency in IEEE 802.11 WLAN when deciding whether to use the piggyback scheme for a control frame. Simulation results show that this algorithm can reduce average frame delay and channel utilization by approximately 24% and 25%, respectively, even when there is one station with a low transmission rate. 
411981	41198148	Beamforming strategy using adaptive beam patterns and power control for common control channel in hierarchical cell structure networks	Beamforming techniques have been successful in solving interference problems for traffic channels, but their use for control channels has not been thoroughly researched. A new (semi-) centralized beamforming strategy is proposed for hierarchical cell structure (HCS) networks, which adapts beam patterns and controls transmit power to enhance the performance of the common channel. This strategy is demonstrated through examples in two-tier HCS networks, consisting of macro and pico cells, with low complexity. System-level simulations show that this strategy provides a significant improvement compared to random beamforming and conventional systems that do not use the proposed algorithm. The proposed scheme can be applied to multiple adjacent cells, even if it is initially designed for the pico cell. The performance metric used is the cumulative distribution function of user geometry or channel quality, as the number of outage users is more important for the common control channel than the sum rate.
411982	41198227	Shape-Simplifying Image Abstraction	This paper presents an algorithm for creating stylistic abstractions of photographs. The method uses mean curvature flow and shock filter to simplify shapes and colors while preserving important features. A constrained mean curvature flow is also developed to better convey directionality and shape boundaries. The algorithm is iterative and allows for intuitive control over the level of abstraction. Users can also incorporate masking to control the speed of abstraction and protect certain regions. Experimental results demonstrate that this method produces highly abstract and feature-preserving illustrations from photographs. 
411982	41198262	Intrinsic Image Decomposition Using Structure-Texture Separation And Surface Normals	Intrinsic image decomposition is a complex problem that has been extensively studied for many years, but it still presents challenges. This is due to commonly used constraints being too restrictive to accurately capture the rich textures present in natural images. In this paper, a new image model is proposed that effectively handles textures in decomposition, even with simple constraints. Additionally, a new constraint is introduced that utilizes surface normals from an RGB-D image to promote consistency in the shading layer. This constraint is based on a locally linear embedding framework and is effective for Lambertian surfaces. The combination of this new image model and constraint outperforms existing approaches in producing high quality results.
411983	4119831	Generating counterexamples of model-based software product lines.	A model-based software product line (MSPL) uses a variability model to capture the differences in a domain and base models to represent the core artifacts. Realization models connect the features in the variability model to the base model elements and trigger operations based on a configuration. However, managing the design space of an MSPL can be extremely complex for engineers due to the large number of variants and the need for conformity to rules. To address this, the paper proposes using counterexamples (or antipatterns) to identify invalid product models, which can help in creating guidelines and testing oracles. A generic process using the common variability language (CVL) is provided to randomly search for MSPLs, and a tool called LineGen is developed to support this process. The effectiveness of this approach is validated with different formalisms and in an industrial scenario.
411983	41198355	Extraction and evolution of architectural variability models in plugin-based systems	Variability management is crucial in building and updating software systems, allowing them to be tailored to individual customers and deployment contexts. This is often achieved through extensible systems with numerous configuration options provided by plugins. In an ideal scenario, a software architect can easily generate a specific system variant by selecting the necessary plugins. To achieve this, variation points and constraints must be properly modeled and maintained over time. However, this is a challenging and time-consuming task. In this article, the authors propose a reverse engineering process to create a feature model of a plugin-based architecture. This approach utilizes automated techniques to extract and combine different variability descriptions, including a software architecture model, plugin dependencies, and the software architect's knowledge. By analyzing differences between versions of architectural feature models, the approach allows for effective control of both the extraction and evolution of variability. The proposed process was successfully applied to a large-scale system and the authors share their experience in this context.
411984	41198418	Towards a more semantically transparent i* visual syntax	The i* modelling language is commonly used in Requirements Engineering to facilitate communication between technical and non-technical stakeholders. However, recent research has revealed flaws in its visual syntax, which can affect the effectiveness of model-mediated communication. In this paper, the focus is on one specific quality criterion, Semantic Transparency, which refers to the ability of notation symbols to convey their meaning. The authors propose an empirical approach to address this issue and present a series of experiments aimed at identifying a new symbol set for i* and evaluating its semantic transparency. While this work does not solve all problems in the i* notation, it serves as an important step towards creating visually effective requirements modelling languages. The authors suggest that this approach can be applied to other quality criteria and other notations in the future.
411984	41198493	Discovering sustainability requirements: an experience report	The concept of sustainability has become a major concern for our society, and the design and use of software-intensive systems plays a crucial role in achieving this goal. However, current software requirements engineering practices do not specifically address sustainability requirements, unlike other quality requirements such as security and usability. This paper discusses a software project where sustainability requirements were given equal importance and systematically identified, analyzed, and documented. The authors aimed to evaluate the effectiveness of existing techniques in supporting these activities. This experience highlights the need for more attention to be given to sustainability concerns in requirements engineering and suggests that even small steps can lead to more sustainable systems. It also provides valuable insights for researchers in the field of requirements engineering and sustainability.
411985	41198552	Extraction and evolution of architectural variability models in plugin-based systems	Variability management is an important aspect in building and updating software systems, allowing for customization and adaptation to meet customers' needs and deployment contexts. Extensible software systems, built on plugin-based architectures, offer a wide range of configuration options through plugins. To efficiently generate system variants, the software architect must accurately model the variation points and constraints between architectural elements. This task can be time-consuming and prone to errors. In this article, a reverse engineering process is proposed to produce a variability model of a plugin-based architecture. This involves automated techniques to extract and combine different variability descriptions, such as a hierarchical software architecture model, plugin dependency model, and software architect knowledge. By analyzing differences between versions of architectural feature models, the software architect can control the variability extraction and evolution processes. The approach has been successfully applied to a large-scale plugin-based system, FraSCAti, with different versions of its architecture.
411985	41198538	Early Consistency Checking Between Specification And Implementation Variabilities	The software product line (SPL) engineering approach requires consistency between the specified variability and the core-code assets. However, current support for checking this consistency is limited, especially when multiple variability implementation techniques are used. This can lead to variability inconsistencies, which can be detected early in the development process through the proposed method. This method models implemented variability using variation points and variants and uses slicing to check for inconsistencies between specification and implementation levels. The approach has been successfully applied in four case studies and is available for public use, providing a quick and automated way to ensure consistency in the development process.
411986	41198625	NAP: practical fault-tolerance for itinerant computations	Mobile agents can be used to support itinerant computation, which is a program that moves from host to host in a network. The program can have a pre-defined itinerary or dynamically compute the next host to visit. It can also create multiple copies of itself on a single host and visit the same host repeatedly. However, this type of computation is vulnerable to failures such as processor failures, communications failures, and program bugs. To address this, NAP is a protocol that uses failure detection and recovery to provide fault tolerance for itinerant computations. NAP is implemented in TACOMA and offers guarantees for its effectiveness.
411986	41198631	Efficient detection of a class of stable properties	This article presents a protocol for detecting a type of stable property, called locally stable properties, in distributed systems. The protocol utilizes a decentralized approach to construct a subset of local states that are mutually consistent, using a modified version of vector time stamps. The protocol is adaptable and can be refined for specialized property detection, and has been proven to be efficient through the derivation of two existing protocols. 
411987	41198720	A hybrid intelligent algorithm for solving the bilevel programming models	This paper presents a hybrid intelligent algorithm that combines genetic algorithm (GA) and neural network (NN) to solve bilevel programming models. GA is used to select potential combinations, while a meta-controlled Boltzmann machine (consisting of a Hopfield network and a Boltzmann machine) determines the optimal solution. The algorithm is applied to solve two-level investment problems, where the upper layer determines optimal company investments and the lower layer decides optimal department investments. Examples are provided to demonstrate the effectiveness and practicality of this method. 
411987	41198729	A genetic algorithm based double layer neural network for solving quadratic bilevel programming problem	This paper presents a hybrid intelligent algorithm for solving the quadratic bilevel programming problem. It combines an intelligent genetic algorithm and a double layer neural network to efficiently select potential solutions and determine the optimal solution for the lower level. The neural network is formulated using a meta-controlled Boltzmann machine, which combines the Hopfield model and the Boltzmann machine. Numerical experiments show that this approach is effective in solving quadratic bilevel programming problems.
411988	41198844	A framework for integrating real-time MRI with robot control: application to simulated transapical cardiac interventions.	The use of real-time intraoperative image guidance has opened up new possibilities for surgical interventions, such as image-guided robot assistance. However, current methods only allow for visual perception during the procedure. This study introduces a framework for performing robot-assisted interventions with real-time MRI guidance. The framework processes the MRI data in real-time and integrates it with robot control, providing a visualization and force-feedback interface for the operator. Experimental results show that this approach significantly improves the speed, accuracy, and safety of procedures, reducing the duration and incidents of tissue collisions. 
411988	41198818	Visual and force-feedback guidance for robot-assisted interventions in the beating heart with real-time MRI	Robot-assisted surgical procedures are constantly improving in order to provide better patient treatment and reduce healthcare costs. One way to enhance these procedures is by incorporating real-time imaging during surgery, which can provide crucial information for the operator. In this study, a guidance approach was developed that uses real-time MRI to assist with robot-assisted procedures in a beating heart. This approach offers both visualization and force-feedback guidance to safely maneuver an interventional tool in the dynamic environment of the heart's left ventricle. Testing on a simulated scenario of transapical aortic valve replacement showed improved control and manipulation, providing effective and accurate assistance to the operator in real-time.
411989	41198951	Power-Aware Computing in Wearable Sensor Networks: An Optimal Feature Selection	Wearable sensory devices are being increasingly used in healthcare and well-being applications. These devices use classification algorithms to detect specific events in real-time. However, selecting the right features for these algorithms is important to ensure efficient use of resources and accurate detection of critical events. In this paper, a power-aware feature selection method is proposed to minimize energy consumption in classification applications. This approach takes into consideration the energy cost of individual features and uses a graph model to represent their correlation and computing complexity. The problem is formulated using integer programming and a greedy approximation is presented to select features in a power-efficient manner. Experimental results show significant energy savings of over 30 percent while maintaining high classification accuracy. 
411989	41198940	A wireless communication selection approach to minimize energy-per-bit for wearable computing applications	Body Sensor Networks (BSN) offer a way to continuously monitor human movements, improving medical care and enabling remote patient monitoring. However, BSNs face challenges in terms of wearability, with battery size having a significant impact. To address this, a burst communication technique is proposed in this paper, utilizing data buffering to reduce energy-per-bit cost and allowing for larger packet sizes or more efficient communication schemes. An energy model is developed to optimize the transmission schedule, taking into account both signal processing requirements and task deadlines. The proposed approach is demonstrated in sway monitoring BSN applications, showing a significant improvement in energy cost compared to traditional streaming methods. Results also highlight the relationship between task deadline extension and energy cost. 
411990	41199031	Zero-Effort Camera-Assisted Calibration Techniques for Wearable Motion Sensors	Activity recognition using wearable motion sensors is crucial for monitoring health and wellness. However, these algorithms are designed to work with a specific orientation of the sensors on the body. If the sensors are accidentally displaced, it is important to recalibrate their new location and orientation. Traditionally, this requires the user to perform specific movements or manually input information about sensor placement. In this paper, the authors propose a camera-assisted calibration method that eliminates the need for extra effort from the user. The calibration occurs seamlessly when the user appears in front of a camera and performs any activity of their choice. Experimental results show the effectiveness of this approach.
411990	41199079	Impact of sensor misplacement on dynamic time warping based human activity recognition using wearable computers	Monitoring daily activities is crucial for detecting diseases early and improving the quality of life, especially for the elderly. A network of wireless wearable sensors can track daily motions and provide a continuous stream of data for activity recognition. Dynamic Time Warping (DTW) is a commonly used method for pattern matching in time-series data, known for its robustness to variations in time and speed. However, for activity recognition, DTW is limited in its ability to handle changes in sensor location and orientation. This can result in decreased accuracy if sensors are misplaced. To address this, the study uses a marker-based motion capture system to simulate different sensor configurations and assess the worst-case scenarios for sensor misplacement. This research aims to improve the performance of DTW for real-time detection of daily activities, such as sit-to-stand movements.
411991	41199145	The minimum number of triangles in graphs of given order and size.	In the 1940s and 50s, Erdős and Rademacher posed the question of finding the maximum number of triangles in a graph with a given number of vertices and edges. This problem has been studied extensively and was recently solved by Razborov in 2008. Further developments by Nikiforov and Reiher have led to an exact solution for large graphs with edge density not approaching one. This result confirms a conjecture made by Lovasz and Simonovits in 1975 for almost all cases. 
411991	41199111	Coloring d-Embeddable k-Uniform Hypergraphs.	This paper explores the Four Color Theorem in relation to hypergraphs, specifically those that can be embedded into \(\mathbb {R}^d\). The maximum chromatic number of these hypergraphs is investigated, with lower and upper bounds being determined. It is shown that for \(d\ge 3\), there are hypergraphs in \(\fancyscript{H}_{2d-3,d}\) with a chromatic number of \(\Omega (\log n/\log \log n)\), while for hypergraphs in \(\fancyscript{H}_{d,d}\) with \(n\) vertices, the chromatic number is bounded by \({\mathcal {O}}(n^{(d-2)/(d-1)})\). 
411992	41199241	Testing in context: framework and test derivation	The paper discusses the challenge of testing a component within a modular system. The component's "context" is defined as the rest of the system, which serves as its operational or testing environment. The authors propose a framework for testing in context using a model of communicating finite state machines. They address the issues of test executability and fault propagation in the presence of the context by computing an approximation of the component's specification. This approximation allows for tests to be executed and faults to be propagated through the context. The paper also presents a conformance relation for test derivation, based on the reduction relation between an implementation and the approximation of the specification. The authors also provide an approach for test generation in the case of deterministic implementations.
411992	41199252	Petri net-based protocol synthesis with minimum communication costs	Protocol synthesis is a method used to create a protocol specification for a distributed system of networked computers, based on a service specification. This helps to reduce design costs and errors by defining the communication between application components. The paper proposes a new synthesis method that uses extended Petri nets and an integer linear programming model to generate optimized protocol specifications. This model considers various cost criteria such as the number and size of messages, frequency of execution, communication channel costs, and resource placement costs. An application example is provided, along with experimental results. This approach can be applied to various application areas to minimize communication costs and improve overall efficiency. 
411993	41199337	Experiences of using a PKI to access a hospital information system by high street opticians	This paper discusses a system that allows opticians to access patient data from a hospital's Diabetes Information System through a standard web browser. The system uses strong encryption and digital signatures to ensure the security of the data as it travels through the internet. The paper describes the public key infrastructure and the changes made to the web interface to suit the opticians' needs. The results of pilot testing and feedback from opticians are also discussed. The authors found that a well-designed system with a seamless PKI can improve patient care and data quality, but slow internet speeds and difficulty in creating a user-friendly interface can be challenges. They also highlight the importance of maintaining all components and administrative procedures to ensure the system's availability.
411993	41199317	Obligations Of Trust For Privacy And Confidentiality In Distributed Transactions	This paper introduces the concept of the obligation of trust (OoT) protocol, which aims to enhance privacy and authorization in distributed transactions. The protocol allows two parties to dynamically exchange their privacy and authorization requirements and commitments, promoting a more user-centric and symmetric approach to privacy protection. The authors demonstrate the applicability of the OoT protocol in various scenarios and highlight its potential to improve upon existing mechanisms. By providing digitally signed acceptance of obligations messages, the protocol also strengthens legal protection for parties involved in a dispute. This paper contributes to current research in trust negotiation, privacy protection, and authorization by combining these three elements into a standardized protocol. 
411994	41199431	Subjectively Interesting Component Analysis: Data Projections that Contrast with Prior Expectations	High-dimensional data can be difficult to explore without effective low-dimensional projections. Principal Component Analysis (PCA) is a commonly used method for finding such projections, as it is simple and often captures the most relevant structure in the data. However, not all users may find the dominant structure interesting. To address this, a new method called Subjectively Interesting Component Analysis (SICA) has been introduced. This method uses information theory and a user's prior expectations to find projections that are truly surprising and interesting. The optimization problem is straightforward and results in a balance between explained variance and novelty. Case studies on various types of data demonstrate the effectiveness of SICA in finding subjectively interesting projections. 
411994	41199448	Subjectively Interesting Subgroup Discovery on Real-valued Targets.	Deriving insights from high-dimensional data is a challenging task in data mining due to the large number of variable combinations to consider. Can this search for interesting patterns be automated? In this study, the focus is on learning efficiently about real-valued attributes. A method is proposed to identify subgroups in the data that are highly informative, using the Subjective Interestingness framework FORSIED. These informative subgroups are described using a range of attribute types and are selected based on their maximization of information with respect to one or more target attributes. This approach allows for the incorporation of prior knowledge in the search for the most informative patterns.
411995	4119955	Automating News Content Analysis: An Application to Gender Bias and Readability	This article discusses the use of text-analysis technologies to aid social science research by analyzing patterns in news content. The system collects and annotates a large amount of textual data, allowing for the extraction of trends and patterns. The study focuses on 3.5 million news articles and identifies a relationship between their topics, gender bias, and readability. This demonstrates the potential for pattern analysis technology to automate tasks typically done by humans in the social sciences, making large-scale studies possible. 
411995	41199556	Modelling and predicting news popularity	In this study, the problem of predicting the popularity of online news articles is explored. Instead of simply classifying articles as popular or unpopular, the concept of popularity is viewed as a competitive situation where the most appealing articles are considered popular. A linear function is used to model this appeal, and is learned from a training set of popular and non-popular articles from various news outlets. The method is able to accurately predict which articles will become popular and identify keywords that influence the appeal function. This approach allows for comparison of reader preferences among different news outlets using limited information such as article content, publication date, and popularity status.
411996	41199653	Automating Customisation of Floating-Point Designs	This paper presents a method for customizing the representation of floating-point numbers using re-configurable hardware. The method determines the appropriate size of the mantissa and exponent for each operation, in order to satisfy a cost function with a specified error compared to a reference representation. The implementation of this method is iterative and supports IEEE single-precision or double-precision floating-point representation as the reference. This tool can produce custom floating-point formats with varying sizes of mantissa and exponent, and can be applied to different arithmetic representations and hardware designs. The results demonstrate that this method can improve the speed and size of hardware for calculations with large dynamic ranges compared to using the reference representation. 
411996	41199674	Automating custom-precision function evaluation for embedded processors	Embedded processors often lack dedicated floating-point units due to resource and power limitations. This results in slow operation when floating-point arithmetic is needed, such as with the IBM PowerPC processor in Xilinx Virtex-II Pro FPGAs. To address this issue, a customizable mathematical library using fixed-point arithmetic for elementary function evaluation has been developed. The library approximates functions using polynomial or rational approximations based on user-defined accuracy requirements. The input and output data are compatible with IEEE single-precision and double-precision floating-point formats. Tests show that the 32-bit polynomial method offers 80 times faster performance than the Xilinx single-precision library, while the 64-bit method offers 30 times faster performance.
411997	41199781	Multilevel logic synthesis for arithmetic functions	Arithmetic functions, a type of Boolean function, can be efficiently described using the AND and XOR operators. This is exemplified by n-bit adders. A new logic synthesis method for arithmetic functions is presented in this paper, using their natural representations in the field GF(2). This method involves algebraic factorization to reduce the number of literals. However, a direct translation of the AND/XOR representations often results in large area costs due to XOR gates. To address this issue, the paper proposes a redundancy removal process that reduces many XOR gates to single AND or OR gates without affecting the function. This process only requires simulating a small set of input patterns and has shown to produce circuits with 17% improvement in area and 50% reduction in run time compared to Berkeley SIS 1.2. The resulting circuits also have good testability and power consumption properties.
411997	411997106	Scan encoded test pattern generation for BIST	This paper describes a new scan-based BIST (built-in self-test) scheme that can achieve high fault coverage without modifying the mission logic or using complex hardware. The approach uses scan order and polarity to create a ROM that encodes a few test vectors, which are then used to generate other vectors by randomly complementing their coordinates. This method effectively tests for random pattern resistant faults, which is a common issue with traditional LFSR-based BIST. The proposed method has a lower hardware cost and more efficient algorithm compared to previous methods, and experimental results show that it can achieve high fault coverage with a smaller test set.
411998	41199811	Look-Up-Table Controller Design For Nonlinear Servo Systems With Piecewise Bilinear Models	This paper discusses the use of servo control for nonlinear systems that are approximated by piecewise bilinear models, which are fully parametric. The input-output feedback linearization technique is utilized to stabilize the PB control systems. The paper presents two approaches to designing servo controllers for these nonlinear systems, including a two-degree of freedom controller and a model following controller. The effectiveness of the proposed method is demonstrated through illustrative examples, where the controller is represented as a Look-Up-Table. Overall, this paper provides a practical and effective method for designing servo controllers for nonlinear systems with PB models. 
411998	41199845	Lut Controller Design With Piecewise Bilinear Systems Using Estimation Of Bounds For Approximation Errors	This article presents a proposal for stabilizing nonlinear control systems that are approximated by Piecewise Bilinear (PB) models. The PB models are fully parametric and use a Look-Up-Table (LUT) to represent the controller. The authors suggest using Input-Output (I/O) feedback linearization to stabilize PB control systems and believe that combining PB modeling with conventional feedback linearization is an effective method for analyzing and synthesizing nonlinear control systems. They also introduce a method for designing robust stabilization controllers that takes modeling error into account. The feasibility of these proposals is confirmed through examples.
411999	41199956	Adaptation in open systems: giving interaction its rightful place	Adapting software in open systems, where autonomous and diverse participants interact, is a challenge. In these systems, each participant must adapt, but often needs to interact with others to do so. Current approaches to software adaptation, which rely on control-based abstractions, do not work well in these settings. To address this, the authors build on a recent model of interaction through social commitments. They formalize the idea of a participant's strategy for a goal, including the commitments required, and propose a framework for adaptation based on this strategy. This allows for the use of any selection criteria, such as trust. The authors provide examples from the emergency services field to illustrate their approach.
411999	41199989	A Goal-Based Organizational Perspective on Multi-agent Architectures	A Multi-Agent System (MAS) is a group of independent agents that work together to achieve common goals. This paper proposes different architectural styles for MAS, based on concepts from organization theory and strategic alliances literature. These styles are modeled using the i* framework and evaluated for software quality attributes like predictability and adaptability. The paper also suggests using micro-level patterns to further describe the architecture of a MAS. An e-business example is used to illustrate the proposed styles and patterns. This research is being conducted within the Tropos methodology for agent-oriented software development.
412000	41200056	The BTR-Tree: Path-Defined Version-Range Splitting in a Branched and Temporal Structure	The BTR-Tree is a new access method for branched-and-temporal databases that allows for the updating of both historic and current versions. It is based on the BT-Tree and is designed to balance space and access time tradeoffs. The BTR-Tree splits at a previous version while maintaining the posting property, reducing redundancy in the structure. It has better space efficiency and similar query efficiency compared to the BT-Tree, with no added complexity in search and posting algorithms. 
412000	4120008	Measuring and Optimizing a System for Persistent Database Sessions	The need for high availability in both data and applications is increasingly important for businesses. While database systems have support for recovery, server outages can still result in lost work for applications. This can lead to degraded application availability and require manual restarts. The Phoenix/ODBC system offers persistent database sessions that can survive a crash without the application being aware, improving availability and eliminating the need for application programming to handle crashes. It can be used with any application that accesses a database through ODBC and can be adapted for various data access protocols. An extension has also been developed to optimize response time and reduce overhead for OLTP workloads, with performance evaluations showing modest overhead.
412001	4120011	A concern architecture view for aspect-oriented software design	Aspect-oriented programming is gaining popularity, but there has been a lack of support for the independent description and incremental design of aspects. A conceptual framework is proposed where aspects are seen as enhancements that modify an existing design. The Concern Architecture model is used to group aspect designs and identify dependencies and potential conflicts in systems with multiple aspects. Aspects are described generically with required and provided elements, similar to formal parameters, and their binding to an existing design shows where modifications will occur. A Concern Architecture Diagram shows the overlap and partial order of aspects and concerns. An example of a UML profile and the design of a digital sound recorder illustrates the effectiveness of this approach.
412001	41200127	A calculus of superimpositions for distributed systems	A superimposition is a program module that adds functionality to an existing distributed program, even across language modularity constructs like processes or objects. There are two ways to combine superimpositions to create new ones: sequential combinations, where one is applied after the other, and merging combinations, where each is applied independently. The applicability conditions and result assertions of the component superimpositions are compared to determine if a combination is possible, and if so, the specifications and code of the resultant superimposition are obtained. By using combinations of superimpositions from libraries, less manual construction is needed and programming techniques can be standardized. Examples include dining philosopher algorithms, handling process addition and deletion, and snapshot algorithms.
412002	41200261	MediaRank: Computational Ranking of Online News Sources.	In today's political climate, the quality of news has become a major concern for both the public and the academic community. Traditional news media is facing growing distrust, making it difficult to establish a common understanding of the truth. In response, a team has developed MediaRank, an automated system that ranks over 50,000 online news sources based on four criteria: peer reputation, reporting bias, financial pressure, and popularity. This paper presents the major contributions of MediaRank, including its open and interpretable rankings for various languages and its new methods for measuring influence and bottomline pressure. Additionally, the paper analyzes the effect of media bias on news sources and finds that despite political differences, high-ranking sources tend to favor left-wing parties in English-speaking countries.
412002	412002103	DeepWalk: online learning of social representations	DeepWalk is a new approach for learning latent representations of vertices in a network. It encodes social relations in a continuous vector space, making it useful for statistical models. The method is inspired by language modeling and unsupervised feature learning and uses local information from truncated random walks to learn latent representations. DeepWalk's performance is demonstrated on multi-label network classification tasks for social networks like BlogCatalog, Flickr, and YouTube. It outperforms other methods, especially when there is missing data, and requires less training data. The algorithm is scalable and can be easily parallelized, making it suitable for real-world applications such as network classification and anomaly detection.
412003	4120034	When the functional composition drives the user interfaces composition: process and formalization	Mashups have simplified the reuse of applications by allowing for easy combination of different applications. However, the resulting composite applications do not allow for data sharing or complex workflows. This can only be achieved by composing applications at the functional level to create new services. However, this process requires the redesign and regeneration of user interfaces in order to interact with the new service. This paper proposes a solution to this issue by implementing a process that abstracts the applications and their functional composition, allowing for the reuse of user interfaces. The process also uses a mixed-initiative framework to solve any composition conflicts, either automatically or with developer intervention.
412003	41200349	An Architecture to Support the Collection of Big Data in the Internet of Things	The Internet of Things (IoT) is a network of physical objects connected to each other, collecting and sharing information. This creates a large amount of data, making the IoT an example of Big Data. This paper proposes a software architecture that supports the collection of sensor-based data in the IoT. The architecture covers the entire process, from collecting data through sensors to storing it in a cloud-based system. It can be used for research or practical purposes, such as creating innovative applications for end-users. The architecture has been tested and validated in a project called SMARTCAMPUS, where sensors are used to gather data on the SophiaTech campus for various purposes. 
412004	4120040	Handling Skew in Multiway Joins in Parallel Processing.	Dealing with uneven data distribution, known as skew, is a major challenge in query processing. In distributed environments like MapReduce, minimizing communication cost is a key factor. This refers to the amount of data transferred from mappers to reducers in a MapReduce job. This paper introduces a new method for handling skew in a multiway join computation using only one MapReduce round with minimal communication cost. The technique is based on the Shares algorithm and aims to optimize communication cost in distributed environments. 
412004	41200470	Optimizing joins in a map-reduce environment	Map-reduce is being utilized to handle large data operations. A new approach is being proposed to optimize joining multiple relations in the map-reduce environment. This approach involves identifying a "map-key" and assigning a "share" to each attribute, which determines how tuples are replicated. An algorithm is also presented to detect and fix issues with incorrect map-key attributes. The approach is then applied to two special cases: chain joins and star joins, resulting in less replication and improved performance. While not always superior, this method is particularly effective for analytic queries involving large fact tables and queries involving high out-degree graphs like the Web or social networks. 
412005	41200569	An Efficient Progressive Bitstream Transmission System for Hybrid Channels With Memory	This article discusses progressive transmission over a hybrid channel that introduces both bit errors and packet erasures. The authors analyze existing solutions and extend them to a channel with memory on both bit errors and packet erasures. They then propose a simple and low-complexity coding scheme that transforms the hybrid channel into a single impairment channel, making it easier to optimize. Both rate-based and distortion-based optimization problems are explored and it is shown that the proposed solution has lower complexities compared to known solutions. Simulation results demonstrate the effectiveness of the proposed solution, and numerical results show a close match between rate-based and distortion-based optimization.
412005	412005158	A Low Complexity Progressive Bitstream Transmission System for Hybrid Channels with Correlated Loss	The proposed coding scheme is designed for the transmission of a progressive bitstream in the presence of correlated bit errors and packet erasures. It simplifies the process by transforming the hybrid channel into a single impairment channel, which can then be optimized using various techniques. The results of numerical simulations show that this scheme outperforms existing solutions in the useful operating regions of the channel. It utilizes joint source-channel coding, progressive transmission, rate allocation, and product codes.
412006	4120063	On early detection of application-level resource exhaustion and starvation.	This article discusses a system designed for early detection of application-level exhaustion and starvation attacks. It utilizes a unique detection algorithm based on timed probabilistic finite automata and does not require source code or debugging information. The system is implemented through kernel monitoring and has low overhead and high accuracy compared to a user-space version. Experiments were conducted using synthetic and real-world attacks, such as Apache Killer and Slowloris, on various applications, including the Apache server. The article also includes a theoretical analysis of the potential advantage for attackers who are familiar with the system, measured by a new metric. This metric can also be used to control resources consumed by demanding inputs to protected programs.
412006	41200681	Resilient and Scalable Cloned App Detection Using Forced Execution and Compression Trees	The Android market has grown in both size and diversity, offering a wide range of apps for different purposes. However, this has also led to the threat of app cloning, where adversaries copy an app and make minimal changes to redistribute it for their own gain or to distribute malicious content. Existing methods to detect clones are often ineffective and time-consuming. In this paper, the authors introduce Dexsim, a dynamic analysis system that uses a unique algorithm to accurately identify similarities in app code. The experiments showed that Dexsim is both scalable and resilient to obfuscation, detecting clones in a short amount of time with high accuracy.
412007	412007147	A cooperative immunization system for an untrusting Internet	Viruses and worms are major security threats in computer systems today, and users often rely on anti-virus programs and firewalls to protect their machines. However, these solutions can be unreliable as they rely on manual configurations and human intervention, making them vulnerable to attacks. To address this issue, a cooperative immunization system is proposed where nodes work together to defend against attacks. This system is evaluated using a virus model and simulations, which demonstrate its effectiveness against viruses and malicious participants. Overall, the cooperative immunization system offers a more robust and efficient method for protecting computer systems from attacks.
412007	4120071	Path-Based Access Control for Enterprise Networks	Enterprise networks are becoming more prevalent and complex, but the methods for managing security policies have not kept pace with advancements in networking technology. Typically, policies are defined on a per-application basis and do not interact with each other. This can lead to vulnerabilities, as there is no way for a web server to communicate its decisions to a firewall, even though they may be relevant. To address this issue, a path-based access control system for service-oriented architecture (SOA) networks is proposed, allowing services to share access-related information with each other. This system can prevent attacks where individual services make correct decisions but the overall network behavior is incorrect. It is demonstrated through graph-based policies and using the KeyNote trust management system.
412008	41200820	Whole brain diffeomorphic metric mapping via integration of sulcal and gyral curves, cortical surfaces, and images.	The paper introduces a new algorithm for registering brain images, called the large deformation diffeomorphic metric mapping algorithm. This algorithm is able to simultaneously map sulcal and gyral curves, cortical surfaces, and intensity images from one brain to another using diffeomorphisms. It is the first time that this type of mapping has been done in a unified manner using a shape space of intensity images and point sets. The paper describes the mathematical equation behind the algorithm and how it is implemented using computationally friendly kernels. The algorithm is then applied to align magnetic resonance brain data and is found to outperform other existing algorithms in terms of mapping accuracy and whole brain alignment.
412008	41200840	Robust diffeomorphic mapping via geodesically controlled active shapes	This paper discusses the use of diffeomorphic active shapes, specifically geodesically controlled diffeomorphic active shapes (GDAS), in the mapping of subcortical template structures. The parametrization of GDAS is done through initial momentum representation, with the dimension constrained using principal component analysis. The goal of this work is to use template surfaces to generate segmentations of the hippocampus, using three data attachment terms: surface matching, landmark matching, and inside-outside modeling. This is achieved through energy minimization and a variational solution, with a gradient descent strategy employed for numerical optimization. The robustness of this algorithm is demonstrated through comparison to an existing diffeomorphic landmark matching algorithm in a large neuroanatomical study.
412009	41200940	Pollution-resilient peer-to-peer video streaming with Band Codes	Band Codes (BC) are a proposed solution for controlled-complexity random Network Coding (NC) in mobile applications, where energy consumption is a major concern. In this paper, the potential of BC is investigated in a peer-to-peer video streaming scenario with both malicious and honest nodes. Malicious nodes launch pollution attacks by randomly altering the content of coded packets, disrupting the video stream for honest nodes. Instead of identifying and isolating malicious nodes, this paper suggests adapting the coding scheme to introduce resilience against pollution propagation. Experimental results show that by adjusting coding parameters, the impact of a pollution attack can be reduced and the quality of the video communication can be restored. 
412009	41200955	Characterization of Band Codes for Pollution-Resilient Peer-to-Peer Video Streaming.	This article discusses the use of band codes (BC) as a solution to pollution attacks in network coding (NC)-based peer-to-peer live video streaming. These attacks occur when a malicious node injects false packets into the network, causing a cascade of erroneous packets. Traditional methods of identifying and isolating malicious nodes are not effective in NC, as the nodes exchange coded packets and it is difficult to distinguish between malicious and unwitting nodes. The authors propose using BCs, originally designed for controlling NC decoding complexity, to recombine the packets and prevent the spread of pollution. Experiments show that this approach effectively restores the quality of the video stream. 
412010	41201092	Transform Coding Techniques for Lossy Hyperspectral Data Compression	Transform-based lossy compression is a promising method for reducing the size of hyperspectral data. As these data are 3-D, the correlation between dimensions must be carefully considered when designing a compression method. This paper focuses on the design of a 3-D transform and rate allocation for lossy compression of hyperspectral data. The authors first select a suitable transform for the data and then determine the optimal allocation of bits for each transformed coefficient. Experimental results show that this approach achieves better compression performance compared to traditional methods.
412010	412010111	Near-lossless distributed coding of hyperspectral images using a statistical model for probability estimation	This paper presents a new algorithm for compressing hyperspectral images with minimal loss of data. The algorithm uses distributed source coding (DSC) and employs an array of low-density parity-check codes. Unlike other DSC techniques, this algorithm determines the encoding rate for each data block using a statistical model, eliminating the need for communication between sources. Additionally, the statistical model allows for better performance by using information from previously decoded bit-planes in the same band. Experimental results on AVIRIS data show a significant improvement in performance compared to existing DSC and traditional techniques, although it does not reach the theoretical coding bounds.
412011	4120114	Improved low-complexity intraband lossless compression of hyperspectral images by means of Slepian-Wolf coding	This paper introduces a new lossless compression method for remote sensing images, which is designed to work efficiently within the limited computational resources available. The method uses distributed source coding and Slepian-Wolf coding to achieve high performance while keeping the encoding complexity low. Tests on AVIRIS data demonstrate that this new approach performs similarly to CALIC and outperforms the popular JPEG 2000 method. Overall, this proposed scheme offers a promising solution for compressing multispectral and hyperspectral images in remote sensing systems.
412011	41201125	Near-lossless distributed coding of hyperspectral images using a statistical model for probability estimation	This paper presents a new algorithm for near-lossless compression of hyperspectral images using distributed source coding (DSC). The encoding process involves using syndrome coding of bit-planes and low-density parity-check codes. The algorithm is different from existing DSC techniques as it uses a statistical model to determine the encoding rate for each data block, eliminating the need for inter-source communication or a feedback channel. The statistical model also allows for improved performance by using information from previously decoded bit-planes. Experimental results with AVIRIS data show a significant performance improvement compared to existing DSC and traditional techniques, although it still falls short of theoretical coding bounds. 
412012	41201253	Network Communities: Something Old, Something New, Something Borrowed …	This paper explores the concept of network communities as a new genre of collaboration in the CHI and CSCW communities. These are robust and persistent communities that bridge the virtual and physical worlds, and require an understanding of both technology and social dynamics. The authors examine various systems and historical examples to describe the advantages they offer to their community of users. Drawing from their own experiences, they expand on the design space by considering three dimensions: boundary negotiations between real and virtual worlds, support for social rhythms, and the emergence and growth of the community. The paper concludes with implications for designers, researchers, and community members based on their findings.
412012	41201272	Learning from seniors in network communities	In this study, researchers looked at SeniorNet, an organization that helps seniors access technology. They focused on how access to technology is influenced by social and cultural factors, specifically the role of community. Through their research, they gained a better understanding of seniors as a group and how they are supported and connected within SeniorNet communities. The researchers also found that their preconceived notions about seniors were challenged as they discovered the diverse range of individuals within the "senior" category. This has implications for addressing access issues for seniors and recognizing the benefits of technology for both seniors and those who interact with them.
412013	41201344	On the Parameterized Complexity for Token Jumping on Graphs.	The token jumping problem involves transforming one set of tokens on a graph into another set, with each transformation moving one token to a different vertex. The sets must be of equal size and all intermediate sets must also be of the same size. This problem is challenging and has been shown to be PSPACE-complete for planar graphs with maximum degree three. In this paper, the problem is further studied and it is found to be W[1]-hard when parameterized by the number of tokens. However, an algorithm is developed that can efficiently solve the problem for general graphs when parameterized by both the number of tokens and the maximum degree. This algorithm can also find the minimum number of token movements needed to transform the initial set into the target set.
412013	41201316	Fixed-Parameter Tractability of Token Jumping on Planar Graphs.	The token jumping problem is to find a sequence of independent sets of the same size that can transform one set into another by moving one token at a time. This problem is difficult to solve, even for simple graphs. However, in this paper, the authors present a fixed-parameter algorithm that can solve this problem for planar graphs, where the parameter is the number of tokens. The algorithm can also find the shortest sequence for a successful transformation. This algorithm can also be applied to a larger class of graphs, as long as they do not contain a complete bipartite graph with a fixed integer t as a subgraph.
412014	41201426	Statistical leakage modeling for accurate yield analysis: the CDF matching method and its alternatives	This study examines the impact of statistical leakage modeling on the yield of memory designs. Different closed form models are evaluated and the CDF matching method is proposed as a comprehensive and effective approach for accurate modeling. The Schwartz-Yeh method is found to match the body and left tail of the distribution, while the Fenton-Wilkinson method focuses on the right tail. This is important for estimating yield in the presence of leaky bitlines, but the CDF matching method is still shown to be more accurate in practical applications. The estimated error in false-read probability can range from 10x to 147x and is expected to increase as technology advances.
412014	41201460	Statistical yield analysis of silicon-on-insulator embedded DRAM	This study analyzes the yield of a 65 nm SOI eDRAM design, taking into account random dopant fluctuations in the cell and micro sense amp, as well as trench capacitor variation effects. By studying different systematic corner and device type considerations, the authors were able to evaluate yield timing windows and identify suitable design guidelines. They used a fast Monte Carlo statistical analysis approach, adapted from an SRAM analysis methodology, to understand yield trends. The results show that the choice of devices and design considerations is critical for the memory operating corners and conditions, and that high vt sense amp devices are more effective in ensuring sufficient design yield windows compared to low Vt devices.
412015	4120153	PaperPhone: understanding the use of bend gestures in mobile devices with flexible electronic paper displays	Flexible displays have the potential to mimic the interaction styles of paper documents, making them a promising technology for use in electronic devices. In this study, researchers evaluated the effectiveness of different bend gestures for performing tasks on a smartphone-inspired flexible E Ink prototype called PaperPhone. They collected 87 bend gesture pairs from ten participants and found that users preferred simpler and less physically demanding gestures that were executed along one axis. They also identified the top three most commonly used bend gesture pairs and found a strong consensus among participants for the polarity of bend gestures in actions that have a strong directional cue. This suggests that bend gestures that take directional cues into account are more natural for users to use.
412015	41201583	PaperFold: Evaluating Shape Changes for Viewport Transformations in Foldable Thin-Film Display Devices	The paper discusses the use of shape changes in a multi-segmented mobile device, called PaperFold, to trigger changes in its graphical interface. This device has foldable and reconfigurable thin-film electrophoretic display tiles, making it thin and lightweight when folded. The design of PaperFold was informed by a participatory study and resulted in 14 preferred shape changes. Another study was conducted to rank the usefulness of these shape changes in triggering common view operations in map and text editing applications. The results showed that users were able to attribute specific view operations to folding, attaching, reorienting, or detaching displays. Participants also preferred collated or full screen views when using two displays, and suggested alternative views like toolbars or a list of apps when adding a third display. Folding the segments into a three-dimensional structure was strongly associated with showing 3D views. 
412016	41201685	Alternative Similarity Functions For Graph Kernels	In collaborative recommendation systems, graph kernels can be used to model the task of recommendation and rating prediction. These kernels are interpreted as inverted squared Euclidean distances in a space defined by the underlying graph. However, other similarity functions can also be used in place of this inverted squared Euclidean function. The exponential diffusion kernel, von Neumann kernel, and random forest kernel have been evaluated in this context and have been found to improve the performance of graph kernels in collaborative item recommendation and rating prediction tasks. This suggests that alternative similarity functions can enhance the effectiveness of graph kernels in these tasks.
412016	41201655	Propagation kernels: efficient graph kernels from propagated information	The article discusses a new general graph-kernel framework called "Propagation Kernels" that efficiently measures the similarity of structured data. These kernels use early-stage distributions from propagation schemes, such as random walks, to capture structural information from node labels, attributes, and edge information. This allows for the use of off-the-shelf propagation schemes and makes the computation considerably faster without sacrificing performance. In addition, if the graphs have a regular structure, the kernel computation can be scaled to large databases of graphs with thousands of nodes. The effectiveness of this approach is supported by experiments on a variety of real-world graphs from different application domains. 
412017	412017104	Visual Representation in the Determination of Saliency	This paper explores the impact of visual representation on the behavior of a visual salience model. Different representations have been used to create an early visual representation of image structure, and experiments show that the choice of representation significantly affects the system's behavior. The reasons for these differences are discussed and applied to vision systems in general. When design choices are arbitrary, the properties of visual representation in human early visual processing can provide insights. Overall, the study emphasizes the importance of filter choice and suggests that log-Gabor filters have desirable properties.
412017	41201710	Integrating Three Mechanisms of Visual Attention for Active Visual Search.	Visual attention methods can be used to improve the performance of algorithms for robotic visual search, thereby reducing computational costs. This can be achieved by integrating three distinct mechanisms of visual attention: viewpoint selection, top-down object-based attention, and visual saliency. These methods have been previously proposed and described, but their integration within a single framework is novel and has not been extensively studied. In scenarios where there is limited information about the environment, this approach has been shown to significantly improve search performance, reducing the time and number of actions required. Extensive experiments on a mobile robot have demonstrated the effectiveness of this method.
412018	41201824	Linear hypertree for multi-dimensional image representation	The linear hypertree is a hierarchical representation of objects in d-dimensional space. It uses a hierarchical locational code to encode the rectangular coordinates of each node. Two decoding techniques are presented - one produces scalar values for the node coordinates, while the other generates them in vector form. The vectorial decoding method only applies coordinate transformations to the universe vertices, making it more efficient. Adjacency concepts in a multi-dimensional environment are defined, and a neighbor-finding algorithm is introduced that operates directly on the locational code to identify adjacent nodes. Procedures for computing the locational codes of larger and smaller neighbors are also included. Finally, an algorithm for finding the shortest path between two nodes is described as an example of the practical use of the decoding formula and neighbor identification.
412018	41201840	Adjacency algorithms for linear octree nodes	This content introduces a formula for converting an octal locational code into rectangular coordinates X, Y, and Z for a linear octree node. The formula is simpler than previous ones proposed. Algorithms for determining the octal locational code of adjacent blocks to a given node are also introduced. These include face, edge, and corner adjacency algorithms, which use direction extensions of eastern, western, front, back, southern, and northern neighbor finding procedures. The conditions for face adjacency are formulated as digit tests on the locational codes of two nodes. These conditions are later used to determine the rightmost digit to be added or removed from the locational code to identify smaller or larger neighbors in a specific direction. The computational complexity for all presented algorithms is O(n), where n is the number of digits in the locational code.
412019	41201966	Experimenting with a multi-agent e-commerce environment	Agent technology is often touted as the most natural way to automate e-commerce processes. However, despite this claim, human decision-making still plays a crucial role in the success of e-commerce systems. This has led to a lack of large-scale agent-based e-commerce applications that are actually implemented and functioning successfully. To address this, an abstract e-commerce environment is proposed where agents of different types can interact and work towards the goal of supporting e-commerce transactions. A prototype system using the JADE agent platform is described, and experiments with the implemented system are reported.
412019	41201974	Designing New Ways For Selling Airline Tickets	Recent research has focused on using multi-agent systems in e-commerce, specifically with autonomous software agents participating in auctions. To apply this concept to the airline industry, a model agent-based e-commerce system was modified to serve as an airline ticket auctioning system. This system can be integrated with a Travel Support System that utilizes a specific travel content ontology. To ensure compatibility with industry standards, an air travel ontology is needed that aligns with the Open Travel Alliance (OTA) messaging system. The goal of this paper is to explain the development of an agent-based system for selling airline tickets that follows the OTA messaging specification and adheres to IATA procedures.
412020	4120200	Geographic origin of libre software developers	This paper discusses the argument that libre (free, open source) software is a product of global development. While there is evidence that developers work in diverse teams and the community is made up of people from different countries, it is difficult to accurately determine the exact composition of the community. Past studies have relied on surveys, which may not be representative of the entire community. To address this issue, the authors propose using databases to infer the geographical location of developers. By analyzing data from SourceForge and mailing lists, they estimate the geographical origin of over one million individuals involved in libre software development. The paper concludes that this method is a good estimate of the global distribution of libre software developers.
412020	41202061	Towards predictor models for large libre software projects	Libre software offers a wide range of publicly available data sources for its development, making it possible to build predictive estimation and evolution models. However, understanding libre software development is challenging due to its unique nature compared to traditional in-house software development. The development team is a mix of hired developers and volunteers, whose level of contribution cannot be predicted in advance. In this study, the researchers analyze data from three repositories of GNOME, a large libre software project, to predict future participation and activity. The results and correlations provide initial estimations for the project's future development.
412021	41202120	Segmentation From Natural Language Expressions	This paper presents a novel approach to segmenting images based on natural language expressions, rather than traditional predefined semantic classes. The proposed model uses a combination of recurrent and convolutional neural networks to extract visual and linguistic information and produce pixelwise segmentation for the target object. Unlike previous approaches, this model is not limited to a fixed set of categories or rectangular regions. It has been tested on a benchmark dataset and shown to outperform other methods by a significant margin. This suggests that this model has potential for accurately segmenting images based on natural language descriptions.
412021	412021149	Cooperative Robust Estimation Using Layers of Support	The authors propose a new approach for representing images containing multiple objects or surfaces. Instead of using edges, they use a multilayer estimation framework that utilizes support maps to represent the segmentation of the image. This approach can accurately represent objects that are split into separate regions or have overlapping surfaces. The framework is based on robust estimation methods and uses a selection criteria based on the Minimum Description Length principle to determine the number of support maps needed for an image. This method has been successfully applied to various scenarios such as range image decomposition, motion field segmentation, and separation of tracked motion features. 
412022	4120225	Truth Discovery in Crowdsourced Detection of Spatial Events.	The widespread use of smartphones has led to the rise of mobile crowdsourcing tasks, such as detecting spatial events as users move around in their daily lives. However, the reliability of these events can be compromised by unreliable participants and low-quality data. This presents a challenge in mobile crowdsourcing known as truth discovery, which involves identifying true events from diverse and noisy reports. This problem is unique due to uncertainties in participants' mobility and reliability. Two new unsupervised models, TSE and PTSE, are proposed to address this challenge by considering location popularity, visit indicators, and participant reliability. These models can effectively handle uncertainties and automatically discover truths without location tracking. Experimental results show that these models outperform existing approaches in mobile crowdsourcing environments.
412022	41202269	Adaptive negotiation in managing wireless sensor networks	The efficient allocation of resources in computer science is a major issue, particularly for tasks such as sensor allocation for environmental monitoring. This problem is complicated by the changing number of tasks and resources, limited resources, and competition among tasks. Many current approaches use agents to manage resources, but successful allocation for new tasks is heavily reliant on existing allocations. In this paper, a new negotiation mechanism is proposed to dynamically rearrange resource allocation for the arrival of new tasks. Experiments have shown that this approach yields better results compared to an agent-based approach without resource re-allocation through negotiation. 
412023	41202360	Rigorous design of robot software: A formal component-based approach	The article discusses the combination of two advanced tools, G^e^noM and BIP, for developing functional modules of robotic systems. While previous research focused on formal methods for the highest level of robot software architecture, the authors address the importance of using these methods for the functional level. They successfully apply the G^e^noM/BIP approach to a complex exploration rover, producing a fine-grained formal model and running it on the real robot to check for deadlock and safety properties. The authors have also extended this approach to include real-time capabilities, distributed processing, user-friendly language for constraints, and integration with a temporal plan execution controller. This approach can now be applied to both the lowest and highest levels of robot architecture. 
412023	41202373	Incremental Generation of Linear Invariants for Component-Based Systems	There is a need for more efficient and scalable methods for generating invariants in concurrent systems. Existing methods do not fully utilize the system's structure and algebra, leading to longer computation times and reduced scalability. A recent approach has been developed for generating Boolean invariants for systems described in the BIP component framework, which includes a rich algebra for describing component interactions. Further advancements have been made in generating linear interaction invariants, which are more precise than Boolean invariants. An incremental approach has also been proposed to reuse previously computed invariants. These techniques have been implemented in a tool called DFINDER and have shown superior performance in experiments on various models.
412024	41202477	An indirect robust continuous-time adaptive controller with minimal modifications	This paper investigates the effectiveness of an indirect continuous-time adaptive controller by implementing parameter projection to limit the estimated parameters within a known range. The analysis shows that the system remains globally stable in the presence of unmodelled dynamics and external disturbances, without requiring any specific signals in the closed-loop system. The controller also maintains the properties of earlier conventional adaptive controllers when the controlled plant meets certain ideal assumptions. This method does not require prior knowledge of the unmodelled dynamics for implementation.
412024	4120245	Decentralized adaptive stabilization in the presence of unknown backlash-like hysteresis	This paper addresses the challenge of designing decentralized output feedback adaptive controllers for unknown interconnected systems with hysteresis. Two new schemes are proposed using the backstepping approach, where each subsystem's general transfer function and interactions between subsystems are considered. The first scheme does not require knowledge of the bounds of unknown parameters, while the second scheme uses a projection operation when the parameters are inside known compact sets. Both schemes take into account the effects of hysteresis and interactions in devising local control laws, ensuring bounded signals in the closed-loop system. The transient system performance can be adjusted by choosing suitable design parameters. The second scheme allows for strong interactions and ensures perfect stabilization in the absence of hysteresis.
412025	41202559	Probabilistic object bases	Current object database systems are not equipped to handle uncertain attributes of objects. To address this issue, the authors propose a new algebra to handle object bases with uncertainty. They introduce concepts of consistency and provide an NP-completeness result, along with classes of probabilistic object bases for which consistency can be easily checked. The authors also discuss the importance of conjunctions and disjunctions in operations and how their probabilities depend on the probabilities of primitive events and their relationships. They demonstrate the application of their algebra in query optimization and have developed a prototype server on ObjectStore. Experiments are conducted to evaluate the efficiency of different rewrite rules.
412025	412025129	Probabilistic reasoning about actions in nonmonotonic causal theories	The language PC+ is a more advanced version of the action language C+, designed for reasoning about actions with both probabilistic and nondeterministic effects. The formal semantics of PC+ is based on probabilistic transitions between sets of states. By incorporating the concepts of history and belief state, important problems in reasoning about actions can be represented in a concise manner within PC+.
412026	41202620	Retrieving geometric information from images: the case of hand-drawn diagrams.	This paper presents a general algorithmic approach to retrieving meaningful geometric information from image data in any geometric domain. The approach is tailored to the specific domain and can be concretized to yield specific algorithms. As an example, the domain of plane Euclidean geometry is formally specified and concrete algorithms are presented for retrieving typical geometric objects, relations, and labels from hand-drawn diagrams in this domain. The feasibility of these algorithms is demonstrated through experiments. The paper also showcases how this approach can automatically discover implied geometric knowledge from images, illustrated through an example of generating nontrivial geometric theorems from retrieved information.
412026	41202615	Towards a geometric-object-oriented language	This paper introduces a new language for performing symbolic geometric computation, reasoning, and visualization. The language allows for the creation of geometric objects using parametric data, as well as modifying and performing basic operations on these objects. Special attention is given to handling degeneracy and uncertainty through conditions and assumptions. Geometric statements can be declared by defining relationships between different objects. The proposed system, which is based on this language, enables users to rigorously perform geometric computations, prove theorems, and generate diagrams and interactive documents. The paper outlines the design of the language, its capabilities and features, as well as its main components. It also includes specifications for some of its functions, reports on experiments with a preliminary implementation, and discusses potential difficulties and research questions. 
412027	41202735	Transparent multi-core speculative parallelization of DES models with event and cross-state dependencies	This article discusses the transparent parallelization of Discrete Event Simulation (DES) models on multi-core machines using speculative schemes. The proposed approach allows simulation objects to access and modify the state of any other object, creating what is called cross-state dependency. This differs from traditional PDES platforms where access is limited to the object being processed. To ensure consistency in parallel execution, an advanced memory management architecture and synchronization mechanisms are introduced. This approach has been integrated with the ROOT-Sim platform, but can also be applied more broadly. It aims to improve performance and efficiency while maintaining transparency in handling cross-state and traditional event-based dependencies.
412027	41202717	Transparent Speculative Parallelization of Discrete Event Simulation Applications Using Global Variables.	Parallelizing discrete event simulation (DES) applications has long been a method for improving their speed and making large and complex simulation models manageable. This is typically done through parallel DES (PDES) techniques, which involve breaking the simulation model into separate objects that execute concurrently and rely on event-exchange or scheduling to handle dependencies. However, this approach restricts the use of shared information, such as global variables, within the application code. To address this limitation, a new method for managing global variables in parallel DES code written in ANSI-C is proposed. This method is based on multi-versioning and a rollback/recovery scheme to ensure consistency and efficiency. The approach is fully transparent and has been tested on various case study applications with positive results.
412028	41202813	CrowdScreen: algorithms for filtering data with humans	The problem of filtering a large set of data items based on human-verifiable properties is common in crowdsourcing applications, but has not been formally optimized. Most solutions use heuristics instead of formal optimization. This study presents different versions of the problem and develops deterministic and probabilistic algorithms to optimize the expected cost and expected error. These algorithms outperform other strategies in experimental testing and can be applied in various crowdsourcing scenarios. They can also be integrated into query processors that use human computation.
412028	4120283	Finish Them!: Pricing Algorithms for Human Computation.	This article discusses the importance of setting and varying the price of human computation tasks in order to meet time and cost constraints. It points out that often the price is set beforehand and not adjusted, leading to either unnecessary high costs or longer processing time. The authors propose algorithms based on a pricing model from previous research to optimally set and change the price over time. These algorithms use techniques from decision theory and have been shown to reduce costs by up to 30% compared to previous methods. Additionally, the authors have developed methods to speed up the computation process, allowing for on-the-fly adjustments to prices. 
412029	41202958	Rank-Aware Query Processing and Optimization	This dissertation proposes a framework for supporting ranking in relational database systems through the integration of ranking algorithms and operators in query processing and optimization. Two rank-join algorithms are introduced, one for joining multiple ranked inputs on key attributes and another for general join conditions. These algorithms use individual orders of input relations and result in ordered join results based on a user-specified scoring function. Practical issues and optimization heuristics are addressed to integrate the new operators in query processors. A rank-aware query optimization framework is also introduced, based on a dynamic programming algorithm, to generate optimal rank-aware query plans. A probabilistic model is introduced for estimating the input cardinality and cost of a rank-join operator. Experimental evaluation shows the superior performance of the proposed techniques in joining ranked inputs and the accuracy of the estimation model.
412029	41202956	Adaptive rank-aware query optimization in relational databases	Rank-aware query processing is becoming increasingly important in modern applications. Efficient and adaptive evaluation of top-k queries is crucial for these applications, and a new framework has been introduced to fully integrate rank-join operators into relational query engines. This framework extends the System R dynamic programming algorithm and uses a probabilistic model for estimating the input cardinality of these operators. Additionally, several adaptive execution strategies have been introduced to respond to estimation errors and changes in the computing environment. An extensive experimental study shows the effectiveness of this framework and the significant speedup achieved through adaptive execution of ranking plans.
412030	41203017	A declarative approach to distributed computing: Specification, execution and analysis.	The use of logic programming for distributed algorithms is gaining interest for various network applications. These applications involve multiple devices sharing data and computation, with the ability to exchange information and collaborate. A declarative approach is proposed in this paper, where algorithms and communication models can be specified as action theories and executed as collections of distributed state machines. Devices are represented as automata that can communicate through messages. The approach is also analyzed using existing theories on causal theories and Answer Set Programming. The paper also presents results on the application of this approach to different types of network protocols.
412030	4120304	Declarative distributed computing	In this paper, a language for writing distributed applications is introduced, along with an operational semantics for a single computational node based on Datalog. The paper also presents a framework that can capture the semantics of a network of computational nodes working together. This framework can express different communication models and can be used to check various properties of the distributed computation. The framework is developed using Answer Set Programs.
412031	41203121	Hybrid Abductive Inductive Learning: A Generalisation of Progol	Progo15 and the Bottom Generalisation method are widely used in Inductive Logic Programming (ILP). However, they are limited in their ability to find hypotheses within the scope of Plotkin's relative subsumption. This paper uncovers a new limitation of Progo15 and proposes a new approach called Hybrid Abductive Inductive Learning that combines ILP and Abductive Logic Programming (ALP). This approach, implemented through the HAIL proof procedure, not only overcomes the limitation of Progo15 but also allows for the generation of multiple clauses from a single example and the derivation of hypotheses outside of Plotkin's relative subsumption. A new semantics, called Kernel Generalisation, is also introduced to encompass the hypotheses produced by HAIL.
412031	41203127	Induction on Failure: Learning Connected Horn Theories	Inverse Entailment (IE) is a popular learning technique that has been used to develop various learning systems. These systems, such as Progol, generate hypotheses in the form of single or multiple clauses. However, they all share a limited search space in which each clause must explain a given example or an abductive explanation for it. This paper introduces a new IE approach, called Induction on Failure (IoF), which expands the search space to include Connected Theories. A proof procedure for IoF is proposed and a prototype implementation is described. Additionally, a new semantics called Connected Theory Generalisation is introduced, which extends the existing Kernel Set Subsumption and allows for hypotheses to be generated using the IoF approach.
412032	412032213	Temporal quality assessment for mobile videos	The paper discusses the challenges of assessing video quality in mobile devices, such as limited computation power and wireless network issues. The authors propose a Temporal Variation Metric (TVM) to measure the temporal information of videos, which shows a high correlation with optical flow. They then use this metric to create a reduced-reference temporal quality assessment metric, Temporal Variation Index (TVI), which can predict users' Quality of Experience (QoE) and estimate network conditions. Subjective assessments show a 92.5% correlation between TVI and Mean Opinion Score (MOS) ratings, and video streaming experiments demonstrate an accuracy of 95% in predicting packet loss and delay.
412032	41203243	Evaluating perceptual video quality for mobile clients in 802.11n WLAN	This paper explores the performance of HD video streaming in 802.11n WLANs when users are moving. Experiments were conducted in a low electromagnetic interference outdoor wireless testbed. The study found that both user speed and distance from the access point (AP) affect video quality. A non-linear regression model was built using subjective scores and objective video quality metrics, which showed a 69% correlation with video quality. The model also found that distance has a greater impact on video quality than speed. However, physical factors alone cannot accurately predict video quality. Overall, this study highlights the importance of considering both speed and distance when optimizing video streaming in WLANs.
412033	41203327	MST construction in O(log log n) communication rounds	The article discusses a basic model for overlay networks in which all processes are connected to each other and messages contain a maximum of O(log n) bits. A distributed algorithm is presented for this model, which can construct a minimum-weight spanning tree in O(log log n) communication rounds. This is the first algorithm to achieve a time complexity of less than ω(log n) with small message sizes.
412033	41203355	Distributed MST for constant diameter graphs	This paper discusses the problem of creating a minimum-weight spanning tree (MST) in a distributed manner for graphs with a constant diameter, where each message has a limited size of B bits. It is proven that the time needed to compute an MST for graphs of diameter 4 or 3 can be as high as &OHgr;(3√n/B) and &OHgr;(4√n/2√B), respectively, even with randomized algorithms. However, it is also shown that for graphs with diameter 2 and B = O(log n), an MST can be computed deterministically in O(log n) time. These results add to a previously known lower bound of &OHgr;(2√n/B) for graphs with diameter &OHgr;(log n). 
412034	41203442	An interoperable delivery framework for scalable media resources	This article discusses a framework for delivering scalable media resources, such as videos, in a standardized format. The framework supports both video on demand and multicast streaming and uses MPEG-21 Digital Item Adaptation for efficient and interoperable content adaptation. The server and clients of the framework use the MPEG Extensible Middleware and MPEG Query Format for querying available resources. The framework has been integrated into the VLC media player and the architecture for both video on demand and multicast is described. A comparison of the performance of the framework's generic MPEG-21 metadata-based approach to a specific scalable video coding approach is also provided.
412034	41203415	Bitstream syntax description: a tool for multimedia resource adaptation within MPEG-21.	This paper outlines a method for adapting different multimedia resources using a single media processor. The method involves transforming an XML description of the media resource's bitstream syntax to generate an adapted version of the bitstream. The paper also introduces two technologies, BSDL22 and gBS Schema, which aid in parsing the bitstream, structuring and marking the XML description, and generating the adapted bitstream. These technologies can be used separately, but a combined approach has been developed to optimize their effectiveness. The focus of the paper is on the gBS Schema and the harmonized approach using both BSDL and gBS Schema.
412035	4120350	Online learning of correspondences between images.	The proposed method is used for finding corresponding points between images in a sequence. This is done by projecting 3D points onto two images and then determining the corresponding point in the other image. The projections and surface shape are unknown, making it difficult to find correspondences without access to the 3D scene. However, the proposed method uses an iterative approach with a chi-square divergence optimization to accurately and quickly determine the mappings between points. This algorithm outperforms other methods in terms of convergence and accuracy in various experiments and can run in real time.
412035	41203519	Real-time visual recognition of objects and scenes using P-channel matching	The paper proposes a new method for real-time view-based object recognition and scene registration. This is important for tasks like robotics, retrieval, and surveillance. The proposed method uses P-channels, a type of information representation that combines features of histograms and local linear models. This approach is inspired by biological systems and is robust against common distortions like clutter and occlusion. The algorithm extracts intensity invariant image features, encodes them into P-channels, and compares them to prototype P-channels in a database. The method is evaluated on the COIL database and achieves excellent results. Results from scene registration using a fish-eye camera are also presented. 
412036	4120367	The weakest failure detector for solving consensus	In the paper, "Consensus in Asynchronous Distributed Systems with Crash Failures," the authors discuss the minimum amount of information needed to solve the Consensus problem in asynchronous distributed systems with crash failures. They prove that a failure detector, which provides very little information about crashed processes, is sufficient to solve the problem. This is known as the {0 failure detector. The authors also demonstrate that any failure detector must provide at least as much information as {0 in order to solve Consensus. Therefore, {0 is considered the weakest failure detector for solving Consensus in asynchronous systems with a majority of correct processes.
412036	41203658	Unreliable failure detectors for reliable distributed systems	Unreliable failure detectors are introduced as a way to solve Consensus in asynchronous systems with crash failures. These detectors are characterized by two properties - completeness and accuracy. The study shows that Consensus can still be achieved even with unreliable failure detectors that make infinite mistakes. It is also determined which detectors can solve Consensus despite any number of crashes, and which ones require a majority of correct processes. It is proven that Consensus and Atomic Broadcast are reducible to each other in these systems, and a specific failure detector is found to be the weakest for solving Consensus.
412037	4120370	Efficiently refactoring java applications to use generic libraries	Java 1.5 generics allow for the creation of reusable container classes that ensure safe usage through type enforcement by the compiler. This eliminates the need for potentially unsafe down-casts when retrieving elements from containers. A refactoring technique has been developed to replace raw references to generic library classes with parameterized references. The refactoring uses type constraints to infer type parameters and removes redundant casts. The refactoring has been implemented in the Eclipse development environment and has been evaluated by refactoring Java programs that use the standard collections framework to use Java 1.5 generics. On average, 48.6% of casts are removed and 91.2% of compiler warnings related to raw types are eliminated. This approach is more scalable, can accommodate user-defined subtypes, and is integrated in a popular IDE, setting it apart from previous techniques.
412037	4120377	Program analysis for mobile: how and why to run WALA on your phone.	With the widespread use of mobile devices, security has become a major concern. This includes protecting the devices themselves as well as the data they contain from attacks and theft. One way to ensure security is through static analysis of the device's software. Various tools have been developed to analyze individual apps for potential vulnerabilities, but now there are also tools that can detect threats from multiple apps on a single device. This type of analysis is best performed on app stores, but it can be challenging in the Android environment where apps can be installed from various sources. Therefore, there is a growing interest in running analysis directly on the device, but this approach also has its own challenges, including the need to build and deploy analysis tools on mobile devices and concerns about resource usage and battery drain. The authors present their work on running program analysis on mobile devices, specifically using the WALA framework on Android. They demonstrate that it is feasible to build and deploy WALA for Android and that its performance is reasonable. 
412038	412038169	Conditional Density Estimation with Dimensionality Reduction via Squared-Loss Conditional Entropy Minimization	Regression is a commonly used method for estimating the conditional mean of output based on input. However, it lacks effectiveness when dealing with complex data that has multiple modes, varying levels of variability, and asymmetry. In these cases, it is better to estimate the conditional density itself, but this is difficult in high-dimensional spaces. One approach to address this is to first use dimensionality reduction (DR) and then perform conditional density estimation (CDE). However, this two-step process can lead to significant errors. To address this issue, the authors propose a new method that simultaneously performs DR and CDE by formulating DR as a problem of minimizing a squared-loss version of conditional entropy. This eliminates the need for an additional CDE step and has been tested on various data sets with promising results.
412038	412038199	Direct conditional probability density estimation with sparse feature selection	Regression is a common statistical analysis method used to estimate the relationship between input and output variables. However, when the conditional probability density is complex, traditional regression methods may not provide sufficient information. To address this issue, researchers have developed estimators of conditional densities, such as the least-squares conditional density estimation (LS-CDE). However, LS-CDE can still be inaccurate if the input contains irrelevant features. To improve upon this, a new approach called sparse additive CDE (SA-CDE) has been proposed. This method applies LS-CDE to each input feature and uses a group-sparse regularizer to automatically select relevant features. An efficient optimization method has also been developed for SA-CDE, making it applicable to large and high-dimensional datasets. Experiments with different datasets have shown the effectiveness of SA-CDE in noisy conditional density estimation problems.
412039	41203952	Bayesian statistical model checking with application to Stateflow/Simulink verification	The article discusses the problem of model checking stochastic systems, which involves determining if a system satisfies a temporal property with a certain probability threshold. The authors propose a Statistical Model Checking (SMC) approach based on Bayesian statistics, which is applicable to a class of hybrid systems with stochastic transitions. This approach combines randomized sampling of system traces with hypothesis testing or estimation to efficiently solve the verification problem. While not guaranteed to be correct, the Bayesian SMC approach can make the probability of giving a wrong answer arbitrarily small. The authors demonstrate the effectiveness of their approach on a fuel control system model and suggest its potential for other stochastic models. 
412039	41203957	Safe Reinforcement Learning via Formal Methods: Toward Safe Control Through Proof and Learning.	Formal verification is a method of ensuring the safe operation of systems, but it relies on the system matching the verified model. However, models are often incomplete, especially in Cyber-Physical Systems where physical models are costly and difficult to verify. On the other hand, reinforcement learning-based controllers are flexible but do not guarantee safety. This paper proposes a way to combine formal verification with verified runtime monitoring to ensure safe learning. The key is to limit exploration to verified control choices and adjust the control strategy when a model violation is detected. This approach maintains safety guarantees while also retaining the performance benefits of reinforcement learning. The effectiveness of this approach is demonstrated using a simple model of adaptive cruise control for autonomous cars.
412040	412040101	Symmetric public-key encryption	Public-key encryption is typically seen as assymetric, as it only allows for messages to be encrypted using a user's public key. However, the use of interactive protocols can allow for a symmetric use of public keys. This means that the same public key can be used for both encrypting messages sent to a user and messages sent by the user to others, without compromising the key. These protocols also allow for probabilistic encryption, which is when the same message can have different encrypted versions. The proposed public-key cryptosystem based on these protocols only requires one key owned by a cryptographic server. Additionally, the protocols allow for authentication of both the sender and receiver of a probabilistically encrypted message, and are secure against chosen-message and chosen-ciphertext attacks. 
412040	41204030	Efficient Dynamic-Resharing "Verifiable Secret Sharing" Against Mobile Adversary	The article introduces a new and efficient version of Verifiable Secret Sharing (VSS) that addresses the threat of a mobile adversary controlling a limited number of trustees at any given time. VSS allows for the secure sharing of a secret among a group of trustees, ensuring validity and preventing premature opening by a small group. This type of sharing has gained attention in contexts such as escrowed cryptography. The dynamic-sharing VSS presented in the article can handle attacks from a mobile adversary by using simple direct methods and public-key functions. The authors also present constant round protocols and reduce VSS to simpler forms.
412041	41204137	Dynamic dictionary matching	The dynamic dictionary matching problem involves finding patterns in a changing dictionary of strings. The goal is to efficiently search a given text for occurrences of any pattern in the dictionary. An algorithm is presented that can insert or delete patterns from the dictionary with a time complexity of O(m log |D"i|), where m is the length of the pattern and |D"i| is the size of the dictionary after the operation. Searching for patterns in the text has a time complexity of O((n + tocc) log |D"i|), where n is the length of the text and tocc is the total number of occurrences of patterns in the text. 
412041	41204142	On-line construction of two-dimensional suffix trees	An on-line data structure is one that is continuously updated to correspond to the input it has received so far. This is seen in the example of a suffix tree, where the algorithm builds the tree as it reads characters from a string. The authors introduce a new technique called implicit updates, which allows for the on-line construction of the Lsuffix tree for an n x n matrix and for simple algorithms for LZ1-type on-line lossless image compression. These algorithms show significant improvement over existing techniques and may lead to faster implementations. This is the first effective technique for on-line construction of two-dimensional suffix trees. 
412042	41204245	Reasoning About Constraint Models	A framework has been proposed to reason about characteristics of models expressed in languages such as AMPL, OPL, Zinc or Essence. However, the framework has shown that certain reasoning problems, like detecting symmetries, redundant constraints, or dualities between models, cannot be solved even for simple problem instances. To assist human modellers, automation of these reasoning tasks would be helpful. Two case-studies have been described to explore the possibility of automating these tasks. The first uses the ACL2 inductive prover to prove the existence of a symmetry in a model, while the second identifies a tractable subset of MiniZinc and uses a decision procedure to prove a parameterized constraint in a model. 
412042	41204258	Constraint Acquisition as Semi-Automatic Modeling 	Constraint programming is a popular technology used in solving complex problems in industries. However, it requires significant expertise in constraint reasoning. This paper proposes a framework for automatically learning constraint networks from a set of instances, including acceptable solutions and undesirable assignments. This approach can aid beginners in articulating their constraints and help experts develop efficient models for specific problems. The paper also outlines a research agenda for interactive constraint acquisition, automated modeling, and automated constraint programming. This framework has the potential to make constraint programming more accessible and efficient for a wider audience.
412043	4120433	An empirical simulation-based study of real-time speech translation for multilingual global project teams	Real-time speech translation technology is currently available, but its impact on communication in global software projects is not fully understood. The goal of this study was to investigate the use of combining speech recognition and machine translation to overcome language barriers in remote software requirement negotiations. The study used Google Web Speech API and Google Translate service, two groups of four subjects speaking Italian and Brazilian Portuguese, and a test set of 60 technical and non-technical utterances. The results showed that while there was satisfactory accuracy in terms of speech recognition, it was affected by differences in speakers and utterances. It was also found that accurate transcripts were crucial for successful translations, highlighting the importance of speech recognition in speech translation technology. Overall, the study suggests that speech translation technology can be used to facilitate communication among globally distributed team members in their native languages.
412043	4120436	A Controlled Experiment on the Effects of Machine Translation in Multilingual Requirements Meetings	The article discusses the challenges of language barriers in global software projects and how machine translation technology can help in remote requirements meetings. A controlled experiment was designed to investigate the effects of using machine translation services in these meetings. Participants, who were native Italian or Portuguese speakers, were asked to use a communication tool and prioritize and estimate requirements. Initial results show that real-time machine translation was well-received and did not disrupt the flow of conversation. However, the full impact of this technology is yet to be seen in situations where language barriers are more significant.
412044	41204413	Building a Knowledge Experience Base for Facilitating Innovation.	In this paper, the authors introduce a framework designed to facilitate knowledge transfer within and outside of an organization for the purpose of innovation. The framework includes a Knowledge Experience Base (KEB) that gathers Knowledge Experience Packages (KEP) to help formalize and package knowledge and experience from innovation stakeholders. This encourages the gradual explanation of implicit knowledge from those who possess it, making the transfer process easier and reducing costs and risks. By using this framework, organizations can improve their ability to transfer knowledge and drive innovation. 
412044	41204430	Empirical Validation Of Knowledge Packages As Facilitators For Knowledge Transfer	The transfer of research findings into production systems requires explicit and understandable knowledge for stakeholders. This is a challenging task, as many researchers have explored alternative methods to traditional approaches like books and papers for knowledge acquisition. The concept of a Knowledge Experience Package (KEP) is proposed as a structured alternative. The KEP contains both the conceptual models and documentation of the research results, as well as the experience gained in acquiring them in business processes. The structure of the KEP allows for easier acquisition of necessary knowledge chunks, and the experience provides insight into potential scenarios. A comparison experiment showed that KEPs are more effective for knowledge transfer than traditional methods.
412045	41204546	First GrC model - Neighborhood Systems the most general rough set models	The Neighborhood System (NS) is a formal model that revisits the concept of infinitesimals, which played a crucial role in the development of calculus, topology, and non-standard analysis. It is shown in this paper that Ziarko's variable precision model can be expressed using NS. Additionally, NS is the most comprehensive rough set model, as it includes topology, binary relation, and covering as special cases. A new operation called "and" is introduced, which generates a base of a topology, referred to as a knowledge base. The approximations based on this knowledge base can be interpreted as learning, which distinguishes it from traditional rough set approximations. 
412045	41204567	Granular Computing and Modeling the Human Thoughts in Web Documents	The concept of human thoughts in a document set can be represented by a polyhedron, with a point representing a single thought, a simplex representing a concept, and a connected component representing a complete concept. The simplest unit is the simplex, which is made up of high frequency and closely related keywords. The structure of these keywords forms a "informal" formal language that describes the human thoughts in the document set. The model theory of this language is used to create a desired model of the human thoughts in the document set.
412046	41204684	Designing Directories in Distributed Systems: A Systematic Framework	In this paper, the authors introduce a framework for designing directory-based distributed applications. They evaluate various directory designs using this framework and provide a case study for a multicast application. The framework is based on a model that extends the concept of "process knowledge" in distributed systems. However, they argue that this definition of knowledge is too strong for many applications and propose a weaker concept of "estimation". They define phrases such as "process p in state s estimates with probability 0.9 that process q is active" and use this concept to specify directory design as an optimization problem. This approach allows for systematic analysis of different directory designs, considering factors such as bandwidth, computation, and storage. This paper contributes to the understanding of distributed systems, directories, and design frameworks, as well as providing a practical tool for performance modeling in multicast applications.
412046	4120467	Verification of distributed systems with local–global predicates	The paper discusses a method for creating and validating distributed systems with both discrete and continuous state spaces. The focus is on systems where only a few components change state while the rest remain unchanged. A proof technique is introduced to ensure overall properties like invariants and convergence by verifying local properties within subsystems. The effectiveness of this methodology is demonstrated through concrete examples. Additionally, a library of theorems and proofs in PVS is provided to aid in developing and verifying programs in this class. The paper also outlines a way to transform these libraries to Java.
412047	41204737	View selection for designing the global data warehouse	A global data warehouse collects data from various databases and sources, and can be seen as a collection of materialized views. The selection of which views to materialize is an important decision in the design of the warehouse. However, commercial products do not have automatic tools for this process. To address this, a method is proposed to generate materialized views that satisfy all the input queries. This process is complex as it involves detecting and utilizing common subexpressions. The method also considers the space allocated for materialization and aims to minimize the overall cost of query evaluation and view maintenance. Algorithms have been designed and tested for this purpose.
412047	41204713	Detecting Redundancy in Data Warehouse Evolution	A Data Warehouse (DW) is a collection of materialized views that are used to answer queries. These views are connected through common subexpressions and are constantly evolving. New views may be added and old ones may be dropped to improve query performance. However, this process can result in redundant views. This paper discusses a method for identifying and removing these redundant views without affecting query evaluation or view maintenance. By representing multiple queries and views as an AND/OR dag, the paper proposes a way to detect views that are not necessary for propagating changes from source data to the DW. This method can also be used to identify and utilize common subexpressions between views for efficient propagation of changes.
412048	41204850	V-scope: an opportunistic wardriving approach to augmenting TV whitespace databases	The article discusses the potential of TV whitespaces for wireless communications and the reliance of secondary users on spectrum occupancy databases to determine available channels. However, the accuracy of these databases may be low due to their reliance on propagation models. The authors present V-Scope, a vehicular sensing framework that uses spectrum sensors on public vehicles to collect and report measurements, to evaluate the accuracy of these databases. The system has been deployed on a public transit bus in a mid-sized US city, and the results show that databases tend to overestimate the coverage of certain TV broadcasts, leading to unnecessary blocking of whitespace spectrum in a large area. The authors also suggest using these measurements to improve existing propagation models in databases.
412048	4120486	A vehicle-based measurement framework for enhancing whitespace spectrum databases	The use of spectrum occupancy databases in current TV whitespace networks leads to a significant amount of unused spectrum. This is due to interference from secondary devices and leakage from TV broadcasts, which is not captured by the databases. To address this issue, the use of spectrum measurements is proposed, specifically through a system called V-Scope that utilizes sensors on public vehicles to collect and report data. This system can better determine whitespace spectrum, estimate channel quality, and validate device locations. In a deployment on a metro bus in a US city, it was found that a commercial database under-utilized certain whitespace channels in 71% of locations. V-Scope was able to reclaim this wasted spectrum in 59% of locations and accurately select suitable channels in 72-83% of locations with a localization accuracy of 16-27m.
412049	41204951	Proximity theorems of discrete convex functions	A proximity theorem is a statement that states that an optimal solution to an optimization problem can be found within a certain distance from a solution to its relaxation. These theorems have been utilized in developing efficient algorithms for discrete resource allocation problems. This paper focuses on establishing proximity theorems for L2-convex and M2-convex functions, which are important for solving the polymatroid intersection problem and the submodular flow problem. The results of this study build upon previous findings for L-convex and M-convex functions, expanding the scope of application for proximity theorems in solving discrete convex problems.
412049	41204953	Cone superadditivity of discrete convex functions.	A function is considered cone superadditive if it satisfies a specific inequality involving a partition of R n into convex cones. This concept is important in nonlinear integer programming as it can help determine global minimality using local optimality criteria. The paper focuses on L-convex and M-convex functions and proves their cone superadditivity with regard to conic partitions that are not dependent on specific functions. Both discrete and continuous variables are considered.
412050	41205026	High-performance public-key cryptoprocessor for wireless mobile applications	The article presents a high-speed public-key cryptoprocessor that utilizes three levels of parallelism in Elliptic Curve Cryptography (ECC) over GF(2n). It employs a Parallelized Modular Arithmetic Logic Unit (P-MALU) to accelerate modular operations using two types of parallelism. The scalar multiplication sequence is also sped up through the use of Instruction-Level Parallelism (ILP) and processing multiple P-MALU instructions simultaneously. The system can be programmed for different types of elliptic curves and scalar multiplication algorithms. Synthesis results show that scalar multiplication of ECC over GF(2163) on a generic curve can be computed in 20 and 16 µs with the binary NAF and Montgomery methods, respectively. On a Koblitz curve, the TNAF method can achieve a scalar multiplication of 12 µs. This fast performance enables over 80,000 scalar multiplications per second and enhances security in wireless mobile applications.
412050	41205054	Superscalar coprocessor for high-speed curve-based cryptography	The proposed superscalar coprocessor is designed for fast curve-based cryptography, specifically for accelerating scalar multiplication. By utilizing instruction-level parallelism (ILP) and processing multiple instructions simultaneously, the coprocessor can fully benefit from its superscalar capability. Implementation results demonstrate a significant speed improvement of 1.8, 2.7, and 2.5 times for Elliptic Curve Cryptography (ECC) over GF(2163), Hyperelliptic Curve Cryptography (HECC) of genus 2 over GF(283), and ECC over a composite field, GF((283)2), respectively. This is achieved by exploiting parallelism in curve-based cryptography and using a single instruction for all field operations. The coprocessor also allows for efficient point/divisor operations and provides a fair comparison between the three curve-based cryptosystems.
412051	41205112	Opportunities from Open Source Search	The internet search industry has a successful business model that offers free services to users. This raises the question of why there should be open source search offerings. This paper explores the concept of open source search and explains why the computer science community should be involved. The Alvis Consortium is developing infrastructure for open source search engines using peer-to-peer and subject specific technology. This approach is based on the belief that there is a sufficient supply of open source components for certain tasks. Open source search is seen as a promising area for information extraction and retrieval components and the use of intelligent agents.
412051	41205123	Applying discrete PCA in data analysis	The paper discusses different methods for analyzing principal components in discrete data, including grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. The authors explore extensions to these methods and their application to various statistical tasks. They also propose a hierarchical version of the methods and additional techniques for Gibbs sampling. The algorithms are compared through a text prediction task and information retrieval. The results show that these methods can be seen as a discrete version of ICA and can be useful for analyzing discrete data in various applications.
412052	41205232	Emerging opportunities for theoretical computer science	This report highlights the importance of a strong theoretical foundation in computer science and emphasizes the need for a closer integration between theory and practice. It suggests that the value, impact, and funding of theory can be enhanced if guided by this principle. In order to achieve a greater synergy between theory and application, the report recommends increased financial resources for theory and a closer interaction between theoreticians and researchers in other areas of computer science and other disciplines. It also proposes two applied research initiatives, Information Access in a Globally Distributed Environment and The Algorithmic Stockroom, as well as a broader approach to graduate education to better prepare both theoreticians and practitioners for collaboration. 
412052	4120526	Massively Parallel Symmetry Breaking on Sparse Graphs: MIS and Maximal Matching.	The increasing popularity of massively parallel computation (MPC) paradigms like MapReduce has sparked interest in understanding their true computational capabilities. This has led to a focus on leveraging the advantages of MPC, such as free local computation, to improve the round-complexity of algorithms inherited from traditional parallel and distributed models like PRAM or LOCAL. Maximal independent set (MIS) and maximal matching are two commonly studied problems in this field, with known $O(log n)$ round algorithms since the 1980s. However, recent research has produced an improved MPC algorithm for these problems, with a round complexity of $O(log alpha + log^2log n)$ and truly sublinear space per machine. This algorithm is particularly interesting as it is parameterized by the arboricity of the input graph, a measure that is often small for sparse graphs. This is the first significant improvement over the known algorithms for these problems on a wide range of graphs, and it also outperforms previous PRAM/LOCAL algorithms. Additionally, the algorithm has exponential improvement in terms of $n$, the size of the input graph, compared to previous algorithms. 
412053	41205344	Testing Planarity of Partially Embedded Graphs	This article discusses the problem of extending a partial planar drawing of a subgraph to a planar drawing of the entire graph. The authors show that this problem remains polynomial-time solvable, thanks to the "TONCAS" behavior which states that the necessary conditions for planarity are also sufficient. This is determined by the rotation system and containment relationships between cycles, as well as the decomposition of the graph into its connected, biconnected, and triconnected components. The authors also introduce a linear-time algorithm for the simultaneous graph drawing problem, where one of the input graphs or the common graph has a fixed planar embedding. However, generalizations of the problem, such as minimizing the number of edges that need to be rerouted, are proven to be NP-hard.
412053	41205385	A Split&Push Approach to 3D Orthogonal Drawing	The article discusses a new approach for creating three-dimensional orthogonal drawings of graphs with a maximum degree of six. The method involves breaking the drawing into smaller pieces and gradually improving its structure until it resembles the final version. The effectiveness of this approach is tested through experiments comparing it to other algorithms. 
412054	41205468	Switch-Regular upward planar embeddings of trees	The problem of determining whether a directed graph can be drawn without any crossings, with all edges flowing upwards, is known as the upward planarity testing problem. This has been extensively studied in past research. A new variation of this problem is introduced in this paper: determining whether a directed graph can be drawn in a specific way, known as a switch-regular upward planar drawing. These drawings have practical applications in developing efficient checkers and compaction strategies. The paper presents a characterization for directed trees that can be drawn in this way and proposes an optimal linear-time algorithm for testing and embedding them.
412054	41205478	Upward-rightward planar drawings	Upward drawing is a commonly used method for representing directed graphs visually. In this convention, vertices are assigned specific points on a plane and edges are drawn as curves that increase vertically according to their direction. However, not all directed graphs can be drawn in this way without any edge crossings, and determining if a graph can be drawn upward planar is a difficult task. To address this, a relaxed version called upward-rightward drawing has been studied. In this type of drawing, every directed path must either go upward or to the right. The paper shows that this type of drawing can be achieved for all planar directed graphs with straight-line edges in linear time and polynomial area. 
412055	4120553	Clustering cycles into cycles of clusters	The paper focuses on studying clustered graphs with a cycle as their underlying graph. The researchers specifically look at 3-cluster cycles, which have three clusters at the same level. They demonstrate that testing for c-planarity, or the ability to draw the graph without crossing edges, can be efficiently done for this type of clustered graph. A drawing algorithm is also provided. The researchers also use formal grammars to characterize 3-cluster cycles. The study is then expanded to include clustered graphs with a cycle structure at each level of the inclusion tree, and the same efficient c-planarity testing and drawing algorithms are shown to apply.
412055	41205537	Relaxing the constraints of clustered planarity	A clustered graph is a visual representation of data where vertices are represented as points and edges as curves, and clusters are shown as closed regions. A c-planar drawing of a clustered graph has no crossings between edges, regions, or edges and regions. The complexity of determining if a clustered graph can have a c-planar drawing is a long-standing unsolved problem in Graph Drawing research. It is known that for a c-planar drawing, the underlying graph must be planar, but this is not a sufficient condition. To better understand the problem, a relaxed version allows for some types of crossings. It is possible to have a drawing with only edge-edge or edge-region crossings, but not necessarily with only region-region crossings. Upper and lower bounds have been established for the minimum number of crossings in these types of drawings. A polynomial-time algorithm is also provided for testing if a drawing with only region-region crossings exists for a certain type of graph. This condition can be tested in polynomial time for a significant class of graphs.
412056	4120561	On cost-aware monitoring for self-adaptive load sharing	This paper discusses the tradeoff between the utility of monitoring and its cost in self-adaptive load sharing systems, where a stream of jobs is distributed among a group of servers. The authors propose a model called Extended Supermarket Model (ESM) to determine the optimal number of servers to be monitored for minimal service time at an optimal cost. They also present self-adaptive load-sharing algorithms for centralized and distributed settings, and evaluate them using simulations and a real testbed. The results show that self-adaptive load balancing is more effective than cost-oblivious mechanisms, and the proposed algorithms perform well in a distributed setting without a dedicated monitoring component.
412056	41205629	Travelling Miser Problem	n this paper, the authors propose a new framework for handling low priority but resource-consuming traffic. This framework allows the network nodes to delay data by storing it locally, which improves the network's goodput without interfering with higher priority traffic. The authors also provide an optimization problem and enhanced scheduling strategy to ensure minimal cost schedules and flexible timing constraints for management traffic. The proposed framework is evaluated through a simulation study and demonstrates improved network performance with no difference in priority between user and management traffic.
412057	41205732	Addressing the challenges of future large-scale many-core architectures	The current trend in processors is to have more cores with varying characteristics, allowing for higher performance in different applications. However, effectively utilizing these processors is a challenge, as assuming all cores are the same for scheduling tasks is no longer valid. To address this, a task assignment policy is proposed that takes into account the different characteristics of cores, such as their grouping and distance from memory controllers. This policy also allows for task migration to optimize both execution time and power consumption. The paper outlines the assignment algorithm and how it will be implemented on a many-core system.
412057	4120570	Using Personality Metrics to Improve Cache Interference Management in Multicore Processors.	As processors continue to increase in the number of cores, there are challenges that arise, such as increased competition for shared resources like the Last-Level Cache (LLC). This is particularly relevant in scenarios like Data Centers, where Decision Support System queries are commonly executed. In this study, the focus is on the cache interference while executing these queries using a multicore processor with up to 16 cores and different LLC configurations. To better understand the effects of co-execution, two new metrics are developed - social and sensitive - to classify the behavior of the queries. These metrics can be used to manage cache interference and improve the overall performance of co-executing queries. 
412058	41205819	A relational symbolic execution algorithm for constraint-based testing of database programs	Constraint-based program testing involves using symbolic execution to generate test data for a specific execution path in a program. This technique can be applied to different paths in the code to automatically generate suitable test sets for code units. To improve the reliability of software, this technique can be generalized for programs that manipulate databases. A proposed relational symbolic execution algorithm for testing simple Java methods that interact with a relational database is described. This algorithm models the database tables and method variables as constrained relational variables and uses Alloy constraints to generate test cases. A tool implementing this algorithm is demonstrated with examples.
412058	41205845	A Symbolic Execution Algorithm for Constraint-Based Testing of Database Programs.	Constraint-based testing is a method used to generate test data for imperative programs, with symbolic execution being a commonly used technique. As databases are prevalent in software, testing database programs is crucial for ensuring reliability. This study proposes and experiments with a symbolic execution algorithm specifically designed for constraint-based testing of database programs. The algorithm works by modeling the program and database interactions using a formal language called SimpleDB, and generating path constraints for testing purposes. The results of the study show that the proposed algorithm is effective in generating test inputs that guarantee adherence to specific paths in the program.
412059	41205934	Web Application Security Using JSFlow.	Web applications are commonly targeted by code injection attacks and attacks through buggy or malicious libraries. However, current protection measures are often ad-hoc and reactive, resulting in a variety of specialized but fragile mechanisms. This abstract introduces JSFlow, an information-flow aware interpreter for web application security, which focuses on controlling what applications can do with accessed information rather than just restricting access. This removes the need for trust in entities with granted access, making it more resilient to bypassing by third party code and code injection attacks. Through two practical attacks on a web application called Hrafn, the effectiveness of JSFlow is demonstrated. It provides a consistent defense against untrustworthy and malicious code, ensuring confidentiality of sensitive data. 
412059	41205937	May I? - Content Security Policy Endorsement for Browser Extensions	Cross-site scripting (XSS) vulnerabilities are a major issue on the web, and one way to combat them is through Content Security Policy (CSP). However, the adoption of CSP has been slow due to its interaction with browser extensions. A study of free extensions from Google's Chrome web store revealed three types of vulnerabilities caused by the conflict between extensions and CSP: third party code inclusion, enabling XSS, and user profiling. Some extensions with millions of users were found to be vulnerable. To promote the use of CSP, the authors propose an extension-aware CSP endorsement mechanism between the server and client, which was demonstrated to be effective in a case study with the Rapportive extensions for Firefox and Chrome.
412060	41206038	LCO-MAC: A Low Latency, Low Control Overhead MAC Protocol for Wireless Sensor Networks	Wireless sensor networks (WSNs) often use a duty-cycling scheme in their medium access control (MAC) protocol to reduce energy consumption. However, this scheme can lead to high end-to-end latency and control packet overhead. To address this issue, a new MAC protocol called LCO-MAC has been proposed. This protocol allows a DATA packet to be transmitted through multiple hops in a single duty cycle, reducing end-to-end latency. Additionally, one control packet can serve multiple roles, acting as an RTS and CTS in the initial transmission period and as an ACK during actual data transmission. Simulations have shown that LCO-MAC improves energy efficiency and decreases end-to-end latency compared to the current RMAC protocol. 
412060	41206024	Collaborative Distributed Admission Control (CDAC) for Wireless Ad Hoc Networks	The paper discusses the importance of admission control algorithms in ensuring Quality of Service (QoS) for multimedia applications over wireless home networks. It proposes a framework for performing distributed admission control in a collaborative wireless environment, where a wireless device will not add a new flow if it would impact the resources for existing flows. The paper also suggests modifications to the 802.11x network to increase bandwidth efficiency and provides performance analysis for multiple flows with different throughput requirements. Based on this analysis, two distributed admission control algorithms are proposed and their performance is confirmed through simulations.
412061	41206114	Collaborative Distributed Admission Control (CDAC) for Wireless Ad Hoc Networks	Admission control algorithms have been studied extensively to ensure Quality of Service (QoS) for multimedia applications over wired Internet. As wireless home networks become more prevalent, it is important to utilize these mechanisms to improve the performance of wireless multimedia applications. This paper proposes a framework for distributed admission control in a collaborative wireless environment, where a device will not add a new flow if it would compromise existing flows due to resource limitations. The authors suggest a modification to 802.11x networks to increase bandwidth efficiency, provide a performance analysis for networks with multiple flows, and propose two distributed admission control algorithms based on transmission opportunity and contention window. Simulation results support the theoretical predictions of the proposed algorithms.
412061	41206110	Video Streaming with Network Coding	The popularity of multimedia streaming applications over the Internet has increased rapidly in recent years. Two effective methods for delivering these contents are Content Delivery Networks (CDN) and Peer-to-Peer (P2P) networks. One common feature of these networks is their ability to support path diversity streaming, where a receiver can receive multiple streams from different paths. This paper proposes a network coding framework for efficient video streaming in CDNs and P2P networks, using multiple servers/peers to stream to a single receiver. This technique eliminates the need for synchronization between senders, integrates easily with TCP, and reduces server storage in CDNs. The Hierarchical Network Coding (HNC) is also proposed for use with scalable video bit streams to combat bandwidth fluctuations. Simulations show that this network coding approach can result in up to 60% bandwidth savings compared to traditional methods.
412062	4120621	Designing core ontologies	The lack of a formal basis for modeling complex, structured knowledge in distributed information systems hinders their integration. To address this issue, the use of core ontologies is proposed. These ontologies are highly axiomatized and based on foundational principles, making them precise and modular. They also allow for the reuse and integration of existing domain knowledge. Three core ontologies have been developed for events and objects, multimedia annotations, and personal information management. These ontologies can be simultaneously used and integrated in a complex socio-technical system such as emergency response. The design approach, lessons learned, and aesthetic aspects of these core ontologies are discussed.
412062	41206219	A core ontology on events for representing occurrences in the real world	The paper discusses the importance of events in semantic ambient media applications and highlights the limitations of existing event models. To address these issues, the authors present the Event-Model-F, a formal model based on the ontology DOLCE+DnS Ultralite. This model allows for a comprehensive representation of various aspects of events, such as time, space, objects, and relationships between events. It is developed using a pattern-oriented ontology design approach and can be easily extended for specific domains. The paper also describes the implementation of an application programming interface for easy integration of the Event-Model-F in different applications. The usefulness of the model is demonstrated through its application in a socio-technical system for emergency response.
412063	41206342	Siamese: scalable and incremental code clone search via multiple code representations	The paper introduces a new code clone search technique, called Siamese, which is accurate, incremental, and scalable to large code bases. Siamese utilizes multiple code representations, query reduction, and a custom ranking function to improve performance. It outperforms seven other clone detection tools with a 95% and 99% mean average precision on two clone benchmarks and can detect the largest number of Type-3 clones compared to other code search engines. Siamese is also scalable, able to search through 365 million lines of code in just 8 seconds. The paper also discusses two use cases for Siamese, online code clone detection and clone search with automated license analysis.
412063	41206313	Comparative stability of cloned and non-cloned code: an empirical study	Code cloning is a practice in software engineering that has been a topic of controversy due to conflicting claims about its impact on software maintenance. Recently, a new measurement technique called code stability has been introduced to assess the effect of code cloning by measuring the changeability of a code region. While previous studies have generally found that cloned code is more stable than non-cloned code, they have been limited in their scope and methodology. In this paper, the authors present a comprehensive study on code stability using three different measurement methods and a diverse set of subject systems written in multiple programming languages. Their findings show that the type of cloning, programming language, system size and age, and development strategy all play a role in the stability of cloned code. Additionally, they find that there is no consistent change pattern between cloned and non-cloned code regions within a system. 
412064	41206460	Improving Color Constancy by Photometric Edge Weighting	Edge-based color constancy methods use image derivatives to estimate the illuminant. Different edge types, such as material, shadow, and highlight edges, exist in real-world images and can greatly affect the performance of illuminant estimation. This paper presents an analysis of the impact of different edge types on edge-based color constancy methods. A taxonomy is introduced to classify edge types based on their photometric properties, followed by a performance evaluation using these edge types. The results show that specular and shadow edges are more important for illuminant estimation than material edges. To address this, the paper proposes an iterative weighted Gray-Edge algorithm that emphasizes these edge types. Results from images in controlled and uncontrolled environments demonstrate improved performance compared to regular edge-based methods, with reductions in median angular error of up to 25% and 11%, respectively. 
412064	41206422	Edge-Based Color Constancy	Color constancy is the ability to measure the colors of objects regardless of the color of the light source. The gray-world assumption is a well-known method for achieving color constancy, which assumes that the average reflectance of objects in the world is achromatic. In this paper, a new hypothesis called the gray-edge hypothesis is proposed, which assumes that the average difference in edges within a scene is achromatic. This leads to the development of a new algorithm for color constancy, which is based on the derivative structure of images rather than the traditional zero-order structure. A framework is also proposed that combines various known color constancy methods with the new gray-edge algorithm. The performance of this framework is tested on two large datasets and compared to state-of-the-art methods, showing comparable results with the added benefit of being more computationally efficient.
412065	41206544	Near-Optimal Distributed Maximum Flow	We have developed a distributed algorithm that can efficiently approximate single-commodity maximum flow in undirected weighted networks. It runs in a nearly optimal number of communication rounds, (D + root n) . n(o(1)), in the CONGEST model. This is a significant improvement over the previous bound of O(n(2)) and is close to the lower bound of (Omega) over tilde (D + root n). The algorithm involves two key results: a distributed construction of a spanning tree with average stretch n(o(1)) in (D + root n) . n(o(1)) rounds, and a distributed construction of an n(o(1))-congestion approximator using O(logn) virtual trees. The algorithm uses randomization and has a high probability of success.
412065	4120657	Distributed connectivity decomposition	The paper discusses the issue of congestion and information flow in distributed network algorithms. It presents time-efficient algorithms for decomposing graphs with high edge or vertex connectivity into multiple spanning or dominating trees, allowing for a flow close to the connectivity. The algorithms have round complexity of ~O(D+√n) and ~{O}(D+√nλ) for vertex-connectivity and edge-connectivity respectively. Lower bounds of ~](D+√n/k) and ~Ω(D+√n/λ) are also shown, and the vertex-connectivity decomposition can be applied to centralized algorithms. These results have implications for centralized approximation of vertex connectivity and distributed algorithms for oblivious broadcast routing. 
412066	41206631	Serving Online Requests with Mobile Servers.	This study focuses on an online problem where mobile servers must be moved to efficiently serve a set of online requests. There are n nodes and k servers that can be moved between them. Requests are issued one at a time and must be served continuously. The cost of serving requests is determined by the number of servers and requests at each node. The algorithm must ensure that the service cost remains within a certain factor and an additive term of the optimal cost. Two minimization objectives are considered: minimizing server movements and minimizing total cost. It is proven that for every k, the competitive ratio of any deterministic algorithm must be at least Omega(n). However, by extending the objective to include current service cost, a competitive ratio of 1+epsilon can be achieved with a linearly increasing additive term for every constant epsilon > 0.
412066	41206654	Bounding interference in wireless ad hoc networks with nodes in random position	The interference minimization problem in wireless networks involves assigning transmission radius or power levels to nodes to ensure a connected communication graph while minimizing interference. The model introduced by von Rickenbach et al. (2005) uses balls to represent transmission ranges and symmetric edges in the graph. This problem is known to be NP-complete in two dimensions and has no known polynomial-time approximation algorithm. However, in this paper, the authors propose a solution for this problem in typical wireless ad hoc network settings. They show that the topology of the closure of the Euclidean minimum spanning tree of randomly selected node positions has a maximum interference of O(logn) with high probability. This bound is extended to a broader set of probability distributions and a local algorithm is presented to construct the graph with an upper bound on expected maximum interference. An empirical evaluation of the algorithm through simulation is also provided.
412067	41206737	Frequent subgraph summarization with error control	Frequent subgraph mining is an important but challenging problem in research. The large number of discovered subgraphs makes it difficult to analyze and understand the patterns. This paper proposes a summarization model that uses an independence probabilistic model to accurately restore frequent subgraphs and their frequencies. The model allows users to specify an error tolerance and uses a top-down approach to discover summarization templates while keeping the frequency restoration error within the tolerance. Experiments on real graph datasets show that the model can effectively control the error within 10% with a concise summary.
412067	41206771	Succinct summarization of transactional databases: an overlapped hyperrectangle scheme	Transactional data, which are commonly found in databases, have been analyzed using methods such as frequent itemsets mining and co-clustering. However, there is a new research problem of succinctly summarizing these databases by linking the high level structure to a large number of frequent itemsets. This problem is formulated as a set covering problem using overlapped hyperrectangles and has been proven to be NP-hard. A polynomial time approximation algorithm called HYPER with a ln(k) + 1 approximation ratio has been developed, along with a pruning strategy to speed up processing. An efficient algorithm has also been proposed to summarize hyperrectangles by allowing false positive conditions. Real and synthetic datasets have been used to demonstrate the effectiveness and efficiency of these approaches.
412068	41206889	Robust face pose classification method based on geometry-preserving visual phrase	Developing a feature for accurately classifying facial poses is a difficult task due to the varying spatial layouts of key facial points. To address this challenge, a pose classification framework using local geometry-preserving visual phrases (GVP) is proposed. This framework includes a weighting strategy on GVP that improves the discriminability of individual words and spatial layouts within high order phrases. This results in a flexible and robust method that can account for multiple factors affecting facial poses. Experimental results on two databases demonstrate the effectiveness of this approach compared to other methods such as PCA, LDA, and tf-idf weighted Bag-of-Words.
412068	412068157	A jointly distributed semi-supervised topic model	Latent topic models are commonly used to analyze the underlying semantic meaning of documents and images, particularly in object categorization. However, these models lack the ability to ensure that the learned topics are relevant to class labels. Manually aligning and labeling training images is expensive and subjective, making it impractical for real-world applications. To address this issue, this paper introduces a semi-supervised approach that uses a small amount of partial labels to generate more suitable topics for classification. The proposed method, which incorporates joint distribution from multi-conditional learning, includes semi-supervised LDA and pLSA models. Experimental results on natural scene categorization and head pose classification tasks demonstrate the effectiveness of this approach, showing promising results even with only partial labels in the training process. 
412069	4120696	A Visual Silence Detector Constraining Speech Source Separation	We propose a method for separating speech signals using audio and visual information. Our algorithm first identifies periods of low mouth activity in video recordings and uses a classifier to detect silent intervals during these moments. The source separation problem is then solved for the entire recording. We tested our approach on two speech corpora with two speakers and two microphones, one with simulated room mixing and one with recorded conversations. The results were positive, with the use of the visual silence detector improving the performance of the source separation algorithm. This method shows potential for enhancing speech separation in audiovisual recordings.
412069	41206971	A Segmentation Framework For Phase Contrast And Fluorescence Microscopy Images	Imaging living cells without staining them is a commonly used technique in biotechnology for studying protein function. This method, called noninvasive imaging, allows for the observation of living specimens without altering them. Fluorescence and contrast images are typically used together to achieve better results. However, segmenting contrast images is challenging due to defocused scans, lighting artifacts, and overlapping cells. In this study, the researchers investigated the optical properties involved in image formation and proposed segmentation strategies that take advantage of these properties. Their proposed approach combines phase and fluorescence information and uses a specialized contour model to accurately segment cell images. The results of their experiments support the effectiveness of this strategy for cell image segmentation.
412070	4120702	If: An Intermediate Representation For Sdl And Its Applications	The project discussed is focused on improving a specification/validation toolbox by integrating a commercial toolset called ObjectGEODE with other validation tools like CADP and TGV. Due to the complexity of most protocol specifications, the project has explored combining techniques such as static analysis, abstraction, and model-checking. This has resulted in the development of an intermediate representation for SDL called IF, which represents systems as timed automata communicating asynchronously through buffers or synchronization gates. This intermediate representation allows for easier integration with existing tools and provides a way to define different notions of time for SDL. The project's results have been validated through experimentation.
412070	41207057	Automated Validation of Distributed Software Using the IF Environment	The paper discusses IF, an open validation environment designed for distributed software systems. Due to the complexity of these systems, no single tool can effectively cover the entire validation process. IF was created with an intermediate language and can connect multiple validation tools, incorporating advanced techniques. The practicality of this approach was confirmed through successful results on various case studies, such as telecommunication protocols and embedded software systems.
412071	41207164	The Pascal Visual Object Classes Challenge: A Retrospective	The Pascal Visual Object Classes (VOC) challenge is a benchmark for image recognition algorithms, consisting of a dataset with annotations and standardized evaluation software, as well as an annual competition and workshop. The challenge has five categories: classification, detection, segmentation, action classification, and person layout. This paper reviews the challenge from 2008-2012, providing insights for both algorithm designers and challenge organizers. The authors introduce new evaluation methods, such as bootstrapping and normalized average precision, to compare algorithm performance. They also analyze the progress of the community and identify common errors. The paper concludes with recommendations for future challenges and a discussion of the strengths and weaknesses of the VOC challenge.
412071	41207149	Renewal Strings for Cleaning Astronomical Databases.	The SuperCOSMOS Sky Surveys (SSS) and other large astronomical databases often contain false records due to various factors such as telescope errors and orbiting objects. These records can be a significant issue for astronomers, as they can make up a large portion of potentially relevant data. A new technique called renewal strings, which combines the Hough transform, renewal processes, and hidden Markov models, has been developed to effectively address this problem. This method has been successfully applied to the SSS data, creating a dataset of false detections with confidence measures that can be used to filter out unwanted data. These methods are adaptable to other astronomical survey data. 
412072	4120721	Ramsey-Type Results for Unions of Comparability Graphs	The comparability graph of a partially ordered set with $n$ elements always contains either a clique or an independent set of size $\sqrt{n}$. This also applies to graphs that are the combination of two comparability graphs on the same set of vertices, where the minimum size of a clique or independent set is $n^{1 \over 3}$. However, there are cases where the size of these sets is no more than $n^{0.4118}$. This same pattern holds for graphs that are unions of a fixed number $k$ comparability graphs, as well as for unions of perfect graphs.
412072	4120724	New Bounds on Crossing Numbers	The crossing number, cr(G), of a graph G is the minimum number of crossing points in any drawing of the graph in the plane. The minimum crossing number for graphs with n vertices and at least e edges, denoted by (n; e), is shown to tend to a positive constant as n approaches infinity and n e approaches n2=e3. This holds for graph drawings on any surface with a fixed genus. Better bounds are proven for graphs with certain monotone properties, such as not containing a cycle of length four or six. Specifically, for graphs with n vertices and e 4n edges, the crossing number is at least ce4=n3 (resp ce=n4), where c > 0 is a constant. This answers a question posed by M Simonovits.
412073	41207390	Efficient Object Detection and Segmentation for Fine-Grained Recognition	This article presents a new algorithm for detecting and segmenting objects in images for fine-grained recognition. The algorithm first identifies potential object regions and then segments the entire object using propagation. Additionally, the algorithm can zoom in on the object, center it, and normalize it for scale to improve recognition accuracy. The algorithm was tested on various datasets and showed significant improvements in performance, especially for difficult datasets such as bird species. It is more efficient than other methods and has been successfully applied to different types of objects. The algorithm outperforms state-of-the-art methods by as much as 11% on benchmark datasets and consistently improves the performance of a baseline algorithm by 3-4%. It also showed a 4% improvement on a large-scale flower dataset with 578 species and 250,000 images. 
412073	41207347	Regionlets for Generic Object Detection	Generic object detection is a challenging task due to variations in different object classes and the need for efficient computations. To address this, a cascaded boosting classifier is proposed, which integrates regionlets - base feature extraction regions that are defined proportionally to the detection window at various resolutions. These regionlets are organized in groups to capture fine-grained spatial layouts within objects and their features are aggregated to tolerate deformations. The proposed method also utilizes segmentation cues to generate object bounding box proposals, limiting evaluation to thousands of locations. This approach achieves competitive performance on popular multi-class detection benchmark datasets without using any contextual information. Further, the use of support pixel integral images and deep convolutional neural networks improves performance on the ImageNet Large Scale Visual Object Recognition Challenge.
412074	41207410	Multi-Output Regularized Feature Projection	Dimensionality reduction by feature projection is a widely used technique in pattern recognition, information retrieval, and statistics. When there are available outputs, it is beneficial to consider supervised projection, which takes into account both the inputs and target values. This is especially useful in applications with multiple outputs, where several tasks need to be learned simultaneously. In this paper, a new approach called Multi-Output Regularized feature Projection (MORP) is introduced, which preserves input information and captures correlations between inputs and outputs. By using a latent variable model and solving a generalized eigenvalue problem, this approach can greatly improve prediction accuracy and has shown promising results in predicting user preferences for paintings and image/text categorization. 
412074	41207433	trNon-greedy active learning for text categorization using convex ansductive experimental design	This paper introduces a non-greedy active learning approach for text categorization using least-squares support vector machines. The method is based on transductive experimental design and aims to effectively utilize the information from unlabeled data. The optimization problem in this approach is NP-hard, but the authors have formulated it into a continuous optimization problem and proved its convexity. They have also developed an iterative algorithm to efficiently solve the problem. Experimental results on two text corpora showed that this new approach outperforms the traditional greedy method and has potential for active text categorization tasks.
412075	412075118	A modular multiple classifier system for the detection of intrusions in computer networks	The protection of computer networks is crucial in modern computer systems. To combat threats, various software tools have been created. One of these is Intrusion Detection Systems, which identify intruders that manage to bypass initial security measures. A new approach to network intrusion detection is proposed in this paper, using a pattern recognition system that combines multiple classifiers. The system is designed with a modular structure, with each module detecting intrusions in a specific service offered by the network. Different feature representations of network traffic are fused together, and the effectiveness of this approach is evaluated. The potential for successful intrusion detection using classifier fusion is also discussed.
412075	41207534	Intrusion detection in computer networks by multiple classifier systems	In modern computer systems, ensuring the security of computer networks is crucial. To protect against threats, various software tools are being used. One such tool is intrusion detection systems, which aim to identify an intruder who has bypassed initial security measures. A paper proposes a pattern recognition approach to network intrusion detection using multiple classifiers. This method combines different classifiers to improve data fusion and address certain challenges. The paper also discusses the potential benefits and unresolved issues related to using classifier combination for network intrusion detection.
412076	41207676	PHY modulation/rate control for fountain codes in 802.11a/g WLANs.	The paper examines the combined performance of fountain codes and 802.11a/g PHY modulation and coding. Both theoretical and experimental channel models are considered, with a focus on maximizing goodput and minimizing energy. Unlike studies in cellular networks, it is found that in 802.11a/g WLANs, a cross-layer approach of using higher-layer fountain coding with PHY layer modulation and FEC coding does not result in significant improvements. The optimal PHY modulation/rate for uncoded multicast traffic is also similar to that for fountain-coded multicast traffic in various network conditions. This suggests that in 802.11a/g WLANs, cross-layer design for multicast rate control may not be beneficial and PHY layer rate control can be done without considering the use of fountain coding at higher layers.
412076	41207665	Proportional Fair Coding for Wireless Mesh Networks	This paper discusses the optimization of multihop wireless networks that carry unicast flows for multiple users. Each flow has a deadline for delay and the wireless links are modeled as binary symmetric channels. Increasing the airtime for one flow decreases the airtime available for other flows sharing the same link. The paper presents a solution for the joint allocation of flow airtimes and coding rates that achieves proportionally fair throughput allocation. This is achieved through a sequence of convex optimization problems, with the final solution providing insight into the nature of the allocation. This is the first time such an analysis has been conducted and one of the first times delay deadlines have been considered in utility fairness.
412077	41207713	Optimal decision fusion with applications to target detection in wireless ad hoc sensor networks	Decision fusion is a method of decentralized decision making where local decisions are combined to reach a larger decision. A new approach called complementary optimal decision fusion (CODF) has been proposed for target detection in wireless ad hoc sensor networks. This method has been extensively compared to other decision fusion methods using standard datasets, and has been found to have superior performance. The CODF algorithm can also be applied to other signal processing problems involving multiple modalities, agents, and media.
412077	41207745	Vehicle classification in distributed sensor networks	This paper discusses the task of classifying different types of moving vehicles using a distributed, wireless sensor network. The authors conducted an experiment and collected a data set consisting of 820 MByte of raw time series data, 70 MByte of preprocessed spectral feature vectors, and baseline classification results using the maximum likelihood classifier. They provide details on the data collection process, feature extraction and pre-processing steps, and the development of a baseline classifier. The database is available for download at a specific website since July 2003.
412078	41207837	New Analysis of Manifold Embeddings and Signal Recovery from Compressive Measurements.	Compressive Sensing (CS) is a technique that takes advantage of the fact that a sparse signal can be accurately represented with a small number of compressive measurements. This has been proven through theoretical guarantees, showing that sparse signals can be recovered from noisy compressive measurements. This paper focuses on a different approach, using manifold models instead of sparse models. By using tools from empirical processes, the authors are able to improve upon previous results and establish bounds for embedding low-dimensional manifolds under random measurement operators. They also demonstrate the effectiveness of manifold-based models for signal recovery and parameter estimation in noisy compressive measurements. Overall, this work supports the idea that manifold-based models can be successfully used in compressive signal processing.
412078	41207842	Random Projections of Smooth Manifolds	The authors propose a new method for reducing the dimensionality of manifold-modeled data using random linear projections. They focus on the effect of these projections on a smooth, well-conditioned K-dimensional submanifold in ℝN. They establish a sufficient number of projections M to preserve pairwise distances between points on the manifold with high probability. This approach is similar to the concept of Compressed Sensing, where sparse signals can be recovered from a small number of random linear measurements. The number of projections M is linear in the information level K and logarithmic in the ambient dimension N, with a logarithmic dependence on the volume and conditioning of the manifold. This method can also reveal important properties about the manifold. It differs from existing techniques in manifold learning, which typically use adaptive, nonlinear mappings from sampled training data.
412079	41207989	Compressive Sensing for Background Subtraction	Compressive sensing (CS) is a new approach for image recovery that allows for using lower sampling rates than traditional methods. It relies on the sparsity of signals in certain bases, such as wavelets, to reconstruct images from a small set of random projections. This paper presents a method for directly recovering background subtracted images using CS, which has applications in communication constrained multi-camera computer vision problems. The proposed method involves casting background subtraction as a sparse approximation problem and using convex optimization and total variation to find solutions. Unlike traditional methods that learn the background, this approach learns a low dimensional compressed representation of it, making it more efficient. The paper also discusses simultaneous appearance recovery of objects using compressive measurements, and provides results on data captured using a compressive single-pixel camera. The proposed method is also shown to be effective for image coding in communication constrained problems, with results from using data captured by multiple conventional cameras for 2D tracking and 3D shape reconstruction.
412079	412079110	FlatCam: Replacing Lenses With Masks and Computation	FlatCam is a lensless camera that uses a coded mask on top of a conventional sensor array. This allows each pixel to capture light from multiple parts of the scene, and a computational algorithm is used to reconstruct the image. The mask is placed very close to the sensor, making the camera thin. A separable mask is used for easy calibration and image reconstruction. A prototype camera was built using a commercial sensor and mask to showcase the design's potential. Overall, FlatCam offers a unique approach to capturing images with a thin form-factor and efficient processing.
412080	41208062	Randomized polygon search for planar motion detection	This paper presents a randomized algorithm for estimating the motion parameters of a planar shape without prior knowledge of point-to-point correspondences. The algorithm searches for points on two shapes measured at different times to determine the centroids, and then randomly searches for congruent polygons to determine the rotation. This approach eliminates the need for manual correspondence matching and improves the accuracy of motion parameter estimation.
412080	412080113	Motion analysis by random sampling and voting process	In computer vision, motion analysis is a crucial problem. A new method is proposed that combines congruence checking and geometric hashing techniques to estimate motion parameters for both fully measured and partially occluded objects in 2D and 3D Euclidean spaces. The method assumes that the object's vertex positions are determined through range sensing or stereo techniques. The study also examines the effects of quantization errors on the estimation of motion parameters through random sampling and shows that increasing digitalization resolution leads to more accurate results.
412081	41208118	An Integrated Approach For Simulating Interdependencies	This paper discusses the challenges of simulating interdependent critical infrastructures and proposes a solution through a simulation framework that integrates sector-specific simulators into a general environment. The framework allows for the modeling of individual infrastructures and their internal dynamics, as well as capturing inter-domain relationships and merging information from different simulators. This approach aims to overcome the difficulties of modeling multiple heterogeneous infrastructures and expressing their internal dependencies and interdependencies. Overall, the proposed framework provides a comprehensive simulation of critical infrastructures, addressing the complex and important issue of interdependency.
412081	41208136	A Holistic-Reductionistic Approach For Modeling Interdependencies	Modeling and analyzing critical infrastructures is crucial in identifying potential vulnerabilities and threats. Some approaches take a holistic view and use abstract models, while others focus on specific interactions among components. This paper suggests a combination of both approaches, where critical infrastructures are represented at multiple levels of abstraction and intermediate entities are introduced to provide aggregate resources or services. This mixed approach allows for a more comprehensive understanding of the complex interdependencies within critical infrastructures.
412082	41208215	What groups do, can do, and know they can do: an analysis in normal modal logics	This article discusses various logics that are used to reason about agents' actions, abilities, and knowledge. These logics include Pauly's Coalition Logic CL, Alternating-time Temporal Logic ATL, the logic of 'seeing-to-it-that' (STIT), and their epistemic extensions. The authors introduce a simplification of the STIT language and propose a new semantics for it, as well as extending it to include groups. They also add a temporal operator and standard S5 knowledge operators to this logic, resulting in a new logic called E-X-Ldm G. The authors compare this logic to other epistemic extensions and conclude that it is better suited for expressing agents' abilities and strategies.
412082	41208233	A computationally grounded dynamic logic of agency, with an application to legal actions	The article introduces a new logical framework, called Dynamic Logic of Propositional Control (DL-PC), that allows for the expression of the concept of 'seeing to it that'. This concept, studied by Belnap, Horty, and others, is captured in DL-PC through the use of dynamic operators. The logic also incorporates the Chellas and deliberative stit theories. The satisfiability problem for DL-PC is shown to be decidable. The framework is then extended to DL-PCLeg, which includes operators for normative concepts such as 'legally seeing to it that' and 'illegally seeing to it that'. The decidability result for DL-PC also applies to DL-PCLeg.
412083	41208341	The Dynamics of Epistemic Attitudes in Resource-Bounded Agents	The paper introduces a new logic for reasoning about how beliefs are formed in agents with limited resources. It distinguishes between explicit beliefs and background knowledge, using a non-standard semantics and specific axioms to capture this distinction. The logic includes mental operations of perception and inference, modeled as special model-update operations. The paper also discusses results on axiomatization, decidability, and complexity for the logic. Overall, the logic provides a formal framework for understanding how agents form beliefs through perception and inference, taking into account the limitations of their resources.
412083	41208318	On the Epistemic Foundation for Iterated Weak Dominance: An Analysis in a Logic of Individual and Collective attitudes.	This paper presents a logical framework for representing individual and collective attitudes, both static and dynamic. The logic is applied to game theory, specifically the analysis of the epistemic conditions for iterated deletion of weakly dominated strategies. The paper introduces a semi-qualitative approach to uncertainty using the concept of plausibility, in contrast to other analyses which use a quantitative representation of uncertainty through probabilities. The paper includes a complete axiomatization and a proof of decidability for the logic. This framework provides a useful tool for understanding and reasoning about attitudes and decision-making in various contexts.
412084	41208410	Sufficient dimensionality reduction	Dimensionality reduction is a crucial problem in unsupervised learning, with applications in statistics and cross-classified data analysis. This paper introduces a new approach to this problem using information theory, which aims to preserve the mutual information contained in the original data while reducing its dimensionality. Unlike previous methods, this approach directly extracts continuous feature functions from the co-occurrence matrix, serving as approximate sufficient statistics for one variable about the other. It can also be seen as a generalized, multi-dimensional, non-linear regression, with the resulting dimension reduction described by two conjugate differential manifolds coupled through Maximum Entropy I-projections. An iterative information projection algorithm is presented and proven to converge, demonstrating its effectiveness in various applications such as text categorization and information retrieval. 
412084	4120848	Sufficient Dimensionality Reduction - A novel Analysis Method	This paper introduces a new information theoretic nonlinear method for dimensionality reduction of co-occurrence data. Unlike previous clustering-based approaches, this method extracts continuous feature functions directly from the co-occurrence matrix, serving as approximate sufficient statistics for one variable about the other. It can be seen as a generalized, multi-dimensional, non-linear regression that unifies aspects of supervised and unsupervised learning. The resulting dimension reduction is described by two conjugate d-dimensional differential manifolds coupled through Maximum Entropy I-projections, with Riemannian metrics determined by the observed expectation values of the extracted features. The paper presents an iterative information projection algorithm for finding these features and proves its convergence. This algorithm is similar to association analysis in statistics, but with a new feature extraction context and information theoretic and geometric interpretation. It is effective in selecting a small set of features and has shown promising results in text categorization and information retrieval.
412085	41208533	FLUXO: a simple service compiler	In this paper, the authors introduce FLUXO, a system that separates the logical functionality of an Internet service from the architectural decisions made for performance, scalability, and reliability. FLUXO utilizes a dataflow-based programming model, runtime request tracing, and analysis techniques to optimize the service architecture for these factors. The paper presents the authors' vision for simplifying the construction of Internet services and demonstrates how FLUXO can be used to express various performance optimizations. By decoupling the logical functionality from architectural decisions, FLUXO allows for more flexibility and adaptability in designing Internet services.
412085	41208539	CMC: a pragmatic approach to model checking real code	The paper discusses the difficulty of finding system errors that only occur after a specific sequence of events and how model checking can be an effective solution. However, building models for code is often too time-consuming and can lead to missed errors. The paper presents a new model checker, CMC, that directly checks C and C++ implementations, reducing the effort and likelihood of missed errors. This is demonstrated through its successful application to three implementations of the AODV networking protocol, uncovering 34 distinct errors. The paper suggests that this approach could be beneficial for other systems and networking protocols.
412086	412086187	The auction: optimizing banks usage in Non-Uniform Cache Architectures	The rise of wire delay in cache design has led to non-constant access latencies in last-level cache banks. To address this issue, Non-Uniform Cache Architectures (NU-CAs) have been proposed. In chip multiprocessor (CMP) architectures, an efficient last-level cache is crucial in reducing requests to off-chip memory due to the speed gap between processor and memory. A bank replacement policy is needed to effectively manage NUCA caches, but the decentralized nature of NUCA has hindered previous policies. A new mechanism called The Auction is proposed, which allows replacement decisions to be spread across the entire cache. This enables global replacement policies, leading to improved performance and reduced energy consumption. Three approaches of The Auction are evaluated and shown to be effective in managing the cache and reducing off-chip memory requests.
412086	41208697	SAMIE-LSQ: set-associative multiple-instruction entry load/store queue	The load/store queue (LSQ) is a complex component in modern processors that has a significant impact on processor performance. The SAMIE-LSQ, a highly banked, set-associative, multiple-instruction entry LSQ, has been developed to improve performance while also reducing energy consumption. This LSQ classifies memory instructions based on the address being accessed and groups instructions accessing the same cache line together. By doing so, it reduces the number of address comparisons needed for memory disambiguation and decreases activity in the data translation lookaside buffer, cache tag, and data arrays. Additionally, the SAMIE-LSQ utilizes caching to save time and energy by reusing translations and avoiding unnecessary checks. Compared to a conventional LSQ, the SAMIE-LSQ has a lower delay and saves a significant amount of energy for the load/store queue, L1 data cache, and data TLB with minimal impact on performance.
412087	41208759	Towards cost-sensitive assessment of intrusion response selection	The concept of cost-sensitive intrusion response has gained attention due to its focus on balancing the potential damage caused by intrusions with the cost of responding to them. However, implementing this approach can be challenging because of the need for consistent and adaptable measurements of cost factors based on system requirements and policies. In this paper, a framework is presented for selecting cost-sensitive intrusion response. This framework includes measurements for potential costs associated with handling intrusions and an evaluation method for response effectiveness, intrusion risk, and cost. An implementation of this framework is also demonstrated using real network traffic. 
412087	41208740	A Cost-Sensitive Model for Preemptive Intrusion Response Systems	The increasing prevalence of complex and fast-spreading cyber attacks has highlighted the need for both improved intrusion detection and advanced automated intrusion response systems. This paper introduces a new cost-sensitive model for intrusion response, which involves deploying response actions preemptively based on cost-benefit analysis. By comparing the cost of deploying a response with the potential damage caused by an unattended intrusion, the model can make informed decisions about when to deploy a response. Additionally, the model can adapt responses to changing environments by evaluating the success and failure of previous responses. The effectiveness of this approach is demonstrated through evaluation using a damage reduction metric.
412088	41208810	A comparative analysis of biclustering algorithms for gene expression data.	This article discusses the need for new data mining methods to analyze high-dimensional biological data. Biclustering algorithms have been used to discover local patterns in gene expression data, but it is unclear which algorithms are most effective. The article addresses this issue by comparing 12 recently published or lesser-known algorithms using synthetic data sets and gene expression data from the Gene Expression Omnibus. The algorithms were evaluated based on their performance on different conditions, such as varying noise and number of biclusters. Gene Ontology enrichment analysis was also performed on the resulting biclusters. The results suggest that the choice of biclustering method and parameters should be based on the desired model and its ability to handle noise, and that algorithms capable of finding multiple models are more effective in capturing biologically relevant clusters.
412088	41208840	PRASE: PageRank-based Active Subnetwork Extraction	The integration of protein-protein interaction networks and gene expression data has shown promise in identifying significant biomarkers for diseases such as cancer. However, existing algorithms struggle to handle the newer RNA-Seq gene expression data or produce large, difficult-to-analyze subnetworks. To address this issue, a new approach called PRASE has been proposed, which uses the PageRank algorithm and a preprocessing step to prioritize genes that may not be differentially expressed but still play important roles in critical pathways. This workflow has been successfully applied to three cancer datasets, demonstrating its ability to extract more focused and informative subnetworks compared to traditional methods.
412089	41208959	Improving security of virtual machines during live migrations.	Live migration of virtual machines (VMs) is a process that allows a running VM to be transferred to a new hardware component with minimal interruption. This is commonly used in cloud architectures, where users are usually unaware of and unable to prevent live migrations of their VMs. However, if a VM is live migrated to a different data center in another country, it can raise security and privacy concerns. In this paper, the authors propose methods to detect live migrations from within the affected VM and analyze how the migration process can be delayed to allow for security measures to be taken. They have developed a "live migration defence framework" (LMDF) to enforce security policies within a VM. The proposed methods and techniques were evaluated in a cloud setup and partially in the Amazon Elastic Computing Cloud (EC2). 
412089	41208930	Data-centric phishing detection based on transparent virtualization technologies	This article introduces a new phishing detection system that uses transparent virtualization technology and isolates its components. It can be added as a security feature for virtual machines in the cloud. The system uses VM introspection to create color-based fingerprints of web pages being viewed in the VM's memory. By comparing these fingerprints, the system can identify and prevent phishing attacks that redirect to fake web pages, as well as "Man-in-the-Browser" attacks. This is the first anti-phishing solution to utilize virtualization technology. The article discusses the design and implementation of the system and presents results from testing it with real-world data.
412090	4120904	MRT—a visualization tool addressing problems “outside” the classical rendering domain	We have developed an object-oriented software architecture for a 3D rendering environment that greatly increases productivity for programmers. The platform, called MRT, is made up of customizable building blocks that make 3D image synthesis more accessible. It is based on objects rather than drawings and has been proven to be highly customizable and extendable for a diverse user population. Our partnership with a German mobile communication network supplier resulted in the creation of a prototype package for simulating 3D radio wave distribution in urban environments within just three weeks. This was made possible by the compact and efficient nature of MRT, which also outperformed existing solutions.
412090	41209060	Automatic procedural model generation for 3D object variation	Procedural models are a useful tool for generating variations of 3D objects. They allow for the automatic creation of these variations by changing parameters within the model. However, creating a procedural model can be time-consuming. To address this issue, we propose a method for automatically generating a procedural model from a single 3D object. This model consists of a sequence of parameterizable procedures that represent the construction process of the object. By linking the procedural model to the original object surface, changes made to the model can be transferred to the original object, enabling the generation of variations. The user can easily adapt the model by defining variation parameters within the procedures, and we have successfully tested this approach on various object types. 
412091	41209151	Learning discrete categorial grammars from structures	The discrete classical categorial grammars are a class of grammars similar to the reversible class of languages proposed by Angluin and Sakakibara. This class can be identified from positive structured examples using a new algorithm that has a quadratic time complexity. This extends previous work by Kanazawa, as our algorithm can handle multiple types associated with a word and has a polynomial time complexity. We provide linguistic examples to demonstrate the applicability of this class.
412091	41209110	A characterization of alternating log time by first order functional programs	The article discusses the characterization of functions that can be computed in NC1, a class of functions that can be computed in logarithmic time on an Alternating Turing Machine. The characterization is done using first order functional programming languages and involves the use of Sup-interpretations, which help in giving space and time bounds and capturing program schemas. This research is aimed at predicting program resources and is connected to the concept of Quasi-interpretations. It falls under the field of implicit computational complexity.
412092	41209233	Gossip Galore: A Conversational Web Agent for Collecting and Sharing Pop Trivia	This paper introduces a self-learning agent that uses information extraction, web mining, question answering, and dialogue system technologies to gather and update knowledge about celebrity gossip in the music world. The agent can answer questions about musicians, bands, and related people, and uses data mining to create a social network among them. The agent is able to have interactive conversations with users through natural language processing and can provide answers in various forms such as text, graphs, and speech. This is made possible through minimally supervised machine learning for relation extraction and a knowledge-intensive question answering technology. 
412092	41209253	Gossip Galore: An Embodied Conversational Agent for Collecting and Sharing Pop Trivia from the Web	This paper introduces a new self-learning agent that gathers and learns information from the internet, specifically in the realm of celebrity gossip in the music industry. The agent can continuously update its knowledge by monitoring the web and engaging in dialogue with users. Fans can ask for gossip about musicians and their associates, and the agent uses technologies such as information extraction, web mining, and question answering to provide accurate and interactive responses. The agent also uses data mining to create a social network of artists and related individuals. Overall, this agent allows users to freely ask questions and receive information through various methods, including text, visualizations, and speech.
412093	41209329	Tight Lower Bounds for st-Connectivity on the NNJAG Model	The directed st-connectivity problem involves determining if there is a path from a specific node s to another node t in a directed graph. A time-space lower bound is proven for the probabilistic NNJAG model, which uses n nodes in the input graph and has a space of S and time of T. The lower bound states that if S is in O(n^(1-delta)), then T is at least 2^(log^2(n/S)); otherwise, T is at least 2^(log^2(nlogn/S)/loglogn) multiplied by the square root of nS/logn. This greatly improves previous lower bounds and is tight when S is in O(n^(1-delta)). As a result, the first tight space lower bound of at least log^2(n) is obtained for the NNJAG model, which was previously unknown. 
412093	4120931	Improved on-line broadcast scheduling with deadlines	The article discusses an online broadcast scheduling problem with deadlines, with the goal of maximizing the weighted throughput. A new algorithm, BAR, is presented for the case where all requested pages have the same length, which is proven to be 4.56-competitive. This improves upon a previous algorithm which was shown to be 5-competitive. For the case of pages with different lengths, a ( $\Delta+ 2\sqrt{\Delta}+2$ )-competitive algorithm is proposed, improving upon the previous (4Δ+3)-competitive algorithm. A lower bound of 驴(Δ/log驴Δ) is also proven, with better lower bounds for smaller values of Δ. 
412094	41209465	Discovery of Narrativity on the WWW based on Perspective Information Access	The proposed framework aims to discover narrative relationships between objects on the internet by providing perspective information access. While it is easy to access web resources, it can be difficult to find information that meets specific user requests. Most people rely on search engines, but this requires specifying appropriate keywords. The proposed framework is designed for users who are seeking new information related to other information, even if the relationship between them is unknown. It offers a perspective path to guide users to the desired information, creating a narrative between the source and destination. This framework is meant to support users in their search for information.
412094	41209428	Querying with preferences in a digital library	A digital library acts as a mediator between a community of users and federated sources on the Web. The library indexes all available documents and returns URLs of relevant documents when a user submits a query. The size of the answer set, or the number of documents returned, can affect user satisfaction. This paper focuses on the issue of answer sets that are too large and introduces the concept of "personalized queries" which allow users to set an upper limit on the number of documents returned and specify their preferred order. The paper proposes a framework for formally addressing this problem and offers a method for evaluating personalized queries.
412095	41209529	A Scenario-View Based Approach to Analyze External Behavior of Web Services for Supporting Mediated Service Interactions	This paper presents a new approach to addressing mismatches in Web service interactions by focusing on both control-flow and data-flow. Current approaches only consider control-flow, but this new approach generates scenarios and views that describe the external behavior of Web services, including data dependencies. A scenario is defined as a set of execution paths for a public process, and a view is generated to analyze this scenario. By describing the external behavior of a Web service as a set of views, this approach is useful for service modelers and users to better understand and resolve behavioral mismatches, ultimately facilitating more effective Web service interactions.
412095	4120958	Better Behavioral Description for Dynamic Semantic Web Services Collaboration	Semantic Web services are designed to automate service operations and minimize human intervention, facilitating seamless interaction between services, reducing operation errors, and maximizing efficiency. While these services are well-described in terms of function, their descriptions of behavior, particularly for complex services, are lacking. This can result in behavioral mismatches between providers and requesters, even when they are functionally compatible. This paper introduces a model for describing the behavior of Web services, including both public and private processes, and emphasizes the significance of public processes in complex interactions. The paper also proposes an approach to enhancing the behavioral description of Web services by building upon existing semantic Web services conceptual models.
412096	41209615	ATL Transformation for the Generation of SCA Model	The Service Component Architecture specification (SCA) is a technology that supports the development, deployment, and integration of Internet applications. While it can manage dynamic availability and handle differences between components, it cannot solve all problems. As software systems continue to evolve, development and maintenance become more complex. To address this, the Model Driven Engineering (MDE) approach has been used. This paper focuses on applying MDE automation to convert UML 2.0 models to SCA models. Two metamodels, UML 2.0 component and SCA, are studied and transformation rules are defined in the ATL language to ensure traceability between them. This approach aims to simplify the development process and improve system maintenance.
412096	41209611	MDE approach for the generation and verification of SCA model	SCA is a technology used for developing and integrating internet applications. It helps manage dynamic availability and heterogeneity among components. However, it is not a complete solution for all problems. As software systems become more complex, Model Driven Engineering (MDE) is being used to simplify the development and verification process. This paper proposes the use of MDE to create and verify SCA models. Two transformations are applied, one using UML 2.0 metamodel to obtain SCA models and the other using event-B metamodel to verify their properties. The UML 2.0 component metamodel, SCA metamodel and event-B metamodel are studied and transformation rules are defined using the ATL language.
412097	412097132	Optimizing Tasks Assignment on Heterogeneous Multi-core Real-Time Systems with Minimum Energy	Embedded real-time systems, especially for mobile devices, face the challenge of balancing system performance and energy efficiency. To address this issue, a study was conducted on the relationship between energy consumption, execution time, and completion probability of tasks on heterogeneous multi-core architectures. An Accelerated Search algorithm was proposed, using dynamic programming, to find a combination of task schemes that can be completed within a given time with minimum energy consumption and a desired confidence level. A Directed Acyclic Graph (DAG) was used to represent task dependencies and a Minimum-Energy Model was developed to determine the optimal task assignment. The heterogeneous multi-core architectures can adjust voltage levels with Dynamic Voltage and Frequency Scaling (DVFS), resulting in varying execution times and energy consumption. The experimental results showed that this approach outperformed existing algorithms, with a maximum improvement of 24.6%.
412097	412097123	A P2P query algorithm for opportunistic networks utilizing betweenness centrality forwarding	Opportunistic networks, enabled by high-end mobile devices with wireless interfaces, offer promising applications. In these networks, messages are relayed through the mobility of nodes in a store-carry-forward paradigm. However, this relay process faces practical challenges such as delay and delivery rate. To address this, a new P2P Query algorithm called Betweenness Centrality Forwarding PQBCF is proposed. This algorithm uses the Betweenness Centrality BC metric from social networks to measure the active degree of nodes in the network. Nodes with a higher BC are preferred as relays, leading to better query success rate and lower query delay. Comparisons with other algorithms show that PQBCF performs better in terms of query success rate and delay while using fewer resources.
412098	41209858	Crafting urban camouflage	The workshop aims to explore the concept of controlling personal visibility in public spaces by using computer vision tracking systems. Participants from various fields are invited to engage in hands-on activities and discuss strategies for managing personal visibility in relation to design and technology. The focus is on creating speculative design scenarios rather than implementable designs, with the goal of expanding designers' understanding of presence in public spaces beyond the physical to include digital representations. This workshop hopes to stimulate new ideas and perspectives on the intersection of architecture, technology, and personal privacy in public spaces.
412098	41209879	Social gravity: a virtual elastic tether for casual, privacy-preserving pedestrian rendezvous	The authors introduce a virtual "tether" for mobile devices that enables groups to easily and privately meet up. This design uses accelerometers, magnetometers, and GPS to create a dynamic virtual object that represents the closest meeting point, allowing users to coordinate without revealing their positions. The system, called Social Gravity, uses haptic feedback to make social connections tangible in a virtual world. The authors conduct field trials and compare results with simulations, finding that the system is effective in facilitating social coordination even with unreliable positioning. They also provide design guidelines for similar geosocial interaction systems. The Social Gravity system can be implemented using standard hardware and is shown to be successful in enabling efficient and simple group meetups.
412099	41209935	Control and Data Flow Visualization for Parallel Logic Programs on a Multi-window Debugger HyperDEBU	A fine-grained highly parallel program involves multiple threads of execution, making it important to understand the overall execution situation in order to debug it effectively. This is where visualization comes in, and HyperDEBU, a debugger designed for the parallel logic programming language Fleng, helps with this by visually representing the control and data flows of the program based on the user's intention. Breakpoints, which represent the user's intention or perspective, are used by HyperDEBU to visualize the program's execution. This enables efficient debugging through the use of visual examination and manipulation tools.
412099	41209930	HyperDEBU: A Multiwindow Debugger for Parallel Logic Programs	The paper introduces HyperDEBU, a multiwindow debugger designed for highly parallel programs written in the language Fleng. This debugger offers a variety of views and levels of analysis to assist users in efficiently locating bugs in their programs. Its support for committed choice languages makes it particularly useful for fine-grained parallel programs. 
412100	41210069	Teaching Petri Nets Using P3	The paper introduces P3, a Petri net software tool designed for teaching the Architecture and organization of computers (AOC) course. P3 offers a graphical modeling interface and interactive simulation with conflict resolution, as well as four analysis tools including two new ones developed specifically for learning. P3 also has the ability to share models with other Petri net tools through XML/XSLT support. The paper discusses the AOC course and compares the outcomes of students who used P3 with those who did not. It also includes feedback from teachers and students on P3's features. Overall, P3 is a useful tool for teaching and learning about Petri nets in the AOC course.
412100	412100131	Interoperable Petri net models via ontology	The paper introduces a Petri net infrastructure designed for sharing Petri nets on the Semantic Web. Unlike previous solutions that only offer model interchange mechanisms between Petri net tools, this infrastructure includes a Petri net ontology that is closely related to the Petri Net Markup Language (PNML). The ontology was developed using UML and Protege, and is represented using RDF and OWL. The team also created a software tool, P3, which can convert Petri net ontology compliant models to various Petri net tool formats using XSLT. To demonstrate the use of the ontology, the team developed a simple educational web application that utilizes RDF-annotated ontology-based Petri net learning materials. 
412101	41210178	The CMUnited-98 champion small-robot team.	The article discusses the research opportunities presented by robotic soccer and highlights the achievements of the CMUnited-98 small robot team. The team utilizes a multiagent system with global perception and distributed cognition and action. The hardware design of the physical robots includes differential drive, a robust mechanical structure, and a kicking device. The team has also developed a new motion algorithm that allows for successful collision-free motion in the dynamic soccer environment. At the strategic level, the team utilizes role-based behaviors and a collaboration algorithm to improve team performance. The CMUnited-98 team performed well in the RoboCup-98 games, scoring 25 goals and only suffering 6 in 5 games.
412101	41210172	CMUnited-98: a team of robotic soccer agents	This paper discusses the research contributions and achievements of the CMUnited-98 small robot team in the field of robotic soccer entertainment. The team consists of multi-agent systems with global perception and distributed cognition and action. The hardware design of the robots, including differential drive, robust structure, and a kicking device, is described. The team's global vision processing algorithm and reactive motion control algorithm are also briefly introduced. The robots demonstrate successful collision-free motion and strategic positioning through role-based behaviors and a collaboration algorithm. The results of the team's performance in the 1998 RoboCup games are also summarized, including a total of 25 goals scored and 6 goals suffered in 5 games.
412102	41210290	A Filtering Approach to Stochastic Variational Inference.	SVI is a method that uses stochastic optimization to efficiently compute Bayesian models with large datasets. It can be viewed as an approximate parallel coordinate ascent, balancing bias and variance to approach the optimal solution from batch variational Bayes. A new model is proposed to automate this process by inferring the next optimum from a sequence of noisy observations. This allows the variational parameters to be updated using Bayes rule, rather than a pre-defined schedule. When the model is a Kalman filter, it recovers the original SVI algorithm and can also incorporate additional assumptions such as heavy-tailed noise. In experiments, this method outperforms traditional SVI and other adaptive algorithms in two different domains.
412102	41210269	Black Box Variational Inference.	Variational inference is a popular method for approximating posteriors in complex latent variable models. However, it often requires extensive model-specific analysis, which can hinder the development of new models. In this paper, the authors propose a "black box" variational inference algorithm that can be applied to various models without the need for additional derivations. The algorithm uses stochastic optimization and Monte Carlo sampling to compute a noisy gradient, and various techniques are introduced to reduce its variance. Compared to black box sampling methods, the proposed method achieves better predictive likelihoods in a shorter time. Additionally, the authors demonstrate the effectiveness of their approach in exploring a range of models for longitudinal healthcare data.
412103	412103108	Improving accuracy in path delay fault coverage estimation	A new method has been developed to estimate path delay fault coverage by counting newly sensitized path faults in a simulated vector pair. However, this estimate may be pessimistic due to shared paths. To improve accuracy, a range of approximate methods have been proposed, with higher accuracy requiring more CPU time. These methods use flags to indicate if a path segment has already been included in a previously detected fault. A fault is only counted as newly detected if it includes an unflagged segment. This method has a small overhead when there are few fan-in and fan-out branches per gate, and accuracy improves as the length of path segments increases. Results show that this approach provides accurate estimates when using short path segments.
412103	41210317	SIGMA: A simulator for segment delay faults	Lucent Technologies has proposed an efficient technique for simulating combinational circuits using the segment delay fault model. This involves tracing activated segments through a depth-first search and using a segment numbering scheme to determine the number of faults to be simulated. A labeling technique is also used to generate edge labels, allowing for easy access to previously detected segment faults without storing large fault lists. Experimental results have shown the advantages of this model, including manageable numbers of faults for small segment lengths and the potential for increased delay defect coverage through testing of previously overlooked segments.
412104	41210451	Privacy preserving decision tree learning over multiple parties	Data mining over multiple data sources poses challenges due to legal and competition concerns. Cryptographic methods have been proposed as a solution, but their complexity makes them impractical for large numbers of sources. This paper proposes an efficient algorithm for building a decision tree over an arbitrary number of distributed sources while ensuring privacy. The algorithm is based on the ID3 algorithm and allows data sources to run data mining algorithms over the union of their data without revealing any extra information to other sources. This addresses the classification problem and enables data mining in scenarios where multiple sources are involved.
412104	412104239	PRISM: indexing multi-dimensional data in P2P networks using reference vectors	P2P systems research has become increasingly important due to the popularity of file sharing applications. However, these systems often lack efficient search techniques. Existing systems only offer keyword search through a centralized index or query flooding. A new scheme based on reference vectors is proposed to improve multi-dimensional data sharing in P2P systems. This scheme supports a wider range of query operations, such as k-NN queries and content-based similarity search, compared to current systems. The idea is to store multiple replicas of an object's index at different peers based on the distances between the object's features and reference vectors. This allows the system to return accurate results by accessing a small fraction of the participating peers.
412105	4121052	Robust satisfaction of temporal logic over real-valued signals	The article discusses the use of temporal logic formulae to specify constraints on the behavior of continuous and hybrid dynamical systems with uncertain parameters. It introduces different measures of robustness that indicate how close a system's trajectory is to satisfying or violating a property. A method is presented for computing these measures and their sensitivity to system and formula parameters. This approach can aid in verifying nonlinear and hybrid systems against temporal properties through simulation. It can also be applied to identify parameter subsets that guarantee a desired level of robustness in satisfying a formula.
412105	4121057	Symbolic Controller Synthesis for Discrete and Timed Systems	This paper discusses algorithms for creating controllers for discrete and real-time systems. These controllers are synthesized by finding winning strategies for games defined by automata or timed-automata. Since the state-space of these systems grows exponentially with the number of components, traditional methods involve searching through all possible states, which is time-consuming. Symbolic methods, on the other hand, use formulae to represent sets of states, allowing for a more efficient search. While in some cases these methods may be as slow as traditional ones, they are effective for many practical problems. This paper expands the use of symbolic methods to include synthesis and real-time systems.
412106	41210623	Efficient mining of recurrent rules from a sequence database	This study introduces a new problem of identifying significant recurrent rules from a sequence database. Recurrent rules follow the pattern of "whenever a series of events occurs, eventually another series of events occurs" and are useful in capturing behaviors in various domains, such as software specifications. These rules are an improvement on existing sequential and episode rules as they consider repeated events within and across sequences, and do not have a "window" limitation. The rules are formalized in linear temporal logic and a new concept of rule redundancy is introduced to efficiently mine a representative set of rules. Performance studies and a case study have demonstrated the scalability and usefulness of this approach.
412106	41210627	Mining patterns and rules for software specification discovery	Software specifications play a crucial role in the development of software. However, they are often lacking, incomplete, and outdated in the industry, causing various software engineering problems. These issues can lead to high costs and potential bugs and compatibility issues. To address this, researchers have developed data mining techniques to extract specifications from software engineering data, such as program execution traces. This involves viewing a program trace as a sequence of events and using pattern and rule mining techniques to analyze a set of traces. Studies have shown that this approach is scalable and can successfully recover specifications from real industrial applications. 
412107	41210743	A stepwise optimization algorithm of clustered streaming media servers	Optimizing Clustered Streaming Media Servers (CSMS) is crucial for achieving cost-effective and high-performing systems. This involves balancing performance, quality of service (QoS), and costs. To address this, a stepwise optimization algorithm is proposed, which models the problem as a directed acyclic graph and uses a divide and conquer approach to reduce complexity and speed up the process. A simulation system is also developed based on a theoretical performance model and practical parameters to accurately generate information. A case study is presented to demonstrate the effectiveness of the algorithm and guide the design of practical CSMS systems. This approach has a significant impact on the practicality and efficiency of CSMS.
412107	4121072	QoS Enhancement for PDES Grid Based on Time Series Prediction	The combination of parallel and distributed discrete event simulation (PDES) with Grid technology is a new trend in simulation. However, ensuring Quality of Service (QoS) in this process is challenging. To enhance QoS, a Grid-based framework is proposed that can predict PDES performance based on its unique features like predictable and periodic inputs. Using this framework, a prediction algorithm is developed using time series theory for large scale Grid simulations. Experiments conducted in GridSim demonstrate the effectiveness of these methods in improving QoS. This approach has the potential to revolutionize simulation by providing accurate predictions and improving QoS levels.
412108	41210832	Computing with semi-algebraic sets represented by triangular decomposition	This article builds upon previous work on triangular decompositions of semi-algebraic systems and introduces new theoretical results that improve these decomposition algorithms. The authors also introduce a technique called "relaxation" which simplifies the decomposition process and reduces the number of redundant components. Additionally, they propose procedures for basic set-theoretical operations on semi-algebraic sets represented by triangular decomposition. Experimentation shows that these techniques are effective. The authors also present new results on the theory of border polynomials of parametric semi-algebraic systems, including a geometric characterization of its "true boundary." These contributions aim to optimize the computation of triangular decompositions and make them more practical for real-world applications.
412108	41210836	Computing with semi-algebraic sets: Relaxation techniques and effective boundaries	This paper focuses on parametric polynomial systems and their applications in real root classification and triangular decomposition of semi-algebraic systems. The authors present new findings in the theory of border polynomials, specifically a definition for the "true boundary" and propose a technique called relaxation to optimize the decomposition algorithms. This technique simplifies the process and reduces the number of components in the output. The paper builds upon previous works by the authors and provides further insights into the use of parametric polynomial systems in solving semi-algebraic problems.
412109	41210936	Fast Regular Expression Matching Using Small TCAM	This paper introduces a hardware-based approach for regular expression (RE) matching using ternary content addressable memory (TCAM). This is a popular method for packet classification in modern networking devices. The proposed approach uses three techniques - transition sharing, table consolidation, and variable striding - to improve TCAM space utilization and RE matching speed. The effectiveness of these techniques was tested on eight real-world RE sets and the results showed that even small TCAMs can store large deterministic finite automata (DFAs) and achieve high RE matching throughput. For example, a 0.59-Mb TCAM chip can store eight DFAs with 25,000 states each, while a single 2.36-Mb TCAM chip can process multiple characters per transition and achieve a potential RE matching throughput of 10-19 Gb/s for each DFA. 
412109	4121099	Fast regular expression matching using small TCAMs for network intrusion detection and prevention systems	In this paper, the authors propose a new hardware-based approach for regular expression (RE) matching using off-the-shelf Ternary Content Addressable Memories (TCAMs). They introduce three techniques, namely transition sharing, table consolidation, and variable striding, to reduce TCAM space and improve RE matching speed. The proposed approach was tested on 8 real-world RE sets and showed potential for high RE matching throughput using small TCAMs. For example, a 0.59Mb TCAM chip was able to store a DFA with 25,000 states using only 12-42 TCAM bits per state. By using a different TCAM encoding scheme, the authors achieved RE matching throughputs of 10-19 Gbps for each DFA with just a single 2.36 Mb TCAM chip.
412110	4121100	Preventing and unifying threats in cyberphysical systems	The topic of CPS security has been widely researched, but only a few studies focus on the general aspects of threats. Most of the existing work is centered on attack detection, which helps to identify threats but does not offer solutions to prevent them. Current system approaches involve detecting and correcting inputs and outputs, but with a large number of inputs and distributed systems, it is important to identify and stop threats at their point of entry. This can be achieved by utilizing a Security Reference Architecture, which has been used to trace attacks but not to stop them. The authors propose using this approach to identify and prevent threats in CPSs, as many of them have similar effects and can be stopped in similar ways. Using threat patterns is seen as a more effective way of describing attacks compared to other models.
412110	41211022	Security patterns and secure systems design	Analysis and design patterns are established methods for building high-quality object-oriented software. These patterns use experience and best practices to create basic models that can be applied to new designs. Security patterns combine the extensive knowledge of security with the structure of patterns to provide guidelines for secure system design and evaluation. Companies such as IBM, Sun, and Microsoft are adopting these patterns. Examples of security patterns include Authentication, Authorization, and Role-based Access Control. They are applied through a secure system development method that considers possible attacks, defines user rights, and implements security policies. These patterns are illustrated using UML models and can be found in the book "Security Patterns" (Wiley 2006). 
412111	41211141	Open Code Coverage Framework: A Framework For Consistent, Flexible And Complete Measurement Of Test Coverage Supporting Multiple Programming Languages	Test coverage is a crucial aspect in determining the effectiveness of software testing. However, current measurement tools for test coverage have various issues, including high costs, inconsistency, and inflexibility. To address these problems, a new measurement framework called the Open Code Coverage Framework (OCCF) has been proposed. This framework supports multiple programming languages and allows for independent addition of language and test coverage criteria support. Additionally, OCCF offers two methods for changing measurement range and elements, making it more flexible. In a sample tool implementation for C, Java, and Python, OCCF was able to measure four test coverage criteria and was also confirmed to support C#, Ruby, Java Script, and Lua. In a comparison experiment with non-framework-based tools, OCCF was able to reduce the required lines of code and time for implementing new test coverage criteria significantly.
412111	41211114	POGen: a test code generator based on template variable coverage in gray-box integration testing for web applications	Web applications are complex and run on various browsers and platforms, making it difficult to conduct proper integration testing. This paper introduces a new test coverage method called template variable coverage, which aims to improve the detection of faults in the connections between subsystems and specific environments. The authors also propose a tool called POGen, which generates skeleton test code with accessor methods to support the template variable coverage criterion. Experiments show that this coverage method is highly effective in detecting faults and that POGen can reduce testing costs. This highlights the importance of establishing efficient integration testing methods and using appropriate tools to ensure the quality of web applications.
412112	41211217	Validating Security Design Patterns Application Using Model Testing.	Software developers often lack specialized knowledge in security, which can result in improperly applied security patterns. These patterns, which contain reusable security knowledge, may not effectively mitigate threats and vulnerabilities if applied incorrectly. To address this issue, a new method is proposed for validating the application of security patterns. This method includes extended security patterns, such as requirement- and design-level patterns, as well as a model testing process. Early in the development process, developers specify the threats and vulnerabilities in the system, and the method then assesses whether the security patterns have been properly applied and if the vulnerabilities have been resolved.
412112	41211252	Security patterns: a method for constructing secure and efficient inter-company coordination systems	The increasing use of the Internet, intranets, and other large networks has led to a rise in interest for mobile agents as a way to build distributed systems. However, ensuring security while also maintaining efficient implementation has been a challenge. In this paper, the authors propose a method that addresses this issue and demonstrate its application in a real-world system. The approach involves using patterns, which are templates for system behavior, to specify different options that meet the security requirements. These patterns include agent migrations, communication between applications, and security procedures. By modeling the performance data associated with each pattern, developers can select the most efficient implementation for their system. The authors show the effectiveness of their approach through the Environmentally Conscious Product (ECP) design support system, which requires coordination among multiple companies.
412113	41211313	The impacts of personal characteristic on educational effectiveness in controlled-project based learning on software intensive systems development	This study examines the impact of team composition and learning process on educational effectiveness in software-intensive business systems courses. The lack of an established method to determine optimal team composition prompted the use of the Five Factors and Stress theory and modified grounded theory approach to measure personal characteristics and identify the learning process of team members. The study found that having team members with different personal characteristics leads to increased knowledge and skills acquisition. Moreover, teams that focus on a smaller number of learning process topics also demonstrate higher educational effectiveness. These findings have implications for improving educational effectiveness in similar practical courses.
412113	41211372	Using an Automatic Collection Method to Identify Patterns during Design Activity	Design is a crucial aspect of software development, but it is subjective and varies based on the designer's knowledge and skills. The design process can be modeled as a sequence of "Create", "Delete", and "Modify" actions on a UML class diagram. To better understand design strategies, an automatic approach was proposed to collect information about the process. The strategies considered were top-down, bottom-up, breadth-first, depth-first, and opportunistic. By analyzing frequent patterns and the position of actions in the sequence, two types of relationships were identified: micro-patterns and macro-patterns. Case studies were conducted to evaluate the approach, revealing that there is no universal design strategy, but rather a combination of strategies used by designers. 
412114	41211480	Towards a semantic model for Java wildcards	Java wildcards are a way to express more types in Java and expand the range of programs that can be typed. While previous research has focused on syntactic models and proofs for type systems related to Java wildcards, there has been little study on the semantics of wildcards. This paper presents a semantic model for Java wildcards based on semantic subtyping, which interprets types as sets of possible values. The model is defined in terms of runtime types, rather than the structure of runtime values, to reflect the nominal type system of Java. The model also accounts for variance introduced by wildcards and shows the soundness of syntactic subtyping. However, completeness is not guaranteed in the general case, but a restricted type language is identified for which syntactic subtyping is both sound and complete. 
412114	41211436	Reversible sessions with flexible choices	The authors propose a new calculus for concurrent reversible multiparty sessions that includes a flexible choice operator inspired by the concept of connecting action. This operator allows for different sets of participants in each branch, making it suitable for describing communication protocols with optional participants. The calculus also supports a compact representation of process history and types, facilitating the definition of rollback and implementing a strategy for backward computation. A session type system is also presented, demonstrating that the calculus enforces session fidelity, forward progress, and backward progress. 
412115	41211575	Lazy Bayesian Rules: A Lazy Semi-Naive Bayesian Learning Technique Competitive to Boosting Decision Trees	LBR is a learning technique that aims to improve the performance of naive Bayesian classifiers by addressing the issue of attribute interdependence. It does this by creating a conjunctive rule and using a subset of training examples to induce a local naive Bayesian classifier for classifying a test example. This approach has been found to be effective in reducing the bias and variance of the naive Bayesian classifier, resulting in improved performance. LBR is a useful tool for dealing with the limitations of naive Bayesian classification and can lead to better classification results. 
412115	41211567	Selective Augmented Bayesian Network Classifiers Based on Rough Set Theory	The naive Bayes classifier is a popular tool in interactive applications due to its efficiency and theoretical base. However, its assumption of attribute independence can lead to lower accuracy. To improve this, various techniques have been developed to relax this assumption. One such technique is TAN, which incorporates limited forms of inter-dependence between attributes. Another approach is using rough sets theory, which allows for inexact or partial dependencies within a dataset. In this paper, a variant of TAN using rough sets theory is proposed and compared to other TAN-based classifiers. The new approach shows improved accuracy and requires less computation than previous methods. It is a selective restricted trees Bayesian classifier.
412116	41211619	Contractible subgraphs in 3-connected graphs	The article discusses a conjecture by McCuaig and Ota regarding the existence of a contractible subgraph in 3-connected graphs. A subgraph H of a 3-connected graph G is considered contractible if H is connected and G with H removed is still 2-connected. The conjecture states that for any value of k, there exists a function f(k) such that any 3-connected graph with at least f(k) vertices will have a contractible subgraph with k vertices. The article presents a proof of this conjecture for k ⩽4 and explores its implications in various types of graphs, such as maximal planar graphs, Halin graphs, and AT-free graphs.
412116	41211622	On the number of contractible triples in 3-connected graphs	McCuaig and Ota's research demonstrated that any 3-connected graph with at least 9 vertices contains a contractible triple, meaning a connected subgraph of three vertices where the remaining graph is still 2-connected. This study goes further to show that such graphs have at least one tenth of the total number of vertices as contractible triples. Additionally, if the graph is cubic, meaning each vertex has exactly three edges, then there will be at least one third of the total number of vertices as contractible triples, which is the maximum possible.
412117	41211755	A context management infrastructure with language integration support	Context-management systems have been developed to support context-aware applications. These systems usually have APIs and query languages for analyzing context. However, they do not have effective support for reacting to context changes within the constraints of the framework. In this paper, a new context-management system is presented that combines context reasoning with context-dependent behavior using dynamic adaptation techniques like aspect- and context-oriented programming. The system offers various levels of integration with programming language extensions and allows for dynamic aggregation of local and distributed context sources. A query library for the JCop language has been implemented as a first step, and an example application is demonstrated to showcase its capabilities.
412117	41211736	Efficient layer activation for switching context-dependent behavior	The paper introduces the concept of Context-oriented Programming, which aims to provide programming language extensions that allow programs to dynamically adapt to their execution context. This is achieved by dividing programs into layers that can be activated or deactivated based on the context, and can be composed in any order. The implementation of this model is efficient, with a dynamic representation of layers that shows competitive performance in both activation/deactivation and overall program execution. The paper also provides an example of how this approach can be used in place of aspect-oriented programming. Overall, Context-oriented Programming offers a flexible and efficient way to modify program behavior at runtime.
412118	41211841	Another Look at Complementation Properties	This paper discusses a variety of attacks based on the complementation property of DES. The authors use symmetry relations in the key schedule and rounds to create distinguishers for any number of rounds, leading to a generalization of the complementation property. They also explore the use of fixed points in these relations to develop new types of attacks. The paper presents a self-similarity property in the SHA-3 candidate Lesamnta, which reveals unexpected weaknesses in its compression function. The authors demonstrate how this property can be used to find collisions much faster than generic attacks. Additionally, a related-key differential attack on round-reduced versions of the XTEA block-cipher is proposed, exploiting weaknesses in the key-schedule to recover the secret key more efficiently. The authors also identify a class of weak keys that can be used to attack a large number of rounds in the cipher.
412118	41211839	Synthesis of Fault Attacks on Cryptographic Implementations	Fault attacks refer to a type of attack where an adversary physically manipulates a cryptographic device, such as a smartcard, in order to extract secret information. These attacks have been a focus of research since the first successful attack on modular exponentiation. With an increasing focus on concrete implementations, finding new fault attacks has become more difficult. To address this, a new approach has been proposed that uses mathematical properties, known as fault conditions, to systematically analyze the security of cryptographic implementations. By identifying these fault conditions, it is possible to automatically discover new attacks. This approach has been successfully applied to find multiple fault attacks on RSA and ECDSA, some of which are new and of independent interest. 
412119	41211973	A Logic Programming Approach to Scientific Workflow Provenance Querying	Scientific workflows are essential for facilitating and expediting scientific discoveries. These workflows integrate and organize diverse data and services for conducting virtual experiments. To ensure the reproducibility and validation of scientific results, the management and querying of provenance information has become crucial. This paper presents a logic programming approach, FLOQ, for querying and managing scientific workflow provenance. The authors identify important characteristics for a provenance query language and demonstrate how FLOQ meets these requirements. They also show how their previous model, virtual data schema, can be easily translated into FLOQ. The paper includes examples to demonstrate the effectiveness and versatility of FLOQ, including solving the provenance challenge queries. 
412119	4121197	Scalable Provenance Storage and Querying Using Pig Latin for Big Data Workflows	Provenance is information about the origin and history of a data product, which is crucial for assessing its quality and reliability. In the context of data-centric scientific workflows, much research has been done on storing and querying provenance. However, with the increasing volume, velocity, and variety of big data, a new generation of workflows called big data workflows are being developed. This has led to the need for a scalable storage and querying system for big data workflows. This paper proposes a system that leverages Pig Latin and OPQL to translate provenance queries and support the W3C PROV-DM standard. The system is evaluated on provenance datasets and has potential for efficient storage and querying of big data workflows.
412120	41212059	CiNCT: Compression and retrieval for massive vehicular trajectories via relative movement labeling.	This paper introduces a compressed data structure for representing moving object trajectories in a road network. Unlike existing methods, this structure supports pattern matching and decompression from any position while maintaining high compression rates. It is based on the FM-index, a data structure known for its fast and compact pattern matching abilities. To further improve compression, the authors incorporate the sparsity of road networks and introduce the concepts of relative movement labeling and PseudoRank. Experimental results show that their method outperforms existing trajectory compression methods and FM-index variants in terms of data size and query processing time.
412120	41212048	Efficient error-tolerant query autocompletion	This paper discusses the problem of query autocompletion with error tolerance, where users' input may contain errors. Previous approaches use a trie data structure to index data strings and maintain prefixes within a certain edit distance from the query. However, this method becomes inefficient for short queries and large alphabets. The paper proposes a new algorithm, IncNGTrie, which significantly improves the speed of autocompletion by only maintaining a small set of active nodes. The paper also addresses the issue of efficient duplicate removal and suggests optimization techniques to reduce the index size. Extensive experiments on real datasets demonstrate the effectiveness of the proposed method.
412121	41212136	Managing Process Model Complexity Via Abstract Syntax Modifications.	The use of Business Process Management (BPM) technology has become increasingly popular, requiring stakeholders to understand and agree upon the process models used in BPM systems. However, users often struggle with the complexity of these models. The main challenge is to enhance the understanding of process models, and there is a significant amount of literature devoted to this topic.
412121	41212138	Business process variability modeling : A survey	Organizations often have multiple versions of a business process, but traditional process modeling languages do not explicitly support representing these variants. This has led to a significant amount of research in the past decade, resulting in various approaches to business process variability modeling. These approaches add specialized features to conventional process modeling languages to capture customizable process models. A customizable process model can represent a family of process variants, with each variant being derived by adding or deleting fragments based on customization or domain options. This survey provides a systematic overview of these modeling approaches, comparing them and identifying common and unique features. It also highlights the abundance of customizable process-modeling languages, but a lack of tool support and empirical evaluations. 
412122	41212227	Achieving side-channel protection with dynamic logic reconfiguration on modern FPGAs	Reconfigurability is a key feature of modern FPGA devices, allowing for different hardware circuits to be loaded on demand. This makes it difficult for attackers to predict what will happen at a specific location in the FPGA. The authors present a hardware implementation of the PRESENT cipher with built-in side-channel countermeasures using dynamic logic reconfiguration. Their design utilizes Configurable Look-Up Tables (CFGLUT) in Xilinx FPGAs to quickly change the hardware internals of the cipher, improving resistance against side-channel attacks. Experimental results on a Spartan-6 platform show that even with 10 million recorded power traces, the first-order leakage cannot be detected with state-of-the-art methods.
412122	41212248	ParTI: Towards Combined Hardware Countermeasures against Side-Channeland Fault-Injection Attacks	Side-channel analysis and fault-injection attacks pose significant threats to cryptographic implementations, making it crucial to implement appropriate countermeasures before deployment. However, these countermeasures differ in nature - side-channel attacks require techniques to hide key-dependent information, while fault-injection attacks can be countered with redundancy in computation for error detection. Implementing a single countermeasure already incurs costs in performance and area, and combining multiple countermeasures is expensive and can have unwanted side effects. This study introduces a countermeasure that combines a provably-secure masking scheme with an error detection approach for cryptographic hardware implementations, using the lightweight LED cipher as a case study. The implementation achieves first-order resistance against side-channel attacks and superior fault detection compared to simple duplication, with an added area demand of 12%.
412123	4121235	ASSIST: adaptive social support for information space traversal	The paper discusses the challenge of finding relevant information in a vast hyperspace, which has been extensively studied. The use of social systems, such as Web 2.0 technologies, for retrieval tasks has become increasingly popular. These systems collect and use community knowledge to benefit users. The paper proposes a form of retrieval that combines multiple sources of social wisdom, specifically social search and social navigation. A personalized engine called ASSIST is introduced, which integrates social support mechanisms for information repositories. The goal is to improve retrieval in the hyperspace and an empirical study of the technology's effectiveness is presented.
412123	41212327	Social Information Access for the Rest of Us: An Exploration of Social YouTube	Information Retrieval systems aim to provide relevant information to users based on their current goals and needs. With the advancements in technology, it has become possible to understand user preferences and access patterns, allowing for the customization of services to meet their needs. The ASSIST platform integrates social support into existing information access systems and has been successfully deployed in a goal-driven environment. In this study, the use of ASSIST in the less-focused domain of YouTube is explored. The study analyzes YouTube access patterns and demonstrates the impact of the ASSIST architecture on user behavior in this domain. 
412124	41212498	Set similarity join on massive probabilistic data using MapReduce	This paper discusses the challenge of efficiently performing set similarity join on large probabilistic datasets using the MapReduce paradigm. The authors propose two approaches, Hadoop Join by Map Side Pruning and Hadoop Join by Reduce Side Pruning, which use different methods to filter out candidate pairs and reduce the comparison cost. They also present a hybrid solution that combines both approaches. Experiments on Hadoop-0.20.2 show that these approaches outperform the naive method of Block Nested Loop Join and have good scalability. This is the first work to address this problem using MapReduce and provides a new method for processing massive probabilistic data.
412124	41212478	Efficient query processing for XML keyword queries based on the IDList index	Keyword search in XML data has been a popular research topic in recent years, with focus on efficiently answering queries based on specific semantics. Existing methods have been hindered by the common-ancestor-repetition problem. In this paper, the authors introduce a new inverted list called IDList, which contains ordered nodes that directly or indirectly contain a given keyword. They also show that finding query results based on certain semantics can be simplified by using ordered set intersection, a well-optimized technique used in information retrieval and database systems. The authors propose various algorithms that utilize set intersection and hash search to improve efficiency. Extensive experiments on large datasets show that their methods outperform existing ones significantly.
412125	41212538	An abstract formalization of correct schemas for program synthesis	This paper discusses the importance of incorporating both structured program design principles and domain knowledge into program schemas for hierarchical program synthesis. While most current approaches only focus on the syntactic aspect of schemas, this paper proposes a semantic approach that includes a formalized template and a first-order axiomatization of the problem domain. This framework allows for a formal definition of correctness and enables the use of correct schemas to guide program synthesis. By incorporating both syntactic and semantic elements, this approach aims to improve the effectiveness and accuracy of program synthesis.
412125	41212523	Correct-schema-guided synthesis of steadfast programs	Program schemas play a crucial role in (semi-)automated software development as they capture both structured program design principles and domain knowledge. While most researchers represent schemas using purely syntactic methods, we propose a semantic approach by formalizing schemas as open logical theories with open logic programs. By using a concept called steadfastness, we can define and reason about the correctness of schemas and use them to synthesize correct programs. This allows for a more thorough and comprehensive representation of the knowledge captured by schemas, making them indispensable for hierarchical program synthesis.
412126	41212690	Provably secure authenticated key agreement scheme for distributed mobile cloud computing services.	Mobile cloud computing has become increasingly popular, but this also brings concerns about security in distributed environments. In 2015, Tsai and Lo proposed a privacy-aware authentication scheme for these services, but it was found to be vulnerable to server impersonation and leaks of sensitive information. To address these issues, a new authentication scheme with stronger security features was proposed. It was rigorously analyzed and found to be secure against various attacks. The scheme also has lower computational costs for mobile users and was evaluated using a network simulator. This new scheme is considered more suitable for practical applications compared to the previous one proposed by Tsai and Lo.
412126	41212685	An Improved Lightweight RFID Authentication Protocol for Internet of Things.	 RFID technology is widely used for automatic identification and has become a core technology for the Internet of Things (IoT). To ensure security and privacy, RFID authentication is essential. However, many existing RFID authentication protocols have security weaknesses and inefficient performance. In this paper, the authors review Kaur et al.'s protocol and identify its security deficiencies. They propose a new lightweight anonymous authentication protocol for RFID systems using elliptic curve cryptography, which has been shown to achieve mutual authentication, confidentiality, anonymity, and resistance to various attacks. Performance evaluation shows that the proposed protocol reduces computation and communication costs significantly compared to previous protocols.
412127	41212745	Exploring the Design Space of AAC Awareness Displays.	Augmentative and alternative communication (AAC) devices are crucial for individuals with speech disabilities. However, one challenge with these devices is their inability to convey nonverbal communication cues that usually complement or substitute for verbal speech. This paper explores the design of awareness displays that can supplement AAC devices, considering their impact on how the users are perceived by others. The design process involved creating prototypes and receiving feedback from people with ALS, their caregivers, and communication partners. The study found a tension between abstractness and clarity in the designs and also raised concerns about how these displays may further label individuals using AAC as "different." Overall, this research provides insights into the design of AAC awareness displays to enhance communication for individuals with disabilities.
412127	41212777	AACrobat: Using Mobile Devices to Lower Communication Barriers and Provide Autonomy with Gaze-Based AAC.	Gaze-based alternative and augmentative communication (AAC) devices allow people with neuromuscular diseases to communicate through eye movements. However, these devices have slow input, leading to communication breakdowns and decreased autonomy, quality, and engagement for users and their conversation partners. Previous attempts to improve these devices have focused on increasing input speed, but with limited success. This work approaches the issue from a different angle, viewing AAC devices as groupware and designing interactions that facilitate better conversations for all involved. Through qualitative research and user preferences, design guidelines were identified and incorporated into AACrobat, a system that includes a mobile companion app. Early feedback from three case studies with users with ALS suggests that AACrobat improves communication and user satisfaction.
412128	41212862	Vehicle Orientation Analysis Using Eigen Color, Edge Map, And Normalized Cut Clustering	The paper introduces a new method for estimating the orientation of vehicles in still images. This is achieved through a clustering framework that utilizes eigen color and edge mapping. A novel color transform model is used to segment vehicles from their background, which is invariant to changes in contrast, background, and lighting. However, this model alone cannot accurately extract the shape of the vehicle, so the distributions of edges and colors are combined to form a high-dimensional feature space. This is then reduced using normalized cut spectral clustering, which minimizes dissimilarity between groups. The orientation can be determined by analyzing the eigenvectors. Unlike traditional methods, this approach only requires one still image and has shown promising results in experiments.
412128	41212819	Vehicle Retrieval Using Eigen Color and Multiple Instance Learning	The paper presents a new method for retrieving images of vehicles from databases using eigen color and multiple instance learning. Traditional methods face challenges in accurately describing vehicles due to variations in color and shape under different conditions. The proposed approach uses a color transform model to extract regions of interest from databases and the MIL method to learn specific visual properties from query images. An eigen color transform is introduced to improve accuracy and adaptability, and experimental results show the effectiveness of the approach. This method does not involve time-consuming optimization and can quickly obtain desired visual concepts for different user requests. 
412129	41212948	Ten Blue Links on Mars.	In this paper, the authors discuss the challenge of providing a high-quality search experience on Mars, where the speed-of-light propagation delays result in significant latency. They explore the feasibility of overcoming this challenge and providing a tolerable user experience by considering the tradeoff between waiting for responses from Earth and pre-fetching or caching data on Mars. The authors present two case studies that use publicly available data to demonstrate the effectiveness of baseline techniques. They argue that this research problem is not only relevant for potential Mars colonization, but also for Earth-based scenarios with similar constraints, such as searching from rural villages in India. 
412129	41212921	Evaluating Streams of Evolving News Events	People tend to track news events based on their level of interest and available time. For major events they are highly interested in, they may check for updates multiple times an hour and stay updated on all aspects. However, for minor events that are less interesting, they may only check once or twice a day for a few minutes to stay informed. In order to improve user performance and avoid information overload, systems that provide updates on evolving events should appropriately filter the updates. However, predicting user performance on these systems can be challenging. The standard evaluation methodology used for Web search and other retrieval tasks may not be suitable for this context. In this paper, a simple model is developed to simulate user behavior of checking the system for updates. This model is tested using data from the TREC 2013 Temporal Summarization Track, and it is found that the primary measure used in this track closely corresponds to a user who checks the system once a day for an average of one minute. However, users who check more frequently and for longer periods of time may have a different perception of the system's performance. Therefore, it is recommended that future experiments clearly state assumptions about user behavior and use effectiveness measures that reflect these assumptions.
412130	41213052	Designing a Physical Aid to Support Active Reading on Tablets	Tablet computers and eReaders are becoming more popular for reading, but it is believed that print documents are better for in-depth understanding. Previous attempts to improve digital reading have focused on mimicking physical features or adding digital tools, but this paper proposes combining both approaches. A study was conducted to identify the needs of users while actively reading on tablets, resulting in the design of a smart bookmark to support active reading. The paper also outlines an interaction space for the bookmark and presents a user study to evaluate its effectiveness.
412130	4121301	MagnID: Tracking Multiple Magnetic Tokens	This paper discusses the limitations of tangible systems due to complex and expensive sensing infrastructures, making them less accessible for widespread use. To address this issue, the paper proposes the use of simple active magnetic tokens, which can be detected and located using a standard triaxial magnetometer found in most mobile devices. The system is accurate, with a recognition rate of 99% for token positions from six pre-calibrated locations. The paper also describes the hardware and software components of the system, along with five demonstration applications showcasing its capabilities. 
412131	41213160	Novel Modalities For Bimanual Scrolling On Tablet Devices	The paper discusses two studies that examine the use of new methods for scrolling on tablet devices using both hands. The techniques involve a combination of physical dial, touch, and pressure input to control the speed and direction of scrolling. The effectiveness of these techniques is compared to traditional one-handed techniques in a targeted task. The results show that participants were able to select targets more quickly and with less effort when using the bimanual techniques. This suggests that bimanual scrolling may be a more efficient and user-friendly option for tablet devices.
412131	412131183	Comparing evaluation methods for encumbrance and walking on interaction with touchscreen mobile devices	This paper compares two methods for evaluating the effects of carrying objects and using mobile devices while walking. The methods involve walking on a treadmill and walking around a predefined route while following a pacesetter. The results of a target acquisition experiment showed that accuracy significantly decreased when carrying a bag or box while walking at the preferred speed. The paper also discusses the advantages and limitations of each method and suggests that treadmill walking may not be the best approach for studying walking speed in future mobile studies.
412132	41213241	Two-way source coding with a common helper	The two-way rate-distortion problem involves a helper sending a limited-rate message to both users using side information. This results in a Markov form of Helper-User 1-User 2. The achievable rates and distortions for this setup are characterized, and it is found that a binning scheme similar to Wyner-Ziv can be used to achieve the optimal rate. The side information at the decoder is the "further" user, User 2. The proof of this result uses a new technique involving undirected graphs to verify the Markov relations.
412132	412132107	Problems we can solve with a helper	This work focuses on studying source coding problems with a helper who provides limited side information to the parties involved. The first problem studied is the Wyner-Ziv problem, where the helper sends rate-limited side information to both the encoder and decoder in addition to the existing memoryless side information. A mathematical characterization of achievable rates is derived under certain Markov conditions on the source and side information. The problem of cascade rate distortion with a helper is also examined, with partial results obtained for cases where the side information is not necessarily common. This work provides insights into the use of side information in source coding problems.
412133	41213349	Extended analysis of motion-compensated frame difference for block-based motion prediction error.	In the past, designing and optimizing hybrid video codecs often relied on trial and error. However, having a theoretical model is important in order to understand and improve existing codecs. In this paper, a first-order Markov model is used to create an approximate model for the block-based motion compensation frame difference signal. The model assumes that pixel deformation is directional rather than uniform within a block. The study also shows that the accuracy of motion-compensated codecs is affected by imperfect block-based motion compensation. Experimental results demonstrate that the derived model accurately describes the statistical characteristics of the MCFD signal and reveals the impact of imperfect motion compensation on the performance of video codecs.
412133	412133181	On re-composition of motion compensated macroblocks for DCT-based video transcoding	Heterogeneous video transcoding is an important technique for achieving portability between different encoding formats and network environments. A frame-skipping transcoder is commonly used to reduce the bitrate of compressed videos, but traditional methods can lead to quality degradation and high computational complexity. To address this issue, a DCT-based frame-skipping transcoder has been proposed, but the motion compensated macroblock transcoding process in the DCT domain remains a bottleneck. In this paper, a new architecture is proposed to reduce the computational complexity by utilizing pre-computed shift operators and avoiding re-encoding errors in the dominant region. An adaptive transcoding architecture for boundary regions and error compensation methods are also introduced, resulting in a faster and more robust transcoding process. Experimental results show improved performance compared to traditional methods.
412134	41213434	Recommendation system for HBB TV: Model design and implementation	Hybrid broadcast and broadband (HBB) television has opened up new possibilities for services, including personalized recommendation systems. With the vast number of programs available, it is impossible for viewers to keep track of everything in real time. Recommendation engines were created to address the issue of information overload and make it easier for viewers to find content they are interested in. This paper presents the design and implementation of a recommendation system for HBB TV. The system uses an enhanced Naïve Bayes model to predict user ratings based on past observations. It is flexible and robust, able to handle sparse data sets and has shown promising results in experiments using a Yahoo movie data set. 
412134	41213464	Discovering biclusters in gene expression data based on high-dimensional linear geometries.	DNA microarray experiments involve identifying groups of genes that have similar patterns of transcription. This is important for understanding gene function, classifying tissues, and identifying regulatory motifs. However, conventional clustering algorithms often fail to detect patterns in data where only a subset of genes have consistent patterns over a subset of conditions. To address this issue, biclustering has been proposed, which focuses on identifying consistent patterns in subsets of genes and conditions. However, most existing biclustering algorithms have limitations and are based on heuristic methods. In this paper, a new geometric perspective for biclustering is presented, where biclusters are viewed as hyperplanes in a high dimensional data space. This approach allows for the detection of different types of patterns and has been shown to be effective in identifying biologically significant subsets of genes in microarray data. 
412135	41213516	Visual tracking via shallow and deep collaborative model.	The paper proposes a robust tracking method that combines a generative model and a discriminative classifier. Features are learned by shallow and deep architectures, with the generative model using a block-based incremental learning scheme and the discriminative model using deep learning techniques. The generative model also incorporates a local binary mask to handle occlusion, while the discriminative model learns generic features that are robust to background clutters and foreground appearance variations. The two models work together to achieve a balance in handling occlusion and target appearance change, which are challenging factors in visual tracking. The proposed method is evaluated against state-of-the-art algorithms and shown to be accurate and robust.
412135	4121353	Visual tracking via adaptive structural local sparse appearance model	Sparse representation has been used in visual tracking to find the best candidate with minimal reconstruction error using target templates. However, most existing methods only consider the overall representation and do not fully utilize the sparse coefficients to distinguish between the target and the background. This can lead to failure when there are similar objects or occlusions in the scene. In this paper, a new tracking method called the structural local sparse appearance model is introduced. This approach combines partial and spatial information of the target using a novel alignment-pooling method. This helps to accurately locate the target and handle occlusions. The method also incorporates a template update strategy that combines incremental subspace learning and sparse representation, which reduces drifting and the impact of occlusions. Experiments on challenging datasets show that this method outperforms several state-of-the-art methods.
412136	41213648	Generic Attacks on Secure Outsourced Databases.	Various protocols have been suggested for securely outsourcing database storage to a third party server, ranging from those with strong cryptographic security to more practical implementations. However, new attacks have shown that confidentiality of the data can still be compromised. The need for a formal understanding of the efficiency and privacy trade-off in outsourced database systems is recognized, and abstract models are proposed to capture them. These models identify two sources of leakage - access patterns and communication volume. Generic reconstruction attacks are developed for systems supporting range queries, where either of these sources is leaked. These attacks are shown to be optimal and can successfully recover secret attributes of all records in a database after a certain number of queries. Experimental results demonstrate the effectiveness of these attacks on real datasets.
412136	41213629	Efficient Set Intersection with Simulation-Based Security	This study focuses on finding a solution for combining private datasets between two parties, which is useful for online collaboration. The researchers propose protocols that utilize homomorphic encryption and hashing techniques for secure computation in both semi-honest and malicious environments. The protocol for the semi-honest environment is secure in the standard model, while the one for the malicious environment is secure in the random oracle model. The protocols have a linear overhead in terms of communication and computation. Different variants of the semi-honest protocol are also implemented and tested, revealing that the best performing variant may vary depending on the input size.
412137	41213769	Automatic initialization for facial analysis in interactive robotics	This paper discusses the importance of the human face in communication and presents a soft real-time vision system that allows a robot to analyze faces and recognize facial expressions as non-verbal cues. The system uses a robust detection scheme to identify faces and basic facial features, and then extracts facial parameters using active appearance models and support vector machine classifiers to identify individuals and their facial expressions. The paper evaluates four different methods for initializing the AAM algorithm and their performance in classifying facial expressions using the DaFEx database and real-world data from a robot's perspective.
412137	41213735	Vision systems with the human in the loop	The emerging cognitive vision paradigm involves using machine learning and automatic reasoning in vision systems to learn from what they see. These cognitive vision systems possess the ability to evaluate and adapt to their surroundings, making them highly robust. This article discusses three such systems: one for content-based image retrieval and two prototype systems that use contextual knowledge for visual analysis. All three systems are designed to interact with humans. The article also delves into the topic of assessing cognitive systems, including the use of psychological evaluations and usability experiments. The potential of these experiments is highlighted through examples of human-machine interactions. 
412138	41213869	How to forget a secret	A new type of attack has been discovered that can affect any cryptographic protocol. This attack exploits the physical memory of a participant, allowing the attacker to access all previous states. To protect against this type of attack, a new cryptographic primitive called "erasable memory" has been introduced. This primitive allows for the secure deletion of secret information. By using a small amount of erasable memory, a large non-erasable memory can be transformed into a large erasable memory. This transformation can be achieved using a simple assumption and can be implemented using a block cipher, making it efficient. Suggestions for implementing small amounts of erasable memory are also provided.
412138	41213856	Private Selective Payment Protocols	This article discusses a payment protocol where a server can make a payment to one of several clients, based on a given function. This type of protocol is commonly used in financial transactions like auctions and lotteries. The authors propose a private selective payment protocol for this scenario, where the server does not learn any private information about the clients. They also present an efficient protocol for the case where the function selects the client with the highest private input. This protocol is information-theoretically private and protects against a curious server. The key technique used is a new form of oblivious transfer called symmetrically-private conditional oblivious transfer.
412139	412139108	The principled design of large-scale recursive neural network architectures--dag-rnns and the protein structure prediction problem	The article proposes a methodology for designing large-scale recursive neural network architectures called DAG-RNNs. This methodology involves three steps: representing a domain using directed acyclic graphs, parameterizing relationships between variables using feedforward neural networks, and applying weight-sharing within subsets of the graphs. This approach allows for efficient processing and training of various data structures with different sizes and dimensions. The resulting models are probabilistic, but their internal dynamics are deterministic, making them suitable for tackling large-scale problems. The article also discusses specific classes of DAG-RNN architectures based on lattices, trees, and structured graphs, and their applications in predicting protein structural features. The article concludes by discussing extensions, relationships to graphical models, and implications for neural architecture design. The protein prediction servers can be accessed online at www.igb.uci.edu/tools.htm.
412139	41213936	Machine learning methods for computational proteomics and beyond	Protein structure prediction is a key challenge in biology, particularly in the genomic age where a large proportion of newly discovered genes have unknown structure and function. With the exponential growth of sequence and structure data, machine learning methods have become essential in tackling this problem. Among these methods, neural networks have achieved remarkable success, including the development of the most accurate secondary structure predictors. This article provides an overview of the authors' work and the current state-of-the-art in several areas of protein structure prediction, such as secondary structure, solvent accessibility, contacts, and three-dimensional and interchain beta-sheet structures. The authors' methods are based on graphical models and use deterministic recursive neural networks for faster learning. Lessons learned from these methods can also be applied to other complex neural architectures.
412140	4121404	On Cognitive Dynamic Systems: Cognitive Neuroscience and Engineering Learning From Each Other	The concept of cognitive dynamic systems combines engineering and cognitive neuroscience to create a better understanding of cognitive perception and control. Bayesian inference is used to improve sparse coding, a method used in neuroscience, and incorporate information filtering to enhance cognitive perception. Bellman's dynamic programming is then applied to cognitive control, resulting in a new reinforcement learning algorithm with desirable properties. The paper also addresses how to integrate cognitive control and perception, as well as how to manage risk, by using probabilistic reasoning and a preadaptive control mechanism. This mechanism involves a closed-loop feedback structure that utilizes past experiences to control motor actions and executive attention.
412140	41214070	A Novel Model-Based Hearing Compensation Design Using a Gradient-Free Optimization Method	A new model-based hearing compensation strategy and gradient-free optimization procedure is proposed for designing a learning-based hearing aid. Inspired by physiological data and auditory nerve models, the strategy aims to improve speech enhancement and compensate for hearing loss by approaching it as a neural coding problem. A Neurocompensator is designed to achieve this, and a gradient-free optimization procedure, based on the ALOPEX algorithm, is used to learn its parameters. The methodology, learning procedure, and experimental results are presented, along with a discussion on the use of unsupervised learning and optimization methods. This approach has potential for improving the performance of hearing aids.
412141	41214132	The Prediction of Student First Response Using Prerequisite Skills	Educational data analytics has mostly focused on predicting student next problem correctness, but this may not be the most useful for teachers. Instead, it is important to predict more meaningful aspects of student knowledge over a longer period of time. This paper proposes a method that uses prerequisite information, recorded by learning systems like ASSISTments and Khan Academy, to predict students' initial knowledge on a subsequent skill. Comparing this method to the standard Knowledge Tracing model and majority class, the results show that it is a reliable way to predict student knowledge without sacrificing accuracy. This method could help teachers better understand and support their students' learning.
412141	41214180	The Opportunity Count Model: A Flexible Approach to Modeling Student Performance.	The study focuses on the use of detailed performance data to improve the accuracy of predicting student performance in intelligent tutoring systems. The Opportunity Count Model (OCM) is introduced, which uses separate models for different opportunity counts (OCs) rather than one blanket model. The OCM is constructed using Random Forest (RF) and considers detailed performance data from tutor log files. Results show that OC is important in modeling student performance and that the detailed performance data varies across different OCs. This suggests that considering OC can lead to stronger student models and improve the effectiveness of intelligent tutoring systems.
412142	41214276	Stochastic optimization for collision selection in high energy physics	Artificial intelligence is becoming increasingly important in basic science research, particularly in high energy physics. AI methods can assist in precision measurements, such as determining the mass of the top quark. This is achieved by selecting collisions at high energy particle accelerators that produce top quarks, while minimizing other particles (background). Current methods for collision selection rely on heuristics or supervised learning, which are not always optimal. A new approach using stochastic optimization has been developed to directly search for selectors that minimize statistical uncertainty in the top quark mass measurement. This method has been shown to significantly improve the accuracy of the measurement, contributing to our understanding of the top quark. 
412142	4121426	Empirical Studies in Action Selection with Reinforcement Learning	This article discusses the importance of sophisticated action selection policies for intelligent agents to excel in challenging tasks. It focuses on two popular approaches to reinforcement learning (RL): temporal difference (TD) methods, which learn value functions, and evolutionary methods, which optimize populations of candidate policies. These approaches have had practical successes, but there have been few studies directly comparing them or attempting to combine their strengths. The article presents three empirical studies, including a benchmark task and ways to make evolutionary algorithms more effective at online tasks by borrowing mechanisms from TD methods. It also introduces a novel approach of using evolution to discover good representations for TD function approximators, which outperforms both TD and evolutionary methods alone.
412143	41214335	Weighted Online Problems with Advice.	The class is concerned with online problems where each request must be accepted or rejected and the goal is to minimize or maximize the number of accepted requests while maintaining a feasible solution. All -complete problems have the same advice complexity. This paper studies weighted versions of problems in , where each request has a weight and the aim is to minimize or maximize the total weight of accepted requests. Unlike unweighted versions, there is a significant difference in the advice complexity of complete minimization and maximization problems. The techniques for dealing with weighted requests can also be applied to non-complete problems like Matching and scheduling, achieving better results.
412143	41214326	Bounds on certain multiplications of affine combinations	Given two n × n matrices, A and B, with entries that are affine combinations of a set of variables, a 1 ,…, a m , and b 1 ,…, b m over the finite field GF(2), the maximum value of m is being considered in relation to the product matrix C = A · B. This question has arisen in the context of improving the communication complexity of zero-knowledge proofs. The current upper bound of n 2 is improved to n 2 3 3 + O (n) , with tighter bounds being obtained for smaller values of n. The bounds have been shown to be tight for n = 2, 3, and 4.
412144	4121449	Asynchronous Multiparty Computation: Theory and Implementation	The proposed protocol is an asynchronous method for multiparty computation that ensures perfect security. It has a communication complexity of n^2|C|k, with n being the number of parties, |C| being the size of the arithmetic circuit, and k being the size of elements in the underlying field. The protocol guarantees termination by allowing a preprocessing phase to finish without releasing any information. It is as efficient as a passively secure solution, and can withstand an adaptive and active adversary corrupting less than n/3 players. A software framework called VIFF is also presented, which allows for automatic parallelization of secure operations. Benchmarking results show that this protocol can be applied to practical and complex computations.
412144	41214495	On the amortized complexity of zero knowledge protocols for multiplicative relations	The protocol presented allows for zero-knowledge proofs of committed values satisfying a specific equation, with a linear size in the error probability and logarithmic in the number of values. This leads to a vanishing amortized complexity as the number of values increases. The protocol can also be made information theoretically secure with preprocessing. It can be used to create a perfect zero-knowledge interactive proof for circuit satisfiability and can be generalized for verifying multiple instances of an algebraic circuit with a cost dependent on the circuit's multiplicative depth. The protocol can also be applied to commitments to integers, with a low amortized complexity and improved security compared to previous solutions.
412145	41214563	An ASP-Based Data Integration System	Information integration systems play a key role in combining data from different sources to provide users with a unified view, known as a global schema. While simple integration scenarios have been extensively studied and efficient systems exist, challenges arise when constraints on the data quality are imposed, leading to ambiguous results. Significant research has been devoted to addressing these challenges, but no efficient system has been implemented yet. This paper presents a new data integration system utilizing Answer Set Programming (ASP) and various optimizations to enable consistent query answering (CQA) for large datasets. This system aims to improve the integration process and provide more reliable results.
412145	41214572	The INFOMIX system for advanced integration of incomplete and inconsistent data	Information integration systems are responsible for combining data from different sources to create a unified view, known as a global schema, for users. This allows users to query the data without needing knowledge about the sources. With the rise of the Internet and World Wide Web, there has been an increase in the number of information sources available to users, which are often autonomous, heterogeneous, and widely distributed. As a result, information integration has become a crucial issue in various areas such as distributed databases, cooperative information systems, data warehousing, and on-demand computing. It is estimated that the information integration market will reach $10 billion by 2006.
412146	41214633	The DLV system for knowledge representation and reasoning	Disjunctive Logic Programming (DLP) is a powerful formalism for representing and reasoning about knowledge, capable of expressing properties of finite structures that are decidable in the complexity class ΣP2 (NPNP). This makes DLP more expressive than normal logic programming, which is limited to properties decidable in NP. Disjunction in DLP allows for simpler and more natural representation of problems of lower complexity. DLV is a widely used implementation of DLP and is able to handle complex problems up to ΔP3-complete problems. It also allows for the use of weak constraints to express optimization problems. DLV has been extensively tested and benchmarked, showing its potential for applications such as knowledge management and information integration.
412146	41214618	Enhancing DLV instantiator by backjumping techniques	Disjunctive logic programming (DLP) is a powerful tool for representing and reasoning about knowledge. Its expressive language and efficient systems have made it popular in fields like Knowledge Management and Information Integration. However, as these applications often deal with large amounts of data, there is a need to improve the efficiency of DLP instantiators. This is the first step in a DLP computation, where variables are replaced with constants to create a ground program. The efficiency of this process is crucial for solving real-world problems. To address this issue, a new backjumping algorithm has been developed that uses both the semantic and structural information of a rule to generate only relevant ground instances, resulting in improved efficiency. Experiments on benchmark problems have shown positive results, with the new technique significantly improving the performance of the DLV DLP system.
412147	4121476	Lexical Selection for Cross-Language Applications: Combining LCS with WordNet	This paper discusses experiments conducted to test the usefulness of large-scale resources for lexical selection in machine translation (MT) and cross-language information retrieval (CLIR). The authors propose that verbs with similar argument structures have shared meaning components that are more relevant for argument realization than for individual verb meanings. This is supported by the finding that verbs with similar argument structures, as represented in Lexical Conceptual Structure (LCS), are rarely synonymous in WordNet. Based on this, the authors developed an algorithm for cross-language selection of lexical items that combines the strengths of both LCS and WordNet. The algorithm was implemented using the Parka Knowledge-Based System and tested on a knowledge base containing both types of information.
412147	41214747	Iterative translation disambiguation for cross-language information retrieval	The distribution of translation probabilities greatly affects the effectiveness of cross-language information retrieval systems. This paper presents a new method for calculating translation probabilities using a bilingual dictionary and target language corpus. The algorithm combines term association measures with iterative machine learning, resulting in more robust results than approaches using higher n-grams. By considering only pairs of translation candidates, the method is less affected by data-sparseness issues. The learned translation probabilities are then used as query term weights in a vector-space retrieval system, leading to significant improvements in English-German cross-lingual retrieval compared to a baseline using dictionary lookup without term weighting.
412148	41214852	On minmax theorems for multiplayer games	The authors present a generalization of von Neumann's minmax theorem to a class of separable multiplayer zero-sum games. These games are polymatrix, where every edge represents a two-player game with a zero total sum of payoffs. This generalization implies convexity of equilibria, polynomial-time tractability, and convergence of no-regret learning algorithms to Nash equilibria. This class of games is the broadest class to which tractability results can be applied. The authors also explore generalizations to non-constant-sum games, specifically polymatrix games with strictly competitive games on their edges. They show that the complexity of these games is PPAD-complete, indicating a significant difference from zero-sum games. Additionally, they examine the role of coordination in networked interactions and show that finding a pure Nash equilibrium in coordination-only polymatrix games is PLS-complete, while combining coordination and zero-sum games results in PPAD-completeness. 
412148	41214815	On the complexity of Nash equilibria of action-graph games	We examine the computational complexity of finding Nash equilibria in action-graph games (AGGs), which combine local dependencies and partial indifference to other agents' identities. We present an efficient approximation scheme for computing mixed Nash equilibria in AGGs with certain conditions, but also show that the problem becomes intractable when these conditions are relaxed. Specifically, we demonstrate that even with a tree action graph, it is NP-complete to find a pure-strategy Nash equilibrium and PPAD-complete to compute a mixed Nash equilibrium with an arbitrary number of agent types. These results highlight the difficulty of efficiently computing equilibria in multi-player games and complement previous findings on graphical games and anonymous games.
412149	41214962	Extending FolkRank with content data	The article discusses the problem of recommending tags for new or untagged documents in real-world tagging datasets. Most existing approaches focus on artificially created datasets where the user and document information is known, rather than addressing the challenge of recommending tags for new documents. In this paper, the authors propose a novel adaptation to the FolkRank algorithm by incorporating content data. This allows FolkRank to recommend tags for new documents based on their textual content. The results show that this adaptation improves FolkRank's performance on full tagging datasets, but a simpler content-aware tag recommender outperforms it. The authors suggest that further optimization of FolkRank's weighting method is needed for better results.
412149	41214977	Testing and evaluating tag recommenders in a live system	The challenge of providing tag recommendations for collaborative tagging systems has received significant attention from researchers. However, most of the research has focused on developing and evaluating methods, rather than addressing practical challenges such as integrating recommendation methods into real tagging systems and assessing their performance. This paper describes a tag recommendation framework designed for the social bookmark and publication sharing system, BibSonomy. The framework is easily extensible, open to various methods, and can be used independently from BibSonomy. Two recommendation methods deployed in the framework are also evaluated in this paper. The aim of the framework is to facilitate the development, testing, and evaluation of recommendation algorithms and foster collaboration with researchers.
412150	41215015	Visually exploring movement data via similarity-based analysis	The article discusses the importance of data analysis and knowledge discovery in moving object databases, particularly for tasks like traffic management and location-based services. Similarity search over trajectories is crucial for these tasks, but existing methods often use generic metrics that do not consider the complexity of trajectory data. To address this issue, the authors propose a framework that includes various trajectory similarity measures based on both primitive and derived parameters. These measures can be used for trajectory data mining, such as clustering and classification. The proposed measures are evaluated through experiments on synthetic and real trajectory datasets, highlighting their effectiveness and potential for visual analytics in trajectory data analysis. 
412150	41215060	Algorithms for Nearest Neighbor Search on Moving Object Trajectories	The Nearest Neighbor (NN) search is a crucial aspect of spatial and spatiotemporal database research, especially with the increasing use of Mobile Location Services (MLS). Most existing literature on NN query processing focuses on either stationary or moving query points over static datasets or predicted locations over continuously moving points. However, with the rise of MLS, there is a growing need for efficient k-NN query processing over historical trajectory data to improve existing services and create new ones. This paper explores methods for NN search on R-tree-like structures that store historical information about moving object trajectories. The proposed algorithms differ based on the type of query object and result, resulting in four types of NN queries. The authors also introduce new metrics to support their search strategies, and their implementation on two R-tree variants (TB-tree and 3D-R-tree) is tested on large datasets to demonstrate their scalability and efficiency.
412151	41215169	Recursive Structure And Motion Estimation From Noisy Uncalibrated Video Sequences	This paper introduces a new approach for estimating structure and camera focal length and motion. The method combines discrete and continuous methods to effectively handle image noise and outliers. It builds upon an existing framework and incorporates a simple structure estimation scheme. This allows for the system to handle varying focal lengths of the camera. The structure obtained from previous image frames is used to improve estimates for the current frame, and a RANSAC outlier rejection scheme is utilized. The system's performance is demonstrated through simulated experiments.
412151	41215158	Self-calibration from image derivatives for active vision systems	This paper presents a method for calibrating a camera on a robot when the hand-eye transformation between the two is unknown. The calibration uses spatial and temporal derivatives in an image sequence and does not require feature matching or a reference object. The calibration can be performed on an active robot vision system with controlled motion of the robot hand. The algorithm only uses image derivatives and known robot hand motion. A minimum of 3 non-coplanar translations of the hand are needed for the calculation. The method also calculates the orientation and position of the camera with respect to the robot hand. The full calibration involves 5 distinct motions and has been tested for noise sensitivity in experiments.
412152	41215265	Sensor Noise Modeling Using The Skellam Distribution: Application To The Color Edge Detection	The Skellam distribution is proposed as a noise model for CCD or CMOS cameras, derived from the Poisson distribution of photons that determine the sensor response. It can measure intensity differences in both spatial and temporal domains, and the parameters are linearly related to pixel intensity. This allows for automatic detection of color differences and edge detection in images. The approach does not require Gaussian smoothing and is able to extract fine details of image structures, such as edges and corners, without being dependent on camera settings. The algorithm only needs a confidence interval for a hypothesis test, making it a more efficient and accurate method.
412152	412152186	Difference-based Image Noise Modeling Using Skellam Distribution.	This article discusses the limitations of traditional assumptions about pixel intensity in quantum physics and proposes a new approach using the Skellam distribution. The Skellam distribution is derived from the Poisson photon noise model and allows for a linear relationship between intensity and Skellam parameters. This relationship remains consistent across different scenes, illuminations, and camera parameters. The article also introduces practical methods for obtaining this relationship using color patterns and natural illumination images. By using this new noise model, the article demonstrates its effectiveness in practical applications such as background subtraction and edge detection. 
412153	41215358	Using evolutionary neural networks to test the influence of the choice of numeraire on financial time series modeling	This study introduces an evolutionary approach to testing the impact of different numeraires on financial time series modeling. The method utilizes evolutionary neural networks, which evolve both the network structure and connection weights, along with a new similarity-based crossover technique. The study focuses on two highly traded financial time series expressed in their trading currency, as well as alternative numeraires such as gold, silver, and the euro. The goal is to compare the performance of these different numeraires and their effect on financial time series modeling. This approach may provide insights into the importance of choosing the right numeraire in financial analysis. 
412153	41215351	Electrocardiographic signal classification with evolutionary artificial neural networks	This study presents an evolutionary artificial neural network (ANN) classifier system designed for classifying heartbeats in the PhysioNet/Computing in Cardiology Challenge 2011. The goal of this challenge is to develop a mobile phone algorithm that can provide feedback on the quality of a 12-lead electrocardiogram (ECG) recording. The approach used is to apply evolutionary neural networks, which evolves both the network topology and connection weights, along with a new similarity-based crossover. The study focuses on distinguishing between usable and unusable ECGs obtained from mobile devices. A preprocessing step using the Discrete Fourier Transform is applied before the evolutionary approach to extract ECG features in the frequency domain. The performance and accuracy of the classifier system are evaluated through various tests.
412154	41215468	Modifying edges of a network to obtain short subgraphs	The paper addresses the problem of finding an optimal reduction strategy for a given edge weighted graph with a budget constraint. The goal is to minimize the total length of a minimum spanning tree in the modified network while staying within the budget. The problem is proven to be NP-hard, but the paper presents polynomial time approximation algorithms for a broad class of cost functions. Improved algorithms are also presented for a specific class of graphs with linear cost functions. These results can be applied to other network design problems such as Steiner trees and generalized Steiner networks. 
412154	4121541	Budget constrained minimum cost connected medians	The paper discusses the Budget Constrained Connected Median Problem, which involves finding the minimum cost subtree in an undirected graph while also satisfying a constraint on the total service distance. This problem has practical applications in areas like network design and maintenance of distributed databases. The authors present bicriteria approximation algorithms for solving the problem and prove lower bounds on its approximability. This shows that their algorithms have high performance ratios and are close to the best possible solutions. 
412155	41215545	Topology control in constant rate mobile ad hoc networks	Topology control is a crucial issue in ad hoc networks, where power levels must be assigned to nodes to create a desired network topology while minimizing energy consumption. While previous research has focused on stationary networks, this paper addresses topology control in mobile wireless ad hoc networks (MANETs). A model called the constant rate mobile network (CRMN) is introduced, where the speed and direction of each moving node are known. The goal is to minimize the maximum power used by any node in maintaining a specific graph property, such as network connectivity. The paper presents general frameworks for solving both the decision and optimization versions of the topology control problem under the CRMN model, with efficient algorithms for specific graph properties like network connectivity. This work paves the way for developing efficient and reliable distributed algorithms for topology control in MANETs.
412155	4121555	Topology Control For Simple Mobile Networks	Topology control in ad hoc networks involves optimizing power levels to create a desired network structure while minimizing energy consumption. Previous research has focused on stationary networks, but this paper examines the problem in mobile wireless ad hoc networks (MANETs). Using the Simple Mobile Network model, the goal is to minimize the maximum power used by any node to maintain connectivity. Three polynomial algorithms are proposed for solving this problem, with the fastest one performing as well as the best known algorithm for stationary networks. This study highlights the importance of considering mobility in topology control for MANETs.
412156	41215623	Strategies for mapping dataflow blocks to distributed hardware	Distributed processors must strike a balance between communication and concurrency when distributing instructions among processors. The level of concurrency plays a significant role in determining the importance of this balance. Higher concurrency allows for wider distribution of instructions, while lower concurrency benefits from mapping dependent instructions close together to reduce communication costs. This study examines these tradeoffs for distributed Explicit Dataflow Graph Execution (EDGE) architectures, which execute dataflow instructions in blocks. The results show that the best approach for mapping blocks to cores varies based on the width of the cores. An adaptive strategy that varies the number of cores for each block improves performance over fixed strategies for single and dual-issue cores, but the benefits decrease as the cores' issue width increases. These findings suggest that choosing the right runtime block mapping strategy can increase average performance by 18% while reducing average operand communication by 70%, making it a promising method for balancing communication and concurrency in distributed processors. 
412156	41215648	Techniques for reducing overheads of shared-memory multiprocessing	This study examines techniques to reduce overheads in shared-memory multiprocessor communication using the Scalable Coherent Interface (SCI) and the SPLASH benchmark suite. Three techniques were analyzed: efficient synchronization primitives, weakened memory ordering constraints, and optimization of the cache-coherence protocol for two-node data sharing. Simulations were performed for current and anticipated technology, with the results showing that the QOLB hardware primitive had the largest and most consistent improvement. Relaxing memory ordering constraints also provided a consistent performance boost. The study also found that technological improvements increase both the overheads and the success of the optimization techniques in reducing them. The results suggest that these techniques are largely orthogonal and can be used concurrently to further improve performance.
412157	4121579	Transparent Offloading and Mapping (TOM): Enabling Programmer-Transparent Near-Data Processing in GPU Systems.	Modern GPU systems face a critical bottleneck in main memory bandwidth due to limited off-chip pin bandwidth. 3D-stacked memory architectures have the potential to alleviate this bottleneck by directly connecting a logic layer to the DRAM layers with high bandwidth connections. Recent research has shown that offloading bandwidth-intensive computations to a GPU in each logic layer can significantly improve performance. However, the challenge lies in enabling computation offloading and data mapping without burdening the programmer. To address this, the paper introduces two new mechanisms: a compiler-based technique to automatically identify code for offloading and a software/hardware cooperative mechanism to predict and place frequently accessed memory pages in the closest memory stack. These mechanisms, known as TOM, are able to transparently improve performance by up to 76% without requiring any program modifications.
412157	41215771	A Hierarchical Thread Scheduler and Register File for Energy-Efficient Throughput Processors	Modern GPUs use a large number of hardware threads to hide latency in both function units and memory access. However, this requires a complex thread scheduler and a large and expensive register file. To reduce energy consumption on these massively-threaded processors, two techniques are proposed. The first is a two-level thread scheduler that maintains a smaller set of active threads and a larger set of pending threads. This reduces the number of threads the scheduler needs to consider, improving energy efficiency. The second is a hierarchical register file that replaces the monolithic register file found in current designs. This reduces energy by only allocating entries in the upper levels of the hierarchy for active threads. On average, this can reduce register file energy by 54&percnt; in real world workloads. 
412158	4121584	An Algorithm for Multi-Criteria Optimization in CSPs	Constraint Satisfaction and Optimization are crucial concepts in Artificial Intelligence, but in real-life applications, it may be necessary to optimize multiple functions simultaneously. This paper introduces a new algorithm for solving Multi-Criteria Optimization problems in this scenario. The algorithm is complete, meaning it can find all non-dominated solutions, and it does not rely on any assumptions about the constraints or objective functions. It utilizes Point Quad-Trees to efficiently represent the non-dominated frontier and provides superior results compared to other commonly used methods.
412158	41215823	Constraint Propagation and Value Acquisition: Why we should do it Interactively	In Constraint Satisfaction Problems (CSPs), the values for variable domains must be known before constraint propagation can begin. However, in some cases, acquiring these values can be time-consuming or not possible at the start of the problem. To address this issue, an Interactive Constraint Satisfaction Problem (ICSP) model has been developed. This model allows for the acquisition of variable domain values during the resolution process through Interactive Constraints. This approach has been tested on randomly generated CSPs and in 3D object recognition, and has shown to be effective. 
412159	41215983	Complexity Analysis of Late Binding in Dynamic Object-Oriented Languages	This article discusses the complexity of implementing late binding in dynamic object-oriented programming languages. Late binding refers to the process of resolving procedure calls to method definitions at runtime, rather than during compilation. This is a crucial feature of object-oriented programming and is commonly found in languages like CLOS and Prototype-based languages. The article explores the challenges and potential solutions for efficiently supporting late binding in these types of languages.
412159	41215945	The Temporal Precedence Problem	The Temporal Precedence Problem on pointer machines involves efficiently performing two operations: insert and precedes. Insert adds a new element, while precedes determines if one element was inserted before another. This paper presents a solution to this problem with a worst-case time complexity of O(lg lg n) per operation, where n is the number of elements inserted. It is shown that this problem has a lower bound of Ω(lg lg n) on pointer machines, making the proposed solution optimal for these machines. This analysis highlights the complexity of the Temporal Precedence Problem and provides an efficient solution for pointer machines.
412160	412160230	Joint Channel Estimation and Data Detection for MIMO-OFDM Two-Way Relay Networks	This paper discusses multi-input multi-output (MIMO) two-way relay networks that use orthogonal frequency-division multiplexing (OFDM) modulation. These networks involve two end users exchanging information through a relay in two phases. The relay amplifies and broadcasts signals in the second phase, reducing the time needed for information exchange compared to traditional one-way relay networks. The paper proposes an iterative algorithm for each end user to jointly estimate channel information and detect transmitted data. This involves estimating composite channels and noise covariance matrices resulting from the relay's amplification. The Expectation Conditional Maximization (ECM) algorithm is used for this estimation. Simulation results show that the proposed algorithm performs similarly to having perfect channel information. 
412160	41216099	Doubly Iterative Receiver for MIMO Amplify-And-Forward Relay Networks	This paper discusses a Multi-Input Multi-Output (MIMO) wireless relay network consisting of a source, destination, and relay terminal. The transmission from the source to the destination is divided into two phases, with the relay only amplifying and forwarding signals during Phase II in an Amplify-and-Forward mode. However, this can lead to correlated noise at the destination due to the amplification and forwarding of noise elements at the relay. To overcome this, the paper proposes a doubly iterative receiver using an Expectation Conditional Maximization (ECM) algorithm to estimate channel coefficients and decode transmitted signals. Simulation results demonstrate a high performance in terms of Bit Error Rate (BER). 
412161	4121619	DBGlobe: a service-oriented P2P system for global computing	The DBGlobe project aims to address the challenge of peer-to-peer computing by developing a data management system for data and services hosted by distributed, autonomous, and possibly mobile peers. The approach used is a service-oriented one, where data is encapsulated in services. The system also supports direct querying of data through an XML-based query language. The research covers topics such as infrastructure support for mobile peers and context-dependent communities, metadata management for services and peers, efficient routing of path queries on hierarchical data, and querying using the AXML language which includes service calls within XML documents.
412161	412161109	A Clustered Index Approach to Distributed XPath Processing	Supporting top-k queries over distributed collections of schemaless XML data is a challenging task due to the lack of schema knowledge in autonomous and dynamic systems. This necessitates the need for approximate query processing. Additionally, fetching top-k results incurs high communication and processing costs as they need to be retrieved and ranked from multiple sources. To address these issues, a clustered path index approach is proposed where data is grouped based on structural information. The query is gradually generalized using structural transformations, and results are ranked based on edit distance between path expressions. A compact indexing data structure is used to reduce index construction cost. Experimental results show that this approach significantly reduces communication cost and maintains a low index construction cost.
412162	41216272	Image-Based Food Calorie Estimation Using Knowledge on Food Categories, Ingredients and Cooking Directions.	This paper discusses the importance of accurately estimating food calories from images for mobile applications. It highlights the challenges of current methods that require human help or limited food categories and multiple images. To address this, the paper proposes using deep learning to simultaneously learn food calories, categories, ingredients, and cooking directions. Two datasets were created for the experiment, one from Japanese recipe sites and one from an American recipe site. The results showed that the multi-task CNN outperformed the single-task CNN in both food category and calorie estimation. This was seen in a 0.039 improvement in correlation coefficient for the Japanese dataset and a 0.090 improvement for the American dataset.
412162	41216227	AR DeepCalorieCam: An iOS App for Food Calorie Estimation with Augmented Reality.	A food photo typically features multiple dishes, so in order to accurately recognize them, each dish must be detected in the image. The introduction of Convolutional Neural Networks (CNN) has greatly improved the accuracy of object detection, leading to the development of two calorie estimation apps for iOS: DeepCalorieCam and AR DeepCalorieCam. These apps use YOLOv2, a CNN-based object detector, to identify dishes in a food photo and estimate their calorie content using image-based methods. AR DeepCalorieCam also incorporates augmented reality technology for a more interactive experience. 
412163	41216318	Metamodel-Based Decision Support System For Disaster Management	Software model developers typically use a general purpose language, such as Unified Modelling Language (UML), to create domain application models. However, when these models do not fully meet their needs, a more specific domain modelling language can be a better option. In this paper, a Disaster Management (DM) metamodel is introduced, which can be used to develop a disaster management language. This metamodel serves as a representation of DM expertise and can be used to create a decision support system for DM. The paper outlines the process of creating the metamodel and explains how it can be used to combine and match different DM activities for a specific disaster. This metamodel is a crucial component in creating a decision support system that streamlines access to DM expertise.
412163	41216344	Metamodelling approach towards a disaster management decision support system	The lack of timely and accessible expertise in disaster management (DM) can have disastrous consequences. To address this issue, a framework is proposed to create a decision support system that unifies and expedites access to DM expertise. It is noted that many DM activities are similar across different types of disasters, and an ontology is suggested as a way to describe these activities and their outcomes. This ontology will serve as a representation of DM expertise and allow for the development of a DM decision support system that can combine and match different activities based on the specific disaster at hand. 
412164	41216431	OPPL-Galaxy, a Galaxy tool for enhancing ontology exploitation as part of bioinformatics workflows.	Biomedical ontologies are important for organizing and sharing data in the Life Sciences Semantic Web. However, working with and enriching these ontologies requires flexible and efficient tools. The Ontology Pre Processor Language (OPPL) is an OWL-based language designed to automate changes in ontologies. A new tool, OPPL-Galaxy, integrates OPPL into the Galaxy framework, allowing for automated ontology manipulation. This combination of OPPL and Galaxy can enhance the analysis and exploitation of biomedical ontologies, including automated reasoning. Use cases have demonstrated the effectiveness of OPPL-Galaxy for enriching, modifying, and querying ontologies. Overall, this integration opens up new possibilities for advanced biological data analysis.
412164	41216453	Lessons learned in the generation of biomedical research datasets using Semantic Open Data technologies.	Biomedical research often requires large amounts of data from various sources, which can be difficult to combine and utilize due to their heterogeneity. The Open Data paradigm has emerged as a solution to facilitate data sharing and integration by making data available in a way that is readable by both humans and machines. The Semantic Web provides a technological framework for data integration and offers technologies such as Linked Datasets, which are open datasets linked to other open datasets. These datasets can be rated between one and five stars according to Berners-Lee's classification, with SWIT being a tool that automates the generation of four-star semantic datasets from heterogeneous data sources. This tool has been successfully applied in two health-related projects, leading to valuable lessons learned.
412165	41216558	A methodology for extracting ontological knowledge from spanish documents	The paper discusses a method for extracting knowledge from Spanish natural language texts using a combination of NLP techniques, ontological technology, and case-based reasoning. This approach has been applied in the field of oncology and the results are presented and discussed. The method involves analyzing text fragments, representing knowledge using ontologies, and applying a case-based reasoning methodology. The paper highlights the effectiveness of this approach and its potential for extracting knowledge from other domains as well.
412165	41216517	Populating Ontologies in the eTourism Domain	The Semantic Web aims to make online information easily understandable by machines without human help. This requires the use of ontologies, which serve as the foundation for the Semantic Web. However, the success of the Semantic Web relies heavily on the creation and implementation of ontologies. While there are methods for designing and automating ontology learning, less attention has been given to ontology population. This article introduces a methodology for populating ontologies using both Semantic Web and Natural Language Technologies. The methodology has been applied in the eTourism industry.
412166	41216617	Robust Higher Order Potentials for Enforcing Label Consistency	The paper introduces a new framework for labelling problems that combines multiple segmentations using higher order conditional random fields. This method uses potentials defined on pixel sets generated by unsupervised segmentation algorithms to enforce label consistency in image regions. The framework utilizes the Robust P n model, which is more general than the commonly used P n Potts model, and optimal swap and expansion moves can be computed by solving a st-mincut problem. This allows for the use of powerful graph cut algorithms for inference. The framework is tested on multi-class object segmentation and results show significant improvements in object boundary definition. The method has potential for use in other labelling problems. 
412166	41216663	P-3 & Beyond: Solving Energies With Higher Order Cliques	This paper introduces a new class of energy functions, called higher order clique potentials, and shows that the optimal moves for these functions can be computed in polynomial time using a submodular function. For a subset of these potentials, the optimal move can be found by solving an st-mincut problem, referred to as the P-n Potts model. This allows for the use of efficient move making algorithms, such as alpha-expansion and alpha beta-swap, for minimizing energy functions involving higher order cliques. These functions are useful for modeling natural scenes and have applications in computer vision, as demonstrated in the paper's example of texture-based video segmentation.
412167	41216716	AC: composable asynchronous IO for native languages	The paper discusses AC, a set of language constructs that allow for composable asynchronous IO in native languages like C and C++. This approach allows for multiple IO requests to be issued by a single thread and for long-latency operations to overlap with computation. Unlike traditional asynchronous IO interfaces, AC maintains a sequential programming style without requiring multiple threads or chains of callbacks. The language includes an "async" statement for concurrent IO operations, a "do..finish" block for waiting until async work is complete, and a "cancel" statement for cancelling unfinished IO. The paper provides an operational semantics for the language and evaluates its performance on the Barrelfish research OS and Microsoft Windows. Results show that AC offers comparable performance to existing interfaces while providing a simpler programming model.
412167	41216727	Feedback directed implicit parallelism	This paper presents an automated method for utilizing unused CPU resources in a multi-processor or multi-core machine. The approach involves profiling the program's execution and identifying potential sources of parallelism, then recompiling the program to perform these tasks speculatively through a work-stealing system. The system also detects any attempts to reveal the speculation at runtime. The approach is implemented using GHC 6.6 and tested on 20 programs from the 'nofib' benchmark suite. Results show varying levels of parallelism, with potential speed-ups ranging from 2x to 32x. On a 4-core processor, speed-ups of 10-80% were achieved on 7 programs. This approach is meant to supplement manual parallelization rather than replace it.
412168	41216822	Re-structuring, re-labeling, and re-aligning for syntax-based machine translation	The article discusses how the structure of bilingual material used in standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems. To improve the accuracy of these systems, the authors propose three modifications to the training data: re-structuring the syntactic structure of training parse trees, re-labeling bracket labels, and re-aligning word alignments across sentences. These modifications are implemented using the EM algorithm and have shown to lead to an overall 1.48 BLEU improvement on the NIST08 evaluation set in Chinese/English translation. Combining all three techniques results in the greatest improvement. 
412168	41216849	Extracting parallel sub-sentential fragments from non-parallel corpora	A new method has been developed for extracting parallel sub-sentential fragments from non-parallel bilingual corpora. This is achieved by analyzing similar sentence pairs through a signal processing approach. The method can identify which segments of the source sentence are translated in the target sentence and which are not. This allows for the extraction of valuable training data for machine translation, even from non-parallel corpora. The quality of the extracted data has been evaluated and shown to improve the performance of a leading statistical machine translation system. 
412169	41216956	Locating causes of program failures	Software failures can be caused by various defects in the program. To identify the specific defect that leads to a failure, programmers compare the program states of a failing run and a successful run. However, these state differences can occur anywhere in the program run, making it difficult to pinpoint the exact cause. To address this issue, programmers focus on the relevant variables and values that are associated with the failure, as well as the moments where these variables undergo transitions. This method has been found to be twice as effective in locating the defect compared to previous methods. For example, if the variable argc was initially 3, and at shell_sort(), variable [2] was 0, this would lead to a program failure.
412169	41216978	Sambamba: a runtime system for online adaptive parallelization	To efficiently utilize a microprocessor, the "classic" approach is to optimize a program at compile-time for all potential uses. However, further optimization can be achieved by anticipating the actual usage profile. The Sambamba project implements a runtime system that allows for adaptive and speculative transformations, rather than just anticipation. This is demonstrated through the automatic parallelization of a small C program, which adapts to the availability of idle system resources and results in a 1.92 fold speedup on two cores without oversubscribing the system. This approach allows for more efficient use of the microprocessor and better performance.
412170	41217016	Iterative criteria-based approach to engineering the requirements of software development methodologies	Software engineering involves developing and designing software based on the requirements of the target software. Similarly, engineering a software development methodology (SDM) also requires identifying the requirements of the target methodology. The authors propose an iterative method for eliciting and specifying the requirements of a SDM by using existing methodologies as supplementary resources. This method is part of a larger methodology engineering process aimed at designing and implementing a target methodology. The initial set of requirements is identified through analyzing the development situation and refining them through iterative application to relevant methodologies. The final set of requirements is based on the qualities desired in the target methodology, using knowledge gained from existing methodologies. This approach results in a more complete and rigorous set of requirements for the target methodology. An example is provided to demonstrate the effectiveness of this method in identifying the requirements of a general object-oriented SDM. 
412170	41217028	1st international workshop on combining modelling and search-based software engineering (CMSBSE 2013)	Modelling is an important aspect of software engineering, allowing for the development of intricate systems. Search-based software engineering (SBSE) is a successful method that utilizes automated discovery to find optimal solutions for software engineering problems. This workshop aims to showcase the potential of combining SBSE and modelling, as they have complementary concepts and techniques. It will provide a platform for researchers to share their ideas and advancements in this field, promoting collaboration and growth within the community. Ultimately, this workshop hopes to further enhance the effectiveness and productivity of software engineering through the integration of SBSE and modelling.
412171	4121713	A conditional neural fields model for protein threading.	Alignment errors are a major issue in current protein modeling methods, particularly for proteins with low sequence identity. A new method, called CNFpred, has been developed to improve the accuracy of sequence-template alignment. It uses a probabilistic graphical model called a Conditional Neural Field (CNF) and a non-linear scoring function that considers various protein sequence and structure features. This method has been trained using a quality-sensitive approach, resulting in improved alignments compared to traditional methods. CNFpred performs well on both small and large datasets, and is especially effective for proteins with sparse sequence profiles. This approach can also be applied to protein sequence alignment. 
412171	41217135	Fragment-free approach to protein folding using conditional neural fields.	Protein folding is a crucial process in understanding the structure and function of proteins. However, one of the main challenges in this area is finding an effective way to generate native-like conformations quickly. The popular fragment assembly method, which uses short structural fragments from the Protein Data Bank (PDB), may not always produce accurate results due to the limited availability of fragments and the discrete nature of their combinations. A previous method, known as conditional random fields (CRF), showed promise in generating conformations without relying on fragments but was limited by its ability to incorporate protein sequence information. A new approach, called conditional neural fields (CNF), overcomes this limitation and allows for more accurate modeling of the sequence-structure relationship. When combined with an energy function and Monte Carlo simulation, this CNF method outperformed the CRF method in producing native-like conformations for a variety of test proteins, including one with a previously unknown fold.
412172	4121725	Establishing an Informed S-BPM Community.	Subject-oriented Business Process Management (S-BPM) is a new approach to modeling and executing business processes that involves a shift from traditional functional flow-oriented methods. This paradigmatic shift requires organization designers and developers to change their mindset from thinking in terms of functions to thinking in terms of actor interactions. To facilitate this change, a Community of Practice based on education is proposed as an initiative to trigger a shift towards S-BPM. By revisiting traditional methods and experiencing alternatives, this approach aims to effectively implement S-BPM in organizations.
412172	41217225	Towards intuitive modeling of business processes: prospects for flow- and natural-language orientation	In order to effectively adapt to changes, stakeholders should develop a common understanding of business processes by using a "business-process language". This language should be intuitive and closely match the users' mental representations. In an empirical study, individuals who were not familiar with business-process modeling were introduced to the concept and asked to model a scenario using a notation with open semantics. The majority of participants showed a flow-oriented understanding of modeling, with some also displaying a natural language orientation. The results suggest the need for modeling techniques and tools to support organizational development and minimize individual cognitive workload when sharing process knowledge during change processes.
412173	4121731	Not Just An Empty Threat: Subgame-Perfect Equilibrium In Repeated Games Played By Computationally Bounded Players	This article discusses the problem of finding subgame-perfect equilibria in repeated games. Previous research has shown that finding a Nash equilibrium in these games is efficient if players are computationally bounded, but becomes extremely difficult if this assumption is removed. However, Nash equilibrium is considered a weak solution concept for extensive-form games. This study introduces a new notion of subgame-perfect equilibrium for computationally bounded players and demonstrates an efficient method for finding such equilibria in repeated games, assuming standard cryptographic hardness assumptions. Additionally, the algorithm can be applied to games with a finite number of players and constant-degree graphical games. 
412173	41217339	On the Non-Existence of Nash Equilibrium in Games with Resource-Bounded Players	The author discusses the concept of sequences of games where the same set of players participate in each game. These sequences can be found in various scenarios such as analyzing player running times, electronic money systems, and cryptographic protocols. Assuming the existence of one-way functions, the author proves that there is a sequence of 2-player zero-sum Bayesian games with polynomial action sizes and utility functions, but no polynomial-time Nash equilibrium. This demonstrates the limitations of traditional game theory concepts in scenarios where players have limited computational resources. The author also shows that Nash equilibrium may not exist when players are constrained to a certain number of computational steps. These examples highlight the potential for a "computational arms race" and the need for alternative equilibrium concepts in competitive settings.
412174	41217443	Quantifying paedophile activity in a large P2P system	The presence of paedophile activity on peer-to-peer (P2P) systems is a major concern for society, as it has significant implications for child protection, policy making, and internet regulation. However, due to limited traces and analysis methods, current knowledge of this activity is limited. To address this, researchers focused on the eDonkey P2P system and collected millions of keyword-based queries. They developed a tool to detect paedophile queries and established its accuracy through expert assessment. Using this tool, they estimated that 0.25% of all queries on eDonkey are paedophile in nature, and that over 0.2% of users enter such queries. These are the most precise and reliable statistics on this issue to date. 
412174	41217425	Outskewer: Using Skewness to Spot Outliers in Samples and Time Series	Outlier detection in datasets is an important problem for social network analysis, but current methods often rely on assumptions that are not realistic. A new approach called Out skewer is proposed, based on the concept of skewness and how it changes when extreme values are removed. This method is easy to implement, does not require prior knowledge, and can be used in real-time. It has been tested on two datasets, one related to internet topology and the other on search engine queries, and has shown good performance. 
412175	4121758	StarT-Voyager: A Flexible Platform for Exploring Scalable SMP Issues	StarT-Voyager is a machine used for research in cluster system communication. It consists of a network interface unit (NIU) that connects a PowerPC-based SMP to the MIT Arctic network. The NIU is highly flexible, with its functions easily modifiable through firmware or programmable hardware. This allows for comparison of different communication interfaces and strategies. The NIU has a fast embedded processor and large FPGAs, making it efficient and able to perform primitive operations in hardware to reduce firmware overhead. It currently supports four forms of message passing, S-COMA, and NUMA shared memory, but can be reconfigured to introduce new mechanisms for better usability and performance.
412175	41217528	A general technique for deterministic model-cycle-level debugging	Efficient use of FPGA resources requires implementing complex hardware models in multiple implementation cycles, which may not have a simple relationship with model cycles. This makes it challenging to debug and reconstruct the state of the synchronous system. A good debugging facility should provide control over the target design, easy access to significant state information, and deterministic execution for multicore processors. The paper presents a debugging technique based on LI-BDN theory, which enables deterministic model-cycle-level debugging. This technique was used to develop the debugging infrastructure for Arete, an FPGA-based multicore simulator, with minimal resource and performance penalties. The debugging infrastructure in Arete has only 5% area and 6% performance overheads.
412176	4121766	Approximation Algorithms for MAX SAT: Yannakakis vs. Goemans-Williamson	MAX SAT, also known as the maximum satisfiability problem, is a problem where the goal is to find a truth assignment that satisfies the largest number of clauses, each with assigned weights. This paper discusses approximation algorithms for MAX SAT proposed by Yannakakis and Goemans-Williamson. The authors present an improved algorithm based on Yannakakis' original algorithm. While Yannakakis' algorithm has the same performance guarantee as Goemans-Williamson, the improved algorithm has a better performance guarantee and can achieve a 0.770-approximation. This shows that the improved algorithm is more efficient in finding a near-optimal solution for MAX SAT.
412176	41217677	Approximation Algorithms for the Maximum Satisfiability Problem	The maximum satisfiability problem (MAX SAT) involves finding a truth assignment that maximizes the sum of the weights of satisfied clauses. This paper presents approximation algorithms for MAX SAT, with a 0.76544-approximation algorithm being the best proposed. The previous best algorithm, by Goemans-Williamson, had a performance guarantee of 0.7584. The new algorithms are based on semidefinite programming and the 0.75-approximation algorithms of Yannakakis and Goemans-Williamson.
412177	41217792	A SIMD optimization framework for retargetable compilers	Retargetable C compilers are commonly used for new embedded processors and to explore processor architecture. However, this approach often results in lower code quality compared to hand-written compilers or assembly code due to a lack of dedicated optimization techniques. To address this issue, flexible and retargetable code optimization techniques can be designed specifically for a range of target architectures. This article focuses on optimizing SIMD instruction support, which is common in embedded processors for multimedia applications but can be challenging due to nonuniform architectures and limitations on data types and memory alignment. The article presents an efficient and quickly retargetable SIMD code optimization framework integrated into an industrial retargetable C compiler. Experimental results show significant improvements in code quality for various processors, approaching the theoretical limit. 
412177	41217714	Graph-based code selection techniques for embedded processors	Code selection is a crucial step in the process of generating code for programmable processors. It involves finding the most efficient way to convert machine-independent intermediate code into processor-specific machine instructions. While traditional approaches to code selection work well for general-purpose processors, they may not be suitable for embedded processors due to their unique architectural features. This paper presents two techniques for code selection in embedded processors: one for media processors with SIMD instructions and another for fixed-point DSPs with irregular data paths. These techniques prioritize code quality over compilation speed and utilize data-flow graphs instead of data-flow trees. The use of graph-based code selection allows for better utilization of certain architectural features and results in significantly higher code quality in some cases. 
412178	41217865	Joint Multiple Target Tracking and Classification Using Controlled Based Cheap JPDA-Multiple Model Particle Filter in Cluttered Environment	This paper addresses the challenge of tracking and classifying multiple targets in a cluttered environment. The approach involves using a multiple model particle filter (MMPF) to handle nonlinear filtering with switching dynamic models. Joint probabilistic data association (JPDA) is also utilized to determine the origin of measurements. The efficiency of using Fitzgerald's Cheap JPDA is proven in the literature. Additionally, a controller based on the quality of innovation is implemented to adjust the number of particles. Monte Carlo simulations with two maneuvering targets demonstrate the feasibility and effectiveness of this approach. 
412178	41217819	Evidential Data Association Filter	Multi-target tracking in cluttered environments is a difficult problem due to challenges in associating measurements with tracks and estimating the targets' states. Various approaches have been suggested, with the joint probabilistic data association (JPDA) and its modified versions being commonly used. This paper proposes the use of belief functions to address both the measurement-to-track association and estimation problems. The method utilizes a Bayesian approach to assign basic belief mass and makes decisions based on an extension of the frame of hypotheses. Testing on a nearly constant velocity target showed improved performance compared to the nearest neighbor filter and JPDA filter in highly ambiguous cases. This suggests the feasibility and effectiveness of the proposed method.
412179	41217916	Jgroup-ARM: a distributed object group platform with autonomous replication management	This paper discusses Jgroup-ARM, a distributed object group platform that uses a measurement-based assessment technique to ensure its fault-handling capability. Jgroup is built on top of Java RMI and is designed for use in partitionable systems. ARM aims to improve system dependability through a fault-treatment mechanism and focuses on deployment and operational aspects. The combination of Jgroup and ARM makes it easier to develop, deploy, and manage partition-aware applications. The paper presents experimental results that demonstrate Jgroup-ARM's ability to recover applications to their initial state in various failure scenarios, including multiple network partitions. 
412179	4121797	Application-based dynamic primary views in asynchronous distributed systems	The paper discusses programming network applications using the process group paradigm and the challenges that arise when these applications are deployed in unreliable networks. These challenges include the possibility of the application partitioning into disconnected clusters, resulting in multiple views of the group's composition. The paper proposes a mechanism for efficiently determining the "primary partition" for the group, which allows for the selection rules to be modified at runtime and supports dynamic groups with changing membership. Additionally, this mechanism allows for the re-establishment of a primary partition even after a total failure scenario. These features enable the development of partition-aware applications that can adapt to their operating environment by using observed execution characteristics to establish selection rules.
412180	41218063	Weakening the language bias in LINUS	Propositional inductive learning algorithms have limitations in their ability to incorporate background knowledge and the expressiveness of their knowledge representation. This paper proposes a solution in the form of LINUS, a system that utilizes a more expressive logic programming framework to effectively incorporate background knowledge in learning both propositional and relational descriptions. This allows for the learning of logic programs in the form of constrained deductive hierarchical database clauses and determinate deductive database clauses. This method bridges the gap between propositional learning and more complex knowledge representation, leading to improved learning capabilities.
412180	41218038	The utility of background knowledge in learning medical diagnostic rules	Inductive learning algorithms are commonly used in the medical field to learn diagnostic rules. However, they are limited by the use of a restricted attribute-value language. To overcome this, the inductive learning system LINUS incorporates these learners into a more powerful logic programming framework, allowing for the effective use of background knowledge. This was applied to the early diagnosis of rheumatic diseases, where LINUS was given background knowledge from a medical specialist. Evaluation of the induced rules using the CN2 learner and measurements of their performance showed that the use of background knowledge greatly improved the quality of the rules.
412181	41218114	Using DEMO to Identify IT Services	IT Service Management (ITSM) has become increasingly important as organizations invest in their Information Technology (IT) Infrastructure and customers become more demanding. The Service Catalog is a crucial element of ITSM, formally documenting the available services provided by IT organizations. However, accurately identifying these services can be challenging and can have serious consequences if done incorrectly. This paper proposes a method using the Design & Engineering Methodology for Organizations (DEMO) to identify IT services. The Design Science Research Methodology and customer feedback were used to evaluate the method, showing that the new services list was preferred by customers compared to the previous one. This highlights the importance of accurately identifying IT services to meet customer demands and improve service quality.
412181	41218113	IT Governance Patterns in the Portuguese Financial Industry	IT has been used in large organizations since the 1950s for internal and external purposes. However, the widespread use of technology has led to a critical dependence on IT, requiring a specific focus on IT Governance (ITG). Determining the right ITG mechanisms can be complex, and this paper suggests an exploratory research approach using case studies and interviews with Portuguese financial services organizations to identify common patterns. The goal is to create theories and recommendations for ITG mechanisms that are suitable for different contexts. The research methodology used is Design Science Research (DSR). The paper concludes with limitations, contributions, and suggestions for future work. 
412182	41218218	Fat-trees: Universal networks for hardware-efficient supercomputing	The author introduces fat-trees, a new type of universal routing network that can connect processors in a parallel supercomputer. These networks can be customized to support varying levels of communication, allowing for efficient use of hardware and potential cost savings for applications like finite-element analysis. The author proves that fat-trees are the most optimal routing network of a given size and demonstrates this using a VLSI model that considers wiring as a direct cost. It is also shown that a fat-tree built from a specific amount of hardware can simulate any other network built from the same amount of hardware with only a slightly longer time (polylogarithmic factor greater).
412182	41218246	A hyperconcentrator switch for routing bit-serial messages	The article discusses a VLSI chip designed for highly parallel message routing networks. The chip can quickly concentrate bit-serial messages onto fewer wires, using ratioed nMOS and large fan-in NOR gates. It has a regular layout and incurs only 2 lg n gate delays. The design can be adapted for different technologies and has applications beyond message concentration, such as providing fault tolerance and being used in a processor datapath for various bit manipulations. Multiple hyperconcentrator switches can also be combined to create a larger partial concentrator switch.
412183	41218373	Drift-free tracking of rigid and articulated objects	Model-based 3D trackers are used to estimate the position, rotation, and joint angles of a model from video data. However, they often suffer from drift due to small errors accumulating over time. This is especially problematic for human motion capture and tracking when there are multiple moving objects and occlusions present. To address this issue, the authors propose an analysis-by-synthesis framework that combines patch-based and region-based matching to track both structured and homogeneous body parts. The method is demonstrated to work well for various scenarios, including fast movements, self-occlusions, and clutter. The study also includes a quantitative error analysis and comparison with other model-based approaches.
412183	41218345	Global Consistency Priors for Joint Part-Based Object Tracking and Image Segmentation	This paper discusses the use of Deformable Parts Models (DPMs) to enhance the tracking performance of previously unseen, articulated objects. The DPM is extended with global priors to incorporate foreground/background segmentation cues, and a Dual Decomposition approach is proposed to efficiently solve high-order constraints. The proposed approach is evaluated on the VOT online tracking benchmark and outperforms the baseline in accuracy and robustness. Additionally, the concept of part visibility is introduced to improve the flexibility of a generic DPM generated from a single reference frame, resulting in the visibility-aware DPM (VDPM). This allows for fine-grained articulated object tracking using an automatically generated DPM from a single template image.
412184	41218437	Routing for energy minimization in the speed scaling model	Network optimization aims to minimize energy consumption in telecommunication networks by adjusting the processing speed of network elements according to the amount of traffic they carry. Existing research on this topic focuses on individual elements, but we aim to optimize the entire network. We specifically examine routing with the goal of providing guaranteed speed/bandwidth for a given demand while minimizing energy use. The energy curve, which shows how energy is consumed as processing speed increases, plays a crucial role in determining the optimal routes. We show that for certain energy curves, there is a constant approximation for routing, but for others with a non-zero startup cost, there is no feasible constant approximation. However, we present two other approximations that provide good results. 
412184	41218433	Source Routing and Scheduling in Packet Networks	The study of routing and scheduling in packet-switched networks involves finding a set of paths for packets that does not overload any links in the network. An online routing algorithm has been developed that can find these admissible paths in real-time. The algorithm calculates the shortest path for each packet based on the current congestion of links. A scheduling protocol has also been developed that guarantees a polynomial end-to-end delay for every packet, which is an improvement from previous protocols. The choice of paths can also affect the stability of scheduling protocols, with some protocols remaining unstable regardless of path choice. However, intelligent path selection can make a difference, as seen in the case of a ring network with parallel links. 
412185	41218524	Learning collaboration strategies for committees of learning agents	In multi-agent systems, it is important for agents to decide when and with whom to cooperate. This paper focuses on systems with learning agents, whose goal is to accurately predict solutions to problems. Each agent must determine whether to solve a problem alone or collaborate with other agents. The paper presents a proactive learning approach that allows agents to learn when to form committees and which agents to invite. Experiments show that this results in smaller committees with maintained or improved problem solving accuracy compared to committees composed of all agents. This approach is beneficial for efficient collaboration in multi-agent systems.
412185	41218534	An Ensemble Architecture for Learning Complex Problem-Solving Techniques from Demonstration	The "Generalized Integrated Learning Architecture" (GILA) is a new approach to learning and problem solving that utilizes a set of independent learning and reasoning components coordinated by a central executive. Each component learns from a small number of expert demonstrations and during performance, proposes solutions to subproblems that are then pieced together by the executive. This heterogeneity in the components allows for more effective learning and problem solving. The effectiveness of GILA is demonstrated in the domain of airspace management, where it performs as well as or better than humans and outperforms individual components. This highlights the power of the ensemble architecture for learning and problem solving.
412186	412186146	Listing triconnected rooted plane graphs	A plane graph is a type of drawing of a graph in which no edges cross each other. A rooted plane graph has a specific outer vertex. The set G3(n, g) includes all triconnected rooted plane graphs with n vertices and inner faces of size at most g. This paper presents an algorithm for listing all plane graphs in G3(n, g). The algorithm takes constant time per output and outputs the difference from the previous output.
412186	41218682	Constant time generation of biconnected rooted plane graphs	A plane graph is a drawing of a planar graph without any intersecting edges. In a rooted plane graph, an outer edge is designated as the root. An algorithm has been developed to efficiently list all plane graphs with n vertices, using a constant amount of time and space. This algorithm only generates plane graphs where the size of each inner face is less than or equal to a specified number. This algorithm has a time and space complexity of O(n).
412187	41218743	The asymptotic number of labeled graphs with given degree sequences	The article discusses the number of n × n symmetric non-negative integer matrices subject to certain constraints. These constraints include specified and bounded row sums, bounded entries, and a set of specified zero entries. The article provides asymptotic estimates for this number and relates it to incidence matrices for labeled graphs. This result has potential implications in various fields such as combinatorics and graph theory.
412187	41218718	Locally Restricted Compositions II. General Restrictions and Infinite Matrices.	The study focuses on the compositions of an integer n, where the value of each part is constrained based on previous parts within a fixed distance. These constraints may depend on the index of the part, modulo a fixed integer m. Periodic constraints are naturally seen when compositions are written in a single row. The number of compositions of n is shown to be asymptotically equal to Ar-n, with A and r being constants. The distribution of the counts is expected to follow a joint normal distribution with means vector and covariance matrix proportional to n. The method of proof involves infinite matrices and does not provide accurate estimation methods. The study also examines the longest run, and in some cases, provides almost sure asymptotic estimates for the maximum part and number of distinct parts.
412188	41218875	Analysis of human behavior to a communication robot in an open field	This study explores human behavior towards an interactive robot at a science museum. Understanding how people interact with robots is crucial in developing communication robots for everyday use. The researchers analyzed visitor behavior towards a simple interactive robot at the museum, using information from sound level and range sensors. They identified factors that influence how people approach, maintain distance, and physically and verbally interact with the robot. This allowed them to extract useful information from the sensor data and apply it to communication robots. Overall, this research contributes to the development of robots that can predict and optimize their interactions with humans based on their behavior.
412188	41218843	Interactive Humanoid Robots for a Science Museum	 found that the use of RFID tags contributed to a more natural and personalized interaction between visitors and robotsThis paper discusses a field trial using interactive humanoid robots at a science museum to engage visitors in science. Each visitor was given an RFID tag to wear, which allowed the robots to interact with them autonomously through gestures and speech. The robots also performed exhibit guiding by moving around and explaining exhibits based on sensor information. The trial lasted for two months and received positive evaluations from visitors. The use of RFID tags improved the naturalness and personalization of the interactions between visitors and robots.
412189	41218930	Diagonal Unloading Beamforming for Source Localization.	In sensor array beamforming, algorithms are used to estimate the location of a radiating source. Diagonal loading of the beamformer covariance matrix is commonly used to improve accuracy and robustness. This paper proposes a new method, called diagonal unloading (DU), which adds an additional constraint to the array output vector's covariance matrix. This regularization is achieved by subtracting a set amount of white noise from the matrix's main diagonal. The DU beamformer aims to separate the signal subspace from the noisy signal space and is calculated by enforcing a negative definite covariance matrix. The proposed DU method is compared to other beamforming methods in terms of computational cost and localization performance. The results show that the DU method is comparable to existing methods, making it a simple, effective, and efficient option for array processing.
412189	4121895	Localization and Tracking of an Acoustic Source using a Diagonal Unloading Beamforming and a Kalman Filter.	This article discusses the signal processing framework and results for the IEEE AASP challenge on acoustic source localization and tracking (LOCATA). The framework is designed for single-source scenarios and consists of four main components: pre-processing, voice activity detection, localization, and tracking. The pre-processing includes the short-time Fourier transform and cross power spectral density matrices estimation. The VAD is calculated using a threshold of the CPSD matrices. The localization uses a low-complexity and high-resolution method called diagonal unloading beamforming, and the DOA estimation is smoothed with a Kalman filter. Results are reported for different microphone arrays, including a 7-microphone linear array, a 12-microphone pseudo-spherical array integrated in a humanoid robot, and a 32-microphone spherical array. 
412190	41219047	Can network characteristics detect spam effectively in a stand-alone enterprise?	Prior research has demonstrated that the network dynamics of an email packet and the entire connection can be used to classify the email as spam or not. However, this research has mainly focused on data collected from various sources. In this paper, the authors re-examine these techniques using data from a single enterprise and find that packet properties are not as effective. They also show that incorporating flow characteristics into the model of packet features can improve the accuracy, indicating that some flow features are already captured by the packet features.
412190	41219048	On estimating end-to-end network path properties	Transport protocols rely on accurate information about network conditions to efficiently transfer data. In the context of the Internet, these protocols must make their own estimates based on measurements taken by connection endpoints. There are two main problems in transport estimation: determining the retransmission timer (RTO) for a reliable protocol and estimating available bandwidth for a connection. Using TCP and a large measurement set, we evaluate different algorithms for RTO estimation and find that their minimum values and timer granularity have the biggest impact on performance. We also explore bandwidth estimation techniques and develop a receiver-side algorithm that performs better than previously proposed methods.
412191	41219152	Parallel non-linear optimization: towards the design of a decision support system for air quality management	Large numerical simulation codes have become increasingly popular for solving complex scientific and engineering problems. In the environmental field, these simulations have been used to predict the outcomes of various scenarios and aid in decision making for strategies that balance financial gain with environmental impact. To optimize these decisions, environmental decision support systems have been developed, using high performance computing to evaluate different policy options and find the best solution. This involves solving non-linear optimization problems with discrete and continuous parameters. In this paper, the authors focus on the optimization component of a decision support system and assess the effectiveness of using the quasi-Newton BFGS method, which can be parallelized for faster computation. Through a case study in air quality management, the authors demonstrate the success of this approach, showing significant performance gains and feasibility for solving real-world problems using parallel and distributed supercomputers.
412191	41219191	Run-time thread sorting to expose data-level parallelism	CQC is a complex method used in quantum chemistry to analyze the electronic structure of molecules. One of its key components, the evaluation of Electron Repulsion Integrals (ERIs), presents a challenge for data parallel processing due to controlflow variation between different evaluations. However, it has been observed that the variation is limited and can be classified into distinct sets, making data parallel execution possible. To achieve this, an architecture with high throughput and small memories is proposed. This approach has been evaluated using FPGA synthesis and has potential for use in other manycore architectures. 
412192	41219227	Efficient Large-Scale Distributed Key Generation against Burst Interruption	A distributed key generation scheme distributes a secret key among key servers and computes a corresponding public key. Canny and Sorkin proposed a probabilistic threshold scheme for a large number of key servers with lower communication cost. However, their scheme has some limitations that can be addressed using randomness technique. This paper introduces a secure scheme against dishonest key servers and better performance. Simulation experiments show that their scheme and the proposed scheme can be used in different scenarios.
412192	41219219	Anonymous Password Based Authenticated Key Exchange with Sub-Linear Communication	The paper introduces a new anonymous password-based authenticated key exchange protocol with improved communication cost. The protocol has a sub-linear cost of O(root N), which is better than the previous protocol's cost of O(N), where N is the number of users. The authors also demonstrate the security of the session key against an active adversary in the random oracle model and the anonymity of user identities against a semi-honest adversary in the standard model. Overall, the proposed protocol provides a more efficient and secure way for users to exchange keys anonymously.
412193	41219318	A container yard storage strategy for improving land utilization and operation efficiency in a transshipment hub port.	This paper discusses the challenges of managing a storage yard in a busy transshipment hub where there are high levels of loading and unloading activities. Due to the large amount of container traffic and limited land availability, efficient services are difficult to provide. A consignment strategy with a static yard template has been used, but it sacrifices land utilization. To address this issue, the paper proposes two space-sharing approaches that dynamically reserve storage space for different vessels during different shifts. This also considers workload balancing to reduce traffic congestion. The proposed framework integrates space reservation and workload assignment, resulting in more efficient container handling and reduced yard crane deployment. Experimental results support the effectiveness of this approach.
412193	41219391	Analysis On High Throughput Layout Of Container Yards	Efficient handling resources are crucial for optimizing container flows at a container terminal. A well-designed layout can greatly improve the performance of handling activities. This study focuses on a design process that maximizes throughput capacity and minimizes resource configuration for two types of parallel yard layouts: double-lane and single-lane. Experiments were conducted to analyze the effects on layout structure and resource configuration. It was found that container flows by vehicles have a greater impact on the design compared to processing times for quay cranes and yard cranes. The single-lane layout is ideal for high throughput capacity, while the double-lane layout is better for efficient vehicle flows.
412194	41219473	Slid pairs in the initialisation of the A5/1 stream cipher	A5/1 is a stream cipher used in the GSM system to ensure privacy. It uses a shift register and majority clocking rule for updating its registers. In this study, the initialisation process of A5/1 is analysed. The researchers found that the cipher has a sliding property, where every valid internal state can also be used as a legitimate loaded state. This property allows for multiple key-IV pairs to produce phase shifted keystream sequences. The paper also discusses a possible attack on the cipher based on this property, using only ciphertext.
412194	41219423	Weak key-IV pairs in the A5/1 stream cipher	The A5/1 cipher is a stream cipher used in the GSM system for privacy protection. This paper focuses on the initialisation process of A5/1 and the loading of the secret key and IV. Through analysis, it is shown that there are weak key-IV pairs in A5/1 due to this loading process, which can result in registers containing all-zero values. This can lead to weak keystream sequences and a distinguisher is described that can fully decrypt affected messages in cases where two or three registers contain only zeros. This highlights potential vulnerabilities in the A5/1 cipher and the need for stronger key-IV loading methods.
412195	4121957	Computing loci of rank defects of linear matrices using Gröbner bases and applications to cryptology	The MinRank problem, also known as computing loci of rank defects of linear matrices, is a challenging task in linear algebra with diverse applications in fields such as Cryptology, Error Correcting Codes, and Geometry. This problem involves finding points where the rank of a given square linear matrix is less than a specified integer. The paper aims to develop an efficient algorithm to solve this problem by studying two algebraic formulations: one that generates bi-homogeneous equations and another that involves a determinantal ideal. Under certain assumptions, the paper provides new bounds on the degree of regularity of these formulations, allowing for an estimation of the complexity of computing Gröbner bases. The results have practical implications, such as breaking a cryptographic challenge and evaluating the security of a cryptosystem. The determinantal ideal formulation is found to be more suitable for Gröbner bases computations. 
412195	41219527	Critical points and Gröbner bases: the unmixed case	The article discusses the importance of computing critical points of polynomial maps on algebraic varieties, as they are essential for finding the global minimum of the map. These points are relevant in non-convex polynomial optimization, which has many applications in fields such as control theory, chemistry, and economics. Gröbner basis algorithms have been found to be efficient in computing these critical points, leading to the development of software using the Critical Point Method. The article provides complexity estimates for computing Gröbner bases of systems defining critical points, showing that under certain assumptions, the complexity is polynomial in the number of critical points. Experimental evidence supports these theoretical results.
412196	41219686	Spectral Analysis of Quasi-Cyclic Product Codes	In this paper, we analyze a linear quasi-cyclic product code made up of two quasi-cyclic codes with relatively prime lengths over finite fields. We explore the spectral analysis of this code using the spectral analysis of the row and column codes. We also present a new lower bound for the minimum Hamming distance of a given quasi-cyclic code and introduce a new algebraic decoding algorithm. Additionally, we prove the explicit basis of this product code in terms of the generator matrix using a special order form. Our findings extend previous work on cyclic product codes and we also provide a conjecture for the generator matrix of a general quasi-cyclic product code. We apply our spectral analysis to a specific case and propose a new lower bound for the minimum Hamming distance of a given quasi-cyclic code. Finally, we develop a decoding algorithm for phased burst errors with guaranteed decoding radius.
412196	41219626	An improvement on the bounds of Weil exponential sums over Galois rings with some applications	We have developed a new upper bound for Weil-type exponential sums over Galois rings of characteristic p2, surpassing the previous bound established by Kumar, Helleseth, and Calderbank in 1995. This refined bound is based on the genera of function fields and also includes an analog of McEliece's theorem on the divisibility of the weights of codewords in trace codes over Zp2. These findings have implications for improving the minimum distance estimation of trace codes over Zp2 and the correlation bounds of certain nonlinear p-ary sequences.
412197	41219727	Making learning designs happen in distributed learning environments with GLUE!-PS	The article discusses the limited availability of virtual learning environments (VLEs) that allow teachers to create and reuse learning designs. Many VLEs are not accessible to most teachers due to institutional decisions and contextual constraints. The GLUE!-PS architecture and data model aims to address this issue by allowing teachers to design learning activities using various tools and deploy them in different distributed learning environments. The article demonstrates this by showcasing two authentic learning designs with different pedagogical approaches and how GLUE!-PS helps set up the necessary ICT infrastructure in two different environments (Moodle and wikis).
412197	41219759	Lost in translation from abstract learning design to ICT implementation: a study using moodle for CSCL	The paper discusses the potential loss of information that occurs when teachers' abstract learning design ideas are implemented in virtual learning environments (VLEs) through the life-cycle of computer-supported collaborative learning (CSCL) scripts. The study involved 37 teachers' collaborative learning designs being deployed in Moodle using a specific set of ICT tools. Despite some loss of information, the teachers still found the resulting deployment of learning designs in Moodle to be valid for use in real practice. This suggests that further research should be conducted to improve ICT support for learning design practices in the context of mainstream VLEs.
412198	41219831	Collisions for the WIDEA-8 compression function	WIDEA is a family of block ciphers based on the IDEA block cipher. It uses multiple parallel instances of IDEA and an improved key schedule to create larger block sizes. The design is suggested as a compression function for the Davies-Meyer mode. This paper examines the security of WIDEA when used as a compression function. By taking advantage of the slow diffusion in the key schedule, the paper presents practical free-start collisions for WIDEA-8, with a complexity of 213.53.
412198	41219871	Refinements of the ALRED construction and MAC security claims	The authors introduce three security claims for iterated message authentication codes (MAC functions) and propose a construction method, ALRED, which is based on a block cipher and has proven security in the absence of internal collisions. They use this construction to create two MAC functions, ALPHA-MAC and PELICAN, using the advanced encryption standard (AES). The authors also provide a model for describing internal collisions in ALRED and demonstrate that their proposed security claims are applicable to MAC functions using the ALRED construction. They also explain the reasoning behind the security claims for PELICAN. Overall, the authors present a reliable and secure method for constructing MAC functions.
412199	41219942	Commutativity analysis for XML updates	The article discusses the use of XQuery extended with update operations as an effective approach to support XML updates. While this approach is user-friendly, it poses difficulties in optimization and analysis due to the undecidability of commutativity in most existing XML update languages. To address this issue, the article proposes a conservative analysis method that uses path analysis to determine commutativity. The method involves computing upper bounds for accessed or modified nodes in an expression and can be used to identify commuting expressions. The article also provides a theorem to support this technique and showcases its application in query optimization with updates.
412199	4121993	View Operations on Objects with Roles for a Statically Typed Database Language	The object data model has been extended to handle the evolving nature of data and multiple views for the same data. This includes two sets of operations: object extension operations, which allow an object to change its type dynamically, and object viewing operations, which allow an object to be viewed with a different structure. These operations are related in that they preserve the identity of the object, but differ in that object extension can change the object's behavior while object viewing creates a new view without changing its behavior. This paper focuses on defining object viewing operations in a statically and strongly typed database programming language, and how they relate to object extension and role mechanisms. The use of these operations is demonstrated with examples from the language Galileo 97.
412200	41220049	Towards Fresh and Hybrid Re-Keying Schemes with Beyond Birthday Security.	Fresh re-keying is a security protocol that splits the task of protecting encryption/authentication schemes against side-channel attacks into two parts. One part, the re-keying function, must have certain properties and is based on an algebraic structure that can withstand side-channel attacks. The other part, a block cipher, provides resistance against mathematical attacks and only needs to be secure against single-measurement attacks. This protocol is cheap, stateless, and does not require synchronization between communication parties. However, a recent attack has shown that the first instantiation of fresh re-keying only provides birthday security. In response, this paper proposes two solutions that provide provable security against this attack, one using classical block ciphers and the other using tweakable block ciphers. The paper also discusses the use of fresh/hybrid re-keying for encryption and authentication and warns about their vulnerability to side-channel attacks.
412200	41220031	Collision attacks on the reduced dual-stream hash function RIPEMD-128	This paper focuses on analyzing the security of the RIPEMD-128 hash function against collision attacks. While it has been used as a replacement for other 128-bit hash functions, such as MD5, there has been limited research on its security. The best previous result was a preimage attack for the first 33 steps of the function. However, this work presents new attacks on up to 48 steps, including a collision attack with practical complexity. Additionally, non-random properties for 48 steps are shown, and an example of a collision on the compression function is provided. The authors use complex nonlinear differential characteristics and a general strategy to analyze the function's dual-stream structure, aided by an automatic search tool. 
412201	41220113	Exploiting coding theory for collision attacks on SHA-1	This article discusses the use of coding theory in the cryptanalysis of hash functions, specifically focusing on the SHA-1 algorithm. The authors introduce various linear codes that are used to identify low-weight differences that can lead to a collision. They also incorporate recent findings in the field to improve upon existing approaches. Through their method, they are able to identify differences with extremely low weight and make a conjecture about the complexity of a full collision attack on SHA-1. 
412201	41220142	Second-preimage analysis of reduced SHA-1	Many applications do not require collision resistance in cryptographic hash functions, but rather preimage resistance. This is why SHA-1 is still recommended for use in most applications, except for digital signatures. However, recent research has shown that SHA-1 may not have as much security margin as previously thought when it comes to second preimage attacks. In this paper, the authors demonstrate this by using known differential properties of SHA-1 to reduce the complexity of a second preimage shortcut attack and show a property that violates a natural second preimage resistance assumption. These findings suggest that there may be potential vulnerabilities in SHA-1 that could potentially compromise its security.
412202	41220242	Dynamic constraints and database evolution	The relational model, commonly used for databases, has been extended to study the impact of dynamic constraints on database evolution. This extension incorporates both static and dynamic constraints, with a focus on functional dependencies (fd's) and their dynamic counterparts. The interaction between these constraints is explored, considering the influence of the database's past history on the static constraints and the connection between the static constraints and future database evolution. The concepts of age, age-closure, survivability, and survivability-closure are used to analyze these effects. This extension provides insight into the behavior of databases under dynamic constraints. 
412202	41220227	Dynamic functional dependencies and database aging	This article introduces a simple extension of the relational model that incorporates both static and dynamic constraints to analyze the impact of database evolution. The static constraints, known as functional dependencies (FDs), are combined with dynamic constraints involving global updates, referred to as "dynamic" FDs. The article focuses on how these constraints affect the satisfaction of static constraints over time. The concept of age and age closure is used to examine the influence of the past history of the database on the static constraints. Additionally, survivability and survivability closure are briefly discussed in relation to the potential future evolution of the database. 
412203	4122037	Organizations Analysis with Complex Network Theory	In this paper, the authors propose network measures and analytical procedures for modeling the structure and behavior of different types of organizations, including line, functional, line-and-staff, project, and matrix organizations. They develop network generators for each type to gather information about employee connectivity and organizational properties. The authors also examine the roles and groups of employees within the network and assess their social position and impact. They analyze the structure of the network to identify employees with similar roles and potential for action within the organization. Additionally, the authors estimate the confidentiality of the network by removing certain communication links between employees and determining the percentage of communications that would disconnect the organization into unconnected parts.
412203	41220338	Processing and Analysis of Macedonian Cuisine and its Flavours by Using Online Recipes	This paper discusses methods for analyzing online culinary data, specifically focusing on the Macedonian cuisine as a representative of South-European cuisine with influences from Middle-Eastern and Eastern-European cuisine. Through various analyses on a dataset of Macedonian recipes, the authors explore dietary habits, specific ingredients and their combinations, and the overall flavor profile of Macedonian cuisine. They also propose a metric that measures the contribution of a specific ingredient to a recipe by comparing shared flavors with and without that ingredient. These methods can be applied to analyze any cuisine and can provide valuable information for comparing different cuisines. 
412204	41220424	Efficient Algorithms and Implementations for Optimizing the Sum of Linear Fractional Functions, with Applications	This paper discusses an improved algorithm for solving the sum of linear fractional functions (SOLF) problem in both 1-D and 2-D. The key subproblem is the off-line ratio query (OLRQ) problem, which requires finding optimal values for a sequence of linear fractional functions subject to linear constraints. The algorithm uses geometric properties and parametric linear programming to solve the OLRQ problem in O((m+n)log (m+n)) time. This improves upon previous methods which had a time complexity of O(m(m+n)). The improved algorithm has been shown to outperform other approaches in most cases and has also been applied to problems in computational geometry and other areas. 
412204	41220431	Smallest Maximum-Weight Circle For Weighted Points In The Plane	This paper focuses on finding the smallest possible circle that covers the most weighted points in a given set in the plane. The researchers present an algorithm that can solve this problem in polynomial time, taking into account both positive and negative weights. They also consider a more specific version of the problem where the circle must be centered on a given line and provide an algorithm that runs in O(n(m + n) log(m + n)) time. Additionally, the algorithm can report multiple smaller circles with minimal weight using limited extra space. The paper also discusses different scenarios and provides an algorithm with a faster runtime for cases where all positively weighted points must be included in the circle.
412205	41220516	PurTreeClust: A purchase tree clustering algorithm for large-scale customer transaction data	Clustering customer transaction data is a crucial step in understanding customer behavior in retail and e-commerce companies. This involves organizing products into a product tree, with leaf nodes being individual products and internal nodes representing product categories. To better analyze customer transactions, the concept of a "personalized product tree" or purchase tree is introduced. The PurTreeClust algorithm is then proposed to efficiently cluster large-scale customer data from these purchase trees. This algorithm utilizes a new distance metric and a cover tree for indexing and initial cluster center selection. Additionally, a gap statistic method is proposed to estimate the number of clusters. Experiments on ten large-scale transaction datasets confirm the effectiveness and efficiency of this method compared to other clustering algorithms.
412205	41220543	Finding long and similar parts of trajectories	This article discusses a method for measuring the similarity between two time-dependent trajectories by computing their average distance at corresponding times. It presents algorithms for finding the most similar subtrajectories, assuming the trajectories are given as polygonal lines. When a minimum duration is specified and the subtrajectories must start at the same time, a linear-time algorithm is provided. The article also presents a linear-time algorithm for finding an interval with the minimum average value for a piece-wise monotone function. However, when the subtrajectories can start at different times, the problem becomes more complex and can only be solved approximately with a performance guarantee. The article provides (1+ε)-approximation algorithms for this case, both for specified and minimum duration.
412206	41220638	A generic flash-based animation engine for prob	Writing a formal specification for real-life, industrial problems can be challenging and prone to errors, even for experts in formal methods. It is important to involve domain experts in the process to ensure that the formal model is accurate and in line with their expectations. However, understanding formal models written in a specification language like B may require mathematical knowledge that domain experts may not possess. To address this issue, a new tool has been developed that visualizes B Machines using the ProB animator and Macromedia Flash. This tool allows specifiers to create domain-specific visualizations that can be easily understood by domain experts, helping to ensure that the specification meets their needs.
412206	41220675	Animating and Model Checking B Specifications with Higher-Order Recursive Functions	The B-method is a formal development approach used in critical industries for computer systems. It involves proof activities as well as validation of the initial specification, as having a correct implementation of an incorrect specification can be problematic. Validation can be done through animation and model checking, which checks for certain temporal properties. Previous work has introduced the ProB animator and model checker to support these activities. This work presents two improvements to that previous work. The first is the ability to handle complex functions in realistic specifications, as seen in an example provided by Anthony Hall.
412207	41220745	Deriving Linear Size Relations for Logic Programs by Abstract Interpretation	The proposed method is an automatic approach for determining linear size relations between the arguments of atoms in a definite Horn clause program's least Herbrand model, based on a given norm. It utilizes abstract interpretation, with an abstract domain consisting of affine subspaces or linear varieties and operations based on linear algebra. The main purpose of this technique is for automatic termination analysis, but it can also be used for complexity and granularity analysis, as well as specialisation of constraints in constraint logic languages. 
412207	4122073	Polynomial interpretations as a basis for termination analysis of logic programs	The paper presents a new method for determining if definite logic programs will terminate, using polynomial interpretations. This involves assigning polynomials to function and predicate symbols, similar to techniques used in term rewriting systems. This extends traditional techniques used in termination analysis of logic programs, such as linear norms and level mappings, to incorporate arbitrary polynomials. The paper also introduces a constraint-based approach for automatically generating polynomial interpretations that meet termination conditions. The method is based on established concepts and results in termination analysis, and aims to improve the accuracy and efficiency of termination analysis for logic programs.
412208	41220852	Migration of information systems in the Italian industry: A state of the practice survey	The paper discusses the challenges and complexity of software migration, particularly towards web and distributed architectures, which has become increasingly relevant due to the widespread use of the internet and mobile devices. The objective of the paper is to report on a survey conducted among 59 Italian IT companies to identify their experiences, tools used, and emerging needs and problems in software migration projects. The results show that migration, especially towards the web, is highly important for Italian companies and they are increasingly using free and open source solutions. However, the adoption of specific tools for migration is limited due to lack of skills and knowledge or inadequate options. The paper concludes by suggesting the need for technology transfer between academia and industry to improve the adoption of software migration techniques and tools.
412208	41220829	Software Migration Projects In Italian Industry: Preliminary Results From A State Of The Practice Survey	This paper discusses the importance and complexity of software migration in the maintenance of software, particularly in the context of the widespread use of web and mobile technologies. The authors present the results of a survey conducted among 59 Italian IT companies to understand their experiences and goals in software migration, as well as the technologies they have adopted. The survey focused on in-house migration projects towards the web, service-oriented architectures, and wireless environments. The results show that software migration is a significant phenomenon, with most companies focusing on migrating to the web. However, there is a limited availability and usage of tools to support migration tasks.
412209	41220935	Code Your Own Game: The Case of Children with Hearing Impairments.	The computer science community emphasizes the importance of teaching children coding skills to foster their creativity and not just consume games. However, Deaf and Hard of Hearing children have specific needs when it comes to coding and making activities, even with accessible visual translations. A workshop program was developed and evaluated with 12 DHH children to improve the teaching of coding in current learning environments. A set of guidelines was created through a focus group and content analysis to aid special education teachers, curriculum designers, and K-12 education developers in providing appropriate workshops for DHH children. This initial evaluation shows promise in enhancing the learning experience for DHH students in coding.
412209	41220922	Open source software for entertainment	This tutorial focuses on open source software practices and tools that are suitable for creating interactive and playful systems. The use of open source tools such as Processing and Arduino has increased participation from both technical and non-technical users in the creative production of interactive systems. Maker communities regularly meet to share resources and knowledge for creative hacking, fun, and networking. The tutorial presents related work and experiences in organizing and running maker workshops, with the goal of inspiring participants to experience open software practices and tools. The tutorial also proposes research questions to explore the relationship between open source software and entertainment. 
412210	41221031	Leftover Hash Lemma, revisited	The Leftover Hash Lemma (LHL) is a well-known concept that states that universal hash functions can be used as good randomness extractors. However, LHL-based extractors have two limitations: large entropy loss and large seed length. These limitations can be overcome in certain scenarios, such as deriving secret keys for cryptographic applications. Additionally, a general computational extractor has been developed that can be used for any cryptographic application with low entropy loss. The expand-then-extract approach, which uses a pseudorandom generator to expand a short seed into a longer one before using it in the LHL, has also been studied. It is not always sound, but it can be used with a small number of bits or in certain cases, such as minicrypt. This suggests that it may be secure to use practical pseudorandom generators, even without a proof of security.
412210	4122105	Entropic security and the encryption of high entropy messages	Entropic security is an information-theoretic concept introduced by Russell and Wang and Canetti et al. in the context of encryption and hash functions. It means that knowledge of the output of a probabilistic map (such as an encryption or hash function) does not help predict any function of the input with high min-entropy. This is stronger than the original formulation and is equivalent to indistinguishability on high-entropy input distributions. New results on entropically-secure encryption are then presented, including two general frameworks for construction and lower bounds on the required key length. These results show the near-optimality of the constructions and provide improved parameters.
412211	41221145	Fixed Parameter Algorithms For The Minimum Weight Triangulation Problem	This article discusses and compares four different algorithms for finding the minimum weight triangulation of a simple polygon with a total of n vertices, where (n - k) vertices are on the perimeter and k vertices are in the interior. All four algorithms use a divide-and-conquer approach and dynamic programming to improve efficiency. The algorithms are based on two observations about triangulations - triangle splits and path splits. The first two algorithms use only one type of split, while the last two combine both types for better performance. The authors aim to simplify and deepen the understanding of these algorithms by discussing them and presenting experimental results from a Java implementation.
412211	4122118	A fixed parameter algorithm for the minimum number convex partition problem	 The MNCP problem involves finding the minimum number of convex pieces that can partition a given input of an n-vertex convex polygon with k holes or an n-vertex PSLG with k holes and/or reflex vertices. A fixed-parameter tractable algorithm has been developed for this problem, with time complexities of linear time if k is constant, polynomial time in n if k is O(log n/log log n), or O(nk^6k-5 216k) time otherwise.
412212	41221273	Semantic representation: search and mining of multimedia content	The ability to understand the meaning of multimedia content is crucial for effective access to digital media data. Semantic content descriptions greatly improve the value of large media repositories by making them searchable. However, automatic semantic understanding is a difficult task and most databases rely on low-level features or manual annotations. Recent techniques focus on detecting semantic concepts in videos, such as indoor, outdoor, face, etc. This requires a fixed lexicon and annotated training examples. This paper discusses the use of semantic concept detection to map videos into semantic spaces, creating a compact representation of the content. Experiments on a large video corpus show improved performance in retrieval, classification, visualization, and data mining using this approach. 
412212	4122129	Multimedia content-based indexing and search: challenges and research directions	The rapid growth of digital multimedia content and the increasing variety of distribution channels has raised users' expectations for easy accessibility and searchability. However, current indexing and search methods are struggling to keep up with the vast amount of content being generated. To address this issue, recent advancements in multimedia content analysis are being utilized to effectively tag and categorize multimedia content. A new system called Marvel is being developed, which uses statistical machine learning and semantic concept ontologies to model, index, and search multimedia content using audio, speech, and visual elements. This system aims to reduce manual processing and improve the ability to unlock the value of large multimedia repositories.
412213	41221334	Conjugacy in Free Inverse Monoids.	Conjugacy, a concept commonly used in groups, can also be applied to monoids in two ways: the first version defines conjugacy as two elements being equal when one can be obtained by rearranging the other, while the second version defines it as two elements being equal when they can be transposed. By using Munn's characterization of elements in free inverse monoids, it is shown that when restricted to non-idempotent elements, the relation of conjugacy is the transitive closure of the relation of transposition. Additionally, it is proven that testing for conjugacy between two elements in a free inverse monoid can be done in linear time.
412213	41221345	On Extendibility of Unavoidable Sets	A subset X of a free monoid is considered unavoidable if almost all words in the monoid contain a subword from X. A conjecture by A. Ehrenfeucht suggests that every unavoidable set is also extendible, meaning that by removing one element and adding a new one, the set remains unavoidable. This conjecture is still unsolved, but some progress has been made in finding partial solutions and developing efficient methods to test for properties related to this problem.
412214	41221417	The weight hierarchy of the Kasami codes	The Kasami codes are a group of codes with lengths of [2 2 m − 1, 3 m , 2 2 m −1 − 2 m −1 ]. These codes are commonly used to create sequences with ideal correlation values. The weight hierarchy of the Kasami codes is fully defined, and it has been proven that the chain condition applies to these codes.
412214	41221498	3-Designs from the Z4-Goethals Codes via a New Kloosterman Sum Identity	 This paper discusses the construction of a new type of design, called a t-design, using linear codes over Z4. Specifically, the authors construct a simple 3-design using codewords from the Z4-Goethals code for odd m ≥ 5. The design has parameters 3 − (2m, 7, \frac{14}{3} (2m − 8)) and is obtained by counting the number of codewords of Hamming weight 7 in the code that contain any 3 chosen positions. This counting is simplified using the double-transitivity of the code and is related to Kloosterman sums. The paper also presents a new identity for Kloosterman sums as a result of this construction.
412215	41221545	Analysis of the 802.11e enhanced distributed channel access function	We have developed a mathematical model to analyze the performance of the enhanced distributed channel access (EDCA) function in the IEEE 802.11e standard. Our model uses a discrete-time Markov chain (DTMC) and takes into account important quality-of-service (QoS) features such as contention window, arbitration interframe space, and transmit opportunity differentiation. This model considers the state of the MAC layer buffer, as well as MAC differentiation mechanisms, to provide a comprehensive analysis of performance under different traffic loads. Our results, obtained through both analytical and simulation methods, demonstrate the accuracy of our model for various EDCA parameters and MAC layer buffer sizes. 
412215	4122154	Performance Analysis of the IEEE 802.11e Enhanced Distributed Coordination Function Using Cycle Time Approach	The IEEE 802.11e standard has been recently ratified and it introduces the enhanced distributed channel access (EDCA) function for providing quality-of-service (QoS) in wireless local area networks (WLANs). EDCA uses CSMA/CA and BEB mechanisms and a mathematical analysis framework has been developed to analyze its performance. This analysis takes into account the cyclic nature of distributed random access systems, where each station successfully transmits a packet in a cycle. The results show that there is a specific cycle time for each access category (AC) in EDCA. By validating the results through simulations, it is shown that this analysis accurately predicts the saturation performance of EDCA in terms of throughput, medium access delay, and packet loss ratio. This cycle time analysis is a simpler and more insightful alternative to previous complex EDCA models.
412216	41221687	Optimal Neighborhood Kernel Clustering with Multiple Kernels.	Multiple kernel k-means (MKKM) is a clustering algorithm that aims to improve performance by learning an optimal kernel, which is a linear combination of pre-specified base kernels. However, this approach may limit the kernel's representation capability and not fully consider the negotiation between kernel learning and clustering, resulting in unsatisfactory performance. To address these issues, an optimal neighborhood kernel clustering (ONKC) algorithm is proposed, which enhances the representability of the optimal kernel and strengthens the negotiation between learning and clustering. Theoretical justification shows that existing MKKM algorithms are a special case of ONKC and that it can be extended for better clustering algorithms. An efficient algorithm is designed to solve the optimization problem with proven convergence. Experimental results demonstrate the superior performance of ONKC compared to state-of-the-art algorithms.
412216	41221652	Multiple Kernel k-Means Clustering with Matrix-Induced Regularization.	Multiple kernel k-means (MKKM) clustering aims to improve clustering performance by optimally combining a group of pre-specified kernels. However, existing MKKM algorithms do not adequately consider the correlation among these kernels. This can lead to selecting redundant kernels, which decreases the diversity of information sources used for clustering and ultimately harms performance. To address this issue, this paper proposes an MKKM clustering approach with a novel matrix-induced regularization that reduces redundancy and enhances kernel diversity. This regularization is theoretically justified by its connection to the commonly used kernel alignment criterion. Experimental results on five benchmark data sets demonstrate the effectiveness and advantages of this approach compared to existing MKKM algorithms, indicating its potential for designing better clustering algorithms. 
412217	41221739	Design, Implementation and Deployment of State Machines Using a Generative Approach	The approach discussed in this article involves using a single abstract model to generate a family of finite state machines for designing and implementing a distributed system. The state machines are used to formalize the interactions between system components, leading to increased confidence in correctness. The generated artefacts include diagrams, protocol implementations, and documentation. This methodology allows state machines to be applied to problems that may not have been suitable for them before. The authors demonstrate this approach with a case study of a Byzantine-fault-tolerant commit protocol in a distributed storage system. They explain how the abstract model is defined in terms of an abstract state space and state transitions, and propose a general methodology for using this technique in system development.
412217	4122170	Casper: A Cached Architecture Supporting Persistence	Persistent object systems are beneficial for programmers as they eliminate the need to differentiate between short-term and long-term storage. This allows for a higher level of abstraction where both types of data are treated equally. Concurrency is important in such systems, whether due to parallel processes or multiple users. However, current implementations lack satisfactory support for concurrent access, and there are still research areas to be explored. This paper presents an architecture that enables concurrent access to a shared persistent object store. The system is based on the Mach distributed operating system and utilizes its features to create a persistent distributed architecture. 
412218	41221813	Automatic Reactive Adaptation of Pervasive Applications.	 the vision of a technology-rich environment that provides seamless and distraction-free support for everyday tasks.Pervasive Computing is a concept that envisions seamless and distraction-free support for everyday tasks through distributed applications. This is achieved by leveraging the resources of the users' environment and adapting to changes in the execution environment. To efficiently create adaptive applications, developers require a suitable framework. This paper presents and evaluates an approach for adapting a pervasive computing application during its execution. The approach is based on the PCOM system and an algorithm for automating the initial configuration of a component-based application. The paper describes modifications to the component model to enable automatic adaptation, proposes a cost model to capture the complexity of adaptations, and presents an online optimization heuristic to choose a low-cost configuration. The ultimate goal of Pervasive Computing is to create a technology-rich environment that seamlessly supports everyday tasks without causing distraction.
412218	4122188	Good Manners for Pervasive Computing--An Approach Based on the Ambient Calculus	Human interactions are governed by specific rules that dictate how we speak, greet, and behave in different environments. These social codes vary depending on the context and breaking them can lead to being perceived as uneducated or foreign. With the rise of Pervasive Computing, where applications will be present in our daily environment, there is a need for a new set of manners or codes to regulate the interactions between these applications. This new code will have to consider both existing social codes and technical aspects. A proposed solution is to use the ambient calculus to specify and enforce these manners for Pervasive Computing applications.
412219	41221922	Information security underlying transparent computing: Impacts, visions and challenges	The advancement of computer networks and social information has created both opportunities and challenges in information security. As more people share information and services, the need to strengthen security measures has become crucial. This paper proposes a new network security mechanism called transparent computing, which is based on the extended von Neumann architecture. This separates program storage and execution in the network environment, utilizing a new generation server and client BIOS called EFI BIOS, and coordinated with the MetaOS management platform and related devices. By controlling data and instructions in a block-streaming way, it effectively manages security and protects against malicious software such as worms and Trojan horses. Detailed examples are provided to showcase the benefits and potential of this security approach.
412219	412219136	A Tensor-Based Framework for Software-Defined Cloud Data Center.	Multimedia has become a major component of big data, consisting of video, images, and audio files. Processing and analyzing these large amounts of unstructured data has led to the use of cloud data centers as a preferred solution. However, there are challenges in this approach, such as data representation, efficient networking, and traffic pattern estimation. To address these challenges, a novel tensor-based software-defined networking model is proposed for multimedia big data computation and communication on a cloud data center. This framework includes three tensor models for managing networking devices and predicting traffic patterns, and two algorithms for efficiently updating eigen-vectors and eigen-tensors. Experimental results show that this framework is both feasible and efficient for handling multimedia big data on a cloud data center.
412220	41222028	On Store Languages of Language Acceptors.	Pushdown automata are a type of language acceptor that use a stack to keep track of intermediate steps in a computation. The "store language" of a pushdown automaton, which includes all possible state and stack configurations during a computation, is known to be a regular language. This concept is explored further by examining different models of language acceptors and their store languages. Connections between one-way and two-way machines, as well as between deterministic and nondeterministic machines, are also discussed. The results of these studies have practical applications, such as a technique for proving that families of languages accepted by deterministic machines are closed under right quotient with regular languages. Additionally, lower bounds on the space complexity of Turing machines with non-regular store languages are determined. 
412220	412220119	Generalizations Of Checking Stack Automata: Characterizations And Hierarchies	In this article, we explore different extensions of stack automata that involve multiple input heads and stacks. We examine their computational abilities and compare them to two-way multi-head finite automata and space-bounded Turing machines. We establish hierarchies for these models and show how they relate to existing results. Additionally, we touch on questions of decidability and the complexity of these models in terms of space and time. Overall, our findings offer a deeper understanding of the power and limitations of these extended stack automata.
412221	41222147	Coalgebraic Hybrid Logic	This article introduces a framework called coalgebraic hybrid logic, which allows for reasoning about individual states in a model using modal logics with nominals and satisfaction operators. This framework is as general as coalgebraic modal logic and encompasses various logics with non-normal modal operators such as probabilistic, graded, or coalitional modalities and non-monotonic conditionals. The authors prove a generic finite model property and a weak completeness result, and provide a semantic criterion for decidability in PSPACE. They also present a fully internalised PSPACE tableau calculus. These generic results can be applied to specific hybrid logics, resulting in new findings such as the decidability of probabilistic and graded hybrid logics in PSPACE.
412221	4122211	EXPTIME tableaux for the coalgebraic µ-calculus	The coalgebraic approach to modal logic is a method that allows for the representation of various types of modal logics, such as graded and probabilistic modal logics, in a unified framework. This approach is extended in this paper with the introduction of the coalgebraic µ-calculus, which includes fixpoint operators. The paper proves the completeness of the associated tableau calculus and shows that the satisfiability problem can be reduced to the existence of winning strategies in parity games. This approach is applicable to a wide range of models and provides new complexity bounds for the probabilistic µ-calculus and an extended version of coalition logic.
412222	41222262	Twitinfo: aggregating and visualizing microblogs for event exploration	Microblogs, such as Twitter, contain a vast amount of user-generated content about world events. However, the chronological organization of posts makes it difficult for users to get a detailed understanding of an event. TwitInfo is a system that addresses this issue by providing a timeline-based display of tweets, highlighting peaks of high activity. A unique streaming algorithm automatically identifies and labels these peaks using text from tweets. Users can explore further through geolocation, sentiment, and popular URLs. The system also includes a recall-normalized sentiment visualization for more accurate overviews. Evaluations have shown that users can reconstruct meaningful event summaries quickly, and a Pulitzer Prize-winning journalist has confirmed its usefulness for understanding long-running events and identifying eyewitnesses. Quantitatively, the system can identify 80-100% of manually labeled peaks, providing a comprehensive view of each event.
412222	4122225	Short and tweet: experiments on recommending content from information streams	 The paper discusses the importance of content recommendation on popular micro-blogging site Twitter. The study focuses on designing a recommender system that considers three aspects: content sources, topic interest models for users, and social voting. The researchers implemented 12 recommendation engines and deployed them on the web to gather feedback from real Twitter users. The best performing algorithm increased the percentage of interesting content from 33% to 72%. The paper concludes by discussing the implications of their design and how it can be applied to other information streams. 
412223	41222351	Multiple Coloring of Cone Graphs	This paper discusses the $k$th chromatic number of cone graphs, which are created by adding a "cone" of vertices to an existing graph. The $k$th chromatic number, denoted by $\chi_k(G)$, is the minimum number of colors needed to assign to each vertex in a $k$-fold coloring of the graph $G$. An upper bound for $\chi_k(\mu_m(G))$, the $k$th chromatic number of the cone graph of $G$, is presented in terms of $\chi_k(G)$, $k$, and $m$. It is also shown that there is a connection between the $k$th chromatic number of the cone graph and the circular chromatic number of $G$. Surprisingly, if $\chi_k(G)$ and $\chi_c(G)$ are even, then for sufficiently large $m$, the $k$th chromatic number of the cone graph is equal to the $k$th chromatic number of $G$.
412223	412223100	Circular consecutive choosability of k-choosable graphs	The circular consecutive choosability of a graph is the minimum number of colors needed to assign to its vertices so that each vertex can be assigned a color from a closed interval on a circle of a given circumference. The relationship between this and the traditional choosability of a graph is investigated, and it is proven that for k-choosable graphs, the circular consecutive choosability is at most k+1-1/k. The equality holds for k≥3, and for k=2, the circular consecutive choosability is at most 2 if and only if the graph contains a cycle. However, there are some graphs that are circular consecutive 2-choosable but not 2-choosable. The circular consecutive choosability is found to be 2 for all cycles and for K2,n with n≥2. Some generalized theta graphs also have a circular consecutive choosability of 2.
412224	41222416	Parametric Yield Analysis and Constrained-Based Supply Voltage Optimization	Parametric yield loss is a major concern in technologies where leakage is dominant. This paper discusses the impact of leakage on parametric yield and how it can shrink the yield window. The authors present a mathematical framework for estimating yield under device process variation with power and frequency constraints. The model is validated through simulations and shown to have less than 5% error. The importance of optimal supply voltage selection for maximizing yield is also demonstrated, along with the sensitivity of yield to frequency and power constraints. Finally, the framework is applied to the problem of maximizing shipping frequency while meeting yield and power constraints.
412224	41222441	On-Chip Process Variation Detection Using Slew-Rate Monitoring Circuit	In the current era of nm design, it is crucial to have efficient and accurate detection schemes to combat the impact of process variations on the parametric yield of integrated circuits. A new variation detection technique is proposed in this paper, which uses both slew and delay as metrics to determine the mismatch between the drive strengths of NMOS and PMOS devices. The paper highlights the importance of considering both metrics and presents a new slew-rate monitoring circuit for measuring the slew of a signal from the critical path of a circuit. The design considerations, simulation results, and characteristics of the circuit in a 45 nm SOI technology are discussed, achieving a sensitivity of 1MHz/ps and detecting threshold voltage variations in the order of mV with 0.95MHz/mV sensitivity.
412225	41222540	A GPU-based Streaming Algorithm for High-Resolution Cloth Simulation.	Our team has developed a GPU-based streaming algorithm for efficient and accurate cloth simulation. By utilizing GPU-based kernels and data structures, we were able to map all components of the simulation pipeline, including time integration, collision detection and response, and velocity updating. Our algorithm can handle both intra-object and inter-object collisions, as well as contacts and friction, resulting in realistic simulations of folds and wrinkles. We also discuss the challenges in achieving high throughput on many-core GPUs and demonstrate the parallel performance of our algorithm on various GPU generations. On a high-end NVIDIA Tesla K20c, we observed a significant improvement in performance compared to a single-threaded CPU-based algorithm and a 16-core CPU-based parallel implementation. Our algorithm can accurately simulate a cloth mesh with 2M triangles using only 3GB of GPU memory.
412225	41222517	Efficient and Reliable Self-Collision Culling Using Unprojected Normal Cones.	Our algorithm efficiently detects self-collisions in deformable models by performing both discrete and continuous collision queries on triangulated meshes. It uses a simple and fast linear time approach to perform the normal cone test, reducing it to a series of point-plane classification tests. Additionally, we introduce a hierarchical traversal scheme that reduces the number of normal cone tests and memory usage through front-based normal cone culling. This allows for reliable detection of all collisions in models with hundreds of thousands of triangles. Our algorithm outperforms previous continuous collision detection methods, providing improved performance for complex models.
412226	41222628	Soft Articulated Characters With Fast Contact Handling	The handling of soft articulated characters in computer animations is a difficult task due to the complex interaction between the character's skeleton and surface deformation. To address this issue, a new algorithm has been developed that uses a layered representation to simulate the movement and deformation of high-resolution characters in real time. This algorithm effectively captures the dynamic relationship between the skeleton and skin by incorporating elastic deformation into the character's pose space. It also overcomes computational challenges by separating skeleton and skin computations and efficiently handling collisions using the layered representation. With this approach, the algorithm can handle large contact areas, produce realistic surface deformations, and accurately simulate the impact of collisions on the character's skeleton. 
412226	41222662	Capture and modeling of non-linear heterogeneous soft tissue	This paper proposes a data-driven approach for simulating non-linear heterogeneous soft tissue. By acquiring example deformations of a real object and representing them as stress-strain relationships in a finite-element model, the construction of deformable models is simplified while maintaining the complexity of non-linear behavior. The material is modeled through non-linear interpolation of these relationships, using a simple capture system and efficient simulation algorithm. The method is suitable for interactive computer graphics applications and has been successfully applied to various non-linear materials and biological soft tissue, with high accuracy compared to measured data.
412227	41222730	StereoPasting: interactive composition in stereoscopic images.	"StereoPasting" is a proposed method for efficiently combining 2D and stereoscopic images. By painting "disparity" on a 2D image, a disparity map is gradually created and blended with the target stereoscopic image. The process takes into account expected disparities and perspective scaling, and the 2D object is warped to generate an image pair. This warping is solved in real time through energy minimization. An interactive composition system allows users to edit disparity maps and view results instantly. Experiments have shown the effectiveness and versatility of this method in various applications.
412227	41222737	Content-aware copying and pasting in images	The authors propose a new image copy-and-paste technique that combines elements from matting and gradient-based methods. They make modifications to the diffusion process in the gradient-based method by using the alpha matte of the cloned area as a weight function. This preserves the color style of the selected region when pasting. The approach is implemented using mean-value coordinates and can be efficiently executed on a GPU. Experimental results demonstrate the effectiveness of this method.
412228	4122281	Analyzing settings for social identity management on Social Networking Sites: Classification, current state, and proposed developments.	The increasing use of Social Networking Sites (SNS) has led to new privacy challenges and has prompted users to manage their online identity. This is referred to as Social Identity Management (SIdM), which involves deliberately disclosing personal information to a select group of contacts or other users on the SNS. However, protection against entities such as the site operator or advertisers is not covered by SIdM and needs to be addressed separately. While some SNS have implemented features and settings for SIdM, there is a lack of integration and a reference framework to guide users in managing their identity. This article proposes a reference framework for SIdM based on identity theory, literature analysis, and existing SNS. It also examines the current SIdM capabilities of popular SNS and suggests possible improvements. Lastly, the idea of developing a metric to compare the SIdM capabilities of different SNS is discussed.
412228	41222845	Patterns and Pattern Diagrams for Access Control	Access control is crucial for maintaining security, but there are many different models available, making it challenging for software developers to choose the right one for their application. This often leads to the use of basic models and the neglect of more advanced ones. To address this issue, pattern diagrams can be used to navigate the pattern space and clarify the relationships between different models. This also helps identify available patterns and those that need to be created. Additionally, pattern maps can aid in semi-automatic model transformations for Model-Driven Development. The goal is to provide designers with a tool to select appropriate security patterns from a catalog and to understand how to modify and create new models using existing patterns.
412229	41222957	Extinction-based shading and illumination in GPU volume ray-casting.	Direct volume rendering is a popular method for visualizing volumetric data, but it is still a challenge to incorporate sophisticated illumination models without sacrificing interactive frame rates. This paper presents a new approach for advanced illumination in direct volume rendering using GPU ray-casting. This method includes directional soft shadows, ambient occlusion, and color bleeding effects while maintaining competitive frame rates. It also supports dynamic lights and interactive transfer function changes. Unlike the commonly used a-blending method, which simplifies the original volume rendering integral, this approach uses the original exponential extinction coefficient, which can be cleverly implemented on the GPU to achieve volume lighting effects.
412229	41222933	TAMRESH - tensor approximation multiresolution hierarchy for interactive volume visualization	The challenge of visually analyzing large and complex volume datasets is addressed through a hierarchical tensor approximation (TA) approach. This allows for a compact representation of the data, visualization of features at multiple scales, and rendering at multiple resolutions. The TA factor matrix bases are utilized to define a novel multiscale and multiresolution volume rendering hierarchy, using one set of global bases for all levels and scales. This approach combines multiscalable feature visualization and multiresolution direct volume rendering, and is demonstrated on several μCT datasets. 
412230	41223045	Freeness analysis in the presence of numerical constraints	This paper discusses a new method of abstract interpretation for dealing with mixed systems of numerical and unification constraints. The proposed abstraction is able to effectively handle the interaction between these two types of constraints, extending the concept of possible sharing information in logic programs to include possible dependencies established by constraint systems. This information can be used for low-level optimizations and to delay the addition of constraints in computation, allowing for constraint specialization. When combined with other analyses, this derived information can greatly improve the compilation of Constraint Logic Programming (CLP) programs, reducing the need for a general constraint solver. 
412230	41223020	Inference and learning in probabilistic logic programs using weighted Boolean formulas.	This paper discusses probabilistic logic programs, which are programs that include probabilities for some facts. The paper investigates how to perform classical inference and learning tasks for these programs, which have not been well-addressed before. The paper presents a suite of efficient algorithms for performing tasks such as computing probabilities and learning from partial information. These algorithms are based on converting the program and queries to a weighted Boolean formula, which allows for the use of established methods from graphical models and knowledge compilation. The paper also introduces an algorithm for parameter estimation in the learning setting, which utilizes expectation-maximization. Experimental results demonstrate the effectiveness of these algorithms and the feasibility of learning from interpretations in probabilistic logic programs. 
412231	41223141	Vertex characterization of partition polytopes of bipartitions and of planar point sets	The partition problem involves dividing a set of vectors in d-dimensional space into p parts in order to maximize a convex objective function. This problem has practical applications in various fields such as clustering, inventory management, scheduling, and statistical testing. The convexity of the objective function allows the problem to be reduced to maximizing it over a polytope, which is the convex hull of all possible solutions. This article explores the vertices of the partition polytope when d=2 or p=2, and determines the maximum number of vertices for these cases. The results provide insights into the time complexity needed to solve the partition problem using an oracle. 
412231	41223149	Monotone optimal multipartitions using Schur convexity with respect to partial orders	The (t, n, m)-multipartitioning problem involves dividing t lists of ordered numbers into n sets, with each set containing m numbers from each list. The objective is to maximize a partition function that depends on the sum of elements in each set. The authors use the theory of majorization and Schur convexity to study optimal multipartitions for this problem. They also use these results to disprove a conjecture by Du and Hwang, which suggests that Schur convex functions can be characterized as the partition functions for (1, n, m)-multipartitioning problems with monotone optimal solutions. 
412232	41223235	The Vapnik-Chervonenkis dimension of graph and recursive neural networks.	The Vapnik-Chervonenkis dimension (VC-dim) is a measure of the sample learning complexity of a classification model and is often used to evaluate its generalization capability. While the VC-dim has been studied for common feed-forward neural networks, this paper explores its application to Graph Neural Networks (GNNs) and Recursive Neural Networks (RecNNs). These models are capable of processing graph inputs, which are a more general form than sequences and vectors. The paper provides upper bounds on the VC-dim of GNNs and RecNNs, showing that their generalization capability increases with the number of connected nodes. This finding is similar to previous results for recurrent neural networks, which also have a higher VC-dim compared to traditional neural networks. 
412232	41223241	Recurrent Neural Network Architectures: An Overview	The paper discusses recurrent neural network architectures, dividing them into two subclasses. It is found that all popular recurrent neural network architectures fall into one of these subclasses and cannot be transformed into the other. Two newer architectures designed for specific purposes are also examined. The paper then suggests that once the architectural aspects are settled, the training process can be further explored in a separate paper. 
412233	412233128	CASA-Based Robust Speaker Identification	Conventional speaker recognition systems struggle to accurately identify speakers in noisy environments. To address this, researchers have turned to computational auditory scene analysis (CASA), which uses a time-frequency mask to separate speech signals. In this study, the researchers introduce a new speaker feature called gammatone frequency cepstral coefficient (GFCC), which is based on an auditory model and outperforms traditional speaker features in noisy conditions. To improve performance, CASA separation is applied and either reconstructed or marginalized components are used based on the mask. Combining these methods results in a highly effective system that outperforms other similar systems in a variety of noise levels.
412233	41223313	Robust speaker identification using auditory features and computational auditory scene analysis	The effectiveness of speaker recognition systems decreases in noisy environments. To address this issue, new auditory features and a robust recognition system were proposed based on computational auditory scene analysis. The features were further studied by varying dimensions and including dynamic features. They were evaluated in a speaker identification task under various noisy conditions and one of the auditory features showed better performance compared to conventional speaker features. The recognition system also outperformed an advanced front-end in different signal-to-noise conditions. This highlights the potential of using auditory features and a robust recognition system for handling noisy environments in speaker recognition.
412234	41223416	A Note on Multiple-Access Channels with Strictly-Causal State Information	This content discusses a new inner bound for the capacity region of a memoryless multiple-access channel that is influenced by a known, strictly causal state. The proposed bound includes previous bounds and is shown to be strictly better through an example. A variation of this example is applied to a scenario where the channel is influenced by two independent state sequences, with each transmitter having knowledge of one of the states. This proves a previous conjecture by Li et al. that their inner bound for this scenario is indeed better than previous bounds.
412234	41223427	On the Capacity of the Discrete-Time Poisson Channel	The article discusses the asymptotic capacity of a discrete-time Poisson channel, taking into consideration peak-power and average-power limitations. A new lower bound and an asymptotic upper bound are derived, with the upper bound utilizing the concept of capacity-achieving input distributions that escape to infinity. The lower bound is based on the entropy of a conditionally Poisson random variable, using the differential entropy of its conditional mean. These bounds provide a better understanding of the asymptotic behavior of the channel capacity and can be useful in designing efficient communication systems.
412235	41223532	Power efficient rate monotonic scheduling for multi-core systems	Current real-time systems offer more computational power to handle CPU intensive applications, but this leads to increased energy consumption and heat dissipation. To address these issues, system speed is adjusted in real-time to meet application deadlines and reduce overall energy consumption. The use of multi-core technology also presents opportunities for energy reduction through efficient scheduling, but there is limited research on this topic. This paper proposes a technique for finding the lowest core speed for scheduling tasks, which is shown to be superior to existing methods. Additionally, a task shifting policy is used to balance core utilization and determine a uniform system speed for a given task set, ensuring that all tasks meet their deadlines and energy consumption is reduced.
412235	41223541	Survey on Grid Resource Allocation Mechanisms	Grid is a distributed computing model that provides resources such as computing power, storage, and communication for resource-intensive user tasks. Efficient task scheduling is crucial for achieving high system throughput and meeting user requirements. However, the increasing complexity of Grid systems has made task scheduling more challenging. This paper examines existing resource allocation mechanisms for Grid systems, including centralized, distributed, static, and dynamic architectures. The mechanisms are compared based on various factors such as time complexity, search method, allocation strategy, optimality, and objective function. This comprehensive analysis helps readers understand important concepts and issues in Grid resource allocation, aiding them in selecting the most suitable mechanism for their system or application. 
412236	41223628	Dynamic Organization Schemes for Cooperative Proxy Caching	In a cooperative caching architecture, web proxies work together in a mesh network. If a proxy cannot fulfill a request, it is sent to other nodes in the network. However, since local caches can only satisfy a small percentage of requests, this can result in a high volume of queries being directed to neighboring nodes, which can strain system resources. Due to variations in traffic, overlapping content, and distance between nodes, it is not necessary for each proxy to cooperate with every node in the mesh. Limiting the number of neighboring nodes can also help with scalability, as having N proxies in a mesh can lead to a high number of queries (N2). Therefore, it is beneficial to restrict the number of neighbors for each proxy to a certain value, k.
412236	41223685	Dynamic scheduling of multimedia documents in a single server multiple clients environment	In a single server and multiple client distributed multimedia system, the server needs to quickly respond to clients' requests for multimedia documents while guaranteeing quality of service. However, existing scheduling algorithms have two major drawbacks: they assume all channels are available at the beginning and ignore the cost of scheduling. To address this, a feasible scheduling algorithm should be real-time, have low scheduling cost, and handle multiple requests. This paper proposes two dynamic scheduling algorithms with a worst time complexity of O ( n log nm + nm ), which adjusts scheduling times based on the slack time between requests. This reduces response time and maintains presentation quality. Simulation and analysis show the effectiveness of these algorithms in a realistic environment.
412237	41223750	MM: A bidirectional search algorithm that is guaranteed to meet in the middle.	Bidirectional search algorithms combine two separate searches, one starting from the initial state and the other starting from the goal state. While adding a heuristic to unidirectional search greatly reduces the search effort, bidirectional heuristic search has not been as successful despite years of research. This is due to the lack of a comprehensive understanding of its nature. This paper introduces a new bidirectional heuristic search algorithm called MM, which guarantees that the forward and backward searches will meet in the middle. This allows for a unique comparison with A* and their brute-force variations by dividing the state space into sections based on their distance from the start and goal. Experimental results support the theoretical analysis and identify the conditions where each algorithm is most effective.
412237	41223721	Automatic move pruning for single-agent search.	Move pruning is a technique used to reduce the cost of single-agent search problems by avoiding the generation of duplicate states. This is achieved by suppressing redundant sequences of moves that have the same effect as other sequences. Researchers have developed an algorithm for automatically identifying these redundant move sequences and a method for selecting which ones to prune during search. Careful selection of which sequences to prune is crucial for the success of this technique. Experimental results have shown significant speedups in various domains using this method. The researchers also discuss theoretical results on when move pruning may or may not affect the correctness of different search algorithms.
412238	4122385	Stochastic link and group detection	Link detection and analysis has been crucial in the social sciences and government intelligence community. While most efforts focus on analyzing known networks, the abundance of circumstantial data from the internet and other sources has led to the need for probabilistic evidence of links. To address this, a probabilistic model of link generation based on group membership has been proposed. This model takes into account both observed link evidence and demographic information of the entities. The parameters of the model are learned through a maximum likelihood search, which is made tractable through various heuristics. The model and optimization methods are tested on both synthetic data with known ground truth and a database of news articles.
412238	41223851	Actively learning level-sets of composite functions	Scientists often have multiple experiments and data sets to test the validity of their models and determine possible values for model parameters. By examining multiple data sources, scientists can gather more informative inferences compared to analyzing each data source independently. Standard techniques for combining data involve creating a weighted sum of the observed data, and the process of finding constraints on the model parameter space can be expressed as finding a level-set of this target function. This study proposes an active learning algorithm that selects both a sample from the parameter space and an observable function to compute the next sample. Tests on synthetic and real data show that this algorithm significantly reduces the number of samples needed to identify the desired level-set.
412239	412239134	Productivity of test driven development: a controlled experiment with professionals	The popularity of Extreme Programming has led to a rise in interest for test driven development (TDD). As a result, many experiments have been conducted to determine if TDD is more effective than the traditional method of testing code after it has been written (TAC). However, research on TDD is still in its early stages and there is limited knowledge on the subject. This paper presents an experiment conducted in a Spanish software company to compare productivity between TDD and TAC. The results suggest that TDD may be more efficient, but further research is needed to fully understand its benefits.
412239	41223987	Evaluating performances of pair designing in industry	Pair programming, a practice where two developers work together on a single task, has gained attention from both practitioners and researchers. Evidence from controlled experiments has shown that it can lead to improved quality and delivery time in software development. This practice can also be applied to other phases of the software process, such as analysis, design, and testing. However, applying pair programming to the design phase may not have the same benefits as in the development phase due to the asymmetry between design and coding. A controlled experiment was conducted in a software company to investigate the effects of pair programming in the design phase. The results showed that although it may slow down the task, it can lead to improved quality. These findings were similar to a previous exploratory experiment involving students. 
412240	41224060	Assessing the usability of a visual tool for the definition of e-learning processes	The paper discusses a usability study of a visual language-based tool called ASCLO-S, which is designed for developing adaptive e-learning processes. The tool uses flow diagrams to define different classes of learners and determine the most suitable adaptive learning process for each class. The study used a questionnaire-based survey and an empirical analysis to gather feedback and measure the usability of the tool. The results showed that both the visual notation and the system prototype were user-friendly for instructional designers, regardless of their experience with computers or e-learning tools. The empirical analysis also revealed that the effort required to develop e-learning processes was not affected by the instructional designer's experience, but rather by the size of the process.
412240	41224088	Search-Based Testing of Procedural Programs: Iterative Single-Target or Multi-target Approach?	Researchers have proposed search-based methods for automatically generating test suites for Object-Oriented (OO) software systems. These methods consider all targets (e.g., branches) defined by the coverage criterion simultaneously, in what is known as a multi-target approach. This approach aims to overcome the problem of wasted search budget that can occur with iterative single-target approaches, which generate test cases for each target separately. However, this approach has not been studied in the context of procedural programs. This paper presents OCELOT, a tool for generating test data for C programs that implements both a whole suite approach and a more efficient iterative single-target approach. An empirical study on 35 open-source C programs shows that the iterative single-target approach achieves the same or higher coverage levels with higher efficiency compared to the whole suite approach.
412241	41224134	On Relating Heterogeneous Elements from Different Ontologies	The majority of distributed ontology integration formalisms utilize mapping languages to express semantic relations between concepts of different ontologies. These mappings, known as homogeneous mappings, can handle a large portion of heterogeneities between ontologies. However, there is a need for mapping languages that can also express semantic relations between properties, known as heterogeneous mappings. This is necessary to reconcile different representations of the same real-world aspect in different ontologies. The authors have proposed a rich mapping language that incorporates both homogeneous and heterogeneous mappings, extending the semantics of Distributed Description Logics (DDL). They provide a sound and complete axiomatization for these mappings, which is a crucial step towards representing an arbitrary network of ontologies.
412241	41224136	A Context-Based Logic for Distributed Knowledge Representation and Reasoning	This paper introduces a logic, called Distributed First Order Logic (DFOL), for formalizing distributed knowledge representation and reasoning systems. These systems consist of different subsystems, each with its own language, partial knowledge, and reasoning capabilities. These subsystems can exchange knowledge through query answering. DFOL represents each subsystem as a context with its own language, basic facts, and inference rules. Knowledge exchange is represented by relations on the languages and domains of different contexts. DFOL is based on a semantics called Local Models Semantics, which allows for modeling diverse contexts and their relations. An axiomatization for DFOL is also provided. 
412242	41224290	A fast algorithm to binarize and filter documents with back-to-front interference	This paper discusses a common issue known as "back-to-front interference" that occurs when documents are written on both sides of translucent paper. This interference makes it difficult to accurately binarize the documents, resulting in unreadable images. To address this problem, the paper presents a fast and effective segmentation-based method that uses the entropy of the image's histogram. This approach produces high-quality binarized images, making the documents easier to read and analyze. Overall, the proposed method offers a solution for effectively binarizing documents with back-to-front interference.
412242	41224226	Detailing a Quantitative Method for Assessing Algorithms to Remove Back-to-Front Interference in Documents	Back-to-front interference, also known as bleeding or showthrough, occurs when documents are written on both sides of translucent paper, making the ink visible from one side on the other. This creates unreadable documents when directly binarized. To address this issue, the literature offers various algorithms for removing the interference. This paper introduces a quantitative approach to evaluate the effectiveness of these algorithms. By assessing the ability of algorithms to remove back-to-front interference, this method can aid in the selection of the most suitable algorithm for a given document.
412243	41224357	Fair Rate Allocation In Some Gaussian Multiaccess Channels	The article discusses how to achieve all points in the capacity region of Gaussian multiple access channels through successive decoding and time-sharing. It proposes a criterion of fairness based on the theory of majorization, also known as the Lorenz order in economics. The article shows that a unique solution exists for this criterion in a large class of channels, and this fair solution is the same as the well-known Nash bargaining solution. This is due to the special structure of the capacity region. The article also presents a fast algorithm to compute this fair point in certain cases, making it a strong operational choice.
412243	41224318	On the Fairness of Rate Allocation in Gaussian Multiple Access Channel and Broadcast Channel	A channel's capacity region comprises all possible rate vectors. Selecting a point within this region determines the rate allocation. This paper focuses on fairness in rate allocation and examines various fairness concepts such as max-min fairness, proportional fairness, and Nash bargaining solution. The efficiencies of these concepts in multiuser channels are discussed. The application of these ideas to the Gaussian multiple access channel (MAC) and Gaussian broadcast channel (BC) is explored. It is shown that in the Gaussian MAC, max-min fairness and proportional fairness are equivalent. Efficient algorithms for finding fair points in the capacity region for both Gaussian MAC and BC are proposed, and some basic properties of fair rate allocations are demonstrated.
412244	41224485	On RAC drawings of 1-planar graphs.	1-planar graphs are graphs that can be drawn with each edge crossing at most once. A k-bend RAC drawing is a special type of drawing where each edge has at most k bends and edges only cross at right angles. A graph is considered k-bend RAC if it has a k-bend RAC drawing. A 0-bend RAC graph is also known as a straight-line RAC graph. The relationship between 1-planar and k-bend RAC graphs has been studied, and it has been shown that not all 1-planar graphs are straight-line RAC and not all straight-line RAC graphs are 1-planar. The complexity of deciding whether a graph has a drawing that is both 1-planar and straight-line RAC is still an open question, and it has been proven to be NP-hard. However, it has been shown that every 1-planar graph has a drawing that is both 1-planar and 1-bend RAC. This paper answers both of these questions by proving the NP-hardness of the first question and providing a drawing algorithm for 1-planar graphs that satisfies both conditions.
412244	41224436	The Rectangle of Influence Drawability Problem	A proximity drawing of a graph is a straight-line drawing where adjacent vertices are represented by points that are close according to a proximity measure. A rectangle of influence drawing is a proximity drawing where the measure is based on a rectangle of influence, which is an axis-aligned rectangle with two points at opposite corners. In this study, the drawability of various graph classes using both open and closed rectangle of influence models is examined. It is shown that determining if a graph has a rectangle of influence drawing can be done in linear time, and if so, a drawing can be constructed in the same amount of time. The algorithms used produce drawings with vertices at integer grid intersections and only use integer operations.
412245	41224554	Achieving the degrees of freedom of 2×2×2 interference network with arbitrary antenna configurations	This paper examines the degrees of freedom (DoF) region in a 2 × 2 × 2 interference network, consisting of two sources, two relays, and two destinations, each with varying numbers of antennas. The authors demonstrate that using linear transceivers, the cut-set outer bound can be reached without requiring additional symbol extensions, except for one specific system setup with a one-DoF gap. Different techniques such as interference avoidance, cancelation, neutralization, and alignment are employed by the transceivers to achieve the outer bound, depending on the antenna configuration. These findings provide insight into the optimal design of transceivers for interference networks with multiple antennas.
412245	412245137	Feasibility Conditions for Interference Neutralization in Relay-Aided Interference Channel	Interference neutralization (IN) is a new method for managing interference in networks with relays. This paper examines the feasibility of using IN for relay-aided multi-input-multi-output (MIMO) interference broadcast channels without symbol extension. The study focuses on fully connected symmetric systems with multiple antennas at each base station, relay, and user. The necessary condition for generalized IN is first presented, followed by the necessary and sufficient condition for coordinated IN and pure IN for a specific class of relay-aided MIMO-IBC. It is shown that when each BS and user has the minimum antenna configuration, the sufficient and necessary conditions for coordinated IN are the same as the necessary condition for generalized IN. The results provide insight into how to use relay resources efficiently to eliminate interference in both relay-aided MIMO-IBC and relay-aided interference channels.
412246	41224639	Chunk parsing revisited	Chunk parsing is a method used in natural language processing to identify and group words into meaningful units or "chunks". While it has been a theoretically appealing approach, its practical performance has been lacking. However, a new study introduces a simple sliding-window method and maximum entropy classifiers to improve the accuracy and speed of chunk parsing. The results from testing this method on the Penn Treebank corpus show high precision and speed. Additionally, the study proposes a parsing method that considers the probabilities output by the classifiers, which further enhances the accuracy of the parsing. This suggests that chunk parsing can be a viable option for practical use with the right techniques. 
412246	4122468	Bitext Dependency Parsing With Auto-Generated Bilingual Treebank	This paper presents a method to improve the accuracy of bilingual texts (bitexts) dependency parsing by using an auto-generated bilingual treebank created with statistical machine translation (SMT) systems. This approach eliminates the need for costly and troublesome human-annotated bilingual treebanks. The proposed method involves using an SMT system to translate a monolingual treebank into the target language and then using a monolingual parser to parse the translated sentences. To ensure reliable bilingual constraints, an additional step is taken to verify the constraints using unannotated data. The experiments conducted on a standard test data show that this method outperforms monolingual parsers and is robust to noise. This method also eliminates the need for human translation of the test set, making it suitable for purely monolingual settings. Overall, this method provides a more accurate and efficient way to parse bilingual texts.
412247	41224788	SimSem: fast approximate string matching in relation to semantic category disambiguation	This study explores the use of fast approximate string matching to address issues with spelling variations and to utilize large-scale lexical resources for semantic class disambiguation. The researchers incorporate string matching results into a machine learning-based disambiguation approach by creating a unique set of features that measure the distance between a given text and the closest match in a collection of lexical resources. They collect these resources from various biomedical sources, resulting in a combined database of over 20 million lexical items. By utilizing a fast and efficient string matching algorithm, the researchers are able to query these resources without significantly impacting system performance. The results are evaluated on six different corpora, showing significant improvements on one corpus while being modest or negative on others. The researchers provide potential explanations for these results and suggest future research directions. Their lexical resources and implementation are available for research purposes at: http://github.com/ninjin/simsem.
412247	4122472	Feature engineering combined with machine learning and rule-based methods for structured information extraction from narrative clinical discharge summaries.	The need for a system that can convert medical text into a structured format is high. The system has three main tasks: concept extraction, assertion classification, and relation identification. The system has five steps: pre-processing, marking phrases, extracting concepts, classifying assertions, and identifying relations. The performance of the system was evaluated using precision, recall, and F-measure. The results showed that the system is competitive with other state-of-the-art systems. The system uses a variety of features and takes a careful approach to feature engineering. By switching models and using rule-based classifiers, the system was able to improve performance in concept extraction and assertion classification, especially for data-scarce classes. Using a two-staged architecture also significantly improved performance in relation identification.
412248	41224852	Haptic interaction becomes mainstream	For over two decades, the human-technology interaction research community has recognized multimodal interaction as a potential successor to graphical user interfaces. With the rapid development of personal computers and mobile devices, there has been an increase in computing power and more accurate output through various channels like display and audio. This has also led to the emergence of new ways of interaction for everyday tasks. Recently, there has been a lot of research and development in haptic interaction, which involves touch-based feedback. In this paper, the author introduces their talk on the progress and potential of haptic interaction technology in mainstream information and communication technology.
412248	41224845	Feedback for Smooth Pursuit Gaze Tracking Based Control.	Smart glasses, like Google Glass or Microsoft HoloLens, have the potential to enhance human abilities in everyday situations. However, traditional manual interactions are not practical with these devices. This is where eye tracking technology, which can be incorporated into the frames of smart glasses, becomes useful. This paper explores the use of different feedback modalities (visual, auditory, haptic, and none) in a continuous adjustment technique for smooth pursuit gaze tracking. This type of gaze tracking allows for spontaneous interactions without the need for calibration. The study found that while there were no significant differences in performance, users preferred and found haptic and audio feedback more acceptable. This research contributes to the understanding of interaction techniques for gaze-aware smart glasses.
412249	41224936	A New Method For Efficient Symbolic Propagation In Discrete Bayesian Networks	This paper introduces a new method for dealing with uncertainty in discrete Bayesian networks using symbolic expressions instead of numeric calculations. The method involves representing the conditional probabilities of a set of nodes as ratios of linear polynomials in parameters. This allows for efficient computation of the expressions using standard numerical algorithms. An alternative method called the numeric canonical components method is also proposed, which is faster and simpler. Redundancy is avoided by using message-passing methods. The method can also be used to obtain lower and upper bounds for the symbolic expressions. The paper also addresses the issue of symbolic evidence and presents examples to illustrate the methodology.
412249	41224947	Parametric Structure of Probabilities in Bayesian Networks	The paper introduces a method for handling uncertainty in Bayesian networks using symbolic rather than numeric computations. It explains the algebraic structure of probabilities and shows that prior and marginal probabilities can be expressed as rational functions with simple polynomials. It also demonstrates how numeric algorithms can be adapted for symbolic calculations using canonical components and how this can be used to create automatic code generators for symbolic evidence propagation. An example of uncertainty propagation in a clique tree is provided, along with corresponding Mathematica code. The paper also shows that the upper and lower bounds for marginal probabilities can be determined using canonical components.
412250	4122502	Validating Orchestration of Web Services with BPEL and Aggregate Signatures	This paper presents a framework to ensure integrity and authentication in secure workflow computation using BPEL Web service orchestration. Despite the focus on security for Web services, there is a lack of practical and standardized solutions for securing workflows. The authors propose using aggregate signatures to validate the orchestration, with all partners signing the result of their computation. This method does not require any changes to service implementation and can be applied to linear workflows. A prototype implementation is evaluated for performance and a more general scheme is also proposed for validating generic workflows.
412250	41225038	Improved Security Notions and Protocols for Non-transferable Identification	The paper discusses different security notions for identification protocols, with a focus on non-transferability and physical access by powerful adversaries. The authors introduce a new, strong notion called resettable non-transferable identification, which better reflects real-life scenarios. They also present a generic protocol that satisfies this notion and can be efficiently implemented. The potential applications of this protocol in the next generation of electronic passports are also discussed. The paper highlights the limitations and impossibility results of existing notions and proposes a new achievable one. 
412251	41225137	Agent-Oriented Visual Modeling And Model Validation For Engineering Distributed Systems	Recent Agent-Oriented methodologies for engineering distributed systems follow a model-based approach to software development. This means that they use a specific set of models for each step in the analysis and design phases. To implement this approach effectively, clear guidelines and flexible modeling tools are needed. This paper introduces a modeling environment that integrates an Agent-Oriented modeling tool with other tools, such as a model-checker for verification and a library for graph transformation techniques. The design of this environment follows recommendations from the OMG's Model-Driven Architecture initiative. Examples are provided to illustrate how modeling and validation are supported by this environment.
412251	41225130	Goals at risk? Machine learning at support of early assessment	The process of requirements engineering involves identifying, evaluating, and managing potential risks that could hinder the system from meeting stakeholder needs. However, risk analysis techniques can be time and resource-consuming, adding extra burden to the process. To address this issue, a semi-automated approach is proposed, utilizing existing risk-related information and automated ranking to support risk management. This approach combines knowledge from risk assessment techniques and Machine Learning to involve human evaluators in the decision-making process, continuously learning from their feedback and integrating it with organizational knowledge. The ultimate goal is to enhance an organization's ability to be aware of and manage risks by introducing new interactive and continuous risk management techniques that extract useful knowledge from the organization and decision-maker expertise.
412252	41225236	Shadowed c-means: Integrating fuzzy and rough clustering	A new method of partitive clustering called shadowed clustering has been developed, which uses shadowed sets to reduce computation compared to traditional fuzzy clustering. Unlike rough clustering, the threshold parameter is automatically chosen. The number of clusters is optimized using validity indices. Shadowed clustering can handle overlapping clusters and model uncertainty in class boundaries well, and is robust against outliers. A comparison with other partitive clustering methods shows that this approach is superior, as demonstrated by experiments on both synthetic and real data sets. 
412252	41225253	Shadowed Clustering for Speech Data and Medical Image Segmentation	The paper introduces a new use of the shadowed clustering algorithm for dealing with uncertainty in CT scan image segmentation. By dividing the ambiguity into three zones, the algorithm is able to achieve faster convergence and reduced computational complexity. It is also found to be effective in generating accurate prototypes even in the presence of noise, making it well-suited for unsupervised applications. A comparison with another clustering algorithm shows that the shadowed clustering algorithm does not require external tuning and can automatically determine the threshold. Experiments demonstrate its effectiveness in extracting regions affected by vascular damage in the brain, and expert radiologists validate its usefulness in diagnosing brain infarctions.
412253	412253105	Evaluation and analysis of term scoring methods for term extraction.	This study evaluates five different methods for automatically extracting terms from four different types of text collections: personal documents, news articles, scientific articles, and medical discharge summaries. Each collection has a specific use case, such as identifying authors, suggesting query terms, and expanding patient queries. The goal is to determine which term scoring method is most suitable for a given collection, and how these methods perform on collections that they were not specifically designed for. The results show that collection size and the presence of multi-word terms are important factors in the success of a term scoring method. The most effective method overall is pointwise Kullback-Leibler divergence for both informativeness and phraseness. This study demonstrates that unsupervised term scoring methods can be applied successfully in a variety of contexts.
412253	41225335	ECIR 2008 Tutorials	This tutorial provides a comprehensive overview of advanced language modeling techniques and tools, such as document priors, translation models, and relevance models. It uses expert search as a case study to demonstrate the effects of modeling assumptions. The tutorial is led by Djoerd Hiemstra, an assistant professor at the University of Twente with expertise in information retrieval. Additionally, there are two more tutorials on the topics of search and discovery in user-generated content and building IR applications using the Terrier platform. These tutorials are led by Maarten de Rijke, a professor at the University of Amsterdam, and his PhD student, Wouter Weerkamp, as well as Craig Macdonald, a PhD research student, and Ben He, a post-doctoral research assistant, both from the University of Glasgow. These tutorials offer insights into the challenges and possibilities of accessing and analyzing user-generated content, as well as the design and use of the Terrier platform for IR research and applications.
412254	41225429	The effect of imperfect error detection on reliability assessment via life testing	Life testing is a method used to measure the reliability of software by running it on a large number of test cases and recording the results. The number of failures observed is used to determine the failure probability, even if no failures are observed. However, in practice, failures can occur without being detected. This paper examines the impact of imperfect error detection on the confidence level of the failure probability bound. The results show that imperfect error detection does not necessarily limit the ability of life testing to accurately estimate failure probability in critical systems. However, achieving high confidence levels may require unrealistic assumptions about error detection. Overall, life testing may be more useful for situations where extremely high confidence levels are not necessary.
412254	41225421	Applying hypertext structures to software documentation	Software documentation is crucial for the success of many companies, but it often fails to meet the needs of users due to its static nature. This resource has not been fully utilized and presents challenges in terms of retrieval and integration of different information sources. To address these issues, the University of Virginia is currently working on a prototype system called SLEUTH. This system aims to automate the management of software documentation by using hypertext and typed links for browsing, authoring tools for maintenance, and ad hoc querying for document synthesis. This system has the potential to greatly increase the utility of software documentation for a diverse range of users.
412255	41225560	Parallel Decomposition of Generalized Series-Parallel Graphs	Generalized series-parallel (GSP) graphs are a type of decomposable graphs that can be represented by their decomposition trees. This allows for efficient solving of graph-theoretic problems. A parallel algorithm has been developed for constructing a decomposition tree of a GSP graph in O(log n) time using C(m, n) processors on a CRCW PRAM. This algorithm relies on the number of processors needed to find connected components of a graph with m edges and n vertices in logarithmic time. Additionally, this algorithm can be used to derive properties for GSP graphs that may be useful in other contexts. 
412255	41225514	A faster implementation of a parallel tree contraction scheme and its application on distance-hereditary graphics	The paper discusses a parallel tree contraction scheme that removes leaves and nodes in maximal chains. The time and processor complexities for computing all nearest smaller values and the minimum of n values are denoted by T(n) and P(n), respectively. The authors present a faster implementation of the tree contraction scheme that takes O(logn·T(n)) time using P(n) processors on an arbitrary CRCW PRAM. This is an improvement over the current best results of O(logloglogn) for T(n) and O(n/logloglogn) for P(n). This implementation can also be used to solve problems on distance-hereditary graphs, with a minimum connected γ-dominating set and a minimum γ-dominating clique being found in O(logn·T(n)) time using O(P(n)+(n+m)/T(n)) processors on an arbitrary CRCW PRAM. This has implications for solving other related problems with the same parallel complexities. 
412256	41225685	Data memory minimisation for synchronous data flow graphs emulated on DSP-FPGA targets	 environment for designing and implementing digital signal processing systems on programmable DSP processors and FPGAs. The paper presents an algorithm for determining the smallest possible data buffer sizes for synchronous data flow applications, ensuring a deadlock-free schedule. This is crucial for mapping applications on FPGAs, where there is limited availability of register resources. The algorithm fits into the design flow of GRAPE, making it a valuable tool for efficient DSP system design.
412256	41225616	Data Routing in Dataflow Graphs	A new extension to the cyclo-static dataflow graph model has been proposed, taking into account the significant amount of time and resources spent on data routing in current digital signal processing applications. This model is designed to be used in GRAPE, an environment for rapid prototyping of DSP systems on various target architectures. Examples are provided to clarify the model, and the code generation for data-routing specifications is discussed for a specific synthesis path. The model is validated through a larger example and its implementation. This systematic approach aims to automatically and efficiently synthesize data routing for software or hardware realization.
412257	41225748	A Clustering Approach to Generalized Tree Alignment with Application to Alu Repeats	The Generalized Tree Alignment Problem is a formalization of the multiple sequence alignment problem that focuses on its evolutionary aspect. It involves finding a phylogenetic tree and ancestral sequences that minimize the amount of change needed to explain the given data. This problem is difficult to solve, but a heuristic algorithm has been developed for it. This algorithm uses a combination of agglomerative clustering techniques and sequence graphs to align the sequences and achieve good results according to the scoring function. The effectiveness of this approach is demonstrated on a set of Alu repeats, producing biologically meaningful answers.
412257	4122574	Towards integration of multiple alignment and phylogenetic tree construction.	Molecular evolution relies on reconstructing the history of a group of biological sequences in the form of a phylogenetic tree. This process involves calculating a multiple alignment, which is often treated as a separate problem from tree construction. However, these two tasks are closely linked and can influence each other. A new approach has been developed that simultaneously produces both the tree and multiple alignment by using pre-aligned groups of sequences. This method allows for changes in the alignment during the computation, unlike existing iterative algorithms. A criterion is also implemented to prevent negative edge lengths in the resulting tree. This approach has been successfully applied to aligning protein and nucleic acid sequences. 
412258	41225852	Clustering Large Graphs via the Singular Value Decomposition	The article discusses the problem of dividing a set of points in a multi-dimensional space into a fixed number of clusters, aiming to minimize the total distance between each point and its cluster center. This is the objective of the popular k-means clustering algorithm. The article proves that this problem is difficult to solve even when the number of clusters is only 2. To overcome this difficulty, a continuous relaxation of the problem is considered by finding the best subspace that minimizes the distances of the points to that subspace. This can be efficiently solved using the Singular Value Decomposition (SVD) of a matrix representing the points. The article also introduces a fast randomized algorithm for this problem, which can be applied to large-scale applications.
412258	41225819	Topics in matrix sampling algorithms	This article discusses three key problems in Linear Algebra that are crucial for applications in Machine Learning. These include Low-rank Column-based Matrix Approximation, Coreset Construction in Least-Squares Regression, and Feature Selection in K-means Clustering. The main objective of these problems is to select a subset of columns or rows from a given matrix to obtain an approximation or solution that is as good as using all the columns or rows. The authors present new algorithms for these problems, building on previous work in Matrix Sampling Algorithms. Their contributions include improved algorithms for Low-rank Matrix Approximation and Regression, as well as algorithms for a new problem domain – K-means Clustering.
412259	41225920	Comparing two notions of simulatability	This work explores the relationship between two security notions, standard simulatability and universal simulatability, for cryptographic protocols. A protocol is considered secure if every attack on it can be simulated by an idealized version of the protocol. Two common formalizations of this concept are standard simulatability, which requires that no user can distinguish the protocol from the idealized version, and universal simulatability, which allows for a single user to distinguish. It is shown that universal simulatability implies standard simulatability for perfect security, but not for computational or statistical security. Additionally, a formal definition of a time-lock puzzle is presented, which has implications for the separation of standard and universal simulatability in terms of computational security.
412259	41225918	On the notion of statistical security in simulatability definitions	We explore the concept of statistical security in the context of reactive simulatability, a framework for analyzing multi-party protocols. However, we find that the current definition of statistical security in this framework does not allow for secure composition of protocols. We propose a modified notion of statistical security that does allow for secure composition and provide evidence that the previous lack of composability was due to the specific formulation of statistical security. This revised definition can also be applied to Canetti's universal composability framework, potentially preventing similar issues. 
412260	41226046	Hot or not: Interactive content search using comparisons	The article discusses a method for searching for a specific object in a database using interactive content search through comparisons. The user is asked to select the most similar object from a small list, and then a new list is presented based on her previous selections. This process continues until the target object is included in the list, and the search terminates. The study focuses on the scenario of heterogeneous demand, where target objects are selected from a non-uniform probability distribution. The proposed algorithm uses a doubling metric space and can efficiently search for objects with a cost proportional to the doubling constant and the entropy of the demand distribution. The average search cost is shown to scale with a bound of O(c5H), which is an improvement from previous bounds and is order optimal for a constant c.
412260	41226045	Content search through comparisons	The article discusses the issue of navigating through a database of similar objects using comparisons when there is a diverse demand for these objects. This problem is similar to designing small-world networks. It is proven that this problem is NP-hard, leading to the proposal of a new mechanism for small-world network design. The mechanism's performance is bounded under heterogeneous demand. The mechanism also has an equivalent application in content search through comparisons, which leads to the establishment of both upper and lower bounds on this type of search.
412261	41226154	Robust graph regularized nonnegative matrix factorization for clustering.	Nonnegative matrix factorization (NMF) and its graph regularized extensions have been popular in machine learning and data mining. However, they are prone to errors and noise due to the use of squared loss function in measuring graph regularization and data reconstruction. To address this issue, a new robust graph regularized NMF model (RGNMF) is proposed. It assumes that there may be corrupted data entries, but the corruption is sparse. An error matrix is introduced to capture this sparse corruption, resulting in a more accurate factorization of the data. Additionally, the use of -norm function helps to reduce the impact of unreliable regularization errors. An iterative updating algorithm is proposed to solve the optimization problem, and experimental results show that RGNMF outperforms other state-of-the-art methods.
412261	4122618	Feature selection for imbalanced data based on neighborhood rough sets.	Feature selection is a crucial step in data mining that involves choosing the most relevant features to create a more effective learning model and save time and memory. Imbalanced data is common in real-life applications such as medical diagnoses, intrusion detection, and credit ratings, making feature selection for imbalanced data an important area of research. In this study, the authors propose a method for feature selection using neighborhood rough set theory, which takes into account the uneven distribution of classes. The method uses a discernibility-matrix-based feature selection technique and a novel algorithm called RSFSAID. The authors also investigate the impact of different parameters on feature selection and use a particle swarm optimization algorithm to determine the optimal parameters. The results of experiments on public datasets show that the proposed algorithm outperforms four other algorithms in terms of classification performance for imbalanced data.
412262	41226234	Algorithmic Barriers from Phase Transitions	In many random constraint satisfaction problems, there are tight estimates for the largest constraint density for which solutions exist. However, known polynomial-time algorithms for these problems stop finding solutions at much lower densities. For example, coloring a random graph requires twice as many colors as its chromatic number, which has yet to be improved upon despite efforts from researchers. This "resilience" of the factor of 2 against algorithms is explored by analyzing the evolution of the set of k-colorings of a random graph. The factor of 2 is shown to correspond to a phase transition in the geometry of the set, and this phenomenon is also observed in other random CSPs. A general technique is developed to prove this using statistical physics principles.
412262	41226246	Why almost all k-colorable graphs are easy	Coloring a k-colorable graph using k colors (k ≥ 3) is a difficult problem, but considering average case analysis can lead to better outcomes. This study focuses on the uniform distribution over k-colorable graphs with n vertices and cn edges, where c is a sufficiently large constant. The researchers rigorously prove that most of these graphs have proper k-colorings that are clustered in one group, with only a small number of vertices having different colors. They also present a polynomial time algorithm that can find a proper k-coloring for the majority of these random graphs, indicating that they are mostly "easy" to color. This is in contrast to very sparse random graphs, where experimental results suggest that certain edge densities are challenging for many coloring methods. The explanation for this is that the solution space for these graphs is more complex and "clustering", as supported by statistical physics analysis. This study provides a rigorous confirmation of this explanation.
412263	41226361	Dynamic frequency scaling algorithms for improving the CPU's energy efficiency	This paper focuses on improving the energy efficiency of service center server CPUs by using dynamic frequency scaling to balance computational performance and power consumption. Two algorithms, an immune inspired algorithm and a fuzzy logic based algorithm, were created and tested. The immune inspired algorithm uses a model based on human antigens to classify the server's power/performance state and determine actions to optimize power consumption. The fuzzy logic algorithm adjusts the CPU's performance based on workload, while also filtering workload spikes to avoid excessive p-state transitions. These techniques aim to reduce the overall energy consumption of service center server CPUs.
412263	41226381	A reinforcement learning based self-healing algorithm for managing context adaptation	This paper proposes a self-healing model for addressing context adaptation. The model uses reinforcement learning to make decisions at runtime and ensures self-healing by monitoring the system's execution environment and selecting appropriate healing actions. The reinforcement learning approach is used to select the best sequence of actions to maintain context resources in a policy-compliant state. This model aims to improve the system's ability to adapt to changing contexts and maintain a desired level of performance. 
412264	41226459	Clustering Sparse Graphs.	The development of a new algorithm for clustering sparse unweighted graphs is presented. The goal is to partition nodes into clusters with high density within clusters and low density across clusters. This is challenging due to the small edge densities within and across clusters. The algorithm takes into account the noisiness of sparse graphs and uses a different penalization approach for missing edges within clusters and present edges across clusters. It is tested on the well-known "planted partition" model and is shown to outperform previous methods in terms of clustering sparser graphs with smaller clusters. This is also supported by empirical evidence.
412264	41226437	Matrix Completion With Column Manipulation: Near-Optimal Sample-Robustness-Rank Tradeoffs	This paper addresses the issue of matrix completion when some columns are corrupted by a malicious adversary. Existing algorithms for matrix completion can perform poorly when even a single column is corrupted. This problem arises in robust collaborative filtering, where certain users manipulate the system to skew predictions. The paper proposes a new algorithm that combines a trimming procedure and a convex program to minimize the nuclear and $ell _{1,2}$ norms. The theoretical results show that the algorithm can complete the matrix even with a growing number of corrupted columns, without assuming anything about the locations or values of the manipulated columns. These results are nearly optimal and provide insights into the tradeoffs between sample size, robustness, and rank in matrix completion.
412265	41226568	Personalized recommendation of social software items based on social relations	This study focuses on personalized recommendations for social software items such as web-pages, blog entries, and communities. The recommendations are based on the user's social network information, which is collected from different sources within the organization. The research compares the effectiveness of recommendations based on the user's familiarity network and their similarity network. Additionally, the impact of providing explanations for recommended items, including related people and their relationship to the user and the item, is also examined. Results from a user survey and a field study suggest that recommendations based on the familiarity network are more effective and that providing explanations can increase interest in recommended items. 
412265	41226524	Personalized social search based on the user's social network	This research focuses on personalized social search, where search results are customized based on the user's social connections. Three types of social networks are studied for personalization: familiarity-based, similarity-based, and overall networks. The effectiveness of these strategies is evaluated through an offline study and a user survey within the organization. Results show that social network-based personalization is more effective than non-personalized social search, and all three social network strategies outperform a topic-based personalization approach. This is supported by both the offline study and user survey, where participants exposed to the different personalization strategies showed a preference for social network-based methods.
412266	41226624	Characterizing VeSFET-Based ICs With CMOS-Oriented EDA Infrastructure	This paper discusses the application of standard cell design methodology to create vertical slit field effect transistor (VeSFET)-based ASICs using modern CMOS EDA tools. The study focuses on a family of VeSFET canvases, specifically chain canvases, that offer improved performance and power consumption compared to circuits using isolated transistor canvases. The designs were compared to those implemented with a commercial low power CMOS library and corresponding VeSFET libraries. Results showed significant power reduction in VeSFET-based designs compared to CMOS-based designs with the same performance. This highlights the potential for VeSFETs to be a promising alternative to CMOS technology in ASIC design.
412266	41226632	Iddq Testing for High Performance CMOS - The Next Ten Years	CMOS scaling, which involves reducing the size of transistors on integrated circuits, has a direct impact on the subthreshold current and the effectiveness of Iddq testing. This trend is expected to continue for at least the next ten years, with a factor of 2**(1/2) reduction in line widths every three years and an increase in chip size leading to a 2-3 times increase in the number of transistors per IC. However, this also affects various device parameters, including the leakage current, which can make it challenging to distinguish between good and defective devices using Iddq testing if defect size also scales with technology.
412267	41226726	Event-Triggered Optimal Control With Performance Guarantees Using Adaptive Dynamic Programming.	This paper focuses on event-triggered optimal control (ETOC) for continuous-time nonlinear systems. A new event-triggering condition is proposed, which allows for the design of ETOC methods based on the Hamilton-Jacobi-Bellman (HJB) equation. Performance guarantees are provided through upper and lower bounds. An adaptive dynamic programming (ADP) method is developed using a critic neural network (NN) to approximate the value function of the HJB equation. It is proven that semiglobal uniform ultimate boundedness can be achieved for states and NN weight errors with the ADP-based ETOC. Simulation results show the effectiveness of this method.
412267	41226721	DRS: Dynamic Resource Scheduling for Real-Time Analytics over Fast Streams	A data stream management system (DSMS) allows users to register continuous queries and receive result updates in real-time. This is particularly important for applications with time constraints, where results must be received within a given period. To handle the fast data, DSMSs are often placed on top of a cloud infrastructure. However, due to unpredictable fluctuations in stream properties, cloud resources must be dynamically provisioned and scheduled to ensure real-time response. The proposed solution, DRS, is a dynamic resource scheduler for cloud-based DSMSs. It addresses challenges such as accurately modeling the relationship between resources and response time, determining the best resource placement, and measuring system load with minimal overhead. DRS has been tested with real data and has shown to achieve real-time response with optimal resource consumption.
412268	41226826	Automatic metadata generation for active measurement.	The Internet is a challenging environment for conducting empirical research due to potential bias introduced by local conditions such as CPU or network load. In response, the authors propose a framework for local environment monitoring during Internet measurement experiments. This framework aims to provide a more comprehensive understanding of measurement results and increase the reproducibility of findings. The tool SoMeta is used to implement this framework and evaluate its runtime costs. Through a series of intentional perturbations of the local environment during active probe-based measurements, the authors demonstrate how simple local monitoring can reveal biases in the results. They also discuss the potential for expanding this framework to provide metadata for a wider range of Internet measurement experiments. 
412268	41226852	Scalable Network Path Emulation	As network research becomes more popular, laboratory-based experimentation has emerged as a useful method for evaluating network systems and protocols. To accurately replicate real-world conditions, it is important to create paths between nodes in the lab that mimic those in the Internet. This paper introduces NetPath, a software-based path emulation tool that offers various features such as packet delay, errors, loss, duplication, and reordering. Through experiments, NetPath has been shown to have a higher throughput capacity and precision compared to other software-based emulators. It also improves application traffic behavior and allows for simultaneous path emulation on multiple physical links, making more efficient use of laboratory resources. 
412269	412269248	Stable Delaunay Graphs.	The concept of stability of edges in a Euclidean Delaunay triangulation is introduced, with a parameter $$\\alpha$$ being used to determine if an edge is $$\\alpha$$-stable. A subgraph G is defined as a $$(c\\alpha, \\alpha)$$-stable Delaunay graph if every edge in G is $$\\alpha$$-stable and every $$c\\alpha$$-stable edge in the triangulation is also in G. This concept can also be applied to Delaunay triangulations under different convex distance functions. It is shown that if an edge is stable in the Euclidean Delaunay triangulation, it is also stable in the triangulation under a different distance function, and vice-versa. This relationship leads to a linear-size kinetic data structure for maintaining a $$(8\\alpha, \\alpha)$$-stable Delaunay graph as points move, with a time complexity of $$O(\\log n)$$ for each event during the motion. It is also shown that many properties of the Euclidean Delaunay triangulation are retained in any stable Delaunay graph.
412269	412269290	Kinetic Voronoi Diagrams and Delaunay Triangulations under Polygonal Distance Functions.	This study analyzes the changes in the Voronoi diagram and Delaunay triangulation of a set of points as they move along continuous trajectories, using a convex distance function defined by a convex k-gon. Assuming bounded degree algebraic trajectories for the points, an upper bound of \(O(k^4n\lambda _r(n))\) is established for the number of topological changes in the diagrams. The maximum length of an (n, r)-Davenport–Schinzel sequence, \(\lambda _r(n)\), is also considered, with r being a constant based on the algebraic degree of the point motion. An algorithm is proposed for efficiently maintaining the structures, utilizing the kinetic data structure (KDS) framework.
412270	41227015	Software-directed power-aware interconnection networks	Interconnection networks are used in various parallel computer systems, but recent technology trends have led to increased power consumption. Power-aware networks are needed to address this issue, but current techniques are limited and not tailored to specific applications. Research on power-aware parallelizing compilers is also in its early stages. In this paper, the authors propose software techniques that allow for dynamic voltage scaling of network links during application runtime, based on offline DVS settings generated during static compilation. Simulations show that this approach can reduce link power consumption by up to 76.3&percnt;, with minimal impact on network performance.
412270	41227082	Dynamic Voltage Scaling with Links for Power Optimization of Interconnection Networks	Interconnection networks, originally designed for multicomputers, have mainly focused on performance. However, as they are now being used in various applications, power efficiency has become a crucial factor. As the demand for network bandwidth increases, communication links consume a significant amount of power, making it necessary to consider dynamic voltage scaling (DVS) to minimize power consumption. A history-based DVS policy is proposed, which adjusts link frequencies and voltages based on past utilization, resulting in up to 6.3X power savings and a moderate impact on performance. This is the first study to target dynamic power optimization of interconnection networks.
412271	4122711	Feature selection for ensembles applied to handwriting recognition	The use of feature selection in creating ensembles has been proven to be an effective strategy due to its ability to produce high-quality subsets of features. This paper introduces a new approach to ensemble feature selection using a hierarchical multi-objective genetic algorithm. The algorithm operates in two levels, first selecting features to generate a set of classifiers and then choosing the best team of classifiers. It has been tested in both supervised and unsupervised feature selection scenarios, specifically in the recognition of handwritten digits and month words. Comparisons with traditional methods such as Bagging and Boosting showed that this approach leads to significant improvements in low error rate situations. These improvements are demonstrated through recognition rate comparisons.
412271	41227111	Zoning and metaclasses for character recognition	This paper has two main contributions. First, it explores the use of confusion matrices to determine perceptual zoning for character recognition. The study focuses on concavities and convexities deficiencies in the background pixels of the input image, and discusses four different types of zoning. Results suggest that this method could be a viable alternative to exhaustive search algorithms. The second contribution is the development of a methodology for defining metaclasses in handwritten character recognition. This approach uses disagreement among characters and calculates the Euclidean distance between confusion matrices. Experiments show that using metaclasses can enhance the performance of the system.
412272	41227243	Using Context Similarity for Service Recommendation	Recommender systems have been successful in solving the issue of information overload for consumers by providing recommendations for goods and services. However, existing systems for service recommendation do not consider both service qualities and contextual dimensions simultaneously. To address this gap, a new approach is proposed that combines both criteria using concept abduction as a reliable measure for context similarity. This approach aims to improve the effectiveness of recommendations in situations with limited feedback. Experiments using real-world data demonstrate the usefulness of this method in addressing data sparsity by incorporating rankings from nearby context segments.
412272	41227226	A Framework for the Evaluation of Mashup Tools	Mashup Programming is a popular method of creating web applications, but it is still considered to be in its early stages. In order to better understand the support available for end users in mashup development, a multi-dimensional framework is proposed to evaluate existing tools and platforms. The evaluation focuses on the end-user perspective and considers factors such as capturing users' requirements and goals. The study also covers platforms that have not been previously evaluated. The framework can be used to guide future research and design of mashup development environments. The study highlights the need for a more user-oriented approach in mashup programming and suggests the use of users' goals as a guiding mechanism in creating mashup applications.
412273	4122737	Mobile access to cultural heritage: mobile-CH 2016.	The Mobile-CH workshop, taking place alongside the MOBILE HCI Conference, will serve as a platform for the intersection of cultural heritage research and personalization using technology, particularly mobile and ubiquitous devices. The workshop aims to bring together researchers and practitioners working on cultural heritage to explore the potential of cutting-edge technology in enhancing the experience at heritage sites. The outcome of the workshop will be a collaborative research agenda that will guide future research and encourage partnerships among different disciplines.
412273	41227349	Thematic Maps for Geographical Information Search.	The challenge of exploring cultural heritage information is hindered by fragmented data and a focus on specific repositories or applications. To overcome this, a data integration approach is necessary to generate custom views tailored to the user's needs but also expandable to related content. This paper introduces a model for managing thematic maps and integrating them with query expansion during user interaction. The model is based on an ontological representation of domain knowledge and a semantic interpretation model for identifying concepts in user queries. The model is being tested in the OnToMap Participatory GIS, a platform for interactive community maps and decision-making.
412274	41227448	Coping with WORDNET Sense Proliferation	WORDNET is renowned for its detailed word sense distinctions, but this can pose a challenge for computational tasks like word sense disambiguation. In order to address this issue, one approach is to reduce the number of senses by grouping them into equivalence classes. However, the authors of this paper propose a different approach. They acknowledge that some of the sense distinctions in WORDNET may be questionable, but they prefer to maintain its semantic richness and instead suggest extensions to make word sense disambiguation more manageable.
412274	41227437	Context-Aware Workflow Management	The CAWE framework is a tool for managing context-aware workflow systems using Web Services. It allows for a flexible and extensible specification of context-sensitive workflows, which can be executed by standard workflow engines. The framework was used to develop a prototype application for monitoring patients on blood thinners. CAWE is based on a hierarchical workflow representation and a declarative specification of conditions for selecting the appropriate workflow. A prototype system was developed and tested with medical workflows involving human actors. The resulting medical application can be accessed through the internet on a PC or Smart Phone and supports the coordination of activities for patients with heart diseases. 
412275	41227582	Capacity Theorems for Distributed Index Coding	Index coding is a communication method in which a server sends messages and their respective receivers use side information to reduce the amount of communication needed. Distributed index coding is an extension of this, where multiple servers broadcast different subsets of messages. This paper studies the optimal tradeoff between message and server broadcast rates, known as the capacity region, for a general distributed index coding problem. Inner and outer bounds on the capacity region are established, with matching sum-rates for various scenarios. The proposed inner bound uses a distributed composite coding scheme, while the outer bound is based on the polymatroidal axioms of entropy and functional dependences. These bounds improve upon existing ones and have features demonstrated through examples.
412275	41227567	Chop and roll: Improving the cutset bound	A new outer bound has been established for the capacity region of a general noisy network with multiple messages. This bound is based on an ordered partition of the nodes in the network and can be interpreted as the directed information between inputs and outputs across these subsets. It is a generalization of existing bounds for specific types of networks, such as graphical and Gaussian networks. The new bound improves upon the standard cutset bound, especially in simple examples. 
412276	412276112	Coalitional Games for Transmitter Cooperation in MIMO Multiple Access Channels	Wireless networks rely on cooperation between nodes to achieve higher throughputs. This paper focuses on determining the feasibility and stability of cooperation between rational nodes in a wireless network using cooperative game theory. The stability of the grand coalition of transmitters is examined, taking into account external interference and different receiver strategies. The core of the game is used to determine stability, which is a complex problem. Results show that cooperation is stable for a single user decoding receiver, but unstable for an interference cancelling receiver with a fixed decoding order. However, allowing for time sharing between decoding orders can make cooperation stable at high signal-to-noise ratios. This research highlights the importance of cooperation in wireless networks and its potential to improve individual user rates.
412276	412276114	Minimizing sum distortion for static and mobile fusion center placement in underwater sensor networks.	Optimizing the position of a mobile fusion center in a sensor network is a significant problem, with the goal of minimizing the sum distortion in communication between nodes and the fusion center. The study incorporates a time-multiplexing architecture for communication between nodes and the fusion center, as well as transmission losses along underwater links. Utilizing information theory results, the optimization problem is formulated and solved analytically for a fixed fusion center location and numerically for a mobile fusion center. In the low power regime, the fusion center selectively communicates with a few nodes while turning others off, and in the high power regime, the time allocated to nodes is based on their information content and distance to the fusion center. When the fusion center is mobile, it is placed near the node with the most information in the low power regime, but this difference decreases as power increases.
412277	41227752	A theory of the motion fields of curves	This article discusses a study on the motion field generated by moving 3-D curves observed by a camera. The relationship between optical flow and motion field is explored, showing that the assumptions made in computing optical flow are difficult to defend. The motion field of a nonrigid curve is then studied, introducing the concept of isometric motion and analyzing the spatiotemporal surface and its differential properties. It is shown that the full motion field of a curve cannot be recovered from this surface, but equations can be used to characterize it up to a rigid transformation. The study also examines the use of two cameras for disambiguating stereo correspondences.
412277	41227717	The fundamental matrix: Theory, algorithms, and stability analysis	This paper examines the geometry of a pair of cameras (stereo rig) by not assuming known intrinsic parameters. This is more realistic and captures necessary information for establishing correspondences between images. The commonly used Essential matrix is confusing and this paper introduces the Fundamental matrix, a 3x3 matrix of rank 2, to clarify the projective nature of the correspondence problem in stereo.
412278	41227857	The First International Workshop on Non-Functional System Properties in Domain Specific Modeling Languages (NFPinDSML2008)	The workshop aimed to bring together researchers and practitioners from different fields to discuss the integration of non-functional properties in software systems. This included those from language engineering and experts in Domain Specific Modeling Languages. The focus was on expanding the principles of reasoning about non-functional properties in software systems and model-driven engineering. 
412278	41227824	Model-Driven Performance Evaluation for Service Engineering	Service engineering and service-oriented architecture are modern methods for integrating software systems. In this approach, performance is a key factor in the integration of diverse, distributed service-based systems. To ensure software quality, empirical performance evaluation is used to measure and calculate performance metrics. This involves testing the implemented software to determine its efficiency. A model-driven service engineering approach is presented for empirical performance evaluation of services and service compositions. This approach utilizes temporal databases theory to evaluate the performance of service systems developed using a model-driven approach. Overall, this method helps to ensure the quality and efficiency of service-based systems.
412279	41227949	Feature Curve Metric for Image Classification	The paper introduces a new classifier, called the feature cure metric (FCM) which combines elements from existing classifiers for hand gesture and face recognition. It uses a distance metric between the test sample and prototype samples to improve accuracy. Experiments on various databases show that FCM outperforms other well-known classifiers, such as nearest neighbor, nearest feature line, and center-based nearest neighbor. The proposed approach achieves higher recognition rates on Jochen Triesch Static Hand Posture, Yale face, and JAFFE face databases.
412279	41227950	Two Classifiers Based on Nearest Feature Plane for Recognition	This paper introduces two improved methods, CNFP and LNFP, for recognition based on the nearest feature plane (NFP) approach. These methods incorporate concepts from the nearest neighbor plane (NNP) and center-based nearest neighbor (CNN) classifiers to reduce computational complexity and improve performance compared to the NFP classifier. Experiments on Yale face database and soil object database show that CNFP and LNFP outperform other improved classifiers in terms of both computational complexity and recognition rate.
412280	4122803	An event based approach to web service design and interaction	The paper proposes a new approach to web service design and interaction, where web services participate in shared business events instead of one-to-one method invocations. These events are broadcasted to all participating web services simultaneously, and there is a distinction between transactional and non-transactional events. The concept of business events is highlighted as essential in the analysis and design phase, and the paper demonstrates how this approach can be implemented using SOAP messaging. This approach has the potential to improve efficiency and effectiveness in web service interactions.
412280	41228040	Domain Modelling and the Co-Design of Business Rules in the Telecommunication Business Area	This paper discusses the creation of an enterprise domain model for a young telecommunications company. The company needed a way to integrate its stand-alone business support systems, so an enterprise-wide domain model was developed as an integration layer. However, due to the company's early stage, much of the domain knowledge was vague and not formalized. As a result, there was a need for co-designing business rules with the domain model. This process allowed the company to gain a better understanding of its business and develop a common understanding across functional areas. Overall, the development of the domain model helped the company improve its information infrastructure and business operations.
412281	41228150	Relaxed and Hybridized Backstepping.	This technical note discusses nonlinear control systems with structural obstacles that make it difficult to design traditional continuous backstepping feedback laws. Instead, a new approach is proposed where the origin of the closed-loop system is not globally asymptotically stable, but a suitable attractor is practically asymptotically stable. A hybrid feedback law is designed using a combination of backstepping and locally stabilizing controllers. The nonlinear dynamics are represented using a differential inclusion method, and the results are demonstrated on a system that cannot be stabilized globally using backstepping alone. 
412281	41228152	Combining a backstepping controller with a local stabilizer	The article discusses nonlinear control systems that have unique challenges in designing classical continuous stabilizing feedback laws. These systems cannot be solved using the traditional backstepping method and instead require a hybrid approach that combines a backstepping controller with a locally stabilizing controller. This results in a feedback law that makes the origin of the closed-loop system not globally asymptotically stable, but instead practically asymptotically stable at a suitable attractor. The proposed design method is demonstrated on a specific nonlinear system that does not have any globally stabilizing backstepping controller due to its structure.
412282	41228228	EnrichNet: network-based gene set enrichment analysis.	EnrichNet is a new approach and web-application that aims to improve the analysis of functional associations between gene/protein sets and databases. It addresses four limitations of the commonly used over-representation-based enrichment analysis: it can score non-overlapping gene/protein sets, considers genes with missing annotations, takes into account physical interactions between the gene/protein sets, and recognizes tissue-specific associations. EnrichNet combines a graph-based statistic and interactive sub-network visualization to prioritize putative functional associations and provide a biological interpretation of the results. When applied to sets of genes involved in human diseases, EnrichNet identified new pathway associations by highlighting a dense sub-network of interactions between their corresponding proteins.
412282	41228221	JEPETTO: a Cytoscape plugin for gene set enrichment and topological analysis based on interaction networks.	JEPETTO is a plugin for Cytoscape 3.x that performs integrative human gene set analysis. It uses protein interaction networks and topological analysis to identify functional associations between genes and known cellular pathways and processes. The plugin integrates information from three previously published web servers that specialize in enrichment analysis, pathways expansion, and topological matching. This integration makes it easier for users to analyze their gene sets and interpret the results. The usefulness of the JEPETTO plugin is demonstrated through its analysis of a set of misregulated genes related to Alzheimer's disease.
412283	41228322	Piggyback CrowdSensing (PCS): energy efficient crowdsourcing of mobile sensor data by exploiting smartphone app opportunities	Mobile crowdsourcing is a rapidly evolving field thanks to the widespread use of sensor-enabled smartphones. This includes systems that provide information on highway congestion, but participation in these systems can drain battery resources. To address this issue, the authors propose Piggyback CrowdSensing (PCS), which collects sensor data during phone calls or app usage to minimize energy usage. They use a user-specific prediction model to determine the best times to collect data and found that PCS can collect large-scale sensor data while using significantly less energy compared to existing methods. 
412283	41228331	Unobtrusive sleep monitoring using smartphones	The way we sleep greatly affects our emotions and overall well-being. With the emergence of quantified-self apps and wearable devices, people now have the ability to track and measure their sleep duration, patterns, and quality. However, these methods can be intrusive and require users to modify their habits in order to gain accurate data. In this paper, a new approach called the best effort sleep (BES) model is introduced, which uses smartphones to infer sleep duration in a non-intrusive way. By analyzing various smartphone usage patterns and environmental observations, the BES model can accurately predict sleep duration without the user needing to interact with their phone. This approach was compared to other popular wearable devices and showed promising results in accurately measuring sleep duration.
412284	41228418	BeWell+: multi-dimensional wellbeing monitoring with community-guided user feedback and energy optimization	This paper discusses the development of a new smartphone app, BeWell+, which monitors three aspects of health: sleep, physical activity, and social interaction. The app uses persuasive feedback through an ambient display on the phone's wallpaper to promote better behavioral patterns. The new version of the app includes two new mechanisms: community adaptive wellbeing feedback, which can be tailored to different user communities, and wellbeing adaptive energy allocation, which prioritizes monitoring and feedback on specific health dimensions. The app was tested in a 19-day field trial with 27 participants, and results showed that the app was successful in promoting healthier lifestyles through user understanding and response to feedback. 
412284	41228448	Preference, context and communities: a multi-faceted approach to predicting smartphone app usage patterns	In this paper, the authors propose a multi-faceted approach to predicting smartphone app usage. This approach takes into account three key factors that influence app usage: intrinsic user preferences, environmental factors, and shared aggregate patterns of app behavior. The authors conducted a 3-week field trial with 35 users and analyzed app usage logs from 4,606 smartphone users worldwide to evaluate their model. They found that their approach not only produces more accurate app predictions, but also has the potential to optimize smartphone system performance. This highlights the importance of considering multiple factors in predicting app usage behavior in order to benefit both users and the phone system.
412285	41228557	Sliding Mode Model Semantics and Simulation for Hybrid Systems	This article discusses the characterization and simulation of dynamic physical systems operating in sliding regimes, which involve complex continuous behaviors at multiple temporal and spatial scales. To simplify behavior generation, models use time scale and parameter abstraction techniques, resulting in hybrid systems with both discrete and continuous behaviors. Mode transitions are caused by internal state changes and external control signals, and can sometimes exhibit chattering behaviors. This presents challenges for conventional simulation methods, so the article proposes an efficient and adaptive algorithm that considers the model semantics at the discontinuous boundaries. Simulation results show that this algorithm is more accurate and efficient for sliding mode systems compared to conventional integration methods.
412285	41228544	Monitoring and fault diagnosis of hybrid systems	The paper discusses the challenges of monitoring and diagnosing networked embedded sensing and control systems with hybrid dynamics. Existing model-based approaches focus on diagnostic reasoning, assuming appropriate fault signatures are available. However, integrating model-based techniques with sensor signal acquisition and processing, as well as fault modeling, is crucial for efficient fault detection and isolation. The paper presents a hybrid automata model that parameterizes abrupt and incipient faults, and an approach for diagnoser design using this model. It also introduces a mode estimation algorithm that uses model-based prediction to improve distributed processing of signals. A diagnostic system architecture that integrates modeling, prediction, and diagnosis is described and applied to fault diagnosis of an electro-mechanical machine. The results demonstrate the effectiveness of the approach and describe the design considerations for implementing the algorithms in online applications.
412286	41228629	A Design Model For Lifelong Learning Networks	The implementation of lifelong learning facilities has become a top priority for higher education institutions and distance learning programs, aiming to meet the needs of both industry and society. With the help of ICT networks, these facilities can be accessed seamlessly and ubiquitously at home, work, and in schools and universities. This requires a shift from traditional course and program-focused models to a learner-centered and learner-controlled approach. A conceptual model based on self-organization theory, learning communities, agent technologies, and learning technology specifications has been developed, and an exploratory implementation has been put into practice. Further reflection and development is needed to fully support lifelong learning.
412286	41228627	Testing the pedagogical expressiveness of IMS	The IMS Learning Design specification (LD) was created to improve upon existing learning technology specifications. Unlike other specifications, LD is an abstract model that can adapt content and integrate assessments based on different pedagogical approaches. To evaluate the pedagogical expressiveness of LD, 16 lesson plans were converted into LD using three different methods. Difficulties in expressing the lesson plans included sharing documents within a group, giving instructions before an activity, random group member assignments, creating an inventory for pre-knowledge mapping, and communicating lesson delivery information to teachers. However, no situations were found to be impossible to express with LD. The difficulties identified are discussed and suggestions are given for handling them. The three methods used are compared and recommendations are made for further research.
412287	41228745	Probabilistic Sensor Fusion for Ambient Assisted Living.	There is a growing need to improve the current methods of providing healthcare, especially in the home setting. As part of the Sensor Platform for HEalthcare in Residential Environment (SPHERE) Interdisciplinary Research Collaboration (IRC), researchers are developing a multi-sensor platform with various network connections. However, this presents challenges in combining data from different sensors. To address this, Bayesian models for sensor fusion are introduced, which can identify the most useful modalities and relevant features for specific activities. The model also combines location prediction and activity recognition, resulting in a more efficient and accurate system. Testing on data from the SPHERE house shows its effectiveness and superiority over other benchmark models.
412287	41228740	Combining Bayesian Networks with Higher-Order Data Representations	Higher-Order Bayesian Networks (HBNs) are a type of probabilistic reasoning model that combines the efficiency of Bayesian Networks with the expressive capabilities of higher-order logics. This paper outlines how HBNs can be used to define probability distributions over higher-order terms and provides an example of their application in the Mutagenesis domain. By utilizing probabilistic inference and model learning, HBNs can be used to construct a probabilistic classifier. This approach has potential applications in fields such as Inductive Logic Programming. 
412288	41228868	A Systematic Approach to Platform-Independent Design Based on the Service Concept	This paper discusses the importance and benefits of using the service concept in the model-driven design of distributed applications. A service is defined as the observable behavior of a system without imposing constraints on its internal structure. By utilizing services and designing application parts based on them, the design is not limited by interaction patterns provided by a specific middleware platform. This allows for platform-independence and potential reuse of application parts across various middleware platforms. The service concept is also used in defining an abstract platform, which considers the characteristics of potential target middleware platforms for platform-independent design. The paper also explores the trade-offs and alternatives for platform-specific realization in this approach.
412288	41228836	Interaction refinement in the design of business collaborations	Inter-organizational business collaborations are complex, and their design may require explicit modelling at multiple abstraction levels. This involves specifying the collaboration as a single abstract interaction at a high level, and breaking it down into more concrete interactions at a lower level. Interaction refinement is a design operation that replaces an abstract interaction with multiple related concrete interactions. Having a suitable interaction design concept for modelling at different abstraction levels can make this process easier. It is also important to have supporting design operations to ensure the correctness and conformity of the interaction refinement. This paper introduces an interaction design concept and associated operations to aid in the design of business collaborations at different abstraction levels.
412289	41228917	A reconstruction and extension of Maple's assume facility via constraint contextual rewriting	Maple's symbolic evaluator and assume facility allow for powerful conditional rewriting. Previous research showed that Maple's evaluation can be seen as constraint contextual rewriting (CCR), which integrates decision procedures through a specific interface. This study extends the analysis to the general solver, which handles problems beyond linear arithmetic. A flaw was discovered that causes Maple to give incorrect results in certain contexts, as it falsely assumes the general solver uses all available assumptions. While a quick fix would weaken the assume facility, a more comprehensive approach based on CCR techniques avoids this issue and results in stronger simplification abilities.
412289	41228927	New results on rewrite-based satisfiability procedures	Program analysis and verification require decision procedures to reason on theories of data structures. These problems can often be reduced to the satisfiability of sets of ground literals in a particular theory. If a reliable inference system for first-order logic is guaranteed to terminate on these satisfiability problems, any theorem-proving strategy using that system and a fair search plan can be considered a satisfiability procedure for that theory. In this study, the termination of a rewrite-based first-order engine on theories of records, integer offsets, integer offsets modulo, and lists is proven. A modularity theorem is also introduced, stating conditions for termination on combined theories, given termination on each individual theory. Various benchmarks are presented to test the performance of this approach, including synthetic benchmarks for scalability and real-world problems with large sets of literals. A comparison with other validity checkers shows that the rewrite-based theorem prover E performs well, challenging the belief that general-purpose provers are inferior to reasoners with built-in theories. This study demonstrates the elegance and practical implications of the rewriting approach for decision procedures in program analysis and verification.
412290	41229071	A Theory of Singly-Linked Lists and its Extensible Decision Procedure	The availability of a decision procedure for pointer-based data structures is essential for effective reasoning. However, current solutions only offer approximate solutions that lack precision in verification techniques. In this paper, a new theory called TLL is introduced for singly-linked lists, which can accurately express both data and reachability constraints. The decidability of TLL is ensured and a practical decision procedure is designed, making it compatible with existing decision procedures for first-order logic theories. This provides a more precise and efficient approach to reasoning about pointer-based data structures.
412290	41229015	Light-weight theorem proving for debugging and verifying units of code	Software bugs can be difficult to find, even in small sections of code. To help with this, techniques have been developed that generate a set of formulas to check for errors. However, these techniques rely on a theorem prover to automatically solve the proofs. Building such a tool can be challenging and prone to errors. In this paper, the authors propose combining superposition theorem provers and BDDs to create a highly automatic and flexible prover. Experimental results on C functions manipulating pointers show that this method can handle proof obligations that other tools cannot, and performs better in general.
412291	412291104	A First Approach to Provide Anonymity in Attribute Certificates	This paper discusses the importance of authorization and anonymity in internet applications. Traditional authorization methods are not always effective, but attribute certificates proposed by ITU-T offer a suitable solution. As internet transactions are easily recorded, anonymity has become a desirable feature. The paper proposes a solution to enhance X.509 attribute certificates to include conditional anonymity. A protocol is also presented for obtaining these certificates while protecting user anonymity through a blind signature scheme. The use of these certificates is explored, along with potential problems and open issues. 
412291	412291143	A New Design of Privilege Management Infrastructure for Organizations Using Outsourced PKI	Public Key Infrastructures (PKI) have been used for authentication services in e-commerce applications, but they do not fully meet the needs of these applications. Additional authorization services are needed for users to prove their permissions. Attribute certificates have changed the approach to authorization, and Privilege Management Infrastructures (PMI) provide the necessary support for their use. While PKIs and PMIs are related, they can operate independently, making it appealing for companies to outsource PKI services. However, due to the confidential information contained in attribute certificates, outsourcing PMI services is not always a viable option. This paper presents a new design for PMIs that is suitable for companies outsourcing PKI services but still needing to manage PMIs internally. This scheme also addresses issues with revocation procedures and offers additional benefits for intra-company attribute certification. 
412292	41229212	Automatically Improving SAT Encoding of Constraint Problems Through Common Subexpression Elimination in Savile Row.	Efficient solving of Propositional Satisfiability (SAT) problems relies heavily on the formulation of the problem instance. Preprocessing and inprocessing techniques have been developed to improve the efficiency of SAT solvers, but these methods operate on the low-level representation of the problem which makes it difficult and costly to identify higher-level patterns. Instead of directly reformulating the SAT representation, this approach uses automated reformulations on a higher level representation of the problem called a constraint model. This approach, known as Common Subexpression Elimination (CSE), has been shown to significantly improve the formulation of constraint satisfaction problems and can also have similar benefits when encoding the reformulated model to SAT and solving it with a state-of-the-art solver, resulting in speed improvements of over 100 times in some cases. 
412292	41229233	Propagation algorithms for lexicographic ordering constraints	Finite-domain constraint programming is a powerful tool for solving complex combinatorial problems in various fields. It involves modeling the problem using constraints on a set of decision variables. Symmetry in the decision variable matrices can lead to redundancy in searching for solutions. To address this, an algorithm called GACLexLeq has been developed to establish generalised arc consistency by constraining the rows and columns to be in lexicographical order. This algorithm has been shown to be efficient and can be decomposed into smaller constraints for better performance. Extensions and modifications have also been made to handle strict lexicographical ordering and unequal vector lengths. Experimental results demonstrate the effectiveness of GACLexLeq in various domains.
412293	41229325	Collaborative work with linear classifier and extreme learning machine for fast text categorization	This paper discusses the importance of fast text categorization in the digital age and explores current methods that either sacrifice accuracy for speed or vice versa. The authors propose a new approach that combines a linear classifier and an extreme learning machine (ELM) to achieve both speed and accuracy. The linear classifier, obtained through a modified non-negative matrix factorization algorithm, quickly maps documents from the original term space into the class space. The ELM then uses nonlinear and linear transformations to further improve classification accuracy. Experimental results show that this approach is not only accurate but also significantly faster, with a 180% increase in speed compared to previous methods.
412293	41229352	Dimensionality reduction with category information fusion and non-negative matrix factorization for text categorization	Dimensionality reduction is a technique used to improve the efficiency of classifiers in text categorization. Non-negative matrix factorization is a commonly used method to map high dimensional term spaces into low dimensional semantic subspaces. However, this method is sensitive to noise, data missing, and outliers, leading to unsatisfactory classification performance. To address this issue, a new approach has been proposed that combines the training text and category information to obtain a transformation matrix using basis orthogonality non-negative matrix factorization and truncation. This allows for aggressive dimensionality reduction while maintaining good classification performance. Experimental results demonstrate the effectiveness of this approach in achieving a low dimensional semantic subspace while maintaining accurate classification.
412294	41229462	Post-processing scheme for improving recognition performance of touching handwritten numeral strings	This paper outlines a method for improving recognition accuracy of handwritten numeral strings that are touching. The approach involves using verification factors, such as structural features and recognition probability, to identify and correct mis-segmented digits. A reconsideration condition is introduced, along with an additional segment digit set, to further refine the recognition results. Through experiments on a database of touching handwritten numeral strings, the proposed method showed promising results in terms of both accuracy and reliability.
412294	412294211	The recognition of handwritten numeral strings using a two-stage HMM-based method	This paper proposes a two-stage HMM-based recognition method that addresses the trade-off between segmentation and recognition in an implicit segmentation-based strategy. The first stage involves an implicit segmentation process that uses contextual information to generate multiple segmentation-recognition hypotheses. These are then verified and re-ranked in the second stage using an isolated digit classifier. The method allows for the use of two sets of features and numeral models, one focusing on both segmentation and recognition and the other only on recognition. Experimental results on 12,802 handwritten numeral strings showed that the two-stage method improved recognition rates by 9.9% on average and achieved a recognition rate of 89.6% on touching digit pairs. Overall, the two-stage approach shows promise in compensating for the trade-off between segmentation and recognition in handwritten numeral recognition.
412295	41229530	Timed default concurrent constraint programming	Synchronous programming is a powerful approach for programming reactive systems. It is based on the idea that processes are relations extended over time, and it provides a simple but effective model for timed, determinate computation. This model, called tcc, is extended to include strong time-outs, which are supported in synchronous programming languages. These operations are non-monotonic, but a compositional semantics is provided using Reiter's default logic. This framework, called Default cc, has a basic set of combinators and supports multiform time, orthogonal pre-emption, and executable specifications. It can also be read as logical formulae and compiled into finite state automata, allowing for separate compilation and run-time tradeoffs.
412295	41229552	Timed constraint programming: a declarative approach to usage control	This paper discusses different policy languages for access control, specifically in the form of trust-management systems and usage control. The authors propose a policy algebra in the timed concurrent constraint programming paradigm that addresses issues such as explicit denial, inheritance, and overriding, as well as history-sensitive access control. This policy algebra is declarative and supports equational reasoning through coinductive proofs based on an operational semantics. The authors also demonstrate the ability to perform security analysis with dynamic state-dependent restrictions using a combination of constraint reasoning and model checking techniques based on linear time temporal-logic. Overall, their framework allows for efficient and comprehensive reasoning about policies in access control.
412296	412296123	Maximizing bichromatic reverse nearest neighbor for  -norm in two- and three-dimensional spaces	The Bichromatic Reverse Nearest Neighbor (BRNN) problem has been widely studied in spatial database research. This paper introduces a related problem called MaxBRNN, which involves finding the optimal region that maximizes the size of BRNNs for the L p-norm in two- and three-dimensional spaces. This problem has practical applications, such as finding the best location for a new server to attract the most customers based on proximity. Previous approaches have been limited due to the large number of possible points. However, the authors propose an efficient algorithm called MaxOverlap that takes advantage of certain properties of the problem. Extensive experiments demonstrate the effectiveness of this algorithm. 
412296	412296120	Dynamic skyline queries in large graphs	This paper discusses dynamic skyline queries in a large graph, which report data points that are not dominated by other data points according to the distances between them and query points. While previous studies have focused on dynamic skylines in Euclidean space, road networks, and metric space, there has been no research on dynamic skylines over large graphs. The authors propose a filter-and-refine framework for efficient query processing and a pruning rule based on graph properties to derive candidates for the query. They also use a carefully-designed index structure to compute short path distances between vertices in O(H) time. Experimental results show that their methods outperform existing algorithms significantly.
412297	41229728	Mosaicing-by-recognition for video-based text recognition	Recognizing text from a hand-held video camera can be difficult because the camera may capture multiple frames with overlapping text. To improve the quality of the text image, the frames must be registered and mosaiced, accounting for camera shakes. This paper proposes a mosaicing-by-recognition technique that combines video mosaicing and text recognition into a single optimization problem. Using a dynamic programming-based optimization algorithm, this technique can accurately estimate distortions and achieve high character recognition accuracy, even with various camera movements. Experimental results show that the proposed technique can achieve around 90% perfect distortion estimation and over 95% character recognition accuracy. 
412297	41229725	A new method for multi-oriented graphics-scene-3D text classification in video	Text detection and recognition in video poses a challenge due to the various types of text present, such as graphics, scene, 2D, 3D, static, and dynamic. A universal method that works well for all types is difficult to develop. In this paper, a novel approach is proposed for classifying graphics-scene and 2D-3D texts in video to improve accuracy. The method includes an iterative process to identify text candidates, using stroke width and medial axis values to classify graphics and scene texts, and combining proximity and gradient directions for 2D and 3D texts. Each step of the proposed method is evaluated and compared to existing methods, showing the effectiveness and necessity of video text classification in enhancing text detection and recognition systems. 
412298	41229842	Real-Time Retrieval for Images of Documents in Various Languages Using a Web Camera	The proposed method is a real-time retrieval approach for document images in different languages. It uses queries in the form of images captured by a web-camera to retrieve corresponding document images from a database. This method is an extension of a previous method for English documents, which only used centroids of word regions as feature points and was not applicable to languages like Japanese and Chinese. The proposed method introduces additional features to enable real-time retrieval for document images in various languages. 
412298	4122981	A mixed reality head-mounted text translation system using eye gaze input	This paper discusses a system that allows for real-time translation and reading of text using eye gaze as input on head mounted displays (HMDs). The system is designed for travelers in Japan who need to read signs. The user can indicate regions of interest in the text document using their eyes, which activates optical-character-recognition (OCR) and translation functions. The system provides visual feedback and navigation to aid in the interaction process. The study evaluates two different gaze gestures for activating the OCR function and shows that using eye gaze for OCR is faster than traditional methods. Other benefits include real-time visual feedback, real-time translation, and precise placement of translated text in the user's view.
412299	41229925	Quantified reading and learning for sharing experiences.	This paper discusses two main topics: the "experiential supplement" project, which aims to capture and transfer human experiences to others, and sensing technologies for producing these supplements in the context of learning. The focus is on reading, and methods for quantifying reading activities and estimating English ability and confidence are presented with experimental results. These measurements are taken using various sensors, including eye trackers, EOG, EEG, and first-person vision. The paper highlights the potential of using these technologies to enhance the learning experience.
412299	41229930	Manga content analysis using physiological signals.	The study of physiological signals has become increasingly popular in understanding everyday activities such as watching videos or looking at pictures. Tracking these signals can provide insight into the user's mental state and emotions. Researchers are now exploring the relationship between a reader's physiological signals and their feelings while reading, as well as how it relates to the content of the reading material. In a recent study, researchers used blood volume pulse, electrodermal activity, and skin temperature to analyze a reader's physiological signals while reading manga (Japanese comic books). They found that by using a support vector machine, they were able to accurately predict which genre of manga the reader was reading with 90% accuracy. This research sheds light on the potential for using physiological signals in understanding reading experiences.
412300	41230043	Towards a Conscious Choice of a Similarity Measure: A Qualitative Point of View	This paper examines the importance of similarity measures in various applications, such as case based reasoning and data mining. The authors propose studying these measures based on the ordering relation they create between objects. They use a method from measurement theory to determine the conditions for a numerical measure or a class of measures to represent a given ordering relation. The focus is on the various conditions of independence that must be met. This research highlights the significance of carefully selecting a similarity measure in order to accurately represent the relationship between objects in different contexts. 
412300	41230047	Towards a conscious choice of a fuzzy similarity measure: a qualitative point of view	This paper proposes a study of similarity measures for fuzzy subsets based on the ranking relation they create between pairs of objects. By using a method from measurement theory, the authors establish conditions for the existence of numerical similarity measures that can accurately represent a given ordering relation. These conditions depend on the axioms satisfied by the ordering relation. This research provides a framework for understanding and comparing different measures of similarity for fuzzy subsets.
412301	41230147	Acyclic Directed Graphs to Represent Conditional Independence Models	This paper focuses on conditional independence models that follow the graphoid properties. It explores the use of acyclic directed graphs (DAGs) to represent these models and proposes a new algorithm for constructing a DAG based on an ordering of random variables. The benefits and unique features of using DAGs in this context are discussed. The paper also presents conditions that guarantee the existence of perfect maps, which can be used to develop a procedure for finding perfect maps for certain types of independence models.
412301	41230124	Acyclic directed graphs representing independence models	This paper discusses the representation of probabilistic independence models, specifically those that adhere to graphoid properties. The main focus is on acyclic directed graphs (DAGs) and a new algorithm is proposed for constructing a DAG based on a given ordering of random variables. The benefits and unique features of this approach are also discussed. Additionally, the paper presents a condition for the existence of a perfect map that represents an independence model and outlines an algorithm based on this condition. 
412302	412302108	EMD-L1: an efficient and robust algorithm for comparing histogram-based descriptors	EMD-L1 is a fast algorithm for calculating the Earth Mover's Distance (EMD) between two histograms. It simplifies the original formulation by reducing the number of unknown variables to O(N) and the number of constraints by half. The algorithm is formally equivalent to the original EMD with L1 ground distance and uses an efficient tree-based approach to compute the distance. It has a time complexity of O(N2), making it much faster than previous algorithms. EMD-L1 allows for comparing histogram-based features, which was previously not feasible. Experiments on shape recognition and interest point matching show that EMD-L1 outperforms previous state-of-the-art methods in these tasks.
412302	412302101	Diffusion Distance for Histogram Comparison	In this paper, the authors introduce a new measure called diffusion distance for comparing histogram-based descriptors. This measure uses a temperature field to represent the difference between two histograms and takes into account deformation and quantization effects through a diffusion process. The diffusion distance is computed as the sum of dissimilarities over various scales and is robust to changes in shape, lighting, and noise. It also has a linear computational complexity, making it more efficient than other cross-bin distances with higher complexities. The authors tested the approach on shape recognition and interest point matching tasks and found that the diffusion distance outperforms other state-of-the-art measures in terms of both accuracy and efficiency.
412303	41230322	DAILY PHYSIOLOGICAL SIGNAL MONITORING SYSTEM FOR FOSTERING SOCIAL WELL-BEING IN SMART SPACES	The proposed system is a socially acceptable way to monitor physiological signals using a natural sensing interface, BioPebble, and an intuitive information display, Rainbow display. The BioPebble is a simple and enjoyable stone-type interface embedded in everyday objects, while the Rainbow display uses an intuitive visualization method for novice users. The information is wirelessly transferred to a mobile personal station and mapped according to individual differences. The experiment showed that the BioPebble is more comfortable and aesthetically pleasing than disposable wired interfaces, and the Rainbow display is easily understood by novice users. This system aims to promote social well-being by encouraging regular monitoring of physiological signals for early detection of abnormalities.
412303	412303122	Service Conflict Management Framework for Multi-user Inhabited Smart Home	The paper presents a service conflict management framework for handling conflicts between multiple users in a smart home using context-aware applications. The framework utilizes an ontology to describe applications and their services, an approach determination tree to assign resolution strategies, and a set of resolution strategies. Using this framework, conflicts can be dynamically detected and resolved based on the involved properties, their relationship, and user preferences. The resolution can be automatic or user-driven. The framework was tested in a smart home test-bed and successfully detected and resolved conflicts between services of different applications and within a single application.
412304	41230429	A Logical Framework For Set Theories	Axiomatic set theory is widely accepted as the foundation of mathematics and is useful for managing mathematical knowledge. However, there are gaps between the official set theory formulations and actual mathematical practice. This work presents a new framework for formalizing different strengths of axiomatic set theories, from basic to full ZF. It allows the use of set terms, but ensures their validity through a static check. The framework is based on two set-theoretical principles: extensionality and comprehension, with the option to include induction and the axiom of choice. Comprehension is formulated as a relation between elements and formulas. Different systems vary in their use of safety relations, which are defined syntactically and based on the safety relation used in commercial query languages for databases.
412304	41230413	Formalizing Scientifically Applicable Mathematics in a Definitional Framework.	In [3], a new framework for formalizing mathematics was created, based on first-order set theory. This framework reflects real mathematical practices by using abstract set terms {x vertical bar phi} in a similar way to ordinary mathematical discourse. The paper showcases how fundamental and scientifically applicable mathematics can be developed within this framework using a weak and predicatively acceptable set theory that is essentially first-order. This theory has the key property that all objects used in it are defined by closed terms, allowing for a concrete and computationally-oriented interpretation. The framework can also be extended to handle stronger set theories, such as ZF. 
412305	412305223	Ranking queries on uncertain data: a probabilistic threshold approach	Uncertain data is a common occurrence in certain applications like environmental surveillance and mobile object tracking. Top-k queries, which rank the data based on a certain criteria, are useful in analyzing uncertain data in these applications. This paper focuses on the problem of answering probabilistic threshold top-k queries, which calculates the probability of at least p for an uncertain record to be in the top-k list. The paper presents three algorithms - an exact one, a fast sampling one, and a Poisson approximation based one - for efficiently answering these queries. An empirical study using real and synthetic data shows the effectiveness of probabilistic threshold top-k queries and the efficiency of the proposed algorithms.
412305	412305218	Probabilistic path queries in road networks: traffic uncertainty aware path selection	Path queries, which involve finding the shortest path in terms of travel time between two points in a road network, are widely used in various applications. Currently, simple statistic aggregates are commonly used to answer such queries, but they often fail to capture the uncertainty in traffic. In this paper, the authors propose three types of probabilistic path queries that take traffic uncertainty into account, using basic probability principles. These queries involve finding paths that meet certain criteria, such as a maximum travel time with a high probability. To efficiently evaluate these queries, the authors develop several probability calculation methods and a best-first search algorithm. An empirical study with real and synthetic data shows the effectiveness of the proposed queries and the efficiency of the evaluation methods.
412306	41230628	Unfolding Based Minimal Test Suites for Testing Multithreaded Programs	This paper discusses the challenge of finding the smallest set of tests that cover all executable statements in a multithreaded program. Previous research has used unfoldings to accurately represent the concurrency in such programs and generate test cases. Building on this, the paper presents a method for using unfoldings to generate the minimal test suite that covers all executable statements. However, this problem is shown to be NP-complete in the size of the unfolding. The paper also proposes using SMT-encodings to solve this problem and provides preliminary results for several benchmarks. Additionally, the paper shows that finding the minimal test suite for any terminating safe Petri net is also NP-complete in the size of its unfolding. 
412306	41230630	Linear Encodings of Bounded LTL Model Checking	This content discusses the problem of bounded model checking for linear temporal logic (LTL) and presents efficient encodings that have a linear size. The encodings can also be extended to LTL with past operators (PLTL) but cannot detect minimal length counterexamples. To overcome this limitation, the virtual unrolling technique is introduced, but it results in a quadratic size encoding. The virtual unrolling is also extended to Buchi automata, allowing for the detection of minimal length counterexamples. These encodings can also be made incremental to benefit from incremental SAT technology. Experiments show that these dedicated encodings improve the performance of BMC and are competitive in finding bugs. However, for proving complex properties, BDD-based methods still tend to be more efficient. 
412307	41230714	Oblivious Network RAM and Leveraging Parallelism to Achieve Obliviousness.	ORAM, or Oblivious RAM, is a cryptographic tool that allows a trusted CPU to access untrusted memory without revealing any information about the data being accessed. It has many potential uses in secure processor design and secure multi-party computation for managing large amounts of data. However, due to a lower bound on its efficiency, ORAM incurs a moderate cost in practical applications. To address this issue, the Oblivious Network RAM model of computation is proposed, where a CPU communicates with multiple memory banks in a way that hides the specific address being accessed. This reduces the overall cost of obliviousness and has potential applications in secure processor design and distributed storage with a network adversary. New constructions for simulating programs in this model are also presented.
412307	41230715	SCORAM: Oblivious RAM for Secure Computation	ORAMs are often measured by their bandwidth overhead and client storage, but when used in secure computation protocols for RAM programs, the size of the ORAM circuits becomes more important. This study focuses on the circuit-complexity of various ORAM constructions and finds that asymptotic analysis does not accurately reflect their performance in practical scenarios. A new design called SCORAM is introduced, which is optimized for secure computation protocols and is almost 10x smaller in circuit size and faster than other designs tested for realistic settings. This makes it possible to perform secure computations on large data sets, up to gigabyte sizes.
412308	41230857	Reducing power density through activity migration	Modern microprocessors have uneven distribution of power dissipation, resulting in localized hot spots with higher temperatures than surrounding areas. This can lead to decreased reliability and even failure. To address this issue, activity migration is proposed, which involves moving computation between multiple replicated units to reduce peak junction temperature. By incorporating a thermal model that considers the temperature-dependent leakage power, it is shown that this technique can increase sustainable power dissipation by almost two times while maintaining the same junction temperature limit. Alternatively, peak die temperature can be reduced by 12.4°C at the same clock frequency. The model suggests that migration intervals of 20-200 μs are necessary for maximum sustainable power increase. Different forms of replication and migration policy control are evaluated. 
412308	41230854	Replacing global wires with an on-chip network: a power analysis	This paper explores the potential benefits of replacing global chip wires with an on-chip network in terms of power consumption. By optimizing network links through repeater spacing, link pipelining, and voltage scaling, the energy required to send data across the chip can be significantly reduced. The paper presents an analytical model for large chip designs using a two-dimensional mesh network, and examines potential power savings for two different design scenarios: a circuit-switched ASIC or FPGA design, and a dynamic packet-switched tiled architecture. Results show that for circuit-switched networks, power savings of 35-50% can be achieved with 1 mm links, while the use of multiplexing and signal encoding in packet-switched designs can result in around 20% savings with optimal tile sizes of 2 mm. 
412309	41230938	Optimal maintenance of a spanning tree	This article discusses the importance of keeping track of history in dynamic network protocols and presents a communication optimal maintenance algorithm for a spanning tree in a dynamic network. This algorithm has an amortized message complexity of O(V), where V is the number of nodes, and a message size of O(log &verbar;ID&verbar;). This efficient maintenance of a spanning tree can improve various distributed algorithms such as broadcast, multicast, routing, and termination detection. The authors also introduce a novel technique to save communication by using past information to deduce present information. This technique is one of the main contributions of the study.
412309	41230977	Adapting to asynchronous dynamic networks (extended abstract)	The computational power of different communication models is an important topic in distributed computation theory. In some models, messages are assumed to be delivered quickly, while in others, delays can be unpredictable. The topology of the network is also a crucial factor, with some models allowing for dynamic changes in the network structure. In this paper, the authors demonstrate that the model of an asynchronous, dynamic-topology network is equivalent to a synchronous, static model with the trade-off of only a small increase in time, messages, and space required. This method has been successfully applied to various applications such as Breadth First Search, routing tables, and packet routing.
412310	41231038	Transliteration by bidirectional statistical machine translation	The paper discusses a system that uses statistical machine translation to transliterate between all language pairs in a shared task. This technique does not rely on language-specific assumptions or dictionaries, and instead directly translates sequences of tokens from the source language to the target language. The system is comprised of two phrase-based SMT decoders, one generating the target from the first token to the last, and the other from the last to the first. Results show that a combination of these two approaches is most effective, with the optimal choice depending on the specific languages involved.
412310	41231025	Rescoring a phrase-based machine transliteration system with recurrent neural network language models	This year's shared transliteration evaluation includes a system that uses a phrase-based statistical machine transliteration framework. It incorporates a joint source-channel model, a target language model, and models for controlling sequence length. The joint source-channel model was trained using a Bayesian bilingual alignment. The main focus of the system is on input representation, and to address data sparseness issues, it has been augmented with recurrent neural network models. Experiments on development data showed that using an RNN language model can improve performance for language pairs with large grapheme sets on the target side.
412311	41231135	Using ensemble method to improve the performance of genetic algorithm	Ensemble methods, which involve combining multiple learning algorithms to improve performance, have been extensively researched and applied in machine learning. The basic concept is to use a "weak" algorithm that performs slightly better than random guessing, and "boost" it into a highly accurate "strong" algorithm. Building upon this idea, a paper proposes a new hybrid optimization algorithm called GA ensemble, which uses ensemble methods to enhance the performance of genetic algorithms. This involves running multiple genetic algorithms on the same problem and using bagging to sample from a pool of solutions. Experimental results on various optimization problems demonstrate that the ensemble approach greatly improves the performance of genetic algorithms.
412311	41231160	Optimal motion generation of flexible macro-micro manipulator systems using estimation of distribution algorithm	This paper introduces a new method for generating and optimizing motion in a flexible macro-micro manipulator system using the Estimation of Distribution Algorithm (EDA). This system is considered to be challenging due to its redundancy and lack of a general solution. The EDA algorithm utilizes a Gaussian probability model to generate optimal joint motions for the system. Compared to simple genetic algorithms (SGA), this approach requires fewer parameters and significantly reduces the time for motion optimization. Simulation results demonstrate that the proposed approach is highly effective in generating and optimizing motion for the flexible macro-micro manipulator system. 
412312	41231213	The Mystery of Structure and Function of Sensory Processing Areas of the Neocortex: A Resolution.	This paper discusses various neural models that have been proposed to explain memory in primates. While there is no direct evidence to support one model over others, the authors suggest that models can be evaluated based on their complexity and predictive power. The authors propose a computational framework that assumes neural information processing is done by generative networks. This framework suggests a possible relationship between functional memory units and information encoding in primates, as well as the entorhinal-hippocampal loop. The authors also explore the idea that different layers of the cortex may have different functions. Finally, the concept of the homunculus fallacy is discussed in relation to this computational framework. Keywords include auto-associator, information processing, entorhinal-hippocampal loop, and information maximization.
412312	41231251	Controlled hierarchical filtering: Model of neocortical sensory processing	The article presents a model for processing sensory information that emphasizes the importance of learning internal generative models. These models can predict future events and evaluate the accuracy of those predictions. The model also connects to goal-oriented systems and is based on the structural similarities between a robust controller and the hippocampal entorhinal loop. The generative control architecture is mapped to the neocortex and hippocampal entorhinal loop, with implicit memory phenomena and priming being key aspects of the model. Mathematical theorems ensure the stability and effectiveness of the architecture, and connections to reinforcement learning are also established. The model also makes predictions about the role of feedback connections between neocortical areas.
412313	41231345	Communication Efficient Coresets for Empirical Loss Minimization.	This paper addresses the issue of minimizing empirical loss with l2-regularization in distributed settings where communication cost is significant. Traditional methods like SGD are commonly used, but their high communication cost can lead to decreased performance. To address this, the paper proposes a new approach that uses a small summary of the data, called coreset, to reduce communication cost while maintaining good convergence. The paper presents a general framework for analyzing coreset-based optimization and offers insights into existing algorithms. A new coreset construction is also introduced, along with its convergence analysis for problems like logistic regression and support vector machines. Preliminary experiments on real-world datasets show promising results for this algorithm.
412313	41231311	Budgeted Distribution Learning of Belief Net Parameters	This article discusses the challenge of learning Bayesian belief networks, which are models of probabilistic distributions, when data is not initially available. The focus is on the problem of learning the parameters for a fixed structure that best match the true distribution, while taking into account a limited budget for acquiring data. The article explores various algorithms for this task, including non-sequential and sequential approaches. It is shown that the greedy allocation algorithm is optimal in some cases, but not always. The effectiveness of other heuristic algorithms is also explored.
412314	412314111	From Color Sensor Space to Feasible Reflectance Spectra	The interaction between light and object surfaces produces color signals that are responsible for the outputs of digital acquisition systems. In order to reverse this process and retrieve wavelength information, a lot of research has been done since 1964. The main approach is to use linear models to establish a one-to-one relationship between sensor data and reflectance spectra, while ensuring the quality and naturalness of the recovered spectrum. In this paper, the authors propose a solution to improve the quality of the recovered spectrum by considering smoothness and physical feasibility, which is achieved through the use of linear models. This strategy can be applied to any generic recovery method. 
412314	41231462	Multidomain Pixel Analysis For Illuminant Estimation And Compensation	This paper presents an algorithm for illuminant estimation and compensation in digital still cameras, which is not dependent on the device used. It takes into account both chromaticity and intensity information and uses a diagonal transform for compensation. The algorithm combines a spatial segmentation process with weighting functions to select objects with the most information for accurate chromaticity estimation. It was evaluated on a database of real scene images with controlled illuminant conditions and showed improved performance compared to single domain pixel analysis. The algorithm was designed using an experimental framework developed by the authors.
412315	41231542	CAESAR_SOLVE: A generic library for on-the-fly resolution of alternation-free Boolean equation systems	Boolean equation systems (Bess) are a useful tool for modeling verification problems on finite-state concurrent systems. These problems, such as equivalence checking and model checking, can be solved on the fly without explicitly constructing the state space of the system. A demand-driven approach is used for construction and resolution of the Bess. A generic software library has been developed for on-the-fly resolution of alternation-free Bess. It currently offers four resolution algorithms, including specialized ones for handling specific types of Bess in a memory-efficient way. The library is developed within the Cadp verification toolbox and is used for on-the-fly equivalence checking, model checking, and state space reduction.
412315	41231529	Improved On-the-Fly Equivalence Checking Using Boolean Equation Systems	Equivalence checking is a verification method used to ensure that a concurrent system (protocol) behaves as intended (service) by comparing their underlying labeled transition systems (Ltss) using an equivalence relation. The on-the-fly approach for equivalence checking incrementally explores the synchronous product of Ltss to avoid state explosion and efficiently detect errors in complex systems. However, when the Ltss are equivalent, the global approach outperforms the on-the-fly approach by building the Ltss and calculating equivalence classes with partition refinement. This paper proposes two enhancements to the on-the-fly approach for equivalent Ltss: a faster encoding of equivalence relations and a more efficient local boolean equation system (Bes) resolution algorithm. These enhancements were incorporated into the Bisimulator2.0 equivalence checker and showed significant performance improvements compared to existing on-the-fly algorithms.
412316	41231666	Factors Influencing Digital Forensic Investigations: Empirical Evaluation of 12 Years of Dubai Police Cases.	Digital forensics is a crucial aspect of investigations, where minimizing the number of person-hours spent while ensuring the authenticity of evidence is important. However, the literature shows that there are several challenges that lead to an increase in person-hours. This paper examines these challenges and argues that they do not fully explain the increase in investigation time. The study uses real case records from the Dubai Police and identifies other factors that contribute to the increase in person-hours. The paper concludes by highlighting the various factors that affect person-hours, contrary to what is commonly proposed in the literature.
412316	41231618	Constructing The Cool Wall: A Tool To Explore Teen Meanings Of Cool	This paper introduces a tool called the Cool Wall, which was created to help designers understand what is considered "cool" by teenagers when designing interactive products. The tool is based on theoretical concepts of coolness and consists of seven core categories that are mapped to a hierarchy. The paper discusses the development and construction of the Cool Wall, as well as the results and analysis of three studies. The first study was conducted in a school in the UK and provided clear insights into cool things for teenagers. The second and third studies revealed potential shortcomings in the methodology but also provided valuable insights into the meaning of the images used in the Cool Wall. Overall, the Cool Wall is a useful tool for understanding coolness in the design of interactive products for teenagers.
412317	41231762	The Phenomenology of Remembered Experience: A Repertoire for Design.	The study aims to understand how people experience autobiographical remembering and how design-driven research can benefit from this understanding. Through interviews using the repertory grid technique, the study identifies five categories of personal constructs that characterize people's remembered experiences: contentment, confidence/unease, social interactions, reflection, and intensity. These categories align with previous classifications of personal constructs and models of human emotion. The study suggests that this categorization can be used to evaluate interactive technologies for supporting remembering and highlights the potential for future improvements.
412317	41231761	Expected information needs of parents for pervasive awareness systems	This paper discusses the use of awareness systems to meet the communication needs of busy parents. The authors conducted an online survey with 69 participants to explore the types of information that are most valued by parents. The results showed that personalized and intentional information is preferred over low granularity and automatically collected information. Additionally, the study found that people's attitudes towards sharing information about themselves align with what they want to know about their partners. Furthermore, the direction and symmetry of information flow do not significantly impact people's preferences for information. These findings suggest that awareness systems can effectively support busy parents' communication needs.
412318	41231832	The International Children's Digital Library: Description and analysis of first use	The International Children's Digital Library (ICDL) is a five-year research project aimed at providing children with access to a diverse collection of children's books from around the world. The paper discusses the importance of this research and outlines the development of new interface technologies to make the ICDL accessible to children. It also compares the ICDL to other digital libraries for children and presents an analysis of the first seven weeks of its public use on the web. The ICDL hopes to promote literacy and cultural understanding among children through its international collection of books.
412318	41231841	Evaluation of tablet apps to encourage social interaction in children with autism spectrum disorders	The rising diagnosis rates for Autism Spectrum Disorders (ASDs) have brought attention to the need for effective interventions during childhood to improve independent living later in life. However, most adults with ASDs who received early intervention do not live independently. As a result, there is a demand for new therapies and interventions to help children with ASDs develop social skills necessary for independent living. The launch of the iPad has sparked excitement in the autism community about the potential use of multitouch tablets in interventions. While there are numerous apps marketed as beneficial for children with ASDs, there is little scientific evidence to support their effectiveness. This paper presents a study on the use of a set of apps from Open Autism Software in an afterschool program for children with ASDs. The apps are designed to promote positive social interactions through creative, expressive, and collaborative activities. The study compared activities conducted with and without the apps, finding that children using the apps spoke more, had more verbal interactions, and were more physically engaged. They also made more supportive comments during activities using two specific apps. These results suggest that this approach to using apps can enhance positive social interactions in children with ASDs.
412319	41231946	(Almost) Tight bounds and existence theorems for single-commodity confluent flows	This article discusses the concept of confluent flow, where the flow of a commodity leaves a node along a single edge. It focuses on single-commodity confluent flow problems, specifically the minimization of congestion in routing node demands to a single destination. The article presents approximation algorithms and hardness results for this problem, as well as a polynomial-time algorithm for determining a confluent flow with congestion at most 1 + ln(k) in a directed graph. It also considers a demand maximization version of the problem and shows that the gap between confluent and splittable flows is smaller in k-connected graphs. The existence of confluent flows with low congestion is proven using topological techniques. 
412319	41231924	(Almost) tight bounds and existence theorems for confluent flows	This paper discusses the problem of determining a confluent flow in a directed graph with non-negative demands on its nodes and k sinks. A flow is considered confluent if all the flow leaves a node along a single edge. The goal is to find a confluent flow that routes every node's demand to a sink with minimum congestion. The paper presents approximation algorithms, hardness results, and existence theorems for this problem. The main result is a polynomial-time algorithm that can achieve a congestion of at most 1 + ln(k) in the graph, assuming that there is a splittable flow with congestion at most 1. The paper also considers a demand maximization version of the problem and shows that the gap between confluent flows and splittable flows is small in k-connected graphs. The proof of the existence theorem for k-connected graphs uses topological techniques. 
412320	41232041	Parallel checkpoint/recovery on cluster of IA-64 computers	ChaRM64 is a high availability parallel run-time system designed for parallel programs on a cluster of IA-64 computers. It uses a user-level, single process checkpoint/recovery library and implements a coordinated checkpointing and rollback recovery mechanism, as well as quasi-asynchronous migration and dynamic reconfiguration. This allows it to effectively handle cluster node crashes and hardware faults. The system has been implemented for PVM on Linux and an MPI version is currently being developed. ChaRM64 is one of the few projects that have been successfully completed for IA-64 architecture. 
412320	41232079	Distributed cache memory data migration strategy based on cloud computing	Cloud computing has become increasingly popular, but with its vast scale comes challenges in scalability and high availability. To address these issues, a recursive N boundary network model is proposed for distributed data storage. A data management model is also developed based on the data center network structure, along with a replica distribution strategy and copy selection strategy to improve data availability and balance loads. A data migration algorithm is introduced to minimize costs and reduce energy consumption, with experimental data confirming its effectiveness. In the edge cloud computing architecture, a data migration strategy and pre-stored data migration strategy are proposed to ensure fast response times and minimize data redundancy. Additionally, a data migration strategy based on network performance is suggested to maintain balance between access network transmission and storage. Overall, these strategies aim to improve the functionality and efficiency of cloud computing systems.
412321	41232194	A Discriminative Model With Multiple Temporal Scales For Action Prediction	Intelligent systems need to quickly recognize actions in order to react effectively. This is especially important in applications like detecting criminal activity, where decisions must be made based on incomplete videos. A new model has been proposed that can predict the action class from a partially observed video by considering the temporal dynamics of human actions and incorporating both past and present features. This model also enforces label consistency between smaller segments and the overall video. Experiments on public datasets have shown that this approach outperforms other methods for predicting actions.
412321	41232133	Exploring discriminative pose sub-patterns for effective action classification	This study proposes a new approach for effective action classification using articulated configurations of human body parts. The method involves extracting discriminative pose sub-patterns from a set of 3D poses, based on the observation that these sub-patterns frequently appear in specific action classes. The proposed algorithm, called SSPI, simultaneously performs feature selection and learning by identifying and using these sub-patterns as "magnetic centers" for feature transformation. Experimental studies on a large motion capture dataset show the effectiveness of this approach in achieving superior performance compared to existing techniques.
412322	4123225	Tractable dataflow analysis for distributed systems	Automated behavior analysis is a useful tool for developing and maintaining distributed systems. This paper presents a dataflow analysis technique that can detect unreachable states and actions in distributed systems. The technique is based on an approximate approach by Reif and Smolka but is more accurate due to the use of action dependency and history sets. While it may not detect all errors, it can efficiently detect nontrivial errors in systems with loops and nondeterministic structures. This makes it a practical and tractable tool for analyzing preliminary designs of distributed systems and can be integrated into software development tools. The technique is demonstrated through case studies and a prototype implementation is presented.
412322	41232217	Structuring parallel and distributed programs	 hierarchiesThe paper introduces Darwin, a language for structuring distributed and parallel programs using groups of process instances that communicate through message passing. Darwin can also express dynamically changing structures during execution. The paper includes examples of using Darwin to construct parallel programs and describes how it can be used to map the hardware structure of distributed memory multicomputers. The approach involves separating the structure of interconnected process instances from the algorithms used to describe individual process types. This allows for context-independent process types with well-defined interfaces, enabling reuse and testing in different contexts. The program structure can be defined using a separate configuration language.
412323	41232376	An Efficient Policy System for Body Sensor Networks	Body sensor networks (BSNs) are a promising technology for healthcare, where biosensors continuously monitor a user's physiological parameters. Unlike conventional sensor networks for environmental monitoring, BSNs need to be adaptive and easily managed. Security is also crucial in BSNs, leading to the development of a policy system that enables policy-driven management at the sensor level. This system allows for adaptability through dynamic loading and the ability to enable and disable policies without shutting down nodes. It also enables fine-grained access control through authorization policies. The design and implementation of this policy system are discussed, and experimental results show its viability and potential to accelerate application development for BSNs in healthcare.
412323	41232343	Finger: An Efficient Policy System For Body Sensor Networks	Body sensor networks in healthcare require a strong focus on security and adaptability to changing contexts and application needs. Policy-based management allows for flexible and adaptive behavior by allowing policies to be dynamically enabled and disabled without shutting down nodes. This addresses limitations of sensor operating systems like TinyOS, which do not support code modification. Other methods for network adaptation, like networking programming, are costly and disruptive. The policy-driven approach also allows for fine-grained access control through authorization policies. This paper presents Finger, an efficient policy system that enables policy interpretation and enforcement on distributed sensors for adaptation and access control. It is integrated as a TinyOS component with simple interfaces for easy use by developers and has been evaluated for performance.
412324	41232462	A Computational Method for Segmenting Topological Point-Sets and Application to Image Analysis	A new computational method has been proposed for segmenting topological subdimensional point-sets in scalar images of any spatial dimension. This method involves calculating the homotopy class defined by the gradient vector in a subdimensional neighborhood around each image point. This neighborhood is defined as a linear envelope created by a given subdimensional vector frame. By using the first largest principal directions of the Hessian, this method can segment critical points such as extrema and saddle points, as well as positive and negative ridges and other types of critical surfaces. The result is a hierarchy of point-sets of different dimensionalities that can be used for geometrical grouping using only local measurements. This method is also efficient and has been demonstrated in two examples where an additional image coordinate or local orientation parameter was used.
412324	41232455	A Dynamic Scale–Space Paradigm	The article presents a new framework for describing the dynamics of physical fields in input images, specifically the optic field dynamics. This framework is invariant under a particular gauge group and yields a concise field description in terms of a set of equivalences or invariants. It is also robust to noise and can be used to analyze physical field dynamics at different scales. The framework is based on initializing joint equivalences for the physical field dynamics and using multi-scale filtering techniques to obtain a hierarchy of nested structures. It can be applied to non-scalar fields and is demonstrated through a time-sequence of satellite images. The article also discusses how this framework can be used to unify existing scale-space paradigms and can be extended to include topological or covariant scale-space paradigms.
412325	41232525	Spatio-temporal segmentation using dominant sets	Pairwise data clustering techniques are becoming increasingly popular for image segmentation problems, but they are too computationally demanding for video segmentation due to their scaling behavior with data quantity. This poses a challenge for large datasets, as the number of potential comparisons scales with O(N2), making these approaches unfeasible. To address this issue, strategies to reduce the number of comparisons required by subsampling the data and extending the grouping to out-of-sample points are needed. In this paper, the authors present an approach to out-of-sample clustering using the dominant set framework and apply it to video segmentation. Results show that this approach is comparable to other recent methods in terms of segmentation quality, but much faster.
412325	41232557	Hierarchical Pairwise Segmentation Using Dominant Sets and Anisotropic Diffusion Kernels	Pairwise data clustering techniques are becoming more popular than traditional methods for grouping data based on features. These techniques have shown success in image-segmentation problems. However, they often overlook the benefits of hierarchical coarse-to-fine segmentations, which are common in direct image lattice analysis. In this paper, the authors introduce a new approach to pairwise hierarchical segmentation using dominant sets and an anisotropic diffusion kernel. This allows for scale variation in segment extraction, emphasizing strong boundaries at higher levels of the hierarchy. Experiments on the Berkeley database demonstrate the effectiveness of this approach.
412326	41232614	Fast Generation of Random Permutations via Networks Simulation	The paper discusses the problem of generating random permutations with a uniform distribution, specifically on two models of parallel computations: the CREW PRAM and the EREW PRAM. The main result is an algorithm that runs in O(log log n) time and uses O(n 1+o(1) ) processors on the CREW PRAM, which is the first o(log n) -time algorithm for this problem. On the EREW PRAM, a simpler algorithm is presented that generates a random permutation in O(log n) time using n processors and O(n) space. Both algorithms use a random switching network and simulate it on the PRAM model efficiently. This algorithm is an improvement over previously known algorithms for the exclusive write PRAMs.
412326	41232632	Periodic Merging Networks	The article discusses the problem of merging two sorted sequences on constant degree networks using only compare-exchange operations. The traditional solution, Batcher's Odd-Even Merge and Bitonic Merge, runs in log(2n) time and is considered time-optimal due to the lower bound of log n. The article introduces a new family of merging networks that are periodic, meaning each processing unit performs the same operations at steps t and t+k. These networks have a simple architecture and can merge two n-element sequences in O(log n) time for a period of 3. By increasing the period and using additional techniques, the runtime can be improved to 9...log_3 n 5.7...log n for a period of 4 and 2.25...((k+3)/(k-1+log 3))log n 2.25...log n for a period of k+3. These networks also have a small area complexity compared to Batcher's networks.
412327	41232734	Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning	The challenges of learning, planning, and representing knowledge at different levels of time are important issues in the field of AI. In this paper, the authors propose using options, which are closed-loop policies that allow for actions to be taken over a period of time, to address these challenges within the framework of reinforcement learning and Markov decision processes. Options can be used interchangeably with primitive actions in planning and learning methods. The authors also present results for using options during execution, learning about options, and improving options. These results are established in a simpler and more general setting, without committing to any particular approach, making them applicable to a wider range of problems.
412327	41232779	Using linear programming for Bayesian exploration in Markov decision processes	Reinforcement learning often faces the challenge of balancing exploration of the environment with exploitation of existing knowledge to gain rewards. Various methods have been proposed to address this issue, with many focused on collecting enough samples to accurately estimate the value function. However, Bellman and Kalaba (1959) suggested a different approach of constructing a representation that pairs states with knowledge about the current model. This approach is only feasible for bandit problems and leads to Gittins indices, an optimal exploration method. In this paper, the authors explore ways to make this approach more computationally feasible by using a Markov Decision Process model and sparse sampling techniques. The resulting optimization problem is then solved using Linear Programming, and the effectiveness of this method is demonstrated on different domains.
412328	41232817	Rectilinear Shortest Paths in the presence of Rectangular Barriers	This paper addresses the problem of finding the shortest L1 path from a given source point to a query point, without crossing any of the given barriers (disjoint isothetic rectangles) in the plane. A previous solution for a restricted version of this problem had a time complexity of O(n2). The authors propose a new approach that involves preprocessing the source point and barriers to create a planar subdivision, allowing for quick traversal from the query point to the source point. By proving the monotonicity of the path in at least one direction, a plane sweep technique is applied to divide the plane into O(n) rectangular regions. The resulting algorithm has a complexity of O(n logn) preprocessing time, O(n) space, and O(logn+k) query time, with an additional O(logn) query time for finding the length of the path. A lower bound of Θ(n logn) is shown for the case where the source and destination points are known in advance, making this algorithm optimal for that scenario. 
412328	41232872	Computing the largest empty rectangle	 empty rectangles with left sides supported by the left edge of the bounding rectangle and right sides supported by the right edge of the bounding rectangle.The problem discussed in this paper is finding the largest empty subrectangle within a given rectangle that does not contain any of the N points it contains. This problem is relevant when trying to salvage fabric or sheet metal with flaws. Previous solutions have taken O(N2) worst-case and O(N log N) expected time. However, this paper presents a more efficient algorithm with a time complexity of O(N log N) and a space complexity of O(N log N). The algorithm uses a divide-and-conquer approach and introduces a new concept of Voronoi diagrams and efficient computation methods. It also addresses the special case of finding the largest empty square and presents a solution using Voronoi diagrams. The proposed algorithm recursively divides the points into two halves and determines the largest empty rectangles with support from the bounding rectangle's edges.
412329	41232950	Enumerating Global Roundings of an Outerplanar Graph	The article discusses the concept of global rounding in a connected weighted graph, where a hypergraph is created based on the shortest paths in the graph. A real assignment is given to the vertices of the graph and a global rounding is a binary assignment that satisfies certain conditions. The authors of the paper proposed a conjecture that there are at most \V\ + 1 possible global roundings for the hypergraph. The paper proves that this conjecture holds true for outerplanar graphs and provides a polynomial time algorithm for enumerating all the global roundings in such graphs.
412329	41232926	The structure and number of global roundings of a graph	We are looking at a connected weighted graph G = (V,E) and creating a hypergraph HG = (V, PG) based on the shortest paths in G. A global rounding α with respect to HG is a binary assignment where the sum of the absolute differences between the assigned values and the real values is less than or equal to the corresponding value in PG. We hypothesize that there are no more than |V| + 1 global roundings for HG, and that the set of global roundings is an affine independent set. We have found evidence to support this conjecture.
412330	41233032	Variational Markov chain Monte Carlo for Bayesian smoothing of non-linear diffusions	This paper introduces new Markov chain Monte Carlo algorithms for Bayesian smoothing of partially observed non-linear diffusion processes. The algorithms use a deterministic approximation of the posterior distribution as a proposal for a mixture of independence and random walk sampling. This approximation is obtained by simulating a time-dependent linear diffusion process using the variational Gaussian process approximation method. The algorithms are tested on two diffusion processes and compared to state-of-the-art hybrid Monte Carlo methods. The results show that the new algorithms are accurate and more computationally efficient, except in cases of large observation errors and low observation densities. Overall, the variational approximation assisted sampling algorithm outperforms hybrid Monte Carlo.
412330	41233012	Query by committee	The research proposes a new algorithm called "query by committee" where a group of students are trained on the same data and the next query is chosen based on maximal disagreement. This algorithm was tested on two toy models and it was found that as the number of queries increases, the algorithm provides a finite amount of information gain which leads to a decrease in generalization error. This is in contrast to learning from randomly chosen inputs, where the information gain approaches zero and the generalization error decreases slowly. The researchers suggest that having a finite amount of information gain may be a significant feature of effective query algorithms. 
412331	41233120	Redundant bit vectors for quickly searching high-dimensional regions	Audio fingerprinting applications require searching in high dimensions to find similar items in a database. This task often results in negative answers, as many queries do not match any items in the database. To address this problem, a new method called Redundant Bit Vectors (RBVs) is proposed. This method involves approximating high-dimensional regions as hyperrectangles, partitioning the query space, and using bit vectors for efficient storage and searching. RBVs outperformed linear scan and locality-sensitive hashing in terms of speed, especially for large databases and when queries are not frequently found in the database. In a data set of 239369 audio fingerprints, RBVs was 109 times faster than linear scan and 48 times faster than locality-sensitive hashing.
412331	41233149	Scalable summaries of spoken conversations	In this work, a new way of browsing recorded audio conversations is presented. The method allows for scalable summaries of recognized speech, with the ability to increase the amount of text based on the desired level of detail. The interface allows for viewing of the entire conversation on one screen, with the option to zoom in for a full transcript and play the corresponding audio. This is accomplished through a combination of topic segmentation and informative phrase selection, with decreasing informativeness threshold for more detail. A user study was conducted to evaluate the method and interface against a baseline.
412332	41233217	Two-Level Volume Rendering	The paper introduces a two-level approach for volume rendering called two-level volume rendering. This approach allows for using different rendering techniques for different portions of a 3D data set. This means that different structures within the data set can be rendered using different methods such as DVR, MIP, surface rendering, value integration, or nonphotorealistic rendering. These results are then combined globally through a merging step, typically compositing. This approach allows for selectively choosing the most suitable rendering technique for each object within the data set while maintaining a reasonable level of information in the image. This is particularly useful for visualizing inner structures with semi-transparent outer parts, similar to the focus-plus-context approach seen in information visualization. The paper also presents an implementation of this approach, which allows for interactive exploration of volumetric data at high frame rates.
412332	41233210	RTVR: a flexible java library for interactive volume rendering	This paper discusses the design features of RTVR, a Java-based library for real-time volume rendering. The data structures used, based on voxel enumeration, allow for interactive rendering on low-end hardware. By assigning voxels to distinct objects and using individual look-up tables, object-aware rendering is achieved, with the ability to mix different transfer functions, shading models, and compositing modes in a single scene. This approach offers flexibility and extensibility in comparison to hardware-based volume visualization. The memory-efficient data representation and use of Java also allow for volume viewing over low-bandwidth networks with user control over rendering parameters. The paper also addresses challenges specific to using Java for interactive visualization.
412333	41233348	Multiple Spike Time Patterns Occur at Bifurcation Points of Membrane Potential Dynamics.	In vitro studies have shown that repeated injections of current into neurons can produce a reliable sequence of action potentials. This response can be interpreted as the activity of a group of similar neurons receiving the same input. To study the consistency of this response under different conditions, researchers injected varying levels of current into cortical neurons and found that increasing the amplitude of the fluctuations resulted in more reliable spike times. However, at certain points, small changes in the stimulus caused large shifts in spike times, revealing multiple spike patterns. Increasing the DC offset also increased reliability and produced earlier spike times. Although reliability was reduced at these points, the information about the input was actually increased.
412333	41233389	Attractor reliability reveals deterministic structure in neuronal spike trains	The article discusses the use of periodic current injections in an integrate-and-fire model neuron to study the reliability of spike trains. The results show that the neuron converges to an attractor, producing reproducible sequences of spikes. This attractor reliability is a measure of the stability of spike trains against noise and is quantified as the inverse of the number of distinct spike trains that result from repeated presentations of the same stimulus. This measure can distinguish between neurons that support a spike-time code and those that exhibit a renewal process, which cannot be distinguished using traditional methods. The study also applied these methods to cortical neurons and found that their spike trains had a higher reliability compared to renewal-like processes with the same spike-time histogram.
412334	41233465	Querying capability modeling and construction of deep web sources	This paper discusses the importance of analyzing the querying capability of deep Web sources, which can be accessed through query interfaces. This is crucial for Web applications that need to interact with these sources, such as deep Web crawling and comparison-shopping. The authors propose a querying capability model based on the concept of atomic query, which is a valid query with a minimal attribute set. They also present an approach for automatically constructing this model by identifying atomic queries for a given query interface. The experimental results demonstrate the accuracy of their algorithm.
412334	4123348	Automatic integration of Web search interfaces with WISE-Integrator	The number of databases accessible through Web-based search interfaces is increasing, particularly on e-commerce sites. However, accessing these sites individually can be overwhelming for users. To address this issue, a unified access to multiple e-commerce search engines is important in allowing users to easily search and compare products across different sites. The key to achieving this is by integrating the search interfaces of these engines. Currently, this integration is done manually or semi-automatically, which is inefficient and difficult to maintain. This paper presents WISE-Integrator, a tool that automatically integrates these interfaces using special meta-information and resolving domain differences. The tool also automatically extracts needed information from the interfaces. Experimental results show that WISE-Integrator can achieve high accuracy and produce high-quality integrated interfaces without human involvement.
412335	41233528	Density-Based Clustering over an Evolving Data Stream with Noise	Clustering is an important task in analyzing evolving data streams. Due to limited memory and one-pass constraints, stream clustering must meet certain requirements such as being able to handle outliers and discovering clusters with arbitrary shape. Many algorithms have been proposed for stream clustering, but none have addressed all of these requirements. In this paper, the Den Stream approach is introduced, which uses "dense" micro-clusters to summarize clusters with arbitrary shape and introduces potential core-micro-clusters and outlier micro-clusters to identify clusters and outliers. A pruning strategy is also developed to maintain the precision of micro-cluster weights with limited memory. Experiments with real and synthetic data sets show the effectiveness and efficiency of this method.
412335	4123354	Knowledge Discovery in Large Spatial Databases: Focusing Techniques for Efficient Class Identification	The number and size of spatial databases are increasing due to the abundance of data from satellite images and scientific equipment. As a result, automated knowledge discovery is becoming more important. Most current methods for knowledge discovery are based on relational databases, but this paper focuses on using clustering techniques for class identification in spatial databases. The integration of these techniques with the database interface, through the use of the R*-tree spatial access method, is crucial for efficiency. Various strategies for focusing on relevant parts of the database are discussed, and the proposed techniques are applied to a protein database with positive results. 
412336	41233623	Uncovering functional dependencies in MDD-compiled product catalogues	A functional dependency is a relationship between attributes in a table of data where the values of certain attributes determine the values of others. This is commonly used in database design and can also be applied to product catalogues. However, traditional methods for identifying functional dependencies require a tabular representation of the data, which may not be feasible for large combinatorial catalogues. This paper presents a new approach that uses decision diagrams to efficiently compute functional dependencies in compiled knowledge representations, making it possible to analyze even very large product catalogues. The algorithm was tested on different types of catalogues and revealed potential anomalies that could be useful for improving product recommendations.
412336	41233674	Reasoning about optimal collections of solutions	The challenge of finding the best possible set of solutions to a combinatorial problem is common in various applications. For example, in product configuration, it is important to have a diverse set of solutions to offer to customers. However, due to the complexity of these types of problems, most existing approaches either use heuristics or combinatorial search, which is not practical for large solution spaces. To address this issue, researchers have developed a new method that compiles the problem into a more compact form, allowing for more efficient solution-finding. Their experiments show that this approach outperforms traditional methods in terms of effectiveness and performance.
412337	41233722	Preserving mapping consistency under schema changes	The article discusses the challenges of adapting data mappings in dynamic environments, such as the Web, where data sources can change their data, schemas, semantics, and query capabilities. The authors propose a new framework and tool, ToMAS, for automatically updating mappings as schemas evolve. This approach takes into account not only local schema changes, but also changes that may affect multiple components of a schema. The algorithm identifies affected mappings and generates alternative rewritings that are consistent with the new schemas. It also considers user preferences and strives to maintain them as schemas and mappings evolve. In cases where there are multiple potential rewritings, the algorithm may prioritize those that are closest to the existing mappings. 
412337	41233719	A unified model for data and constraint repair	Integrity constraints are important in data design, but in operational databases, they may not always be enforced, leading to inconsistencies over time. To manage this, various techniques have been proposed to repair data by making minimal or low-cost changes to make it consistent with the constraints. However, in modern applications, constraints may evolve over time, making it unclear whether an inconsistency is due to an error in the data or a change in the constraints. This study presents a unified cost model to compare data and constraint repairs and evaluates its effectiveness on synthetic and real datasets. The results show that the repair algorithms are scalable and accurately identify when a data repair or constraint repair is needed.
412338	41233834	Performance limitations for linear feedback systems in the presence of plant uncertainty	This paper aims to examine the performance limits for feedback control systems by considering the impact of plant uncertainty. Previous studies have assumed an exact model of the plant, but it is believed that plant uncertainty should also be taken into account. By utilizing stochastic embedding, the paper formulates the problem and provides results that allow for evaluation of the best achievable performance in the presence of uncertainty. This also helps to determine whether uncertainty or other factors, such as nonminimum phase behavior, have a more significant impact on the system's performance. Keywords include performance limitations, stochastic embedding, and uncertainty. 
412338	41233857	On the use of one bit quantizers in networked control.	This paper discusses the problem of determining the best control update rate and number of bits per sample for a control law implemented over a fixed bit rate channel in networked control. The paper considers a restricted architecture with linear filters and a quantizer with linear feedback. A procedure for designing the controller and filters is presented, and it is shown that, under these restrictions, one bit per sample is generally the optimal choice, with the control update rate equal to the bit rate. This analysis differs from previous literature by focusing on bits per unit time and using a fixed number of bits in each time period.
412339	41233910	Soundness of Formal Encryption in the Presence of Active Adversaries	This paper presents a method for proving the security of cryptographic protocols against active adversaries. The method uses a simple logic-based language to express security properties and does not require dealing with probability distributions or asymptotic notation. The paper shows that the method is sound, meaning that logic statements can be interpreted in the computational setting to accurately represent the behavior of an adversary. This is the first paper to provide a framework for translating security proofs from the logic setting to the standard computational setting for protocols with powerful active adversaries. 
412339	41233944	A composable computational soundness notion	Computational soundness results demonstrate that if a system is symbolically secure, it can also be considered computationally secure. However, extending these results to cover new cryptographic primitives often requires redoing a significant amount of work. To address this issue, the authors propose a new concept called deduction soundness. This notion takes into account the use of primitives in the presence of functions specified by the adversary, allowing for easier modular extensions. The authors prove that a deduction sound implementation of certain primitives can be extended to include asymmetric encryption and public data-structures without repeating the original proof effort. This notion of soundness is also applicable to different protocol specification languages, but requires a commutation property to ensure computational soundness.
412340	41234051	Examining the technology acceptance model using physician acceptance of telemedicine technology	Organizations worldwide are heavily investing in information technology (IT), making user acceptance a crucial issue in technology implementation and management. While previous research has focused on user acceptance, there is a need for further examination in different contexts. This paper reports on a study that looked at the applicability of the Technology Acceptance Model (TAM) in explaining physicians' acceptance of telemedicine technology in the health-care industry. This study was significant as it involved a new technology, user group, and organizational context. Results showed that TAM was able to explain physicians' intention to use telemedicine technology, with perceived usefulness being a significant factor. However, the relatively low explanatory power of the model suggests the need for additional factors or integration with other IT acceptance models. The study has implications for both user acceptance research and telemedicine management.
412340	4123400	Information Technology Acceptance By Individual Professionals: A Model Comparison Approach	The rise of new and exciting information technology applications aimed at individual professionals has sparked a need to re-evaluate existing technology acceptance theories. This study compares the Technology Acceptance Model, Theory of Planned Behavior, and a decomposed TPB model in the context of healthcare professionals. Survey data from over 400 physicians in Hong Kong was analyzed and results suggest limitations in using TAM and TPB to predict technology acceptance in this setting. Furthermore, this study highlights the need for specific models and instruments for professionals, rather than relying on those used in ordinary business settings. The implications of these findings for technology adoption research and management practices are also discussed.
412341	41234115	A Fast Optimal Robust Path Delay Fault Testable Adder	This paper discusses the complexity of testing the adder function under the robust path delay fault model. The authors prove a lower bound of Omega(n2) for the minimum number of tests required for a complete test set for an n-bit adder. This applies to all known adder designs. They also introduce a fast O(sqrt(n)) adder that can be fully tested for robust path delay faults with a test set of size Theta(n^2). 
412341	412341162	(Quasi-) linear path delay fault tests for adders	.The article discusses the testability of the adder function and proposes a method to reduce the number of tests required. The method is applied to well-known hardware implementations of the adder, such as the Carry Ripple Adder and the Carry Look Ahead Adder. Depending on the structure, the proposed method can achieve a linear or quasi-linear size for a complete timing behavior test of the adder. This makes it possible to perform on-line dynamic testing for many commonly used adders. Overall, the proposed method can improve the testability of adders and make testing more efficient.
412342	41234238	Property analysis and design understanding	Verification is a crucial aspect of circuit and system design, and formal methods such as bounded model checking (BMC) are effective in ensuring high-quality verification. However, formal verification requires in-depth knowledge of the design implementation and finding the right set of properties can be a laborious and time-consuming task. To address these challenges, this paper introduces two techniques that aid in writing properties for BMC. The first technique helps in analyzing and refining properties to avoid redundancy and handle different scenarios. The second technique, known as inverse property checking, automatically generates valid properties based on the desired behavior. These techniques are integrated with a coverage check for BMC, resulting in a more efficient and streamlined verification process. This ultimately reduces the number of iterations needed for full coverage, saving time and effort.
412342	41234244	Measuring the Quality of a SystemC Testbench by using Code Coverage Techniques	SystemC is a language that allows for the creation of executable specifications for hardware/software integration and design exploration. With the increasing importance of verification in circuit and system design, SystemC's versatility and use of C++ make it easy to build testbenches at different levels of abstraction. However, it can be difficult to measure the effectiveness of these testbenches. In this paper, a methodology for measuring the quality of SystemC testbenches is presented. This approach uses code coverage techniques to identify untested parts of the model and provides feedback to the designer. Case studies of a RISC CPU and a TLM-based video processor demonstrate the applicability of this approach.
412343	41234364	Optimizing Majority-Inverter Graphs With Functional Hashing	The Majority-Inverter Graph (MIG) is a new type of logic representation that allows for efficient optimization. It has been shown to outperform existing approaches in reducing logic depth and achieving superior synthesis results. This paper introduces a new MIG optimization algorithm that uses functional hashing and precomputed minimum MIG representations for functions up to 4 variables. Experimental results demonstrate that this algorithm can further minimize heavily-optimized MIGs in size. When used as a starting point for technology mapping, the optimized MIGs can improve both depth and area for arithmetic instances of the EPFL benchmarks, surpassing the current results of state-of-the-art logic synthesis algorithms. This highlights the effectiveness of the proposed methodology. 
412343	41234375	Logic Synthesis for Quantum Computing.	The authors propose a synthesis framework called LUT-based Hierarchical Reversible Logic Synthesis (LHRS) to convert classical logic networks into quantum circuits for quantum computing. This framework uses LUT networks to bridge the gap between conventional logic synthesis and logic synthesis for quantum computing. The input to LHRS is a classical logic network and the output is a quantum network realized using Clifford+$T$ gates. This framework allows for a trade-off between the number of qubits and the number of quantum gates. The effectiveness of LHRS is demonstrated by automatically synthesizing IEEE compliant floating point networks up to double precision, which can be useful in cost estimation for quantum algorithms targeting scientific simulation applications. The LHRS framework is a crucial step towards understanding the practicality of quantum algorithms in the early stages of quantum computing.
412344	412344109	Robust sample preparation on digital microfluidic biochips.	Digital microfluidic biochips (DMFBs) are a promising platform for sample preparation, but errors in fluidic operations can lead to concentration errors. To address this issue, researchers have developed two dilution-chain structures that can generate droplets with desired concentrations even if there are volume variations during droplet splitting. These structures are more effective than current error-recovery methods, which require on-chip sensors and additional time for re-execution. The experimental results demonstrate the success of the proposed method in reducing errors and improving efficiency compared to previous methods. This advancement in sample preparation on DMFBs has potential to greatly improve the accuracy and speed of various bioassays and analyses.
412344	41234448	Passive droplet control in microfluidic networks: A survey and new perspectives on their practical realization.	Two-phase flow microfluidics is a promising technology for creating Lab-on-a-Chip (LoC) devices, which can perform multiple laboratory functions on a single chip. This is achieved by processing droplets containing biological or chemical samples using different elements on the chip. However, the fixed sequence of elements limits the flexibility and reusability of these devices. To address this issue, microfluidic networks have been developed to allow for programmable and flexible LoC devices. This paper discusses the state-of-the-art in microfluidic networking, including passive switching, network topologies, and validation methods. It also introduces a simple Droplet-on-Demand (DoD) system for generating droplets at specific times and volumes, which is crucial for practical use. The potential applications of microfluidic networks, such as drug screening and waterborne pathogen detection, are also discussed. The paper concludes by highlighting future research opportunities and challenges in this field.
412345	41234548	Bayesian Active Clustering with Pairwise Constraints	This paper focuses on improving clustering using pairwise constraints, which specify similarities between pairs of instances. Randomly selecting constraints can be inefficient and can negatively impact clustering performance. To address this issue, the paper introduces a Bayesian clustering model that learns from pairwise constraints and presents an active learning framework. This framework selects the most informative pair of instances to query an oracle for labels and updates the model based on the obtained constraints. The paper introduces two criteria for selecting informative pairs, one based on uncertainty and the other on maximizing information gain. Experiments show that this method outperforms existing techniques on benchmark datasets.
412345	41234545	Active Learning from Relative Comparisons	This work focuses on active learning using relative comparison information, which specifies the similarity between data triplets. Such constraints have been proven beneficial for tasks like defining distance metrics and finding clustering solutions. However, acquiring these constraints can be time-consuming and requires human effort. To address this issue, the authors propose an information-theoretic criterion to select the most useful triplets for learning with minimum user effort. They also introduce a randomized selection strategy to reduce the number of triplets to be examined, allowing for scalability to larger datasets. Experiments demonstrate the effectiveness of this approach compared to baseline policies. 
412346	41234650	Reachability under contextual locking	The pairwise reachability problem is a fundamental issue in the analysis of multi-threaded programs, which aims to determine if two control locations can be reached simultaneously during program execution. This problem is crucial for detecting concurrent statements and is typically undecidable, even with abstracted data and finite lock synchronization. However, certain programming paradigms have been identified that make the problem solvable. This paper introduces a new programming paradigm called contextual locking, which restricts lock usage to specific calling patterns within each thread. The main result is that the pairwise reachability problem is decidable in polynomial time for this paradigm.
412346	4123463	Symbolic computational techniques for solving games	This paper discusses the use of games in modular specification and analysis of systems. The authors compare different symbolic computational techniques for determining winning strategies in games that involve reaching a certain state or ensuring safety. These techniques include using symbolic fixed-point computation with ordered binary decision diagrams, checking for strategies within a specified bound by reducing to satisfiability of boolean formulas, and using two different approaches for reducing bounded cases to satisfiability. The authors also demonstrate how these techniques can be applied to safety games and evaluate them using examples and existing tools.
412347	41234750	Tracking performance of the RLS algorithm applied to an antenna array in a realistic fading environment	This paper explores the application of frequency domain techniques to the recursive least squares (RLS) algorithm in an adaptive antenna array for mobile wireless communication. The RLS algorithm is used to track changes in a fading environment, and the frequency domain approach is applied to the interference canceling problem that arises with antenna arrays in a mobile setting. The results show that using frequency domain techniques can improve the tracking performance of the RLS algorithm in nonstationary environments, making it a promising approach for mobile wireless communication.
412347	41234754	LINKING SEQUENCE BEHAVIOR IN ANC	Adaptive filters have been found to outperform Wiener filters in cases where the reference signal and primary signal have different frequencies. This phenomenon has been explained by the concept of linking sequences. A new study delves deeper into the characteristics of these linking sequences and finds that better performance of the adaptive filter is associated with low variability of the linking sequences. Understanding the behavior of these linking sequences could aid in improving the performance of adaptive filters in noise cancellation.
412348	41234866	Invariant Features for 3D-Data based on Group Integration using Directional Information and Spherical Harmonic Expansion	As the amount of 3D data increases, there is a need for effective classification and search in databases. However, the varying representations and positions of 3D objects make it challenging to compare and classify them. Invariant features, generated through Group Integration, provide a solution to this problem. Two extensions to this approach are proposed in this paper - incorporating local directional information and using Spherical Harmonic Expansion for more descriptive features. The effectiveness of these extensions is demonstrated through their application to 3D-volume data (Pollen grains) and 3D-surface data (Princeton Shape Benchmark).
412348	41234865	Voxel-wise gray scale invariants for simultaneous segmentation and classification	3D volumetric microscopical techniques, such as confocal laser scanning microscopy, are commonly used in biomedical applications to capture images of 3D objects with complex shapes. However, analyzing these images in high-throughput experiments requires reliable and easy-to-use pattern recognition tools. Most existing tools are specialized to specific problems and require expertise to adapt to new problems. Therefore, the authors have developed a tool that combines object segmentation and recognition using nonlinear kernel functions and support vector machines. This allows biologists to adapt the tool to their specific problem by interactively selecting training points for different classes of objects. The authors also present a fast algorithm for computing these nonlinear functions and demonstrate the tool's effectiveness in recognizing different cell cores of the chorioallantoic membrane.
412349	41234989	Identity-based online/offline key encapsulation and encryption	The Identity-Based Online/Offline Encryption (IBOOE) scheme splits the encryption process into two phases to make it more efficient for devices with limited computation power. The first phase involves heavy computations, while the second phase only requires light computations and does not require knowledge of the plaintext or receiver's identity. This scheme also allows for preparation of ciphertext without certificate verification. The authors propose new schemes with improved efficiency, assuming random oracles. These include a scheme secure against chosen-plaintext attack and a new scheme called ID-based Online/Offline KEM (IBOOKEM) which splits the key encapsulation process into offline and online stages. Additionally, they present a generic transformation to achieve security against chosen-ciphertext attack. These schemes are the most efficient in terms of online computation and ciphertext size, making them suitable for deployment on devices with limited computation power and expensive communication bandwidth. 
412349	41234990	Practical ID-based encryption for wireless sensor network	The paper proposes a new practical identity-based encryption scheme called RB-OOIBE for wireless sensor networks (WSN). It splits the encryption process into an offline and online part, with heavy computations done in the offline stage and light computations in the online stage. This allows for easier encryption by sensor nodes with limited computation power and storage. The scheme also allows for re-use of offline ciphertexts for the same receiver, limiting the number of offline ciphertexts an encrypter needs to hold. This makes it easier for sensor nodes to send encrypted data, with only a few offline ciphertexts needed during the manufacturing stage and light online computations for the sensor to process.
412350	412350126	Learning to handle negated language in medical records search	The use of negated language by medical practitioners to indicate the absence of a medical condition can lead to incorrect results when using traditional information retrieval systems. These systems do not distinguish between positive and negative contexts of terms, resulting in non-relevant medical records being ranked highly. To address this issue, a novel learning framework is proposed that takes into account the context of terms within a document when weighting their relevance. This prevents non-relevant documents from being retrieved. The framework was evaluated using the TREC 2011 and 2012 Medical Records track test collections and showed significant improvements over existing baselines. When combined with other techniques, the framework can achieve results comparable to the best TREC systems, but does not address other challenges in medical records search.
412350	41235010	Aggregating evidence from hospital departments to improve medical records search	Medical records are difficult to search due to the implicit knowledge they contain, which is known by medical practitioners but hidden from information retrieval systems. This knowledge can be used to improve the search system by ranking patients based on the relevance of their records to a query. The proposed solution is to group medical records from individual hospital departments, referred to as department-level evidence, to capture this implicit knowledge. Two approaches, federated search and voting, are proposed to build this evidence, along with an extended voting technique. Evaluation of these approaches in the TREC 2011 Medical Records track shows improved retrieval effectiveness, comparable to the best systems without requiring external resources. 
412351	4123512	A Structured Learning Approach for Medical Image Indexing and Retrieval.	The VisMed approach is a framework for automatic indexing and retrieval of large medical image databases. It involves creating vocabularies of meaningful medical terms and matching images based on their distributions of these terms. This approach allows for both similarity-based retrieval with visual queries and semantics-based retrieval with text queries. By combining the results from both types of retrieval, the VisMed approach achieves a higher Mean Average Precision (MAP) compared to using just one type of retrieval. In a study using this approach on a subset of medical images, the best MAP was 0.2821 for mixed retrieval. 
412351	4123515	IPAL Knowledge-based Medical Image Retrieval in ImageCLEFmed 2006.	The paper discusses the contribution of the IPAL group in the CLEF 2006 medical retrieval task, specifically in ImageCLEFmed. The group's approach involves incorporating medical knowledge into a multimodal fusion framework. This includes using the Unified Medical Language System (UMLS) for text and semantic features for images, learned through a structured learning framework. The use of UMLS concepts allows for a higher semantic level and standardization of medical data, improving communication between visual and textual indexing and retrieval. Results showed the potential of this approach, especially when using semantic dimension filtering and within a fusion framework. Additionally, a visual retrieval system was tested but showed poorer results compared to the textual approaches.
412352	41235225	Lookahead and pathology in decision tree induction	The standard method for creating decision trees involves making optimal decisions at each node without considering future nodes. A new approach was tested where the algorithm looks ahead one level to decide which test to use at a node. This method was compared to the standard method using a large number of artificial data sets. The results showed that the standard method consistently produced trees with the same level of accuracy as the more expensive lookahead method. Additionally, the lookahead method often created trees that were larger and less accurate than those produced without it.
412352	4123525	Efficient Algorithms for Finding Multi-way Splits for Decision Trees	Recent papers have highlighted that binary decision trees may not always be the most suitable model for certain domains. In some cases, it may be more effective to partition a set of examples into multiple groups based on a given feature's intervals. However, traditional decision tree methods only consider bi-partitions. To address this issue, a new algorithm has been developed that can efficiently compute an optimal multisplit of an interval into k sub-intervals, where k is less than the number of examples. This algorithm incorporates a penalty function to prevent trivial partitions. Implementation results have shown that this approach can produce better decision trees for certain distributions.
412353	41235370	Eco-friendly reduction of travel times in european smart cities	The article introduces an innovative solution for reducing polluting gas emissions from road traffic in modern cities. The solution is based on a new Red Swarm architecture, composed of intelligent spots with WiFi connections that can suggest customized routes to drivers. The proposal has been tested in four European smart cities using a micro-simulator and an evolutionary algorithm. Results show a significant reduction in gas emissions and travel times when vehicles are rerouted according to the Red Swarm indications. This low-cost solution has the potential to engage the interest of citizens and municipal authorities.
412353	412353185	Smart Mobility Policies with Evolutionary Algorithms: The Adapting Info Panel Case	This article presents the Yellow Swarm architecture, a proposed solution for reducing travel times, greenhouse gas emissions, and fuel consumption in road traffic. The architecture uses LED panels to suggest detours for vehicles during different time slots, determined by an evolutionary algorithm designed specifically for this purpose. The algorithm evaluates various scenarios based on real cities imported into a traffic simulator. Results show that even a small percentage of drivers following the panel indications can lead to improvements in average travel times, emissions, and fuel consumption. This suggests that the Yellow Swarm architecture has potential for effectively reducing the negative impacts of road traffic.
412354	41235420	So-Grid: A self-organizing Grid featuring bio-inspired algorithms	So-Grid is a set of bio-inspired algorithms designed for decentralized construction of a Grid information system. It uses swarm intelligence, where individual entities perform simple tasks but together create advanced intelligence. So-Grid has two main functions: logical reorganization of resources, inspired by ants and termites, and resource discovery, inspired by ants searching for food sources. These functions work together to facilitate intelligent dissemination and efficient discovery of resources. The algorithms are implemented by ant-like agents that autonomously travel the Grid through P2P connections and use biased probability functions. Simulation analysis shows that So-Grid can reduce system entropy and efficiently disseminate content. It also allows users to reach Grid hosts with a larger number of useful resources in a shorter time. This approach has self-organization, scalability, and adaptivity, making it suitable for dynamic and unreliable distributed systems.
412354	41235421	A decentralized ant-inspired approach for resource management and discovery in grids	This paper explores a decentralized and self-organizing approach, inspired by ant behavior, for creating an information system that distributes and organizes metadata for Grid resources. Each "ant" agent uses its own past activity to move and copy resource metadata among Grid hosts, helping to collect similar resources in a specific area of the Grid. This decreases system chaos. A protocol for resource discovery uses the work of these ants, directing client queries to "representative peers" who have information on many relevant resources. The agents control their actions and the queries are sent through the network using self-organizing mechanisms. The research suggests that this approach is effective in organizing resources and making them easily searchable for users.
412355	41235531	A field study of API learning obstacles	A study was conducted on over 440 professional Microsoft developers to determine the obstacles they face when learning new APIs. The study found that documentation and other learning resources were the most significant challenges. Five key factors were identified for effective API documentation: documentation of intent, code examples, matching APIs with scenarios, penetrability of the API, and format and presentation. These factors can help prioritize API documentation efforts in order to improve the learning experience and increase programmer productivity.
412355	41235528	Analyzing temporal API usage patterns	Software reuse through Application Programming Interfaces (APIs) is a key factor in software development. However, as developers work on client programs, their understanding and usage of APIs may evolve over time. To better understand these changes, the concept of Temporal API Usage Mining is proposed. This approach involves analyzing the change history of a client program to detect significant changes in API usage. A framework is described that can extract detailed models showing the addition and removal of API method calls. Machine learning techniques are then applied to these models to semi-automatically identify temporal API usage patterns, such as the consistent addition of API calls at different stages of the client program's life-cycle.
412356	41235642	Asymmetric rendezvous on the plane	The concept of rendezvous problems involves two players moving on a plane and trying to minimize the time it takes for them to meet. In one scenario, the players are aware of their distance apart but do not know which direction to travel in. Another scenario involves one player knowing the other's initial position while the other player only knows the initial distance between them. Results are also discussed for situations where one player's initial position is randomly chosen from a set of points. Overall, the goal is to find strategies that will allow the players to meet as quickly as possible.
412356	412356100	Traveling salesmen in the presence of competition	The "competing salesmen problem" (CSP) is a competitive version of the traveling salesman problem where two players compete to reach a majority of customers in a given graph. The players take turns moving along edges and know the opponent's position. It is proven to be PSPACE-complete, even in bipartite graphs, with a starting distance of 2 between players. The starting player may not be able to avoid losing, but in bipartite graphs, they can always avoid a loss. On a tree graph, the second player can avoid losing by more than one customer. A polynomial strategy for either player to force this outcome is uncertain. For a star graph with n leaves, there is a simple and optimal strategy for both players. If the customer set includes non-leaf vertices, the situation is more complex.
412357	41235717	Cutlike Semantics For Fuzzy Logic And Its Applications	Fuzzy sets can be represented by a nested system of ordinary sets, known as a-cuts. There is a lot of research on whether operations and properties of fuzzy sets can be reduced to those of their a-cuts. This includes questions about reducing operations and properties of fuzzy relations and whether a fuzzy concept can be represented by a collection of crisp concepts. Klir and Yuan (1995) refer to this as "cutworthiness". The authors propose a solution to this problem by using a nested system of crisp structures to define the semantics of fuzzy predicate logic. This approach can provide answers to these questions, as demonstrated by examples such as the extension principle and properties of binary fuzzy relations.
412357	41235724	Grouping fuzzy sets by similarity	The paper discusses factorization of systems of fuzzy sets, where sets are grouped based on their pairwise similarity to a prescribed degree. This is a challenge in fuzzy set theory since the similarity is not always transitive. The solution proposed is to consider maximal blocks of sets that are similar at least to the prescribed degree. A natural complete lattice structure is introduced for these blocks, serving as a factor structure for the original system. Examples of this approach include factorization of fuzzy concept lattices and residuated lattices. 
412358	41235878	Taking part: role-play in the design of therapeutic systems	Understanding user needs is crucial in designing effective human-computer interaction (HCI). However, in fields like mental health care, it can be challenging to access end-users. This is due to the sensitive nature of mental illness and the stigma associated with it, as well as the protected therapeutic setting. Role-play, a technique used in HCI, can be useful in these settings. It allows researchers to gain a deeper understanding of the user's experience, simulate therapy sessions, and train therapists in using technology. This paper explores various role-play formats adapted from therapeutic role-play, drawing upon literature and therapist input. It also discusses the benefits of using role-play in generating empathy and providing feedback on designs. 
412358	41235869	Biological rhythms and technology	Biological rhythms are internal processes that allow living organisms to adapt to and survive periodic changes in their environment. These include variations in the position of the earth and sun. These rhythms, such as body temperature and sleep-wake cycle, are driven by biological processes and can be maintained even without external cues. They have a significant impact on our physical and mental well-being, sleep quality, and mood. However, modern technology does not take these rhythms into account and can disrupt them. To address this issue, the HCI (human-computer interaction) community has been increasingly researching ways to support behavior change, personal insight, and productivity while considering our biological rhythms. This workshop aims to bring together experts in sleep, well-being, and circadian rhythms to discuss the potential of rhythm systems: technologies that align with our biology. The workshop will focus on two areas: measuring and intervening in our biological rhythms with the help of HCI.
412359	41235936	Composition attacks and auxiliary information in data privacy	Privacy is becoming increasingly important in data publishing, but there are challenges in reasoning about privacy. One major issue is the auxiliary information that an adversary can gather from other sources, which can breach privacy even with current anonymization techniques. This paper explores how to reason about privacy when faced with realistic sources of auxiliary information. It investigates composition attacks, where an adversary uses multiple independently released anonymized data sets to breach privacy. The experiments show that even simple composition attacks can be successful against commonly used anonymization schemes. However, certain randomization-based notions of privacy, such as differential privacy, are resistant to composition attacks and allow for modular design without explicitly considering other releases. This expands the range of protocols that can be used for privacy protection.
412359	41235942	Discovering frequent patterns in sensitive data	Discovering frequent patterns from sensitive data is a popular technique in datamining, but it carries a risk of privacy violations. This paper introduces two algorithms that accurately discover and release the most significant patterns and their frequencies while protecting individuals' privacy. These algorithms satisfy a new definition of privacy called differential privacy, which ensures privacy even in the presence of external information. To preserve privacy, the algorithms return slightly inaccurate lists of patterns. The paper also defines a new measure of utility to quantify the accuracy of these algorithms' output. Experiments on real data demonstrate the effectiveness of the algorithms. These techniques are relevant for any data mining output that is ordered according to a robust measure of interest.
412360	41236039	Open distributed processing: an architectural basis for information networks	The ODP Reference Model is an important part of the information network architecture being developed by TINA-C. This model is based on key principles and concepts that are crucial for the design and construction of information networks. One of these principles is binding, which is essential for the structure of an information network. A generic binding protocol is also introduced, which can accommodate the diverse types of binding in an information network. Overall, the ODP Reference Model is a significant framework for building efficient and effective information networks.
412360	4123609	The kell calculus: a family of higher-order distributed process calculi	The Kell calculus is a set of distributed process calculi designed for studying component-based distributed programming. It is based on the π-calculus and follows five key principles for a foundational model: hierarchical localities, local actions, higher-order communication, programmable membranes, and dynamic binding. The paper discusses these principles and defines the syntax and operational semantics of the Kell calculus. It also presents a co-inductive characterization of contextual equivalence using strong context bisimulation, and provides examples to demonstrate the expressive power of Kell calculi. 
412361	41236124	Variations in the binding pocket of an inhibitor of the bacterial division protein FtsZ across genotypes and species.	The rise of antibiotic resistance in pathogenic bacteria has led to a need for new approaches to drug development. Targeting the mechanisms of action of proteins involved in bacterial cell division, such as FtsZ, has shown promise. However, the mechanism of action and effectiveness of FtsZ inhibitors, such as PC190723, on different bacterial species is not fully understood. Using a statistical method and molecular dynamics simulations, researchers examined the structural environment of the PC190723 binding site on FtsZ. They found that the binding site varies significantly between species, genetic mutations, and polymerization states, providing valuable information for the development of FtsZ inhibitors. This highlights the need for a tailored approach to drug development, taking into account species-specific differences in drug targets.
412361	41236156	Knowledge-based Fragment Binding Prediction.	Target-based drug discovery involves evaluating numerous drug-like compounds for potential activity. Focusing on low-molecular-weight fragments can reduce the search space, but methods for determining protein-fragment interactions have limitations. Experimental assays are time-consuming and expensive, while computational approaches have limited accuracy. With the increasing availability of high-resolution structural data, FragFEATURE, a machine learning approach, uses a knowledge base of protein-fragment interactions to predict fragments preferred by a target protein. It achieves high precision and recall in identifying fragments corresponding to known inhibitors and can aid in fragment-based drug design or compound library creation for screening.
412362	41236292	Coupled-learning convolutional neural networks for object recognition.	Convolutional neural networks (CNNs) have gained a lot of attention for their use in computer vision tasks. These networks are inspired by the human brain and have similar properties to its learning process. However, a key difference is that CNNs operate independently while humans rely on effective communication between individuals for their visual system. To address this, a new approach called Coupled-learning Convolutional Neural Network (Co-CNN) has been proposed for object recognition. This method utilizes the dynamic interaction between neural networks to improve their discriminative capability. Unlike other network architectures, Co-CNN optimizes multiple networks simultaneously and incorporates a coupled-learning mechanism to prevent over-fitting. Evaluations on various datasets demonstrate the superiority of Co-CNN over existing algorithms. 
412362	412362105	Joint Bayesian guided metric learning for end-to-end face verification.	The paper addresses the problem of face verification, which involves determining whether two face images belong to the same person or different people. Previous methods have tackled this problem in two separate steps, but the authors argue that this approach does not fully exploit the potential of end-to-end frameworks. They propose a new technique, called Joint Bayesian guided metric learning, which integrates feature extraction and face recognition into a single convolutional neural network (CNN) architecture. The network is trained on a combination of publicly available databases and then fine-tuned using a metric learning technique. The results of the experiments on the LFW dataset show that this approach achieves high performance in face verification.
412363	412363162	Image coding using dual-tree discrete wavelet transform.	This paper explores the use of 2-D dual-tree discrete wavelet transform (DDWT) for image coding. Three methods for reducing DDWT coefficients are compared and noise shaping is found to be the most efficient. The dependence among DDWT coefficients is analyzed and three coding methods are evaluated, with TCE performing the best. The DDWT _ TCE scheme outperforms JPEG2000 and other directional filter bank-based image coders. To improve efficiency, the DDWT is extended to anisotropic dual-tree discrete wavelet packets (ADDWP) and is coded with TCE. Experimental results show that ADDWP _ TCE provides even better performance, outperforming JPEG2000 by up to 2.00 dB. The reconstructed images of these schemes are visually more appealing due to the directionality of wavelets.
412363	412363189	Image Compression Using 2d Dual-Tree Discrete Wavelet Transform (Ddwt)	This paper explores image compression techniques using the 2D Dual-tree Discrete Wavelet Transform (DDWT). This transform has direction-selective basis functions and can be further improved through an iterative projection-based noise shaping method. The study examines the statistical properties and dependencies of DDWT coefficients. Two coding methods, SPHIT and EBCOT, are evaluated for compressing DDWT coefficients. Results demonstrate that SPIHT is more efficient than EBCOT for DDWT, and the DDWT-SPIHT method performs better than JPEG2000 at low bit rates and similarly at high bit rates.
412364	412364111	Anatomical labeling of the anterior circulation of the Circle of Willis using maximum a posteriori classification.	This paper discusses a method for automatically labeling the arteries that make up the Circle of Willis, a key structure in the brain. This labeling is important for comparing different individuals and identifying risk factors for vascular diseases. The method uses pre-labeled examples and machine learning to detect the five main vessel bifurcations in the anterior part of the Circle of Willis. The labeling process is formulated as a maximum a posteriori solution, incorporating both local and global anatomical variations. The method was tested on 30 subjects and had a success rate of 90%. This technique is effective for labeling the arterial segments of the Circle of Willis and can handle anatomical variations. 
412364	41236469	Temporal diffeomorphic free-form deformation: Application to motion and strain estimation from 3D echocardiography.	This paper introduces a novel registration algorithm, TDFFD, for analyzing motion and strain in 3D ultrasound images. The algorithm enforces time consistency by using continuous spatiotemporal B-Spline kernels to represent the velocity field. The displacement field is then obtained through forward Eulerian integration, and the strain tensor is calculated using spatial derivatives. The algorithm considers both image similarity and a regularization term based on incompressibility. TDFFD was compared to two existing algorithms and showed increased robustness to noise and reduced temporal resolution. It was also applied to a database of cardiac ultrasound images and showed potential for assessing Cardiac Resynchronization Therapy outcomes. 
412365	4123654	Concurrent flexible reversibility	The concept of concurrent reversibility has been explored in various fields, but has only been limited to a "rigid" form where a past state can be revisited and the same computation can be restarted, even if it leads to a different outcome. In this paper, the authors introduce croll-π, a concurrent calculus that allows for flexible reversibility. This means that alternatives to a computation can be specified and used upon rollback, and these alternatives are attached to messages. The authors demonstrate the strength of this approach by encoding more complex methods for flexible reversibility and showcasing its application in a calculus of communicating transactions.
412365	41236540	Towards a unifying theory for web services composition	Orchestration is a term used to describe the organization and coordination of web services. Many languages have been developed to facilitate this process, with a focus on handling errors through long-running transactions and compensations. The most commonly used language, WS-BPEL, also has a Recovery Framework, but its complexity makes it difficult to analyze formally. To address this issue, the paper introduces webπ∞, a version of webπ that does not consider time, and a related theory. This framework aims to unify different orchestration languages and can fully encode BPEL.
412366	41236617	Constrained Equational Deduction	Symbolic computation plays a crucial role in automated deduction, algebraic specification, and declarative programming. In order to design a deduction mechanism within equational programming logic, symbolic equational deduction needs to be extended to incorporate semantic information from abstract symbols. This paper proposes constrained equational deduction as a framework for this extension, within a general constraint equational logic programming framework. By utilizing hierarchical constraint information and linking symbolic equational deduction with various constraint solving mechanisms, constrained equational deduction bridges the gap between symbolic and concrete computation. The paper presents a constructive approach that combines a constraint system with symbolic equational constraints to create the constraint equational logic programming paradigm. Computational models for this paradigm, called constrained equational deduction models, are also introduced.
412366	41236667	Option pricing under model and parameter uncertainty using predictive densities	The theoretical price of a financial option is determined by the expectation of the discounted payoff at expiry time, which is computed using the density of the underlying instrument's value at that time. This density is influenced by the chosen parametric model for the underlying's behavior and the values of its parameters, such as volatility. However, these are typically unknown and therefore, a common practice is to assume a specific model and use point estimates of the parameters to calculate the density. To account for this uncertainty, a method called Bayesian model averaging is used, which combines multiple models and their parameter distributions to generate a predictive density for the underlying. This approach utilizes all available information and eliminates the need for point estimates, making it a more accurate and logical way of pricing options.
412367	41236723	Finding corresponding objects when integrating several geo-spatial datasets	The paper discusses the use of join algorithms in integrating geo-spatial datasets, particularly for finding corresponding objects. It proposes methods that can be applied to multiple datasets and compares two approaches that use object locations. One approach processes datasets sequentially, while the other processes them simultaneously. The paper provides join algorithms for both approaches and evaluates their performance in terms of recall and precision. These algorithms are designed to handle imprecise locations and datasets that do not represent all real-world entities. Results from experiments show that one of the algorithms consistently performs well and outperforms the traditional one-sided nearest-neighbor join. 
412367	41236733	Object Fusion in Geographic Information Systems	This article discusses the use of fusion algorithms to match corresponding objects from two geographic databases. These algorithms only use the locations of the objects and are able to work with imprecise locations and incomplete databases. The performance of four fusion algorithms is measured in terms of recall and precision, and the results show that their effectiveness depends on the density and overlap of the data sources. The algorithms are found to outperform the current state of the art method, with one algorithm consistently performing the best at the cost of a small increase in running time. Extensive experimentation and analysis of the results are presented.
412368	41236894	A General Framework for Automatic Termination Analysis of Logic Programs	The paper discusses a framework for automatically analyzing the termination of logic programs, focusing on the finiteness of the LD-tree constructed for a given program and query. A general property of mappings from a subset of infinite LD-tree branches into a finite set is proven, leading to various termination theorems. These theorems are then applied to specific cases, such as predicate and atom dependency graphs. The paper also introduces a new method for proving termination in programs involving arithmetic predicates, which combines a finite abstraction of integers with the technique of query-mapping pairs. Several potential extensions to this framework are also mentioned. 
412368	41236889	Proving Termination for Logic Programs by the Query-Mapping Pairs Approach	This paper introduces a method for proving the termination of queries to logic programs using abstract interpretation. The method involves using query-mapping pairs to abstract the relation between calls in the program and query. Any well founded partial order for terms can be used to prove termination. The method has been implemented in SICStus Prolog as a system called TermiLog, which is accessible online. The system can determine if a given program and query pattern will terminate or potentially result in non-termination. This approach has the benefits of being straightforward and not imposing restrictions on programs.
412369	41236930	Elastic correction of dead-reckoning errors in map building	.The paper discusses a technique called elastic correction that aims to address the problem of imprecision in sensor measures during map building. It involves using a relational graph to represent the environment being explored, with landmarks and inter-landmark routes represented as vertices and arcs respectively. The approach is based on the idea of treating the map as a truss, with each route acting as an elastic bar and each landmark as a node. By taking inconsistent measures and modeling the uncertainty of odometry as elasticity parameters, errors can be corrected through the deformations induced within the structure. This technique shows promise in improving the accuracy of maps created by robots navigating unknown environments.
412369	41236956	Dynamic Clustering of Maps in Autonomous Agents	Spatial knowledge organization and utilization for navigation is a crucial concern in the development of autonomous mobile systems. Dividing the environment map into connected clusters can effectively capture topological features and simplify path-planning tasks. Clustering by discovery is a method for identifying clusters as the agent explores the map, and results in a valid clustering at each step. This study proposes a fitness measure for clustering and presents two heuristic algorithms to maximize it. These algorithms dynamically determine clusters based on topological and metric criteria, with one focused on minimizing entity scattering and the other estimating cluster positions and sizes using a global map of density. The algorithms are compared in terms of optimality, efficiency, robustness, and stability.
412370	41237047	Fingerprint Classification by Directional Image Partitioning	This work presents a new method for automatic fingerprint classification. The directional image is divided into connected regions based on fingerprint topology, creating a synthetic representation for classification. Dynamic masks and an optimization criterion guide this partitioning process. The resulting numerical vectors can be seen as a continuous classification of fingerprints. Different search strategies are explored for efficient retrieval of both continuous and exclusive classifications. Experimental results show superior performance and high robustness compared to other existing methods, making it the best approach for continuous fingerprint classification and retrieval. Tests were conducted on commonly used fingerprint databases.
412370	41237040	The Big Brother Database: Evaluating Face Recognition in Smart Home Environments	This paper discusses a preliminary study on template updating techniques for face recognition in home environments. A new database has been created specifically for this purpose, with a focus on variability in pose and illumination. Though the number of subjects is limited, there is a large amount of images available for incremental learning. The process of creating the database and the characteristics of the collected data are described. This database can be valuable for developing and improving face recognition methods for smart homes. The paper also presents initial results on incremental learning and their impact on recognition performance.
412371	41237130	On the use of words and fuzzy sets	This paper proposes a rethinking of fuzzy sets that does not change their fundamental nature as mathematical entities that extend predicates. The key idea is that predicates organize the universe of discourse, and when this organization is a preorder, a numerical degree can be defined to represent the L-set. The ultimate goal is to expand the current theories of fuzzy sets to encompass language and reasoning, in order to further understanding of the relationship between language and its representations for the advancement of computing with words and perceptions.
412371	41237148	Glimpsing at guessing	The paper proposes a new approach to understanding Commonsense Reasoning, which is seen as a manifestation of the natural phenomenon of "thinking" and a crucial survival skill for humans. The approach utilizes fuzzy sets with a flexible structure and focuses on the concepts of conjecture and refutation. It allows for the definition of informal and formal consequences, as well as hypotheses and creative speculations. The study of Commonsense Reasoning is not limited to logic or mathematics and requires researchers to have a broader perspective. 
412372	41237220	Characterization of Fuzzy Implication Functions With a Continuous Natural Negation Satisfying the Law of Importation With a Fixed t-Norm.	The law of importation is a significant aspect of fuzzy implication functions that has practical applications in fields such as approximate reasoning and image processing. It has been extensively studied and there are open problems related to this property that have been proposed in previous research. In this paper, the authors address one of these open problems by providing a partial solution. They specifically focus on the case of a fixed t-norm and characterize all fuzzy implication functions with continuous natural negation that satisfy the law of importation with this t-norm. The results are detailed for different types of t-norms, including continuous and noncontinuous ones, and also provide characterizations for well-known fuzzy implication functions. 
412372	41237261	Corrigendum to "Fuzzy implication functions based on powers of continuous t-norms" [Int. J. Approx. Reason. 83 (2017) 265-279].	In Massanet et al. (2017), a new property of fuzzy implication functions called the invariance property was introduced. This property is based on the powers of a continuous t-norm and was studied in relation to approximate reasoning. The paper also presented a new family of fuzzy implication functions called power based implications that satisfy this invariance property. However, further research has shown that not all power based implications generated from specific ordinal sum t-norms fulfill this property. Therefore, this paper aims to characterize which continuous t-norms can be used to generate power based implications that satisfy the power based invariance property. Alternatively, a modified version of this property is proposed that ensures all power based implications satisfy it. 
412373	41237352	Distributed Model Predictive Control	The paper discusses a distributed model predictive control (DMPC) scheme where controllers use model predictive control (MPC) policies to control their local subsystems. These controllers exchange their predictions through communication and incorporate information from other controllers to coordinate their actions. The DMPC scheme ensures stability for controllable systems that satisfy a matching condition by imposing stability constraints on the next state in the prediction. An example application of multi-area load-frequency control is used to demonstrate the performance of the DMPC scheme. The results show that this approach can effectively coordinate the actions of multiple controllers and improve system performance.
412373	41237323	Supervisor Synthesis for Real-Time Discrete Event Systems	This paper presents a formal framework for analyzing and controlling real-time discrete event systems (RTDESs). The framework extends Time Petri nets to controlled time Petri nets (CtlTPNs) and utilizes control class graphs (CCGs) to represent the logical behaviors of CtlTPNs. It is proven that the CCG fully captures the logical behavior of the CtlTPN. A real-time supervisor is then developed based on a nondeterministic logical supervisor for the CCG, with a delay for control computations to ensure acceptability in a true real-time environment. An algorithm is presented to construct the maximal controllable sublanguage of a given specification language. It is also shown that the real-time supervisor meets the desired real-time behavior and an online control algorithm is presented for implementing real-time supervisors. An example of packet reception processes in a communication network is used to illustrate the concepts and algorithms.
412374	41237444	Polarized Resolution Modulo	The authors propose a modified version of Resolution modulo where clauses can directly rewrite to other clauses without needing to be transformed into clause form. This requires extending the rules for negative and positive atomic propositions. This method is a combination of clause and literal selection restrictions and can be viewed as a variation of Equational resolution. It is not an instance of Ordered resolution, unlike many other restrictions of Resolution.
412374	41237456	A Completion Method to Decide Reachability in Rewrite Systems	The Knuth-Bendix method is a process that takes a set of equations and rewrite rules and determines if two terms are equivalent based on these rules. A new method has been designed that uses a similar approach, but instead of determining equivalence, it decides if one term can be reduced to another using the given rewrite rules. This has practical applications in proving the decidability of reachability in finite ground rewrite systems and pushdown systems.
412375	41237530	Collaborative Metacomputing with IceT	The IceT metacomputing system utilizes collaborative resource sharing, lightweight software frameworks, and efficient execution in heterogeneous environments. It extends the traditional model for distributed computing to enable controlled and secure exchange of hardware and software resources and supports programming paradigms that balance portability and efficiency. The current system is Java-based and allows non-persistent collections of resources, users, and processes to work together for distributed problem solving. The paper discusses the architecture and design principles of IceT, its operational model, and support for executing distributed applications. It also covers the development of subsystems that support the concept of cooperative metacomputing.
412375	41237520	Alternative approaches to high-performance metacomputing	The article discusses the challenges of parallel programming on both tightly-coupled platforms and distributed heterogeneous resources in high-performance computing. It introduces the Harness project, which uses dynamic reconfigurability to address these challenges. The project's core subsystem, H2O, has a "pluggable" software architecture that allows for flexible customization of distributed computing resources without compromising control or security. This is achieved through the use of "pluglets" that can access specialized features of the underlying resources. The article also mentions other subsystems, such as event handling and fault-tolerant naming services, that make up the comprehensive suite of software for metacomputing. The current status, experiences, and future plans for the Harness project are also discussed.
412376	41237652	Graph Treewidth and Geometric Thickness Parameters	This paper discusses the relationship between the classical graph parameter thickness, the geometric thickness, and the book thickness, which are all based on the number of colors needed to draw a graph without crossing edges. The main result is that for graphs with treewidth k, the maximum thickness and geometric thickness are both equal to ⌈k/2⌉. This also applies to the more restrictive setting of geometric thickness. Another main result is that for graphs with treewidth k, the maximum book thickness is equal to k if k ≤ 2 and k + 1 if k ≥ 3. This contradicts a previous conjecture. Similar results are also shown for outerthickness, arboricity, and star-arboricity.
412376	41237628	Layouts of graph subdivisions	A k-stack layout of a graph is an ordering of the vertices and a partition of the edges into k non-crossing sets. A k-queue layout is similar, but with a different partition of edges. A k-track layout involves a vertex coloring and an ordering of each color class such that no edges cross between classes. The stack-number, queue-number, and track-number of a graph are the minimum k values needed to achieve these layouts. This paper looks at these layouts for graph subdivisions and shows that all non-planar graphs have a 3-stack subdivision, with an improved upper bound on the number of division vertices per edge. It also proves that all graphs have a 2-queue subdivision, a 4-track subdivision, and a mixed 1-stack 1-queue subdivision. These values are optimal for every non-planar graph. The paper also characterizes graphs with k-stack, k-queue, and k-track subdivisions for all possible values of k. The results have applications in 3D polyline grid drawings, where it is shown that every graph has a 3D polyline grid drawing with vertices on a rectangular prism and a number of division vertices per edge of $\mathcal{O}({\rm log}{\sf qn}(G))$.
412377	41237722	Sequential Feature Explanations for Anomaly Detection.	Anomaly detection systems are commonly used in various applications to detect unusual data instances. However, when presenting the most anomalous instance to a human analyst, these systems often do not provide any explanation as to why it was considered anomalous. This leaves the analyst with no guidance on how to investigate further. To address this issue, researchers have studied the concept of sequential feature explanations (SFEs) for anomaly detectors. These are sequences of features that are presented to the analyst one at a time until enough information is provided for them to confidently determine the anomaly. The quality of an explanation is measured based on the number of features required for the analyst to attain confidence. To evaluate these explanations, a framework has been developed using real and simulated data sets. This has led to insights into various novel explanation approaches.
412377	41237738	Improving Automated Email Tagging with Implicit Feedback.	Tagging emails is crucial in managing information overload. Machine learning can assist with this task by predicting tags for incoming messages, displayed on the email itself. The learning algorithm only receives feedback when it makes a mistake, which can slow down learning. This study aims to determine if implicit feedback, such as user's time spent on an email, can provide additional training for the machine learning system. Three algorithms (and two baselines) were proposed and evaluated using data from 14 users using TaskTracer EP2. The results showed that implicit feedback significantly reduced prediction errors, despite not being perfect. This study concludes that implicit feedback can improve the performance of email tagging systems.
412378	41237818	Content-based image retrieval by using tree-structured features and multi-layer self-organizing map	The study presents a new approach for content-based image retrieval (CBIR) using a tree-structured image representation and a multi-layer self-organizing map (MLSOM). The tree structure contains global and local features, providing more comprehensive information for image retrieval. MLSOM efficiently organizes and compresses the image data, allowing for faster retrieval compared to traditional methods. A relevance feedback scheme is also implemented to further improve accuracy. Results show that the proposed system is robust against image alterations and outperforms other CBIR systems in terms of accuracy, speed, and robustness. 
412378	412378127	A new image classification technique using tree-structured regional features	Image classification is a difficult task in computer vision, and traditional methods use flat features with fixed dimensions extracted from the entire image. However, these features may not accurately represent the image content. This paper introduces a new approach using a tree-structured feature set, where the image is organized into a two-level tree with the root node representing the whole image and the child nodes representing smaller regions. This allows for a combination of global and local features to be used in the classification process. The method uses a two-level self-organizing map, consisting of an unsupervised map for processing image regions and a supervised map for overall classification. Experimental results show that this approach outperforms traditional methods.
412379	41237970	Multi-Objective Optimization Based on Brain Storm Optimization Algorithm	The paper discusses the development of various evolutionary and population-based algorithms for solving multi-objective optimization problems. The authors propose a new algorithm, called multi-objective brain storm optimization, which uses a clustering strategy in the objective space rather than the solution space as in the original brain storm optimization algorithm for single objective problems. Two versions of the algorithm were tested and the results showed the effectiveness of the proposed approach in solving different multi-objective problems. The authors conclude that the new algorithm shows promise in solving various multi-objective optimization problems.
412379	41237978	Normalized Ranking Based Particle Swarm Optimizer for Many Objective Optimization.	The paper discusses the issue of comparing solutions in multi-objective optimization problems with more than three conflicting objectives. A simple and effective method is proposed, where solutions are ranked by the sum of normalized fitness values for each objective. Solutions with small values are considered good for minimum optimization problems. To maintain population diversity, solutions with small values and better fitness values are kept in an archive and updated per iteration. This ranking metric is implemented in a particle swarm optimization algorithm and tested on four benchmark problems. The results show that the proposed algorithm is a promising approach for solving multi-objective and many objective optimization problems. 
412380	41238067	Negotiation as an Interaction Mechanism for Deciding App Permissions.	Apps on the Android platform often collect personal data from users, including location, contacts, and photos, and use it as part of their business model. However, many users are not aware of the permission settings or do not make changes to them. This is due to the difficulty of checking and modifying settings for all apps on a device, as well as the lack of flexibility in deciding what happens to their data. To address this issue, the authors conducted a study to explore the impact of more discretionary permission settings at the time of app installation. The pilot experiment showed that allowing users to negotiate which data they are comfortable sharing results in higher satisfaction compared to the traditional all-or-nothing approach. This suggests that negotiating consent can be an effective way to engage users and find a balance between privacy and pricing concerns.
412380	4123802	An Automated Negotiation Agent for Permission Management.	The digital economy relies heavily on data sharing, but individuals have little control over how their personal data is used. As the Internet of Things (IoT) expands, the number of devices accessing and requiring personal data will exceed what individuals can manage manually. To address this, a new approach is needed for managing privacy preferences and obtaining active consent for data sharing. A novel agent-based approach has been introduced to negotiate data sharing permissions between users and services, based on learned preferences from actual users. This approach was evaluated through an experimental tool on smartphones, and it was found that the agent effectively negotiates on behalf of the user without reducing user engagement. Understanding the interaction between individuals and agent-based automation is crucial for successful deployment in real-life and IoT settings.
412381	41238149	An evaluation framework for assessing the dependability of Dynamic Binding in Service-Oriented Computing	Service-Oriented Computing (SOC) is a flexible approach to building applications using distributed services. One of its main advantages is Dynamic Binding, which allows for abstract requests to be connected to specific services at runtime, increasing flexibility and adaptability. However, there is a lack of research on the evaluation of Dynamic Binding Systems (DBS), particularly in terms of system failure and dependability. This paper introduces a new evaluation framework for DBS, utilizing a fault model to simulate potential failures and observe the system's behavior. By treating the DBS as a black box and using distributed components, the evaluation is not limited by the technology or location of the system. Experiments on the NECTISE Software Demonstrator (NSD) show that this framework can detect and provide important information for improving the dependability and performance of DBS.
412381	41238144	Workload Estimation for Improving Resource Management Decisions in the Cloud	Cloud computing can be beneficial for both users and providers if good resource management is implemented. Workload prediction is a crucial aspect in achieving this. While it is possible to estimate the workload of tasks with recurring patterns, it is difficult for tasks without such patterns. This paper proposes a new approach using clustering to group tasks with similar characteristics and use their historical workload data to estimate resources for new tasks. The approach is evaluated using a Google dataset and results show its effectiveness. A workload model is developed based on the dataset and used to estimate the workload of randomly selected tasks from the trace log. This cluster-based method proves to be effective in estimating resources for each task.
412382	41238294	Mapping an unfriendly subway system	This article looks at creating a map of a dynamic network, specifically an urban subway system, in less than ideal conditions. The focus is on a team of asynchronous computational entities, or mapping agents, trying to locate black holes in the constantly changing graph of the subway system. The goal is to solve this problem with the minimum number of agents and movements possible. The article presents and analyzes a solution protocol, and also establishes lower bounds on the number of movements required in the worst case scenario, showing that the protocol is move-optimal.
412382	4123827	Distributed Black Virus Decontamination And Rooted Acyclic Orientations	The presence of a black virus (BV) in a network supporting mobile agents is a major threat. The BV can destroy any agent it comes into contact with and can spread to neighboring sites. The objective is to eliminate the BV with the least amount of network infections and agent casualties. Previous research has found a solution for networks with specific topologies, but this paper focuses on networks of arbitrary and unknown topology. The paper presents a decentralized solution using asynchronous agents with 2-hop visibility, ensuring optimal decontamination with minimum casualties and infections. This solution also applies to a related graph optimization problem.
412383	41238336	Level-k Phylogenetic Networks Are Constructable from a Dense Triplet Set in Polynomial Time	This paper addresses the question of whether there exists a phylogenetic network that is consistent with a given dense triplet set, and if so, can we construct one effectively. Previous research has provided answers and algorithms for networks of levels 0, 1, and 2, but for higher levels, only partial answers were available. The authors present a complete solution to this problem, using a special property of SN-sets in a level-k network. As a result, they are able to find a level-k network with the minimum number of reticulations, if one exists, in polynomial time. 
412383	41238349	LDFS-Based Certifying Algorithm for the Minimum Path Cover Problem on Cocomparability Graphs.	A minimum path cover (MPC) is a set of vertex disjoint paths that covers every vertex in a given graph G(V, E). This problem is a generalization of the Hamiltonian path problem. Cocomparability graphs are a popular family of perfect graphs that include interval, permutation, and cographs. For every cocomparability graph G and acyclic transitive orientation of its edge set, there is a corresponding poset P-G and an MPC of G is a linear extension of P-G that minimizes the bump number. While there are MPC algorithms for subfamilies of cocomparability graphs, previous algorithms for cocomparability graphs were based on poset formulations. This paper presents the first directly graph-theoretical MPC algorithm for cocomparability graphs, which is similar to the algorithm for interval graphs and has a running time of O(min(n(2), n + mloglogn)). 
412384	41238431	Split decomposition and graph-labelled trees: Characterizations and fully dynamic algorithms for totally decomposable graphs	This paper explores the split decomposition of graphs and presents new findings for the class of totally decomposable graphs, as well as two subclasses: cographs and 3-leaf power graphs. The authors provide structural and incremental characterizations, resulting in efficient dynamic recognition algorithms for changes to vertices and edges within each class. These results are based on a new framework of graph-labelled trees, which represent the split decomposition of graphs. The paper also discusses how this framework allows for a bijective mapping between the graph classes and graph-labelled trees, with labels representing cliques and stars. This approach leads to an intersection model for distance hereditary graphs. 
412384	41238415	Distance labeling scheme and split decomposition	A distance labeling scheme is a way to answer queries about the distance between any two vertices in a graph. Each vertex is assigned a label, and by using a specific function, the distance between two vertices can be estimated based on their labels. This paper proposes a method that combines different types of distance labeling schemes using split decomposition of graphs. This results in optimal label length schemes for certain types of graphs, allowing for constant time distance estimation once the labels are created. This approach is particularly effective for distance-hereditary graphs and other families of graphs.
412385	41238575	Delineation of rock fragments by classification of image patches using compressed random features	Monitoring the fragmentation of rocks is crucial for the mining industry. Current methods involve physically sieving rock samples or using 2D image analysis software, which is time-consuming and requires skilled operators. Recent research has shown promise in using 3D image processing, but it is not feasible for all mines to switch to this technology. This paper proposes a new method that uses compressed Haar-like features and a support vector machine to accurately delineate rock fragments from 2D images. The optimal image patch size and number of compressed features have been determined through experimentation. Results show that this method outperforms existing algorithms and is more computationally efficient, making it a viable solution for mining companies.
412385	41238582	Classification of Hand-Written Digits Using Chordiograms	The chordiogram method is being explored for identifying shapes in images. This study focuses on using chordiograms to recognize handwritten characters using the MNIST dataset. Each digit is assigned a feature based on the geometric relationships of its boundary pixels. These features are used to train a support vector machine for classification. The study found that using skeleton features extracted through thinning was more effective than using boundary pixel features, with a character recognition error rate of only 2.4%. However, a slightly better error rate of 2.2% can be achieved with boundary pixel chordiograms, though it results in a larger feature vector. This performance is compared to classifying digits based on their pixel intensities.
412386	41238639	Particle swarm optimisation assisted classification using elastic net prefiltering	A new algorithm has been developed for constructing a linear-in-the-parameters classifier in two stages, specifically for noisy two-class classification problems. The first stage aims to create a prefiltered signal that will be used as the desired output for the second stage. The first stage uses a two-level algorithm to maximize the model's generalization capability, incorporating an elastic net model identification algorithm and particle swarm optimization to select the two regularization parameters. This approach emphasizes the principle of "Occam's razor". The second stage uses an orthogonal forward regression with the D-optimality algorithm to construct the sparse classifier. Extensive experiments have shown that this approach is effective and competitive for handling noisy data sets.
412386	41238641	Nonlinear Identification Using Orthogonal Forward Regression With Nested Optimal Regularization.	The article presents a new algorithm for identifying nonlinear systems using radial basis function (RBF) neural networks. The algorithm aims to maximize generalization capability by using leave-one-out cross validation. Each RBF kernel has its own kernel width parameter and the algorithm optimizes multiple pairs of regularization parameters and kernel widths, one at a time. This leads to a very sparse RBF model with excellent generalization performance. Unlike previous algorithms, the proposed method optimizes both the kernel widths and regularization parameters within a single procedure, reducing computational complexity. The effectiveness of the approach is demonstrated through examples and comparisons with other popular methods. 
412387	41238716	Argument-Based expansion operators in possibilistic defeasible logic programming: characterization and logical properties	Possibilistic Defeasible Logic Programming (P-DeLP) is a logic programming language that integrates argumentation theory and logic programming, incorporating possibilistic uncertainty and fuzzy knowledge. It allows for non-monotonic inference, making it useful for modeling complex reasoning. This paper focuses on two non-monotonic operators in P-DeLP that add new weighed facts associated with argument conclusions and warranted literals to a given program, and compares them to a traditional logic system. The analysis of these operators provides a useful framework for evaluating and comparing argumentation frameworks.
412387	41238735	On Warranted Inference in Possibilistic Defeasible Logic Programming	P-DeLP is a logic programming language that combines elements from argumentation theory and logic programming, while also incorporating the treatment of possibilistic uncertainty and fuzzy knowledge. It allows for the modeling of non-monotonic inference, particularly through defeasible argumentation, and offers a way to capture defeasible inference relationships. This paper focuses on a non-monotonic operator for P-DeLP that represents the expansion of a program by adding new weighed facts associated with warranted literals. The logical properties of this operator are compared with those of a traditional SLD-based Horn logic, providing a useful framework for evaluating and applying argumentation frameworks.
412388	41238839	Parallelism and synchronization in two-level metacontrolled substitution grammars	The concept of synchronized substitution is introduced in two-level metacontrolled substitution grammar, as explained in [6]. In contrast to [6], where the top level is independent of the grammar, this mechanism is highly influenced by the chosen grammars. This is exemplified using parallel grammars like E0L systems, and their language-generating capability is compared to ET0L systems and E0L iterated systems.
412388	41238820	From folksologies to ontologies: how the twain meet	Ontologies are tools used to capture and utilize formal semantics, often created through a controlled process. However, the rise of social bookmarking sites, known as folksologies, has introduced a new trend on the Web. Folksologies allow for informal semantics to be created and adopted by anyone on the Internet through the use of tags. Instead of choosing between ontologies and folksologies, there is a need for hybrid emergent semantics systems that combine elements of both. In this paper, the authors suggest analyzing the larger context of semantics and introduce DogmaBank as a proof of concept for a next-generation emergent semantics system. This system combines lexical and conceptual functionalities and is compared to existing examples such as del.icio.us and Piggy Bank.
412389	41238934	Constructive hypervolume modeling	This paper discusses the modeling of point sets with attributes in geometric spaces of arbitrary dimensions. A point set is a representation of a real or abstract object, while an attribute represents a property of the object at any point in the set. The paper provides a survey of various modeling techniques related to point sets with attributes, such as solid modeling, heterogeneous objects modeling, and volume graphics. Based on this survey, the paper proposes a general model for hypervolumes (multidimensional point sets with multiple attributes) and discusses its components, including objects, operations, and relations. The model uses a function representation approach, where the geometry and attributes are represented by real-valued scalar functions evaluated at points using a constructive tree structure. The paper also demonstrates the application of this model in the context of texturing and discusses its implementation and a specialized modeling language used for hypervolume objects. 
412389	41238915	Constructive function-based modeling in multilevel education	The digital age has greatly impacted the education of children and students, leading to the emergence of new educational technology focused on visual thinking and computer-based tools. This poses a challenge for computer graphics, which has traditionally relied on two paradigms: approximation and discretization. However, these approaches have limitations such as loss of precision and growing memory consumption. To address these issues, a new paradigm of procedural modeling and rendering has emerged, allowing for the creation of complex models using simple operations. This approach aligns with the constructionism theory, which emphasizes active learning and hands-on creation. The use of virtual modeling and graphics tools in education has been found to be effective in teaching mathematical concepts and programming, as well as understanding real-world structures and processes. This approach has also led to the development of qualified students for research and development in this field.
412390	4123902	Certified Information Access	Certified Information Access (CIA) is a method for users to verify the accuracy of information received from a database. This is achieved by the database owner providing a proof that the information matches the actual content of the database. Existing solutions for this problem require a lengthy setup process. Two secure distributed implementations of CIA are described, one where the database owner distributes the evaluation of a computationally intensive function among untrusted peers, and another where the entire computation is outsourced. The main concern in both cases is ensuring confidentiality and correctness of the data. A new primitive, the Verifiable Deterministic Envelope, is introduced as a potential solution.
412390	41239025	The price of defense and fractional matchings	The article discusses the concept of the "Price of Defense" as a measure of loss in security guarantees due to selfish attacks and defenses in a vulnerable network. The Price of Defense was introduced as a worst-case measure in a strategic game where attackers aim to minimize their chances of being caught and a defender chooses edges to catch attackers. The article presents results that show the lower bound of the Price of Defense is at least half the number of vertices in the graph, and introduces the concept of Defense-Optimal graphs which have a Nash equilibrium achieving this lower bound. It also identifies a graph that is Defense-Optimal but has no Perfect Matching Nash equilibrium. The article suggests that the connection to Fractional Graph Theory may hold the key to understanding the combinatorial structure of Nash equilibria in this security game.
412391	41239195	Computational Anatomy Gateway: Leveraging XSEDE Computational Resources for Shape Analysis	Computational Anatomy (CA) is a field that uses mathematical and computational techniques to study the variability in biological shape. The key algorithm used in CA is Large Deformation Diffeomorphic Metric Mapping (LDDMM), which assigns numerical descriptors and a metric distance to anatomical shapes. CA is widely used in the neuroimaging and cardiovascular imaging communities. The process involves estimating a template and calculating diffeomorphic transformations for each subject in a population. This is computationally expensive and requires parallel computing resources, such as those provided by XSEDE's Stampede cluster. The use of NVIDIA Tesla GPUs and Xeon Phi Co-processors can further speed up these calculations. This will have a significant impact on the neuroimaging and cardiac imaging fields, as these shape analysis tools become available through a webservice (www.mricloud.org) using XSEDE's resources through the Computational Anatomy Gateway.
412391	41239185	Gaussian Random Fields on Sub-Manifolds for Characterizing Brain Surfaces	This paper discusses analytical methods for studying the variation of neuro-anatomically significant substructures in the brain using brain images. Specifically, the focus is on characterizing the shape variation of 2-dimensional brain surfaces through the use of smooth templates. The paper presents techniques for quantifying shape variation in populations, including the use of Gaussian random vector fields. An example is provided of how these methods were used to analyze the shape of the hippocampus in a group of normal and schizophrenic individuals. The paper also presents results from a study comparing hippocampal shape in these two groups and proposes a Bayesian hypothesis test for clustering the two groups.
412392	4123928	Secure and efficient data transmission in the Internet of Things	The Internet of Things (IoT) has gained popularity in both industry and academia. This concept involves millions of interconnected objects with sensors that collect data and send it to servers for analysis. To ensure the accuracy of this collected data, it is important to establish a secure communication channel between the sensors and servers. In this paper, a heterogeneous ring signcryption scheme is proposed to achieve confidentiality, integrity, authentication, non-repudiation, and anonymity in a single step. This scheme is proven to be secure against different types of attacks and can be used for data transmission in the IoT, including between different types of cryptographic systems.
412392	41239263	Practical Identity-Based Signature for Wireless Sensor Networks.	Wireless sensor networks require strong security measures, and one important aspect is authentication. A digital signature is crucial for ensuring the integrity, authentication, and non-repudiation of data. In this letter, the authors propose a practical identity-based online/offline signature scheme specifically designed for wireless sensor networks. The scheme has two phases: offline and online. The offline phase involves heavy computations without knowledge of the message, while the online phase only requires light computations when the message is known. Compared to existing pairing-based schemes, this proposed scheme has the lowest computational cost. 
412393	41239392	Towards confidentiality of ID-Based signcryption schemes under without random oracle model	Signcryption is a cryptographic method that ensures both confidentiality and authenticity in an efficient way. It is a single step process that offers better computation and communication costs to enhance security. This paper analyzes two different signcryption schemes - one by Yu et al. and another by Liu et al. - under the without random oracles model. The chosen plaintext attacks prove that these previous schemes are not able to resist indistinguishability on chosen ciphertext attacks, thus failing to provide semantic security for confidentiality. This highlights the need for improved signcryption schemes that can effectively resist such attacks and ensure stronger security measures. 
412393	412393101	Reconciling and improving of multi-receiver signcryption protocols with threshold decryption	Signcryption is a cryptographic technique that combines the features of signature and encryption to provide both confidentiality and authentication. It is important for a signcryption protocol to meet the security requirements of indistinguishability against adaptive chosen-ciphertext attacks (ind-cca2) and unforgeability against adaptive chosen-message attacks (euf-cma2). Qin et al. proposed an identity-based multi-receiver signcryption scheme with threshold decryption that claimed to meet these security requirements. However, their scheme is found to be insecure. This paper presents an improved scheme that meets the security requirements and also offers identity privacy for the sender and receiver. The performance evaluation shows that the new scheme has lower communication overhead. 
412394	41239417	Adaptive pricing for customers with probabilistic valuations	This paper discusses the problem of determining discriminatory prices for customers with uncertain values and a seller who has identical copies of a product. It is shown that, with certain assumptions, this problem can be reduced to the continuous knapsack problem and a new algorithm is developed to solve it. The algorithm is able to handle asymmetric concave reward functions and can also be applied to pricing problems with overlapping goods. Additionally, a framework for learning customer valuation distributions is proposed and tested on real-world pricing scenarios. The results demonstrate that the algorithm outperforms a previously proposed heuristic in terms of speed and accuracy.
412394	41239448	Using Information Gain to Analyze and Fine Tune the Performance of Supply Chain Trading Agents	The Supply Chain Trading Agent Competition (TAC SCM) focuses on dynamic supply chain trading and has over 800 games played by agents worldwide. This paper analyzes the data from 16 games in the 2006 TAC SCM semi-finals to determine which behavioral features differentiate winning agents from others. It also introduces a metric based on information gain to analyze 80 games in the quarter-finals, semi-finals, and finals of the 2006 competition. The results show that winning agents in the final rounds made strategic procurement decisions rather than customer bidding decisions. The analysis presented in this paper was used to improve the entry for the 2007 competition, which was one of the six finalists.
412395	41239523	Embodiment enables the spinal engine in quadruped robot locomotion	The spinal engine hypothesis suggests that locomotion is mainly controlled by the spine, with the legs providing support. Based on this, a quadruped robot called Kitty was created with a compliant, multi-degree-of-freedom spine and no leg actuation. The study demonstrates how the spine's movements, through embodiment, can generate versatile behaviors such as bounding, trotting, and turning. Information theory was also used to analyze the spine's internal dynamics and its effect on the bounding gait, using three different spinal morphologies. The results showed that a rear virtual spinal joint improved locomotion by allowing more freedom for the rear legs to move forward. The study also discusses the relationship between the robot's behavior and its information structure, which varied based on the spine's morphology.
412395	41239551	The function of the spine and its morphological effect in quadruped robot locomotion	This article discusses the role of spinal movements in locomotion for quadruped animals. Two quadruped models with different numbers of spinal joints were developed to demonstrate spine-driven locomotion behaviors. The models were analyzed in terms of gait properties, showing that employing a spinal morphology with two joints greatly enhances stability and speed. This is achieved by allowing the robot to adjust its center of mass and increasing stride length. The model also exhibits two flight phases and a greater flight proportion during each stride, similar to running cheetahs. These findings highlight the importance of spinal morphology in quadruped locomotion. 
412396	4123969	Fully automated analysis of padding-based encryption in the computational model	Computer-aided verification is a useful method for assessing the security of cryptographic primitives. However, achieving automated analyses that are effective against computational attacks has been difficult. This paper addresses this challenge by presenting proof systems for analyzing the security of public-key encryption schemes that use trapdoor permutations and hash functions. By combining techniques from both computational and symbolic cryptography, the authors have developed a toolset that allows for fully automated proof and attack finding algorithms. This toolset has been used to create a database of encryption schemes, documenting attacks against insecure schemes and providing proofs with concrete bounds for secure ones.
412396	41239614	Completing the picture: soundness of formal encryption in the presence of active adversaries	This paper extends previous research on the Dolev-Yao model and the computational model by incorporating key exchange and cryptographic primitives, such as signature. This allows for automatic verification of protocols in the computational model using a formal model. The authors introduce a precise definition of security criteria and a reduction theorem which can be used to prove equivalences between security criteria. The proof of the reduction theorem utilizes novel ideas that may have applications in other scenarios. This work has implications for improving the analysis and verification of security protocols in the computational model. 
412397	41239750	Non-revisiting genetic algorithm with adaptive mutation using constant memory	The continuous non-revisiting genetic algorithm (cNrGA) incorporates the entire search history and parameter-less adaptive mutation to improve search performance. However, as the number of fitness evaluations increases, memory management becomes necessary. To address this, two pruning mechanisms, least recently used and random pruning, are proposed to keep memory usage constant. These strategies also serve as parameter-less adaptive mutation operators. Experimental results show that cNrGA with constant memory outperforms other algorithms and can be extended to handle larger fitness evaluations without affecting performance. This expands the applicability of cNrGA to solve more complex problems.
412397	41239730	A genetic algorithm that adaptively mutates and never revisits	The article discusses a novel genetic algorithm that uses a non-revisiting approach, storing all previously explored solutions in an archive. This method uses a unique design of a binary space partitioning tree archive, which not only efficiently prevents revisits but also serves as an adaptive mutation operator without any parameters. The algorithm is evaluated using 19 benchmark functions and outperforms canonical genetic algorithms, particle swarm optimization algorithms, and even a state-of-the-art method for adaptive mutation. It can handle problems with large search spaces and has minimal CPU overhead and memory usage. The adaptive mutation feature also demonstrates stable and consistent performance without the need for parameter tuning.
412398	4123983	Sound and complete relevance assessment for XML retrieval	In information retrieval research, test collections consisting of documents, user requests, and relevance assessments are necessary for comparing retrieval approaches. Obtaining accurate and complete relevance assessments is crucial for this comparison. In XML retrieval, the challenge of obtaining relevant assessments is complicated by the structural relationships between retrieval results. Unlike flat document retrieval, the relevance of elements in XML is dependent on related elements. As such, creating reliable relevance assessments for XML retrieval has been a focus of the INEX evaluation campaign. After trying various methods, the campaign has settled on a highlighting method for marking relevant passages within documents, which has proven to be the most reliable approach.
412398	41239812	The use case track at INEX 2006	The INEX workshop series has primarily focused on developing algorithms for XML information retrieval using test collections from IEEE and Wikipedia. However, a new track called the use case track has been introduced to explore how end-users can utilize XML IR systems for different purposes. This article summarizes the discussions and results of this track.
412399	41239940	How well do line drawings depict shape?	This paper examines the use of sparse line drawings in representing 3D shapes. A study was conducted where participants were shown images of 12 objects depicted in 6 different styles and asked to orient a gauge to match the surface normal of the object. Results were compared to ground truth data from a 3D surface model and analyzed for accuracy and precision. The study collected a large data set of 275,000 measurements and found that people are able to interpret certain shapes effectively from line drawings, which can be as effective as shaded images or artist's drawings. Errors in depiction were found to be localized and related to specific properties of the lines used. The collected data will be made available for future studies on this topic.
412399	41239932	Interactive rendering of suggestive contours with temporal coherence	This paper discusses the use of line drawings in conveying shape and proposes the use of suggestive contours, which are lines drawn at specific points on a surface, to improve computer-generated line drawings. The authors extend previous work on static suggestive contours to dynamic and real-time settings and discuss techniques for rendering drawings with contours and suggestive contours at interactive rates. They also analyze the movement of suggestive contours with changes in viewpoint and suggest ways to improve the visual appearance of these contours in both static and dynamic scenarios. 
412400	41240054	Design and fabrication of materials with desired deformation behavior	This paper presents a data-driven approach for designing and creating materials with specific deformation properties. The process involves measuring the deformation properties of base materials and using this information to create a non-linear stress-strain relationship in a finite-element model. The validity of this measurement process is confirmed by comparing simulated and actual deformations of fabricated material stacks. Then, an optimization process is used to design layered combinations of base materials that meet desired deformation criteria. Various strategies are employed to streamline the search for optimal solutions. The complete process is demonstrated by designing and fabricating objects using modern multi-material 3D printers.
412400	412400101	Data-driven finite elements for geometry and material design	Crafting the behavior of a deformable object, whether it is a biomechanically accurate character model or a new 3D printable design, is a challenging task that requires constant iteration. Previous algorithms for accelerating three-dimensional finite element analysis suffer from expensive precomputation stages that rely on prior knowledge of the object's geometry and material composition. In this paper, a new method called Data-Driven Finite Elements is introduced to address this issue. This method constructs a reusable library of metamaterials based on a given material palette, allowing for fast coarsening of simulation meshes at runtime. The method is applicable to non-linear material models and has been shown to yield significant speed gains while maintaining accuracy. Examples of its effectiveness are demonstrated on both virtual and 3D printed objects. 
412401	41240140	Evolving mach 3.0 to a migrating thread model	The authors have made changes to the Mach 3.0 operating system to improve its handling of cross-domain remote procedure calls (RPCs). This involves treating RPCs as a single entity instead of a sequence of message passing operations, and using a migrating thread model instead of a static one. This model allows for better transfer of control during RPCs and simplifies kernel code. The new system also provides more control over thread manipulation and improves RPC performance. The authors have kept the old thread and IPC interfaces for compatibility and have demonstrated the effectiveness of their changes with a functional Unix server and clients. The changes have reduced logical complexity and improved local RPC performance. They conclude that the migrating thread model and kernel-visible RPC are beneficial for improving operating systems. 
412401	41240119	Vx32: lightweight user-level sandboxing on the x86	Code sandboxing is a useful tool for isolating and executing potentially untrusted code within an application. However, many existing sandboxing techniques have limitations such as requiring kernel modifications, not fully isolating the guest code, or causing significant performance costs. Vx32 is a user-level sandbox that addresses these issues by using x86 segmentation hardware and a lightweight instruction translator. It allows an application to safely load and execute guest plug-ins within a restricted memory region, and control the guest's access to system calls and instructions. Vx32 has been evaluated through microbenchmarks and whole system benchmarks, and has been successfully used in various applications including an archival storage system, a public-key infrastructure, and an experimental user-level operating system. While it may incur some overhead, it is still more efficient than other virtual machine monitors and does not require kernel modifications.
412402	4124020	Coupled binary embedding for large-scale image retrieval.	Visual matching is a crucial part of image retrieval using the bag-of-words (BoW) model. However, the traditional method of matching based on SIFT descriptors has limitations in terms of discriminative power and only describing local texture features. To address this issue, this paper proposes incorporating multiple binary features at the indexing level and using a multi-IDF scheme to model their correlation. This allows for the integration of binary feature-based matching methods, like Hamming embedding, into the framework. The inclusion of binary color features also improves the precision of visual matching and reduces the impact of false positive matches. Experimental results on four benchmark datasets show that this method outperforms the traditional approach and requires acceptable memory usage and query time. When global color features are also integrated, the proposed method performs competitively with other state-of-the-art methods. 
412402	41240211	Packing and Padding: Coupled Multi-index for Accurate Image Retrieval	The Bag-of-Words (BoW) based image retrieval method often results in false positive matches due to the low discriminative power of the SIFT visual word. This is because the SIFT feature only describes the local gradient distribution and there is information loss during quantization. To address this issue, a coupled Multi-Index (c-MI) framework is proposed for feature fusion at the indexing level. This involves coupling complementary features into a multi-dimensional inverted index, with each dimension corresponding to a specific feature. The fusion of local color features is also incorporated, improving both precision and recall. Experimental results show that c-MI significantly improves retrieval accuracy and is complementary to other techniques. It achieves high scores on benchmark datasets and outperforms state-of-the-art methods.
412403	41240388	Texture Features for Browsing and Retrieval of Image Data	Image content based retrieval is a growing field that has practical applications in digital libraries and multimedia databases. This paper specifically focuses on the use of texture information for browsing and retrieving large volumes of images. The authors propose using Gabor wavelet features for texture analysis and provide a thorough evaluation of their effectiveness. Results show that Gabor features outperform other texture features when applied to the Brodatz texture database, demonstrating their accuracy in pattern retrieval. An example of using this approach for browsing large air photos is also presented. 
412403	412403124	Color and texture descriptors	This paper provides an overview of approved color and texture descriptors for the MPEG-7 standard. These descriptors have been extensively evaluated and developed in the past two years, with evaluation criteria including their effectiveness in similarity retrieval, extraction, storage, and representation complexities. The color descriptors in the standard include a coded histogram using the Haar transform, a color structure histogram, a dominant color descriptor, and a color layout descriptor. Three texture descriptors are also included, one for characterizing homogeneous texture regions, one for local edge distribution, and a compact descriptor for texture browsing. Each descriptor is explained in detail, including their semantics, extraction, and usage. The effectiveness of these descriptors is supported by experimental results. 
412404	41240451	Asking for more than an answer	The study discussed in this article focuses on the situational context that affects the quality of answers in online Q&A services. The researchers used a mixed method analysis to investigate this issue and found that users have three primary expectations when asking a question: quick responses, additional or alternative information, and accurate or complete information. By understanding these expectations, it may be possible to develop personalized approaches to improve information relevance and satisfaction for users of online Q&A services. This research addresses a gap in the current research on online Q&A and sheds light on how users engage in information seeking within this context. 
412404	4124049	User motivations for asking questions in online Q&A services	The study aims to understand what motivates individuals to ask questions in online Q&A services. Yahoo! Answers and WikiAnswers were used as test beds, and a mixed method approach was utilized, including a survey, diary method, and interviews. The results showed that cognitive needs were the main motivation for asking a question, but other factors such as tension-free needs also played a role depending on the user's context. Understanding these motivations can help improve question-answering processes and provide a framework for understanding information needs in online Q&A. This study has broader implications for understanding online information-seeking behaviors.
412405	41240565	Barriers to organizing information during cancer care: &quot;I don't know how people do it.&quot;.	Patients are receiving a large amount of health information and are expected to take an active role in managing it. However, both patients and clinicians are frustrated with the lack of effective organization and use of this information. A study was conducted on cancer patients to identify barriers they face in organizing their health information. Four types of barriers were found: emotional, scalable, temporal, and functional. Suggestions are provided for reducing these barriers through technology or social changes. This study offers guidance for health-information providers, technology designers, and patients and their caregivers.
412405	4124050	Locating Patient Expertise in Everyday Life.	When individuals face new health issues, they often need to learn how to manage their personal health. Some patients turn to other patients for advice and support, using internet-based social software to connect with others who have similar experiences. However, these platforms often lack effective ways for users to find sources of expertise. This study looked at how patients with breast cancer locate expertise and found similarities with professionals in workplace organizations. However, patients also use unique strategies, such as seeking unsolicited advice and using triangulation, due to the high stakes of their health problems. The study suggests five ways that social software could be improved to better support patients in finding expertise from other patients.
412406	41240638	Fixed-Points of Social Choice: An Axiomatic Approach to Network Communities.	This article introduces a social choice theory approach to defining communities within social networks. The authors use a preference network framework in which members rank each other based on their preferences. They develop two approaches to studying communities: one indirectly through preference aggregation functions and the other directly through community axioms. These approaches reveal that community rules form a bounded lattice and have a structural characterization. However, defining community rules based solely on preference aggregation can lead to violations of desirable community axioms. The authors also provide a polynomial-time rule that satisfies seven axioms and weakly satisfies the eighth.
412406	41240610	Finding endogenously formed communities	The problem of determining overlapping communities in data mining and social network analysis without external identification is addressed using a framework based on the relative affinities among members. This framework, called an affinity system, defines communities as groups of members who prefer each other over anyone outside the group. A polynomial bound is provided for the number of self-determined communities and a polynomial-time algorithm is presented for enumerating them. This framework is particularly well-suited for social networks and interactions. The analysis is also connected to (α, β)-clusters and a polynomial bound is shown to exist while a superpolynomial number of clusters can exist in certain networks.
412407	4124078	Left and right convergence of graphs with bounded degree	The theory of convergent graph sequences has been studied in two different cases, dense graphs and bounded degree graphs. Convergence can be defined in terms of counting homomorphisms from fixed graphs into members of the sequence (left-convergence) or counting homomorphisms into fixed graphs (right-convergence). In the dense case, it has been shown that left-convergence and right-convergence are equivalent under certain conditions. This paper establishes a similar equivalence in the bounded degree case, with restrictions on the set of graphs in the right-convergence definition. This implies that for a left-convergent sequence, partition functions of a wide range of statistical physics models will converge. The proof utilizes techniques from statistical physics such as cluster expansion and Dobrushin Uniqueness. This work was supported by OTKA (67867) and ERC (227701).
412407	4124075	Convergent sequences of sparse graphs: A large deviations approach	Sparse graphs are important models in various fields, such as combinatorics, probability theory, optimization, and statistical physics. These models are based on different notions of similarity and convergence, such as Benjamini-Schramm convergence, left-convergence, right-convergence, and partition convergence. However, the relationships between these notions are not well understood. In this paper, a new notion of convergence called Large Deviations (LD) convergence is introduced, which is based on the theory of large deviations. This is done by assigning random weights to the nodes of the graph and constructing corresponding random measures. LD-convergence implies the other three notions of convergence and also reveals new relationships between them. This can be used to express limiting free energies and understand the behavior of sparse graph sequences. 
412408	41240821	Building member attachment in online communities: applying theories of group identity and interpersonal bonds	This article discusses the importance of online communities for organizations and the general public, and the lack of research on what makes some communities more successful than others. The authors applied theory from social psychology to understand how online communities develop member attachment, a key factor in community success. They tested two sets of community features aimed at strengthening either group identity or interpersonal bonds, and found that both sets of features increased participants' visit frequency and self-reported attachment. However, features intended for identity-based attachment had a greater impact, particularly for newcomers. This research highlights the potential for applying social science theories to improve the success of online communities.
412408	41240865	The effects of diversity on group productivity and member withdrawal in online volunteer groups	The "wisdom of crowds" idea highlights the value of diversity in online collaborations like open source projects and Wikipedia. However, research on diversity in offline work groups has shown mixed results. While a diverse group can bring in a range of perspectives and improve outcomes, individual differences can also lead to conflicts and reduced performance. This paper examines the impact of diversity on group productivity and member withdrawal in WikiProjects. The study finds that diversity in experience with Wikipedia can increase productivity and decrease member withdrawal up to a certain point, beyond which members are more likely to withdraw. Interestingly, differences in member interest have a linear relationship with productivity and withdrawal. The study suggests that the low visibility of individual differences in online groups can allow them to reap the benefits of diversity while avoiding the drawbacks. These findings can inform future research on online collaborations.
412409	41240910	Compilation for explicitly managed memory hierarchies	A new compiler has been developed for machines with a managed memory hierarchy. Its main purpose is to organize and schedule bulk operations at different levels of the application and the machine. The performance of the compiler has been tested using various benchmarks on a Cell processor.
412409	41240942	Scatter-add in data parallel architectures	Data parallelism is a common feature in many important applications, and computer systems are designed to take advantage of it. However, some operations in multimedia and scientific applications can slow down the process due to serialization. To address this issue, a new mechanism called scatter-add is introduced, which is a data-parallel version of the scalar fetch-and-op. This mechanism allows for efficient support of data-parallel atomic update computations and can be applied to both single and multi-processor systems. The micro-architecture of scatter-add on a stream architecture shows promising results with minimal increase in die area and significant performance speedups on a variety of applications.
412410	4124106	Policy auditing over incomplete logs: theory, implementation and applications	This article presents an algorithm, called reduce, that checks audit logs for compliance with privacy and security policies. The algorithm addresses two main challenges in compliance checking: realistic policy applicability and incomplete audit logs. Reduce operates on policies expressed in a first-order logic that allows restricted quantification and uses logic programming to identify the restricted form of quantified formulas. It can handle all 84 disclosure-related clauses of the HIPAA Privacy Rule. The algorithm proceeds iteratively and outputs a residual policy that can only be checked with additional information. The article proves correctness, termination, time and space complexity results for reduce and provides an optimized implementation using two heuristics for database indexing. Experimental results show that the algorithm is fast enough for practical use.
412410	41241042	Constraining Credential Usage in Logic-Based Access Control	Authorization logics are used to create access-control policies in logic-based systems. Resource owners issue credentials that specify these policies, and logical inference rules are used to determine the consequences of these policies. Proofs in authorization logics can serve as capabilities for accessing resources. However, since proofs can be derived from multiple credentials, the issuer of a specific credential may not be aware of all the possible consequences of their policy. To address this issue, a system is proposed where credentials can specify constraints on their usage. These constraints can be expressed as functions over authorization proofs, allowing for a wide range of constraints. The formal properties and examples of such constraints are studied.
412411	4124112	Digestor: device-independent access to the World Wide Web	Digestor is a software system that allows for easy access to the World-Wide Web on small screen devices such as PDAs and cellular phones. It works by automatically reformatting web pages to fit the screen size of the device, using a planning algorithm and structural page transformations. This means that users can view web pages on their small screen devices without having to zoom in or scroll. Digestor is implemented as an HTTP proxy, making it a convenient and efficient solution for device-independent web browsing on the go.
412411	41241164	IntelliPrompter: speech-based dynamic note display interface for oral presentations.	Many presenters struggle with the fear of forgetting what to say next during oral presentations. To combat this, they often use written notes alongside presentation slides. However, this can lead to disengagement and low-quality presentations. To address this issue, the researchers developed IntelliPrompter, a speech-based note display system that tracks the presenter's coverage of each slide and dynamically adjusts the note display to highlight the most relevant topic. Two versions of the system were created using Google Glass and computer screens, based on feedback from 36 interviews and analysis of presentation notes. In a study comparing the dynamic screen-based and Google Glass interfaces with a static note system, the dynamic option was preferred by presenters and independent judges.
412412	41241242	"…is it normal to be this sore?":: using an online forum to investigate barriers to physical activity	Regular physical activity is crucial for maintaining overall health, but many adults in the U.S. lead sedentary lifestyles. Lowering perceived barriers to physical activity is an important part of interventions aimed at increasing physical activity. Through a systematic qualitative coding of an online forum used in a three-month healthy lifestyle intervention, the top five barriers to physical activity were identified. Two of these barriers have not previously been reported in the literature. This paper provides design considerations for technologies that can support and encourage physical activity, based on the identified barriers. Understanding the needs of a population is essential in designing effective interventions, and this paper offers valuable insights for those working in this field.
412412	41241231	Barriers to physical activity: a study of self-revelation in an online community.	Regular physical activity is crucial for overall health, but many adults in the US lead sedentary lives. Research shows that reducing barriers to exercise can motivate people to be more active. This article discusses the top barriers identified in a study of message board discussions from a healthy lifestyle intervention promoting physical activity and healthy eating. The findings add to existing knowledge and highlight the importance of considering individual needs when designing technologies to support physical activity. This paper provides valuable insights for those working in this field, emphasizing the importance of understanding the population's needs in the design process.
412413	4124130	UWB-Based Tracking of Autonomous Vehicles with Multiple Receivers.	This paper discusses real-time tracking of an Autonomous Guided Vehicle (AGV) in an indoor industrial setting. The AGV's on-board odometer provides information about its position and orientation, while an external Ultra Wide Band (UWB) wireless network compensates for any error drift. Two new solutions for real-time tracking are proposed: a classical Time Differences of Arrivals (TDOA) approach with a single receiver, and a "Twin-receiver" TDOA (TTDOA) approach that requires two independent receivers on the AGV. The effectiveness of these algorithms is evaluated in realistic conditions, showcasing the tradeoff between the frequency and quality of UWB measurements.
412413	41241339	Improved ultra wideband-based tracking of twin-receiver automated guided vehicles	This paper discusses a new system for tracking Automated Guided Vehicles (AGVs) in indoor industrial settings. The system combines an on-board odometer and an external Ultra-Wide Band (UWB) wireless network to accurately predict the AGV's position and orientation. Two tracking solutions are proposed: one using a single receiver and the other using two receivers on the AGV. The latter allows for increased accuracy and can still estimate the AGV's movement even when the odometer fails. The system's performance is evaluated in realistic conditions, showing that it can achieve sub-centimeter accuracy with low-complexity UWB receivers and commercial odometers. This system has potential applications in warehouse automation.
412414	4124147	Natural language analysis for semantic document modeling	This paper discusses a method for organizing and retrieving web documents through semantic document classification. This approach uses a combination of linguistic tools and a conceptual domain model to create a controlled vocabulary for a document collection. Users can browse the model and select relevant fragments to classify documents. Natural language tools are then used to analyze the text and suggest model fragments based on the selected concepts and relations. These fragments are refined by users and stored in RDF-XML format for retrieval. A prototype of the system is presented, with examples from a medical document collection. Lexical analysis is also used to refine search queries using the domain model.
412414	41241439	Semantically Accessing Documents Using Conceptual Model Descriptions	This paper discusses a method for classifying and retrieving documents on the Web using Natural Language Processing and Conceptual Modeling. The approach involves using a controlled vocabulary to classify documents, which is created using a combination of a lexical analysis tool and the Referent Model language. Documents are classified by identifying high frequency words in the document that also appear in the domain model defining the vocabulary. These sentences are then parsed and stored using RDF-XML syntax, creating a connection between the document and the domain model. This approach is currently being implemented for a document collection published by the Norwegian Center for Medical Informatics (KITH).
412415	41241525	Characterizing network events and their impact on routing	Network events, also known as incidents, are disruptions to the usual functioning of elements in an IP network. These elements, such as routers, network interface cards, and IP links, can fail or malfunction due to various reasons. For instance, a router may need to be rebooted for a software upgrade, an interface card may crash, or an IP link may become overloaded due to a denial-of-service attack. These incidents can affect the traffic of customers, resulting in packet loss, delays, or even complete loss of connectivity to parts of the network. To address these issues, network operators must diagnose the problem and find a solution. The study of network events, their causes, and their impact was conducted using data from a large European provider's Virtual Private Network (VPN) backbone, which connects over ten thousand enterprise networks but does not connect to the internet.
412415	41241534	Joint analysis of network incidents and intradomain routing changes	 causes of these changes, a joint analysis of routing and trouble tickets is performed.The paper analyzes the relationship between intradomain routing instability and network trouble tickets in two different networks: a VPN provider and the Internet2 backbone network. The goal is to better understand the underlying causes of routing instability. A method is developed to correlate trouble tickets with instability events and it is found that there is no single cause for most instability and that the causes differ across networks. The study also compares results to a previous study, finding that certain causes of instability have become less prevalent while others, such as software-related problems, have become more common. The paper highlights the need for a deeper understanding of the causes of network incidents and the potential for joint analysis of routing and trouble tickets in identifying these causes.
412416	41241637	Instant Places: Using Bluetooth for Situated Interaction in Public Displays	Instant Places is a study that explores the use of Bluetooth presence and naming as tools for interactive experiences around public displays. The authors not only use Bluetooth names for identity representation, but also as a means of triggering content on situated displays. Through deploying a prototype in a bar for several weeks, the study examines the effectiveness of these techniques and the resulting social practices. The findings suggest that the simplicity of these techniques does not hinder their ability to sustain interaction and can be easily adapted for new forms of social engagement. 
412416	41241643	Steerable augmented reality with the beamatron	Steerable displays are advanced systems that use a motorized platform and a camera to project graphics at any point in a room. This allows for the superimposition of graphics onto the real world, making them useful for augmented reality and ubiquitous computing scenarios. The Beamatron is a new type of steerable display that incorporates a computer-controlled pan and tilt platform, a projector, and a Kinect sensor. Unlike previous versions, the Beamatron uses live depth data to enable reasoning in 3D, allowing for more dynamic and interactive graphics. Two example applications of the Beamatron include an augmented reality game where a player can control a virtual car in a real room, and a demo that uses speech and gesture to move projected graphics around the room. 
412417	4124175	Information Holodeck: Thinking In Technology Ecologies	This paper discusses the potential for technology to support human thinking, learning, and sensemaking through device ecosystems (DDEs). The authors propose a theoretical framework that incorporates digital information into Vygotsky's sign mediation triangle, introducing the concept of objectification where perceivable objects can be associated with signs to aid in thinking. A qualitative study with graduate students was conducted to evaluate the effectiveness of DDEs in supporting objectification. The findings suggest that DDE technologies can afford objectification in two distinct ways. This work advances a method for examining device ecologies in terms of their potential for learning and offers insights for designing technology that can enhance higher thought processes.
412417	41241723	The effects of visual contextual structures on children's imagination in story authoring interfaces	This paper examines how the use of contextual visual images can influence a child's imagination when creating digital content using authoring systems. The focus is on children aged 8 to 10, as this is a critical period where creativity can decrease. A study was conducted where children used a storytelling system with either a contextual background image or a blank screen. Through video coding and analysis, differences were found in the storytelling process and product between the two conditions. The paper discusses the implications of these findings for the design of authoring systems for children.
412418	41241825	IDSET: Interactive Design System using Evolutionary Techniques	The paper proposes a system called IDSET, which supports the creation of tools using evolutionary techniques. The system has two stages - one where fundamental shapes are generated and another where they are combined. The system generates new shapes at each stage and displays them to the user, stimulating their creativity. Unlike other systems, IDSET starts with creating parts instead of using pre-prepared parts, resulting in a variety of novel tools. The user only needs to evaluate the displayed tools, rather than creating them. The system was tested through computer experiments, and various tools and artistic shapes were successfully created, some of which were not easily imaginable by humans. This demonstrates the effectiveness of IDSET.
412418	41241842	A Natural Language Processing Neural Network Comprehending English	This paper introduces a natural language neural network model that analyzes sentence structure. It consists of 5 layers and uses energy propagation for training and recall. The model is evaluated for its association and noise tolerance, showing impressive results. It is also applied to answer TOEIC test questions and performs at the average level of human examinees, demonstrating its ability to learn and recall knowledge. Overall, the proposed neural network offers a flexible way for artificial intelligence to learn and recall English knowledge.
412419	41241956	Experimental Evaluation of Ontology-Based Test Generation for Multi-agent Systems	Software agents are becoming increasingly popular for managing complex, distributed systems. To ensure the reliability of these systems, there is a growing need for methodologies and techniques that can effectively test multi-agent systems. A new approach, using an agent interaction ontology, has been developed and incorporated into a testing framework called eCAT. This framework can automatically generate and evolve test cases, and continuously run them. In this paper, the authors evaluate the effectiveness of this ontology-based approach through experiments using two BDI agent applications. The results demonstrate the framework's performance and its ability to identify faults in the tested systems.
412419	41241926	Evolutionary testing of autonomous software agents	Testing autonomous software agents is crucial for ensuring their correctness, as they make independent decisions that can have significant consequences. However, testing these agents can be challenging due to their ability to react differently to the same inputs over time. To address this problem, the authors propose an approach that uses evolutionary optimization to generate demanding test cases. This approach involves deriving fitness functions and evaluating them with simulated agents. The results show that this method is effective in automatically finding good test cases, making it a valuable tool for testing autonomous agents. 
412420	41242018	How knowledge representation meets software engineering (and often databases)	This paper examines various personal research projects that utilized concepts from Knowledge Representation and Reasoning to solve problems in Software Engineering and Databases. It explores the degree to which these ideas were directly applicable and how often new techniques had to be developed. The projects demonstrate the potential for cross-disciplinary collaboration and the importance of incorporating diverse perspectives in finding solutions to complex problems.
412420	41242049	Knowledge Representation And Reasoning In Software Engineering	The special issue focuses on using knowledge and specific principles to solve complex problems with computers, rather than relying on general principles. It explores how this approach has influenced research on tools for enhancing software productivity and quality. The idea for the issue stemmed from a workshop on intelligent and cooperative information systems, which took place in April 1991. The workshop discussed various tools and issues related to them, and the special issue aims to further explore these topics. 
412421	41242195	Optimally fast parallel algorithms for preprocessing and pattern matching in one and two dimensions	This article discusses optimal parallel algorithms for string matching problems, specifically in one dimension. The authors have designed an algorithm that can efficiently compute a substring in constant time, which was previously a bottleneck in pattern preprocessing. They have also improved the preprocessing time for constant-time text search and developed algorithms for deterministic and probabilistic string matching in certain cases. Additionally, the article presents a constant expected time Las-Vegas algorithm for string matching, solving a major issue in this field.
412421	412421160	Overcoming the Memory Bottleneck in Suffix Tree Construction	The suffix tree is a crucial data structure for processing strings and has become more relevant with the rise of massive data sets. However, existing algorithms for building and using suffix trees are hindered by memory limitations. The authors propose a new algorithm that optimizes disk accesses by utilizing sort and scan primitives. This algorithm performs well in both sequential and parallel computational models, achieving optimality in terms of disk accesses. Additionally, the algorithm takes into account the difference between random page accesses and block transfers, resulting in improved performance. This approach is shown to have the same optimal tradeoff as sorting.
412422	41242264	Building and Testing a Statistical Shape Model of the Human Ear Canal	The current design of custom in-the-ear hearing aids relies on individual experience and skills rather than a systematic understanding of the variation in ear canal shape. This paper presents a method for creating a detailed model of the human ear canal using laser scanned ear impressions and expert-placed anatomical landmarks. The model is used to analyze potential differences in ear canal size and shape between genders. By warping a template mesh onto the ear canal shapes in the training set and using the resulting vertices to create a 3D point distribution model, a more accurate and reliable means of designing custom hearing aids can be achieved. 
412422	41242256	Analysis of surfaces using constrained regression models.	This study examines the relationship between changes in the shape of the human ear caused by jaw movement and acoustical feedback (AF) in hearing aids. Data from 42 hearing aid users, in open and closed mouth positions, was collected and analyzed to identify regions of the outer ear that play a significant role in AF. The study uses weighted support vector machines (WSVM) to classify the presence or absence of AF with 80% accuracy. Logistic regression penalized with elastic net (EN) is also used to localize the regions of the ear that contribute most to AF. The results are visually interpreted for clinical implications.
412423	41242396	Effective User Relevance Feedback for Image Retrieval with Image Signatures.	CBIR has become increasingly popular as a method for searching and retrieving digital images, due to the large number of images now available online. Relevance feedback (RF) is a technique used in search engines that involves gathering feedback from users about the relevance of search results, and using this information to improve future searches. This paper presents a new approach for using RF with signature based image retrieval, which involves the user in the process and allows them to refine their search queries. Empirical experiments have shown that this approach improves the performance of CBIR in terms of accuracy, speed, and scalability. This is a significant contribution as it addresses the problem of effectively reformulating image queries, for which there is currently no effective solution.
412423	41242327	Protein database search using compressed k-mer vocabularies.	Efficient and accurate search in biological sequence databases is crucial due to the increasing amount of genomic data available for analysis. Various methods have been developed to speed up the search process, such as explicit pairwise alignments and alignment-free techniques. However, these methods sacrifice sensitivity for speed. Other strategies, like inverted indexing and hashing, can quickly retrieve similar sequences from the database, but their precision decreases as the level of similarity decreases. To address these issues, a new search algorithm has been developed that uses a compressed vocabulary of short sub-sequences (k-mers) from the database to map each sequence to a binary feature vector. This approach incorporates biologically relevant symbol substitutions and has been shown to have comparable or better accuracy than existing methods, while significantly reducing search time.
412424	41242472	Performance of a Policy-Based Management System in IPv6 Networks Using COPS-PR	The implementation of a centralized server is the most effective way to ensure the correct Quality of Service (QoS) in new generation core networks. These networks commonly use DiffServ due to its scalability, making it suitable for large networks. A Bandwidth Broker architecture is a policy-based management system designed to centrally manage DiffServ networks through admission control and policy delivery. This paper showcases the implementation of this architecture for managing QoS in DiffServ dual networks (IPv4/IPv6). Developed using JAVA and COPS-PR as an intra-domain communication protocol, the system allows administrators to configure policies based on SLA/SLS. The performance of the architecture was evaluated using a real IPv6 DiffServ testbed, and the results and conclusions are presented.
412424	41242435	Characterizing fault propagation in safety-critical processor designs	The safety-critical systems industry is facing difficulties in achieving reduced time-to-market for electronic designs due to the certification process, which adds extra time and resources to the verification and validation process. To address this challenge, there is a demand for new tools and methods that can efficiently and cost-effectively verify robustness. While microarchitectural simulators have been used in various domains for reliability testing, their effectiveness in robustness verification is yet to be validated compared to traditional methods like RTL or gate-level simulation. This paper explores the use of fault injections in an RTL model of a processor to assess fault propagation. The results of this study will help determine the applicability of using microarchitectural simulators for robustness verification.
412425	41242534	Evaluating and improving semistructured merge	This article compares unstructured and semistructured merge tools in terms of conflict detection and resolution. While previous studies have shown that semistructured merge tools result in fewer reported conflicts, it is unclear if this actually leads to increased productivity and quality. To address this, the authors reproduce over 30,000 merges from 50 open source projects and analyze the false positives and false negatives of each approach. Their findings show that semistructured merge tools have significantly fewer false positives and that these are easier to resolve, but there is no significant difference in false negatives. Based on these results, they develop an improved semistructured merge tool that reduces the number of reported conflicts, has no additional false positives, and has fewer false negatives compared to unstructured merge. 
412425	41242535	Understanding semi-structured merge conflict characteristics in open-source Java projects.	Merge conflicts are a common issue in software development, which can significantly affect developers' productivity. However, the underlying structure of these conflicts has not been studied before. In this paper, a catalog of conflict patterns is derived by analyzing the structure of code changes that lead to merge conflicts. The study focuses on conflicts reported by a semistructured merge tool, which avoids analyzing numerous spurious conflicts. An empirical study of 70,047 merges from 123 GitHub Java projects reveals that most conflicts occur when developers independently edit the same or consecutive lines of the same method. Additionally, conflicts often involve multiple developers and are caused by copying and pasting code across repositories. The study suggests the need for further research and improvements in collaborative software development tools.
412426	41242681	On exploiting static and dynamically mined metadata for exploratory web searching	This paper discusses the limitations of current web search engines (WSEs) in meeting users' exploratory search needs, as they are designed for targeted, keyword-based searches. The authors propose a solution that incorporates both static and dynamically generated metadata to enhance the exploration aspect of web searching. This involves using online results clustering to provide users with an overview of top results, as well as utilizing various static metadata such as domain, language, date, and file type through a dynamic taxonomy and faceted exploration approach. Incremental algorithms are also designed to improve the speed of the exploration process. An experimental evaluation and user study show promising results in terms of effectiveness, flexibility, and user satisfaction. 
412426	41242615	Exploratory web searching with dynamic taxonomies and results clustering	This paper suggests using both explicit and mined metadata to enhance web searching with exploration services. Online results clustering can help users get an overview of the results and narrow down their focus. However, many WSEs only use advanced search options to utilize metadata such as domain, language, date, and filetype. To address this, the paper proposes a method that combines dynamic taxonomies and faceted exploration to make the most of both types of metadata. This approach aims to provide a more effective, flexible, and efficient exploration experience for users.
412427	41242739	Contextualization as an independent abstraction mechanism for conceptual modeling	The concept of context is relevant not only in computer science, but also in other fields. This paper proposes a framework for representing context in information modeling. Context is defined as a set of objects with names and possible references to other contexts. The framework allows for structuring context through traditional abstraction mechanisms such as classification, generalization, and attribution. The paper also explores the relationship between context and traditional abstraction mechanisms and presents a theory for contextualized information bases, including validity constraints, a model theory, and inference rules. The core theory can be expanded to support embedding of specific information models within the contextualization framework.
412427	41242719	Why-provenance information for RDF, rules, and negation	The origin of information on the Web is important for assessing its quality, trustworthiness, and understanding its context. Negative information is also important for reasoning, as long as it is associated with its source. This study focuses on collections of g-RDF ontologies, which consist of a g-RDF graph and a program with derivation rules. The goal is to determine the provenance of derived information, which is achieved by defining provenance stable models and using an algorithm based on Answer Set Programming. The results show that this approach accurately extends RDFS semantics and can handle various types of queries.
412428	4124287	A New Lower Bound on the Number of Perfect Matchings in Cubic Graphs	The article discusses the number of perfect matchings in $n$-vertex cubic bridgeless graphs. It is proven that every such graph must have at least $n/2$ perfect matchings. A list of all 17 graphs with less than $n/2+2$ perfect matchings is provided. These results have implications in the study of perfect matchings and cubic graphs.
412428	41242867	Short Cycle Covers of Graphs with Minimum Degree Three	The Alon-Tarsi conjecture states that all edges in a bridgeless graph with $m$ edges can be covered by cycles with a total length of $7m/5$. However, we have proven that even smaller total lengths are possible for certain types of graphs. For example, cubic bridgeless graphs can be covered by cycles with a total length of $34m/21$, and bridgeless graphs with a minimum degree of three can be covered by cycles with a total length of $44m/27$. These results provide stronger bounds for the shortest cycle cover conjecture. 
412429	4124293	Sliding Mode Congestion Control for data center Ethernet networks	Data centers are increasingly adopting Ethernet as the unified switch fabric, known as Data Center Ethernet. One important enhancement is the end-to-end congestion management, for which Quantized Congestion Notification (QCN) has been established as the standard. However, experiments have shown that QCN can experience queue oscillation at bottleneck links, resulting in decreased link utilization. This is due to the fact that QCN's equilibrium point is mainly reached through sliding mode motion, which is influenced by system parameters and network configurations. To address this issue, the Sliding Mode Congestion Control (SMCC) scheme has been proposed, which is simple, stable, and has a fast response time. SMCC can easily replace QCN and outperforms it in variable traffic and network conditions. Experiments on the NetFPGA platform have demonstrated the superiority of SMCC.
412429	412429113	Phase Plane Analysis of Quantized Congestion Notification for Data Center Ethernet	Ethernet is being improved to become the primary switch fabric in data centers, reducing costs and simplifying network design and management. One important enhancement is congestion management, with the recent ratification of Quantized Congestion Notification (QCN) as the formal standard. However, there has been a lack of in-depth theoretical analysis on QCN, possibly due to its heuristic design and variable structure. This paper uses the phase plane method to analyze the QCN system, presenting its dynamic behaviors and deducing sufficient conditions for stability. It is found that QCN's stable queue is mainly due to its sliding mode motion, which is confirmed by experiments on the NetFPGA platform. These analytical results can help guide proper parameter settings for QCN.
412430	41243071	The Effect of Information Utilization: Introducing a Novel Guiding Spark in the Fireworks Algorithm.	The fireworks algorithm (FWA) is a popular swarm intelligence algorithm used in various applications. A new feature called the guiding spark (GS) is introduced to enhance its performance by utilizing more information. This is achieved by constructing a guiding vector (GV) using the objective function's information from explosion sparks, and adding it to the position of the firework to generate an elite solution called a GS. The new version, called guided FWA (GFWA), shows improved performance in both exploration and exploitation compared to previous versions and other similar algorithms. The simplicity and effectiveness of the GS make it applicable to other population-based algorithms.
412430	41243064	Iterative improvement of the Multiplicative Update NMF algorithm using nature-inspired optimization.	Low-rank approximations of data, such as those based on the Singular Value Decomposition, have been widely used in various data mining applications. The Non-negative Matrix Factorization (NMF) is a special type of low-rank approximation that enforces non-negativity constraints. The Multiplicative Update (MU) algorithm, one of the original NMF algorithms, is known for its fast iterations. However, it requires a large number of iterations to accurately approximate the original data. In this paper, a new iterative update strategy for the MU algorithm, inspired by nature-based optimization algorithms, is proposed to improve its accuracy and reduce runtime. The NMF objective function used in the MU algorithm is non-differentiable, discontinuous, and prone to local minima, making heuristic search algorithms a suitable choice. Experimental results show that the new strategy achieves the same accuracy as the standard MU algorithm with significantly fewer iterations and faster overall runtime.
412431	41243118	An implementation for small databases with high availability	This technique presents a method for implementing small databases in operating systems and distributed systems using the checkpointing mechanism. It combines elements of stable storage and master-slave approaches for fault tolerance and is well-suited for systems that use the processor pool paradigm. It has been successfully used to create a monitoring service for a distributed system, with the benefits of being simple, efficient, reliable, and available.
412431	41243131	Conditional Anonymous Authentication With Abuse-Resistant Tracing and Distributed Trust for Internet of Vehicles	The Internet of Vehicles (IoV) is a system designed to improve traffic management and road safety. Vehicles communicate with each other and with traffic management infrastructure through constant message broadcasting. It is crucial to protect the cybersecurity of the IoV system, as attacks on safety-related messages could be life-threatening. To balance the need for authenticity and privacy, researchers have proposed conditional anonymous authentication, where a trusted third party can reveal the identities of malicious vehicles. However, existing techniques assume a trusted tracer, which may not always be the case. To address this, a privacy-preserving authentication scheme with abuse-resistant tracing is proposed, which prevents a single tracer from revealing the identities of vehicles and distributes the tracing key to prevent any single authority from revealing identities.
412432	4124328	Non-redundant Multi-view Clustering via Orthogonalization	This paper proposes a new clustering paradigm for explorative data analysis that aims to find all non-redundant clustering views of the data. It acknowledges that in real-world applications, data can have multiple reasonable and interesting groupings from different perspectives, especially in high-dimensional data. The proposed framework includes two approaches: orthogonal clustering, which finds alternative clusterings by seeking orthogonality in the cluster space, and clustering in orthogonal subspaces, which seeks orthogonality in the feature space. The paper presents results from testing the framework on synthetic and high-dimensional data sets, demonstrating its ability to discover varied and meaningful solutions. This approach is referred to as multi-view clustering or non-redundant clustering, as it allows for different interpretations of the data without committing to a single clustering solution. 
412432	41243240	Iterative Discovery of Multiple AlternativeClustering Views	In data analysis, complex data can be interpreted in various ways. However, most clustering algorithms only provide one solution, leaving little room for exploration. To address this issue, a new approach has been introduced that allows the user to explore multiple clustering solutions for exploratory data analysis. This approach also considers the possibility that different clusterings may exist in different subspaces. An algorithm has been developed based on an optimization procedure that takes into account cluster quality and novelty. Experiments have shown that this method outperforms other alternatives and can be used for both simultaneous and iterative discovery of multiple clusterings.
412433	41243313	Automated cellular annotation for high-resolution images of adult Caenorhabditis elegans.	Recent advancements in high-resolution microscopy have opened up the possibility of studying gene expression at the individual cell level. The fixed lineage of cells in adult Caenorhabditis elegans makes it an ideal model for investigating complex biological processes such as development and aging. However, manually annotating individual cells in images of adult C. elegans is time-consuming and requires expertise. Automating this task is crucial for conducting high-resolution studies on a large scale. In this study, researchers developed an automated method for labeling a subset of 154 cells in high-resolution images of adult C. elegans. They used a combinatorial optimization approach and a learning algorithm to compare cells in test images with a manually annotated training atlas. The results showed an 84% median accuracy, demonstrating the feasibility of automating cell annotation in adult C. elegans. 
412433	41243352	A principal skeleton algorithm for standardizing confocal images of fruit fly nervous systems.	The fruit fly (Drosophila melanogaster) is a widely used model organism in biology. We are creating a 3D digital map of the fruit fly's larval nervous system by aligning high-resolution images of specific neurons using a method called principal skeleton alignment. This method involves identifying the principal skeleton of a larval image, which is a series of connected lines that represent the shape of the nervous system, and using it to align the images. The method has been successfully applied to both simulated and real images, including those of other patterns such as the adult fruit fly's ventral nerve cord and center brain. Supplemental materials and resources for this method can be found on our website.
412434	41243455	Dividing Protein Interaction Networks by Growing Orthologous Articulations	The growing amount of data on protein-protein interaction networks has led to increased interest in comparing these networks across different species. To do this, researchers have proposed various models and algorithms for network alignment, which involves identifying conserved modules between networks. Traditionally, this involves creating a merged representation of the networks and using greedy techniques to find conserved modules. However, a new approach has been proposed that divides the networks into smaller subnets based on both graph theory and biological indicators, and then performs network alignment on these subnets. This approach has shown promising results in accurately identifying conserved modules and highlights the importance of considering orthology and articulation in comparative network analysis. 
412434	41243445	Dividing protein interaction networks for modular network comparative analysis	The growth of data on protein-protein interaction (PPI) networks has led to increased research on their comparative analysis. Recent studies have focused on network alignment, which involves comparing networks across species to identify conserved functional complexes. This paper presents an algorithm for dividing PPI networks into smaller sub-graphs that are likely to cover conserved complexes, allowing for modular network alignment. The algorithm combines a graph-theoretical property with a biological one to accurately identify orthology and articulation points. Experiments on various PPI networks show that this dividing algorithm effectively covers protein functional complexes and can enhance the performance of network alignment algorithms. The source code is available for academic use upon request.
412435	41243543	Object Reconstruction By Incorporating Geometric Constraints In Reverse Engineering	This paper presents a new method for reconstructing 3D geometric models of objects from range data. It focuses on improving the overall shape of the model by incorporating geometric constraints based on feature positions. The proposed framework allows for incremental addition and integration of constraints, resulting in a balance between minimizing shape fitting error and satisfying constraint tolerances. The paper introduces sets of constraints for planar and quadric surfaces, and demonstrates the effectiveness of the approach through experiments on synthetic and real objects. This work has significant implications for reverse engineering and manufactured object modelling, where feature relationships are commonly present. 
412435	41243564	Shape Reconstruction Incorporating Multiple Nonlinear Geometric Constraints	The paper discusses the reconstruction of 3D geometric shapes using observed measurements and multiple nonlinear shape constraints. The proposed framework allows for the incorporation of constraints during the reconstruction process, balancing the accuracy of shape fitting with the tolerance of the constraints. The paper also presents specific constraints for objects with planar and quadric surfaces, and demonstrates the effectiveness of the approach through real-world applications. This is the first work to provide a comprehensive framework for integrating numerical geometric relationships in object modeling from range data. The technique has the potential to greatly impact reverse engineering and manufactured object modeling, particularly for objects with intended feature relationships.
412436	41243689	Probabilistic management of OCR data using an RDBMS	The digitization of scanned documents is changing the way enterprises manage data. To integrate this data with existing enterprise data, OCR software is often used to convert images to ASCII text and store it in a relational database. However, OCR software can produce errors, leading to failed queries. State-of-the-art OCR programs use probabilistic models to account for these errors, but these models are usually discarded once the data is stored in the database. This study proposes retaining these probabilistic models in the database, but this poses a challenge due to the large size of the data. To address this, a new approximation scheme called Staccato is proposed, allowing users to balance query performance and accuracy. This scheme is integrated with standard RDBMS text indexing. 
412436	41243642	MYSTIQ: a system for finding more answers by using probabilities	MystiQ is a system that uses probabilistic query semantics to find answers in large numbers of data sources with low quality. This is due to various reasons such as different representations of the same data item, imperfect schema alignments, contradictory information, and fuzzy matches. Despite these challenges, users may still want to ask complex queries using SQL constructs like joins, subqueries, and quantifiers. This is useful for tasks like querying scientific data sources or finding rare associations for law enforcement. However, applying standard query semantics to these types of queries would result in empty answers. 
412437	41243760	Human and Machine Collaboration in Creative Design	The article discusses the idea of collaboration between humans and machines in the creative design process. It argues that machines cannot be creative in the same way as humans, but proposes a computational model that allows for collaboration between the two. The article then delves into the specific architectural challenges of such a system, emphasizing the complexity of design and the need for both human and machine contributions. It proposes an open and distributed architecture using software agents to facilitate collaboration and creativity. The article also addresses the roles of different agents and the challenges encountered in developing the system, including how to integrate human and machine contributions in the design process.
412437	4124371	Gesturing on the Steering Wheel: a User-elicited taxonomy	The principle of "Eyes on the road, hands on the wheel" is important when designing in-vehicle interfaces. Gesture interaction is a promising way to achieve this and improve safety. In a study, 40 participants were asked to elicit 6 gestures on the surface of the steering wheel, resulting in 240 gestures. A taxonomy of these gestures was created, providing valuable insights for designing in-vehicle gestural interfaces. This approach can help reduce driver distraction and ensure that drivers keep their eyes on the road and hands on the wheel. 
412438	41243815	Glyph: Visualization Tool for Understanding Problem Solving Strategies in Puzzle Games.	Understanding player strategies is crucial for both academic researchers and industry practitioners when analyzing player behavior. For game designers and researchers, it is important to compare intended strategies with emergent strategies to identify glitches or undesirable behaviors. This is especially relevant in serious games used for education, as player strategies can indicate their cognitive progress towards learning goals. However, current methods for analyzing player strategies have limitations, such as difficulty in scaling up for large numbers of players and subjective biases. To address these issues, a new visualization technique has been proposed that allows for both an overview of strategies and examination of individual player behaviors. This approach has been tested on data from a commercial educational puzzle game and initial results show its effectiveness in understanding player strategies.
412438	41243848	Visual attention in 3D video games	The study of players' visual attention patterns in interactive 3D games is crucial for improving game design and graphics. To achieve fast rendering speed, graphics techniques often use perception-based rendering methods. By understanding players' visual attention patterns, game designers can enhance gameplay through adjustments in level design, textures, colors, and object placement. This paper addresses this issue by presenting results on visual attention patterns in two game types: action-adventure and first-person shooter. Analyzing visual attention in complex 3D game environments poses a challenge due to rapidly changing situations. The paper discusses a new approach to studying visual attention patterns in these environments.
412439	41243923	CATaLog Online: Porting a Post-editing Tool to the Web.	CATaLog online is a new web-based translation and post-editing tool that can be accessed through a web browser. It is a free tool that only requires a simple registration. It offers editing and log functions similar to the desktop version of CATaLog, as well as new features that are described in detail in this paper. This tool allows users to post-edit both translation memory segments and machine translation output. It also provides a comprehensive set of log information, which can be used for project management and studying the translation process and translator's productivity. 
412439	41243954	Irish Treebanking and Parsing: A Preliminary Evaluation.	Language resources are crucial for linguistic research and the advancement of Natural Language Processing (NLP) applications. However, low-density languages like Irish have limited research in this area. This paper presents the first Irish dependency treebank and statistical dependency parser, which are important resources for Irish language development. The authors explain the methodology for creating the treebank and utilizing existing resources. They also discuss language-specific choices, such as prepositional attachment and clefting, and report an inter-annotator agreement of 0.7902 for a manually developed treebank. The authors also describe the initial parsing results achieved through MaltParser and propose a bootstrapping approach for further development. These new language resources will contribute to the advancement of Irish language research and NLP applications.
412440	41244026	Opinion Fluctuations and Disagreement in Social Networks	The article discusses a model of opinion dynamics that involves two types of agents: regular agents who update their beliefs based on information from their social neighbors, and stubborn agents who never change their opinions. In societies with both types of agents, there is no consensus among the regular agents and beliefs continue to fluctuate. The structure of the social network and the placement of stubborn agents influence the dynamics of opinions. The expected belief vector is shown to follow a differential equation and converge to a harmonic vector, while expected cross products of beliefs can be characterized through Markov chains. In large societies, a condition of homogeneous influence emerges, leading to similar distributions of beliefs among most regular agents.
412440	41244017	Constrained Consensus and Optimization in Multi-Agent Networks	The article discusses distributed algorithms that allow multiple agents to align their estimates with a particular value over a network with changing connectivity. This value can represent a consensus among agents or an optimal solution to an optimization problem. The focus is on constrained problems, where each agent's estimate is limited to a different constraint set. The authors present a "projected consensus algorithm" that combines local averaging with projection on individual constraint sets. The algorithm is a version of an alternating projection method with varying weights. Convergence and convergence rates are established. The authors also propose a "projected subgradient algorithm" for optimizing the sum of local objective functions subject to the intersection of constraint sets. This algorithm involves local averaging, taking subgradient steps, and projecting on constraint sets. Convergence to the same optimal solution is shown for certain cases.
412441	41244189	An Efficient Approach Toward the Asymptotic Analysis of Node-Based Recovery Algorithms in Compressed Sensing	The paper proposes a framework for analyzing node-based verification-based algorithms, where the signal length and number of non-zero elements scale with infinity. This framework is applied to study the performance of recovery algorithms in compressive sensing with random sparse matrices. It is found that there is a success threshold for the density ratio of the signal, below which the recovery algorithms are successful and above which they fail. This threshold depends on both the graph and the recovery algorithm. The results also show that there is a strong agreement between the asymptotic behavior and finite length simulations for large signal lengths. 
412441	41244151	A Hybrid Technique Using PCA and Wavelets in Network Traffic Anomaly Detection	Research into network anomaly detection has become increasingly important due to the rise in computer attacks. While there have been various approaches reported in the literature, they are often not freely available. However, the Kyoto2006+ dataset, a labeled network traffic flow dataset, is now publicly available. Most existing approaches for anomaly detection using this dataset rely on clustering techniques. This paper proposes a new approach, Hybrid PCA-Haar Wavelet Analysis, which combines two popular statistical and spectral analysis techniques: Principal Component Analysis (PCA) and Haar Wavelet filtering. The paper presents experimental results using the Kyoto2006+ dataset to demonstrate the effectiveness of this hybrid approach compared to using the two techniques individually. 
412442	41244258	Comparison of LRD and SRD traffic models for the performance evaluation of finite buffer systems	The paper compares different traffic models to determine their ability to accurately represent real traffic in a single server queue with limited buffer space. Models with long range dependence (LRD), short range dependence (SRD), and a combination of both are evaluated. The results show that models such as circulant modulated Poisson processes and fractional autoregressive integrated moving average are effective in simulating heavy load conditions, but may not accurately represent traffic under low to medium load conditions. Additionally, the accuracy of the circulant modulated Poisson process model may be compromised when fitting actual traffic data.
412442	41244264	Analysis of f-ARIMA processes in the modelling of broadband traffic	The paper explores the use of fractional AutoRegressive Integrated Moving Average (f-ARIMA) processes for traffic modelling. These models are able to accurately capture both the long range dependence (LRD) and short range dependence (SRD) features of traffic. This allows for the creation of synthetic traffic traces with a controlled ratio of LRD and SRD components. The practical relevance of this feature is tested through discrete event simulations, specifically looking at queueing performance in various working conditions. The results show that a low-order f-ARIMA model with a strong "low frequency" SRD component can significantly improve queueing performance evaluation compared to a pure LRD model. However, the f-ARIMA model may not be suitable for more complex SRD components, as seen in the case of LAN traffic.
412443	41244310	How Does Software Process Improvement Address Global Software Engineering?	SPI programs have been used for decades to improve the quality and speed of software development. There are many different SPI approaches and a lot of experience available to guide and measure these projects. SPI covers a wide range of aspects from individual developer skills to entire organizations, including optimizing software lifecycle activities and creating organization awareness and project culture. Recently, there has been a growing interest in the topic of Global Software Engineering (GSE) in relation to SPI. A systematic mapping study was conducted and 30 papers were selected for an in-depth analysis. These papers discuss the use of cultural models and agile approaches in GSE, as well as identifying success factors and barriers for implementing SPI in a GSE context. 
412443	41244376	How do software developers experience team performance in lean and agile environments?	This paper explores the challenge of evaluating team performance in a constantly changing business environment. The objective is to understand how software developers experience performance in volatile circumstances and use this knowledge to guide the formation and maintenance of high-performing teams. The study used qualitative multiple-case interviews with 16 experienced practitioners from five organizations. The results revealed 33 major categories of performance factors, which were organized into a theoretical structure to explain how software teams perceive and evaluate their performance. The study concludes that integrating soft factors, such as communication, team spirit, and team identity, into the development process is crucial for enhancing performance experiences in a dynamic business environment.
412444	41244428	Assessing the usability of a visual tool for the definition of e-learning processes	The paper discusses a usability study of a visual language-based tool called ASCLO-S, which is used for creating adaptive e-learning processes. This tool utilizes flow diagrams to define different classes of learners and their suitable adaptive learning processes. The study uses a questionnaire-based survey and an empirical analysis to gather feedback from instructional designers. The results show that both the visual notation and the system prototype are user-friendly for designers with and without experience in computer usage and e-learning tools. The empirical analysis also reveals that the effort required to develop e-learning processes is not affected by the designer's experience, but rather by the complexity of the process. Overall, the tool is considered effective in assisting designers in creating adaptive e-learning processes.
412444	41244465	Class Point: An Approach for the Size Estimation of Object-Oriented Systems	This paper introduces Class Point, a new approach for estimating the size of object-oriented products. It presents two measures that have been theoretically validated and compared to other size measures. An initial empirical validation was also conducted to evaluate the effectiveness of the measures in predicting development effort for object-oriented systems. The results show that Class Point measures are useful and accurate in estimating the size of object-oriented products. The paper highlights the importance of accurate size measures in software development and the potential for Class Point to improve estimation accuracy.
412445	41244532	Weakly-private secret sharing schemes	Secret-sharing schemes are important in cryptography for creating secure protocols. However, the size of the shares in the best known schemes for general access structures is too large to be practical. The lower bound for sharing an l-bit secret with n parties is ω(ln/log n), but no progress has been made in closing this gap. To address this issue, researchers have investigated a weaker notion of privacy in secret-sharing schemes where unauthorized parties cannot rule out any secret, rather than not learning any probabilistic information. Surprisingly, they were able to construct schemes with smaller shares for various access structures, with the size becoming more efficient as the secret size grows. For example, for a specific access structure, they were able to construct a scheme with shares of size l + n log n, and for a 2-out-of-n threshold access structure, they were able to construct a scheme with 2-bit shares, compared to ω(log n) in perfect secret-sharing schemes.
412445	41244524	Separating the Power of Monotone Span Programs over Different Fields	Monotone span programs are a computational model that uses linear algebra operations to represent and evaluate functions. They are closely related to linear secret sharing schemes and have various applications in cryptography and complexity theory. One key question is how the choice of the field in which the algebraic operations are performed affects the power of the span program. This paper proves that monotone span programs over finite fields with different characteristics are incomparable, with a super-polynomial separation between any two fields. This also leads to a super-polynomial lower bound for monotone span programs for a function in uniform-NC2, solving an open problem. The paper also shows that quasi-linear schemes are more powerful than linear secret sharing schemes, providing evidence that non-linear secret sharing schemes are more efficient.
412446	41244690	Join query optimization techniques for complex event processing applications	Complex event processing (CEP) is a technology used in modern applications to monitor and track important events in large data streams. CEP engines examine real-time information and try to identify combinations of events that match pre-defined patterns. This is similar to how traditional data management systems execute multi-join queries. However, there has been little research on using existing join optimization methods to improve the performance of CEP systems. This paper presents the first study of the relationship between these two areas. It proves that the CEP Plan Generation problem is equivalent to the Join Query Plan Generation problem and can be solved more efficiently using join query optimization techniques. Experiments show that these techniques outperform existing strategies for CEP optimization in terms of speed and memory usage. 
412446	412446116	Scheduling processing of real-time data streams on heterogeneous multi-GPU systems	The "online big data problem" is a common issue in modern computer systems, where vast amounts of data streams need to be processed within a known deadline. This paper focuses on adding hard real-time constraints to this problem, which is a challenging task. The system used in this paper has a central compute engine consisting of CPUs and GPUs, and a scheduler is used to decide where each data stream will be processed. The paper presents a new algorithm that achieves high utilization of the entire system (CPUs and GPUs) while meeting real-time constraints. The proposed algorithm was evaluated using an AES-CBC encryption kernel and showed a significant improvement in processing speed compared to a similar single-GPU system.
412447	4124472	(FPT-)Approximation Algorithms for the Virtual Network Embedding Problem.	The Virtual Network Embedding Problem (VNEP) is a common issue in cloud resource allocation, where request graphs need to be mapped onto a physical infrastructure. In the offline setting, the main objectives are profit maximization and cost minimization. This problem is a generalization of routing and call admission problems, but it is not well-understood. This paper presents the first fixed-parameter tractable approximation algorithms for VNEP, which use a novel linear program formulation to account for flexible mapping options and arbitrary request graph topologies. The algorithms have an exponential runtime in the request graphs' maximal width, but for fixed extraction width, they provide the first polynomial-time approximations. Additionally, the paper introduces the concept of extraction orders and shows that computing minimal width extraction orders and decomposable LP solutions is NP-hard.
412447	41244735	Online Admission Control and Embedding of Service Chains	The use of virtualization and software in computer networks allows for the creation and quick implementation of new network services called service chains. These service chains are sequences of virtualized network functions that route traffic between source and destination, such as firewalls, caches, and traffic optimizers. This paper focuses on the problem of admitting and embedding the maximum number of service chains, which involves allocating network functions to handle the traffic between a maximum number of source-destination pairs. An Online variant of this problem is also considered, where requests arrive in a worst-case manner over time. The main contribution of this paper is a deterministic algorithm that is O(logℓ)-competitive, assuming capacities are at least logarithmic in ℓ. The paper also explores lower bounds for offline approximation algorithms and presents an exact algorithm based on 0-1 programming for the general offline problem.
412448	41244890	Query Answering in Expressive Variants of DL-Lite	The paper discusses the use of ontologies in different application domains, such as Data Integration and the Semantic Web, where they provide access to large amounts of data. This poses a challenge in balancing the expressive power of a Description Logic with the efficiency of reasoning. The DL-Lite family of logics was designed to meet these requirements, with a focus on data complexity. The paper proposes an extension of DL-Lite called DL-Litebool, which includes full Booleans and number restrictions. Using a novel reduction to the one-variable fragment of first-order logic, the paper studies the computational and data complexity of reasoning in DL-Litebool and its sub-logics. The results provide insights into the properties of these logics and show an extension of the LogSpace upper bound for answering unions of conjunctive queries in DL-Lite to positive queries and number restrictions.
412448	41244813	Ontologies and Databases: The DL-Lite Approach	Ontologies are used to conceptualize a specific area of interest and are commonly represented using Description Logics (DLs). They are important for describing the meaning of information across different sources. The use of ontologies as a conceptual view for data repositories has gained popularity, but it is crucial that it does not add complexity to accessing the underlying data. To address this, a family of DLs called DL-Lite has been developed, which is designed to capture basic ontology and conceptual modeling languages while maintaining low complexity in reasoning and answering queries. This article discusses the major achievements of the DL-Lite family, focusing on $DL-Lite_{\mathcal{A},id}$ and its algorithms for reasoning and query answering. It also addresses the problem of accessing relational data sources through an ontology and presents a solution to the mismatch between abstract objects and data values. The QUONTO system, which implements these solutions, is also discussed. 
412449	41244967	: A Tool for Automatic Composition ofServices Based on Logics of Programs	This paper presents a technique for automatic service composition and introduces a prototype software, called ESC, that implements this technique. The approach utilizes a finite state machine to characterize the behavior of a service. By using satisfiability in a variant of Propositional Dynamic Logic, the technique is able to automatically compose a client's desired service using a given set of available services. The system is open-source and can work with services described in WSDL and finite state machine languages. This allows for the creation of a composite service that fully realizes the client's specification.
412449	412449169	Automatic Composition of Web Services in Colombo	The paper introduces Colombo, a framework that characterizes web services based on message exchanges, data flow, and effects on the real world. While these aspects have been studied separately in the past, Colombo is the first attempt to address them together in one framework. This is particularly challenging when it comes to automatically composing web services. The paper also presents new techniques for synthesizing composite web services under specific assumptions.
412450	41245048	Project talk: coordination work and group membership in WikiProjects.	WikiProjects have played a significant role in the success of Wikipedia, but their specific contributions and methods of coordination have not been thoroughly examined. In a study analyzing discussions from 138 WikiProjects, it was found that they go beyond just content production and involve non-members in their work. This research suggests that WikiProjects have a more flexible and inclusive approach to collaboration compared to other virtual teams, and may operate similarly to open source software projects rather than traditional groups.
412450	41245042	Recommending collaboration with social networks: a comparative evaluation	This paper discusses the use of social networks in workplace collaboration and how they can be incorporated into groupware systems to improve collaboration. The study evaluates the effectiveness of two different social networks in recommending potential collaborators based on expertise. It is found that these embedded social networks do not always align with individuals' personal networks, raising concerns about their use in groupware. The paper also discusses design considerations for incorporating social networks into systems. Overall, the study highlights the importance of considering social relationships in collaboration and the potential limitations of using social networks in groupware design.
412451	41245123	The Ethics of Knowledge Transfers and Conversions: Property or Privacy Rights?	This article discusses the ethical considerations surrounding the transfer of knowledge within organizations. It explores the question of whether organizations have the right to own and use the knowledge of their employees, or if this knowledge is the property of the individual and protected by human rights. The article suggests that organizational knowledge may be viewed as intellectual property, while personal knowledge is subject to personal privacy rights. This may lead to different knowledge management practices within organizations. Ultimately, the article highlights the importance of considering both ethical and legal implications when managing knowledge within organizations. 
412451	41245144	Genres of Inquiry in Design-Science Research: Justification and Evaluation of Knowledge Production	Design-science research is a relatively new paradigm that recognizes the importance of design in information systems development. This has led to a variety of knowledge goals and processes within a single study, making it difficult to associate any design-science research with a single view of knowledge production. Instead, these studies exhibit up to four different modes of reasoning, known as genres of inquiry, which are influenced by two dualities: the contrast between the knowledge goals of science and design, and the scope of the knowledge produced. As a result, a single design-science study may incorporate multiple genres of inquiry, each with its own criteria for knowledge justification and evaluation. This understanding of the diversity in genres of inquiry can advance the discourse on design-science research and its outcomes.
412452	41245250	Designing an Appropriate Information Systems Development Methodology for Different Situations	The use of information systems development methodologies has grown significantly, but there is no one-size-fits-all approach that works for all situations. This raises the question of when to use which methodology. To address this issue, a design research approach was used to create a radar diagram with eight dimensions. Three action research cycles were then conducted to validate the design in three projects involving IT project managers. The result is an artefact that can guide the selection of a specific methodology for a particular situation. This can help practitioners and researchers make informed decisions about which methodology to use.
412452	41245242	Soft design science methodology	The paper introduces a Soft Design Science approach to design science research, which aims to improve human organizations through design, development, evaluation, and evolution of a technological artifact. This approach combines the design science research process with the soft systems methodology, and iterates the design-build-evaluation process until specific and generalized requirements are met. The effectiveness of this approach is evaluated through a comparison with a previously published design science study using a design-oriented action research method. Overall, the proposed methodology offers a comprehensive and iterative approach to addressing complex problems in human organizations.
412453	41245394	AMiner: Toward Understanding Big Scholar Data.	In this talk, the speaker introduces AMiner, a new academic search and mining system that goes beyond traditional document search. AMiner focuses on modeling researchers, their publications, and the venues where they publish, creating a comprehensive network. The system automatically extracts researcher profiles and integrates them with publication data. Currently, AMiner has collected data on over 130 million researchers and 100 million papers from various databases. The system also connects with professional social networks, enhancing the metadata. A unified topic modeling approach is used to analyze different entities and provide topic-level expertise search. AMiner also offers various researcher-centered functions, such as social influence analysis and collaboration recommendations. Since its launch in 2006, AMiner has attracted over 7 million IP accesses from 200 countries/regions. 
412453	412453159	A Unified Probabilistic Framework for Name Disambiguation in Digital Library	Despite extensive research, the issue of name ambiguity remains a major challenge. Challenges in the field include finding a comprehensive approach for name disambiguation and determining the appropriate number of individuals for the disambiguation process. In this paper, a unified probabilistic framework is proposed to address these issues by incorporating both attributes and relationships. A disambiguation objective function is defined and a two-step parameter estimation algorithm is proposed. Additionally, a dynamic approach for estimating the number of individuals is investigated. Experimental results demonstrate the effectiveness of the proposed framework, outperforming four baseline methods and automatically finding the appropriate number of individuals for disambiguation.
412454	41245463	Web page title extraction and its application	This paper discusses the automatic extraction of titles from HTML documents. While authors should accurately define titles in the title field, this is often not the case. The paper proposes a supervised machine learning approach to extracting titles and outlines a specification for what constitutes a proper HTML title. Two learning methods are used, one utilizing features from the DOM Tree and the other using visual features. Combining these methods results in significant improvements in title extraction accuracy compared to a baseline method. The paper also explores the application of extracted titles in web page retrieval, showing that using both extracted titles and title fields leads to better results than using title fields alone, particularly in the task of finding named pages. 
412454	41245489	Title extraction from bodies of HTML documents and its application to web page retrieval	This paper discusses the issue of incorrect titles in HTML documents and proposes a supervised machine learning approach to automatically extract titles from the body of HTML documents. The authors propose a specification for HTML titles and use format information such as font size and position as features in title extraction. Their method outperforms the baseline method of using the largest font size as title, with a 20.9%-32.6% improvement in F1 score. The authors also evaluate the use of extracted titles in web page retrieval, showing that the combination of extracted titles and title fields is more effective than using title fields alone, particularly in the task of named page finding with a 23.1%-29.0% improvement. The proposed method is evaluated using the TREC Web Track data.
412455	41245516	A Feedback Mechanism for Network Scheduling in LambdaGrids	The future of e-Science applications will require the ability to transfer large amounts of data quickly between various computing centers and data repositories. This can be achieved through the use of a Lambda-Grid, which offers dedicated, high-speed, point-to-point connections that can be reserved for specific applications. However, this can lead to congestion at the end-systems, as their processing speeds have not kept up with networking speeds. To address this issue, researchers have proposed a lightweight end-system protocol that utilizes performance monitoring to detect potential congestion and provide feedback to the sending end-system. This protocol, called RBUDP+, improves the performance of data transfer over Lambda-Grids and has been demonstrated to be effective through network emulation.
412455	412455158	RAPID: an end-system aware protocol for intelligent data transfer over lambda grids	Next-generation e-Science applications require high-speed data transfer between distributed computing centers and data repositories. To facilitate this, lambda grid networks have been built, providing large bandwidth between endpoints through optical circuit-switched lambdas. Efficient transport protocols are needed for these dedicated circuits, as they eliminate the need for network congestion control. Previous research has shown that rate-based protocols, like RBUDP, are more effective than TCP for data transfer over lambdas. However, these protocols are not suited to handle congestion at endpoints, which can still occur with lambdas. In response, a new "Rate-Adaptive Protocol for Intelligent Delivery (RAPID)" has been developed, which uses self-monitoring and proactive feedback to adjust sending rates and improve end-to-end throughput. In testing, RAPID has been shown to reduce file transfer time and improve throughput by up to 25%.
412456	4124561	Minimizing the Data Transfer Time Using Multicore End-System Aware Flow Bifurcation	This study examines the impact of high speed network traffic, commonly used in data centers, on a commodity multicore machine. The researchers found that in certain scenarios, packet loss and degraded throughput can occur due to the machine's inability to process incoming data quickly enough. To address this issue, they propose a flow bifurcation technique that optimizes data transfer time using rate-based protocols and determines the optimal number of parallel flows and rates to utilize available bandwidth. This approach outperforms the widely used GridFTP protocol in computational grids, particularly when end-system losses occur in the receive ring buffer.
412456	41245620	Design and analysis of a model-based Covert Timing Channel for Skype traffic	In a model-based Covert Timing Channel (CTC), a sender uses a statistical model to manipulate the timing of packets generated by an overt application, such as Skype. This presents challenges when implementing the CTC system on real application traffic, as there are maximum delay requirements for the packets and potential buffer issues. To address this, the delay of multiple inter-packet delays (m-IPDs) is used to modulate the encoded symbols. The delay is partitioned to minimize buffer issues and a mathematical model is used to choose the appropriate partition for each encoded symbol. The performance of this CTC system is evaluated in real network settings and shown to be undetectable by statistical tests.
412457	412457152	Dynamic Traffic Grooming in Elastic Optical Networks	Spectrum elastic optical networks offer flexible central frequency and spectrum allocation for lightpaths. However, when provisioning a new connection in these networks, the control plane must solve the problems of electrical-layer routing and optical-layer routing and spectrum assignment (RSA). This involves determining how to route the connection through new and existing lightpaths and establishing new lightpaths with limited spectrum availability. The study proposes a multi-layer auxiliary graph that addresses these issues and allows for different traffic-grooming policies. A spectrum reservation scheme is also proposed to efficiently use the flexibility of lightpaths and reduce operational costs. The results show that incorporating the reservation scheme can improve spectrum efficiency and reduce costs in various traffic-grooming policies.
412457	41245743	Greening the Optical Backbone Network: A Traffic Engineering Approach	Telecom networks consume a large amount of energy, making green strategies desirable for Service Providers (SP) to operate their networks more efficiently. This study focuses on operating optical backbone networks with green strategies. A typical architecture is considered and the Operational Power is minimized using a Traffic Engineering (TE) approach for service provisioning. Service provisioning is broken down into multiple operations and power efficiency is analyzed for optical bypass and traffic grooming. A novel auxiliary graph is proposed to capture the flow of operations and their associated power. A Power-Aware scheme is presented based on this graph, resulting in reduced power consumption compared to a generic traffic grooming approach according to simulation results.
412458	41245820	Energy-Efficient and Thermal-Aware Resource Management for Heterogeneous Datacenters.	This paper focuses on studying the management of resources in heterogeneous datacenters, considering energy efficiency, thermal control, and performance optimization. As datacenters become more diverse, with variations in computing power, power consumption, and cooling systems, it is important to address all these factors for optimal resource management. The paper introduces the concept of a heat distribution matrix to account for the influence of servers on each other, and proposes a heuristic solution for server placement and a greedy framework for online scheduling. It also presents single-objective heuristics and a fuzzy-based priority mechanism to balance performance, energy usage, and cooling. The results, based on simulations and real measurements, demonstrate the effectiveness of the proposed approach. 
412458	4124583	Spatio-temporal thermal-aware scheduling for homogeneous high-performance computing datacenters.	Datacenters play a crucial role in today's computing infrastructure, and recent studies have highlighted the significance of thermal considerations for effective resource management. In this paper, the authors investigate thermal-aware scheduling for homogeneous high-performance computing (HPC) datacenters using a thermal model that accounts for both spatial and temporal correlations of temperature changes. They propose an online scheduling heuristic that minimizes makespan for a set of HPC applications while adhering to a thermal constraint. The heuristic utilizes a novel concept of thermal-aware load for job assignment and thermal management. To meet the temperature constraint, dynamic voltage and frequency scaling (DVFS) is employed, along with load balancing to improve makespan. The simulation results demonstrate the effectiveness of the proposed heuristic, highlighting the importance of considering both spatial and temporal factors in thermal-aware scheduling. The authors also reveal the benefits of using DVFS, which leads to both performance and energy gains in this context. 
412459	4124598	Requirements specification via activity diagrams for agent-based systems.	Goal-oriented agent systems are becoming increasingly popular for developing complex applications in dynamic environments. However, like any software, these systems require clear and well-defined system requirements in order to be successful. In this paper, the authors propose a method for improving the understandability and maintainability of requirements in a popular agent design methodology called Prometheus. This method involves automatically generating UML activity diagrams from existing requirements models, such as scenarios and goal hierarchies. The authors believe that this approach can help reduce ambiguity in the current requirements specification and provide a more structured way of representing variations. The evaluation of this approach through user experiments showed that it enhances understanding of requirements, makes it easier to modify them, and helps ensure coverage in the detailed design of the agents. This approach can also be applied to other methodologies that use similar concepts for specifying requirements.
412459	41245939	An agent-oriented approach to change propagation in software maintenance	Software maintenance and evolution is a costly and time-consuming process in the lifecycle of a software system. This paper focuses on the change propagation problem, which involves identifying secondary changes that need to be made in response to a primary change made to meet new or modified requirements. The proposed solution is an agent-oriented approach that utilizes the Object Constraint Language (OCL) and Unified Modelling Language (UML) metamodel to specify consistency rules in the design model. The underlying change propagation mechanism is based on the Belief-Desire-Intention (BDI) agent architecture and incorporates event-triggered plans. The approach also includes a method for generating repair plans from OCL constraints and a mechanism for selecting the most suitable plan based on cost, taking into account cascading and synergies between constraints. An evaluation of the approach shows its effectiveness, efficiency, and scalability.
412460	41246031	How difficult are exams? A framework for assessing the complexity of introductory programming exams	The level of difficulty of exam questions has a significant impact on student performance. To better understand the expected level of skills and knowledge at the end of a course, this paper explores using the assessment of question difficulty as a gauge. A subjective assessment and a specially designed complexity classification system were used to measure the difficulty of exam questions, specifically in introductory programming courses. The system considers factors such as external references, explicitness, linguistic and conceptual complexity, length of code, and intellectual complexity. The study examined 20 exam papers from five countries and found a wide range of difficulty levels. All measures of complexity were found to correlate with the assessment of difficulty, highlighting the importance of considering these factors in assessing learning standards in programming courses.
412460	41246034	Exploring programming assessment instruments: a classification scheme for examination questions	This paper outlines the creation of a classification system for analyzing introductory programming exams. The scheme was developed to better understand the nature and purpose of formal examination instruments used to assess students in this subject. The categories within the scheme are explained and a sample analysis of CS1 exam papers is presented. This project is part of a larger effort to examine the characteristics and intentions of educators who construct exams for introductory programming courses. 
412461	41246121	Interactive Obstruction-Free Lensing For Volumetric Data Visualization	Volumetric visualization faces challenges with occlusion, which hinders the direct visualization of the area of interest. Existing techniques like transfer functions and volume segmentation have limitations in addressing occlusion, especially when dealing with similar density datasets. A new technique has been proposed to address this issue by allowing the user to define an area of interest and using a lens to reveal hidden volumetric data. The lens is modified with a fish-eye deformation, providing a better view of the surroundings. The technique has been implemented using GPU acceleration for real-time exploration and has been demonstrated in different scenarios like baggage inspection and 3D fluid flow visualization. 
412461	41246164	Version-centric visualization of code evolution	The source code of software systems undergoes multiple changes throughout its lifecycle. To help developers understand these changes and the overall project and product, new techniques for representing and visualizing code evolution from a version-centric perspective are proposed. This includes a line-based display where each file version is represented as a column and time is shown on the horizontal axis. A version-centric layout and a constrained interaction scheme are suggested to make navigation easier. A cushion-based technique is also described to provide more information about stable evolution areas. The effectiveness of these techniques is demonstrated using real-life data sets. 
412462	41246229	Role of calibration, validation, and relevance in multi-level uncertainty integration.	Calibrating model parameters is crucial in predicting complex systems, but lack of data makes it difficult. This paper proposes a method for quantifying uncertainty in system level predictions by integrating calibration, validation, and sensitivity analysis at different levels. The approach considers the validity of models used for parameter estimation at lower levels and their relevance to the system level prediction. A model reliability metric is used to evaluate validity, and Sobol indices are used to measure relevance. These results are then integrated in a roll-up method to predict the system output. This approach allows for more accurate predictions by considering the validity and relevance of lower level tests.
412462	41246238	Model validation under epistemic uncertainty	This paper presents a methodology for evaluating the accuracy of computational models when there is uncertainty in the input data. The uncertainty can be in the form of interval data, sparse point data, or probability distributions with uncertain parameters. Two approaches are used to represent these uncertain variables as probability distributions - a parametric approach and a non-parametric approach. The resulting probabilistic model predictions are compared to experimental observations, and a generalized likelihood function is used for Bayesian updating to estimate the posterior distribution of the model output. The Bayes factor metric is then used to evaluate the validity of the model and estimate the confidence in the model prediction. A numerical example is provided to illustrate the proposed method.
412463	41246345	Eudaemonic Computing ('underwearables')	This paper discusses a framework for wearable computing that prioritizes being unobtrusive and integrated into everyday clothing. This approach, known as 'eudaemonic computing', is exemplified through the creation of the 'underwearable computer'. This computer system is designed to be worn within or under regular clothing and has evolved into a tank-top shape. This design was chosen for its even weight distribution, privacy, and proximity to the body for sensing and output capabilities. The paper also explores the use of vibrotactile technology, specifically the VibraVest, as a means of assisting the visually impaired. The success of this technology suggests the potential for other unobtrusive wearable devices in various aspects of daily life.
412463	4124631	An EyeTap video-based featureless projective motion estimation assisted by gyroscopic tracking for wearable computer mediated reality	This paper presents a cost-effective method for tracking head motion in wearable computer-mediated reality using featureless vision and inertial methods. By combining these techniques, the system is able to accurately register live video images with minimal error and can handle fast head rotations. The tracking system operates on a wearable computer with graphics hardware, which allows for fast image registration. As an example of its application, the system enables wearable computer users to share stabilized views of their surroundings with others. Overall, this method provides an efficient and accurate way to recover projective motion for head mounted cameras or EyeTap devices in wearable computer systems.
412464	41246422	First symposium on the Personal Web	The First Symposium on the Personal Web, sponsored by IBM CAS Research and colocated with CASCON 2010, aims to bring together key researchers and practitioners to focus on the research directions and challenges of the Personal Web as the next step in the evolution of the Smart Internet. The Smart Internet is envisioned as a platform that automatically aggregates data and services to support individual users' goals and tasks. The Personal Web focuses on the user's perspective, allowing for seamless integration across the web based on their context and interests. The research initiatives for the Smart Internet include Smart Interactions, which focuses on discovering and delivering relevant data and services, and Smart Services, which addresses challenges in the web architecture and infrastructure. The Personal Web emphasizes the use of semantically-linked data to enable user-driven integrations and presents new research challenges.
412464	41246465	Second Space: A Generative Model for the Blogosphere	The Blogosphere, or the network of interconnected blogs, is a complex natural phenomenon that requires synthesized data to be accurately analyzed. While graph models are widely used to analyze the web, they are not suitable for modeling the Blogosphere due to its unique structure and dynamic nature. To address this issue, an agent-based simulation model is proposed to construct blog graphs that exhibit properties similar to real-world blog networks. This model captures the linking patterns arising in the Blogosphere through local interactions between blogs. The resulting blog and post networks have key properties such as degree distributions, degree correlation, clustering coefficient, and reciprocity. Comparing to existing web models, it is evident that a specific model for the Blogosphere is needed. The structure of these networks is influenced by the characteristics of bloggers, and they provide a macroscopic and microscopic view of the Blogosphere. 
412465	41246565	On triple systems and strongly regular graphs	A Steiner triple system of order v is represented by a (v(v-1)/6,3(v-3)/2,(v+3)/2,9) strongly regular graph. While most strongly regular graphs with these parameters are the block graph of a Steiner triple system, some exceptions exist for small orders. These exceptions can be explained using the concept of switching. Similarly, Latin squares can also be treated using switching. By applying switching and constructing graphs with specific automorphisms, many new strongly regular graphs have been discovered, such as (49,18,7,6), (57,24,11,9), (64,21,8,6), (70,27,12,9), (81,24,9,6), and (100,27,10,6), which do not come from Steiner triple systems or Latin squares.
412465	4124653	The Steiner triple systems of order 19	The article discusses the use of an algorithm to classify Steiner triple systems of order 19. There are a total of 11,084,874,829 designs, each with its own automorphism group and number of Pasch configurations. Some of the designs are anti-Pasch. The classification process involves constructing an initial set of blocks, completing them with an algorithm, and rejecting isomorphic designs using graph labeling software and a vertex invariant. The designs can also be used in isomorphism tests for strongly regular graphs. The total number of nonisomorphic strongly regular graphs with certain parameters is estimated to be at least 11,084,874,829.
412466	41246637	HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion	Research on human motion and pose estimation has made significant progress in recent years, but there has been no comprehensive evaluation of different methods to determine the current standard. To address this, the HumanEva datasets were created, which consist of synchronized video and ground-truth 3D motion data captured using a specialized hardware system. These datasets contain multiple subjects performing predefined actions with multiple repetitions, resulting in a large amount of data. Standard error measures were defined to evaluate 2D and 3D pose estimation and tracking algorithms. A baseline algorithm for 3D articulated tracking was also developed, using Bayesian filtering and optimization techniques. Experiments showed that image observation models and motion priors are important factors in performance, and that Bayesian filtering performs well in a laboratory environment with initialization. The datasets and software are available to the research community, providing a foundation for the development and comparison of new methods and establishing the current state of the art in human pose estimation and tracking.
412466	41246644	Implicit Probabilistic Models of Human Motion for Synthesis and Tracking	This paper discusses the issue of probabilistically modeling 3D human motion for synthesis and tracking. Due to the complexity of human motion, it is currently not feasible to create an explicit probabilistic model from available data. Instead, the authors propose using methods from texture synthesis, which treat images as representing an implicit empirical distribution. This involves searching a large database of example motions for similar instances of a pattern. To improve efficiency, the authors propose learning a low-dimensional linear model of human motion and using it to structure the example motion database into a binary tree. A probabilistic tree search method is then used to efficiently find a sample human motion that approximates the true distribution. This method can be used for tracking human motion and has been successfully applied within a Bayesian framework. Examples of synthesizing human motion using this model are also presented.
412467	4124679	Empty pseudo-triangles in point sets	The study focuses on empty pseudo-triangles in a set of n points in the plane. These are triangles with vertices at the points and no points inside. The minimum and maximum number of these triangles is determined, with different bounds depending on whether the points are inside a triangle or not. The study also looks at optimization problems, such as finding the minimum or maximum perimeter or area of these pseudo-triangles. The time complexity for solving these problems ranges from O(n^3) to O(n^6) depending on the given conditions. Additionally, the number of star-shaped pseudo-triangles is also considered, with different bounds for their minimum and maximum numbers.
412467	41246734	Kinetic maintenance of context-sensitive hierarchical representations for disjoint simple polygons	The article explains how to create and maintain a tessellation of the free space between a group of k disjoint simple polygonal objects with a total of N vertices, where R of them are reflex. This tessellation is made up of pseudo-triangles and has properties such as containing hierarchical representations of the objects and allowing for efficient intersection with line segments in the free space. The maintenance of the tessellation is done using the Kinetic Data Structure framework and is both compact and responsive. The efficiency of the structure is shown to be O(k + κmaxlog R)log N during motion of one object, and ray shooting queries can be answered in O((k + log R) log N) time.
412468	41246862	Probabilistic Wire Resistance Degradation Due to Electromigration in Power Grids.	Electromigration (EM) is a growing issue in on-chip interconnects, especially in the computing and automotive industries. It can cause wire resistances to increase, leading to performance failure in a product's lifetime. Existing EM models have limitations in accurately capturing modern copper dual-damascene (Cu DD) metallization and accounting for circuit resilience. This paper presents a new method to analyze EM's impact on wire resistance in a circuit. It considers factors such as susceptibility and statistical behavior of void evolution. The proposed criterion identifies wires at risk of failure, taking into account steady state and system lifetime. The study also shows the robustness of on-chip power grids in maintaining supply integrity despite EM failures. 
412468	412468151	The impact of electromigration in copper interconnects on power grid integrity	Electromigration (EM) is a growing issue in on-chip interconnects that can lead to increased resistance and even open circuits. However, classical circuit-level EM models have two shortcomings: they do not accurately reflect the degradation of copper dual-damascene (CuDD) metallization and they do not account for a circuit's resilience in the face of wire failures. This study addresses both of these limitations by using a probabilistic analysis that considers the variable susceptibility of different regions of CuDD wires and the statistical behavior of void formation and growth. The study also demonstrates the robustness of on-chip power grids, which can maintain supply integrity even after EM failures occur.
412469	41246971	High-level algorithm and architecture transformations for DSP synthesis	This survey paper discusses the use of high-level transformation techniques to enhance the performance of digital signal and image processing architectures and circuits implemented using VLSI technology. It highlights the importance of carefully selecting algorithms, architectures, implementation styles, and synthesis techniques in designing successful VLSI processors. These transformations can help reduce silicon area or power consumption while maintaining speed, or increase speed while using the same amount of area. The paper reviews various transformation techniques such as pipelining, parallel processing, retiming, unfolding, and others, and how they can improve the suitability of an algorithm for a specific architectural style. 
412469	41246992	High-level DSP synthesis using concurrent transformations, scheduling, and allocation	This paper presents a new high-level synthesis methodology for dedicated digital signal processing (DSP) architectures in the MARS design system. The proposed algorithm utilizes inter-iteration and intra-iteration precedence constraints to perform concurrent scheduling and resource allocation, resulting in solutions that are comparable to previously published algorithms but with faster execution times. Unlike previous synthesis systems, MARS can handle DSP algorithms with randomly distributed delays, allowing for more efficient and general architectures. By unfolding, retiming, and pipelining the original data-flow graph, MARS is able to synthesize architectures that meet the iteration bound of any algorithm. 
412470	4124709	srcType: A Tool for Efficient Static Type Resolution	The article introduces an efficient tool for resolving static types, which is based on srcML, an XML representation of source code. This tool can determine the type of every identifier in a given code body, providing a dictionary for easy lookup. The type information includes various metadata such as constness, class membership, and file information. The tool is scalable and can generate a dictionary for large codebases like Linux in under 7 minutes. It is open source and can be downloaded from srcML.org under a GPL license.
412470	41247032	A Slice-Based Estimation Approach for Maintenance Effort	Program slicing is a technique used to estimate the effort required for maintenance of a software system. A case study of the GNU Linux kernel, spanning over 900 versions and 17 years, is presented as an example. A system dictionary is created for each version using a lightweight slicing approach, which encodes the static slice profiles for all variables in the system. Changes to the system are then represented at a behavioral level using the difference between the system dictionaries of two versions. The study analyzes three different levels of slice granularity and uses a structured format to create traceability links between changes and other software artifacts. The results show that this approach is both accurate and scalable in predicting maintenance effort.
412471	412471152	Efficient User Support with DACS Scheme	The university network faces various problems when it comes to operation and management. One of these problems is user support, which often arises during changes in setups by users. This can be a time-consuming task for system administrators as they have to provide individual support to each user. Another issue is dealing with annoying communication from certain users, which requires a lot of effort and time to identify the source. To address these challenges, a new user support system called DACS (Destination Addressing Control System) Scheme is being considered. This system aims to simplify user support tasks and make network operation and management more efficient in university networks.
412471	41247135	Simplified Network Management with DACS Scheme	The university network faces various issues during operation and management, one of which is user support problems. One specific problem is dealing with annoying communication that affects the performance and speed of other users. This can be difficult and time-consuming to address, especially with DHCP Service management. To simplify this process, a new form of user support called DACS Scheme is proposed. This system intercepts communication ports and displays a warning message on the infected client's screen, prompting the user to contact the computer management section. This eliminates the need for the system administrator to physically identify the infected client, making network management more efficient.
412472	41247216	Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation	The WMT10 and MetricsMATR10 shared tasks focused on machine translation, system combination, and evaluation. 104 machine translation systems and 41 system combination entries were manually evaluated, and their rankings were used to measure the correlation between automatic metrics and human judgments of translation quality. The study also explored hiring non-expert annotators through Amazon's Mechanical Turk to increase the number of human judgments. 26 metrics were used to assess translation quality.
412472	41247218	(Meta-) evaluation of machine translation	This paper assesses the accuracy of machine translation systems for eight language pairs, specifically translating French, German, Spanish, and Czech to and from English. The evaluation process included a thorough human evaluation, providing rankings and insights into the evaluation process. Timing and agreement among evaluators were also measured, along with the correlation between automatic evaluation metrics and human judgments. The results of this meta-evaluation shed light on the effectiveness of commonly used evaluation methods.
412473	41247323	Bit-split string-matching engines for intrusion detection and prevention	Network Intrusion Detection and Prevention Systems are crucial for providing security to those connected to a network. A key component of these systems is the string-matching algorithm, which allows for decisions to be made based on the actual content flowing through the network. However, as network speeds increase, this task becomes computationally intensive. To address this issue, a specialized device is needed that can maintain performance, be updated with new rules seamlessly, and be efficient enough to be integrated into existing network chips or wireless devices. Our approach involves a special purpose architecture and optimized string-matching algorithms, resulting in a system that is 10 times more efficient than existing methods. This is achieved by breaking down the database of strings into small state machines that search for a portion of the rules and bits of each rule.
412473	41247342	Patchable instruction ROM architecture	This paper discusses the increasing trend of integrating various components onto a single chip in embedded systems. This has led to the need for efficient instruction storage solutions that balance the conflicting goals of reduced area and rapid development times. The two main options currently available, ROM and Flash, have their own trade-offs and limitations. To address this, the authors propose a new architecture called Patchable Instruction ROM, which combines the benefits of both ROM and Flash by providing reduced area and limited post-fabrication software patching. The results show that this approach can achieve a 10% increase in area with only an 11% increase in design time compared to a Flash-based approach, making it a promising solution for embedded systems design.
412474	41247418	Semi-Supervised Classification with Graph Convolutional Networks.	Our approach for semi-supervised learning on graph-structured data uses a convolutional neural network that works directly on the graph. We use a localized first-order approximation of spectral graph convolutions to guide our choice of convolutional architecture. Our model has linear scalability with the number of graph edges and learns hidden layer representations that capture both the local graph structure and node features. Through experiments on citation networks and a knowledge graph dataset, we show that our approach outperforms similar methods by a significant margin. This demonstrates the effectiveness of our scalable approach for semi-supervised learning on graph data.
412474	41247480	Mean Field Network based Graph Refinement with application to Airway Tree Extraction.	The article discusses a method for extracting tree structures from 3D images using a graph refinement approach. The method uses an approximate Bayesian inference framework and mean field approximation to learn model parameters from training data. The model is then applied to extract airway trees from 3D chest CT data. The authors compare the performance of their method to two existing methods, one of which was a top performer in a previous airway challenge. The presented method showed significant improvement in performance compared to these two methods, as measured by centerline distance. 
412475	41247532	Network-level dynamics of diffusively coupled cells	This article discusses the study of molecular dynamics in populations of diffusively coupled cells with fast diffusive exchange. The authors propose conditions for boundedness and ultimate boundedness in systems with a singular perturbation, which expands upon existing stability results. Using these results, they demonstrate that commonly used models of intracellular dynamics lead to coordinated behavior in cell populations, with all cells converging to a common equilibrium point. The authors then focus on a specific example of coupled cells functioning as bistable switches, where the population as a whole exhibits bistable behavior by converging to a state where all cells are close to one of two equilibrium points. Finally, the practical implications of these findings for the robustness of cellular decision making in coupled populations are discussed.
412475	41247598	Identification of models of heterogeneous cell populations from population snapshot data.	Systems biology focuses on understanding the complex intracellular pathways within a "typical cell" through quantitative modeling. However, in biologically important situations, even clonal cell populations can exhibit a heterogeneous response. To study this variability, models for heterogeneous cell populations are needed. This paper presents a Bayesian approach to infer the probability density of parameters that capture the dynamics of individual cells, using population snapshot data from flow cytometric analysis. This approach can handle sparse and noisy data, and allows for assessment of parameter uncertainty. The method is evaluated using artificial data from a tumor necrosis factor signaling network model and is shown to be computationally efficient and accurate even with limited data.
412476	412476130	Transform coding for hardware-accelerated volume rendering.	Hardware-accelerated volume rendering is the standard method for real-time rendering, but limited graphics memory can be a challenge when dealing with large volume data sets. Volumetric compression, where decompression is linked to rendering, has been proven to be an effective solution. However, most existing techniques were developed for software volume rendering and are not suitable for real-time hardware-accelerated rendering. In this paper, a new block-based transform coding scheme is proposed that addresses this issue by consolidating the inverse transform with dequantization. This approach allows for precomputation of most of the reprojection, making decompression fast without sacrificing compression quality. Additionally, a new block classification scheme is introduced to preserve important features during compression. The end result is an asymmetric transform coding scheme that allows for real-time decompression of large volumes while rendering on the GPU.
412476	412476107	Multiresolution view-dependent splat based volume rendering of large irregular data	This article discusses techniques for interactive volume visualization of large, irregular data sets. Two methods for generating multiple resolutions of irregular volume grids are examined, along with a data structure that supports the splatting approach for volume rendering. These techniques are combined with a view-dependent error-based resolution selection to maintain accuracy at different zoom levels. The error tolerance can be adjusted in real time to balance frame rates and rendering accuracy. The article also presents a solution for computing gradients for lighting and demonstrates the technique on unstructured grid data from aerodynamic flow simulations. Overall, the techniques presented offer an efficient way to render irregular or meshless data sets in an interactive manner.
412477	41247743	Ensuring Time in Service Composition	This paper discusses the significance of time in service compositions, particularly in situations where meeting a deadline is essential. It introduces a framework that takes into account the execution time of services in order to improve the overall composition. The reliability of service execution times is also emphasized, considering factors such as workload and availability. This framework aims to enhance service compositions by incorporating timely and reliable service execution. 
412477	41247735	Agent negotiation protocols in time-bounded service composition	The importance of time in service compositions is discussed in this paper, particularly in situations where a service must be delivered within a deadline. The paper proposes that service execution time, workload, and provider availability should be considered to ensure reliable service compositions. This is achieved through the use of negotiation protocols, which allow for flexibility between clients and providers participating in the composition. These protocols increase the likelihood of reaching an agreement by allowing both parties to exchange proposals and adjust the negotiation terms related to service execution time. Overall, considering time as a non-functional parameter and utilizing negotiation protocols can improve the success and reliability of service compositions.
412478	412478109	Why Do Internet Services Fail, and What Can Be Done About It?	The article discusses the architecture, operational practices, and failure characteristics of three large-scale Internet services. Research was conducted through interviews with architects and operations staff, as well as examining operations databases and failure reports. The architecture of these services includes front-end and back-end nodes, redundancy, and custom software. Operationally, there is a close relationship between service developers and operators, and coordination is necessary for problem detection and repair. Operator error is the main cause of failures, and front-end nodes experience more problems but result in less unavailability. The article suggests that implementing online testing and better detection of component failures could help reduce system failure rates.
412478	4124787	Architecture and dependability of large-scale internet services	The growth of large-scale Internet infrastructure services like AOL, Google, and Hotmail has resulted in the development of unique system architectures to meet their scalability and availability needs. These architectures differ significantly from traditional systems and often utilize inexpensive personal computers, customized service software, and redundant clusters and data centers to ensure zero downtime. However, the development of these architectures has been ad hoc and little is known about the reasons for service failures or the operational practices used to maintain them. To address this, a survey is being conducted to better understand the principles behind building highly available and maintainable Internet services. This article presents initial observations from this survey.
412479	41247914	An 8.69 Mvertices/s 278 Mpixels/s tile-based 3D graphics SoC HW/SW development for consumer electronics	The paper discusses a 3D graphics system-on-a-chip (SoC) that supports OpenGL ES 1.0 and has a high performance of 8.69 Mvertices/s and 278 Mpixels/s. It also includes features such as embedded circuitry for monitoring performance and detecting bus errors, as well as the ability to capture bus traces with compression up to 98%. The SoC has a compact size of 15.7 mm2 and operates at a frequency of 139 MHz. 
412479	4124797	Statistical Estimation for Exterior Orientation from Line-to-Line Correspondences	This paper introduces a new objective function for exterior orientation estimation based on the assumption that the noise model for line correspondences is the Fisher distribution. This assumption is suitable for 3D orientation and allows for consistent estimation of unknown parameters. The objective function simultaneously estimates six parameters and provides optimal solutions with better accuracy compared to previous methods. An experimental protocol was designed to evaluate the performance of the new algorithm, which consistently outperformed the competing decoupled least squares algorithm with 10-20% less error. 
412480	41248036	Predicting Instructor'S Intervention In Mooc Forums	Instructor intervention plays a crucial role in MOOCs, where personalized interaction is limited. This paper explores the issue of predicting when instructors will intervene in student discussion forums. The proposed models take into account various factors unique to MOOCs, such as course information, forum structure, and post content. These models use latent categories to summarize individual posts and improve the accuracy of intervention prediction. Experiments on data from two Coursera MOOCs show that incorporating thread structure into the learning process results in better predictive performance. Overall, this study highlights the importance of instructor intervention and offers potential solutions for predicting it in MOOCs.
412480	41248066	Exploiting tag and word correlations for improved webpage clustering	The use of automatic webpage clustering has many benefits for information retrieval tasks. Traditionally, clustering algorithms have only used features from the page-text. However, with the rise of social-bookmarking websites, there is now a large amount of user-generated content, such as tags associated with webpages. This paper presents a new approach that combines page-text and tag information to extract more effective features for improved clustering performance. By learning a shared subspace that maximizes the correlation between these two views, any clustering algorithm can be applied. The results show that this approach outperforms other methods that use tag information in different ways. This approach can also be applied to webpage classification. Future work could explore using tag information for a smaller subset of webpages.
412481	41248137	What does classifying more than 10,000 image categories tell us?	Image classification is a crucial task for both humans and computers, but one of the challenges is the large scale of the semantic space. While humans can recognize tens of thousands of object classes and scenes, no computer vision algorithm has been tested at this scale. This paper presents a study of large scale categorization with more than 10,000 image classes. The authors find that computational issues are crucial in algorithm design, conventional wisdom from smaller datasets may not hold, and the structure of WordNet is strongly related to the difficulty of visual categorization. They also argue for the importance of dataset scale, category density, and image hierarchy in developing automatic vision algorithms for recognizing tens of thousands or even millions of image categories.
412481	41248140	Efficient classification for additive kernel SVMs.	A new method for nonlinear kernel SVMs, called additive kernels, has been developed that allows for efficient classification without relying on the number of support vectors. This class of kernels includes popular ones used in image comparison and has been shown to significantly improve accuracy while maintaining the same runtime as linear SVMs. Experiments on various datasets have proven the effectiveness of this method, making it a crucial component in state-of-the-art systems for object detection and image classification. The technique can also be applied to settings requiring evaluation of weighted additive kernels, such as kernelized versions of PCA, LDA, regression, and k-means. 
412482	41248264	A packet loss estimation model and its application to reliable mesh-based P2P video streaming	This paper presents a model for estimating packet loss probability in a mesh-based P2P network. The model considers factors such as channel packet drop rate, peer dynamics, and FEC protection, to account for the complex and heterogeneous packet loss behavior in a mesh network. The proposed model is able to accurately estimate packet loss and is used to develop a peer selection mechanism that can reduce packet loss propagation by selecting the most reliable child-peers to transmit redundant substreams. Simulation results demonstrate the effectiveness of the proposed model and peer selection mechanism in improving the overall performance of a mesh-based P2P network.
412482	41248294	A Hybrid Sender/Receiver-Driven Error Protection Scheme For Reliable P2p Scalable Video Streaming	The paper suggests a new method for transmitting scalable video packets over peer-to-peer networks with packet loss. This method uses a combination of sender and receiver-driven strategies, where child-peers subscribe to appropriate amounts of source and channel coding packets based on the estimated system uplink capacity. To address fluctuations in bandwidth due to peer dynamics, peers use a consensus-based approach to estimate the available capacity and avoid unreliable joint source-channel coding allocations. Parent-peers also use a sender-driven approach to reject low-contribution subscriptions from potential child-peers and efficiently utilize the available bandwidth. Simulation results show that this hybrid scheme outperforms other current methods in terms of visual quality.
412483	41248376	Rate-Distortion Cost Estimation for H.264/AVC	This paper discusses the usefulness of rate-distortion cost estimation in H.264/AVC applications, particularly for rate-distortion optimization and rate-control. The authors propose a new rate-prediction model and adaptive algorithm to improve the accuracy of estimating the number of coding bits for intra and inter-block encoding. Their approach uses a linear combination of existing coding parameters, which are more closely related to entropy coding and transform coefficients. They also introduce a cost estimation function and block classification method to further enhance the accuracy of their approach. The results show that their proposed schemes outperform previous methods in terms of bit-rate and rate-distortion cost estimation.
412483	4124836	Statistical rate-distortion estimation for H.264/AVC coders	The article discusses the use of statistical analyses of transform coefficients to estimate bit-rate and distortion functions directly from spatial-domain statistics. The proposed rate and distortion estimators are shown to achieve similar results to the true rate-quantization and distortion-quantization curves through simulations. These estimators have two main applications: fast rate-distortion optimized mode decision and improved rate control in video encoding. The simulations show that the estimators have sufficient accuracy and can significantly improve performance in both applications. The rate-distortion optimized mode decision saw a 51.69% reduction in cost computation time and a 0.20 dB improvement in average peak signal-to-noise ratio (PSNR) for rate-control in H.264/AVC.
412484	41248449	Reinforcement learning for robot soccer	Batch reinforcement learning methods are a valuable tool for teaching autonomous robots to learn efficiently and effectively. The authors' recent work focuses on applying this approach to a difficult and complex domain. Their paper explores different versions of the batch learning framework, specifically designed for using multilayer perceptrons to estimate value functions in continuous state spaces. These methods have proven successful in teaching essential skills to their soccer-playing robots, which have competed in the RoboCup tournament. The paper presents three case studies demonstrating the effectiveness of the batch learning approach.
412484	41248439	Reducing policy degradation in neuro-dynamic programming	Neuro-dynamic programming is a method used for learning state-action value functions. However, when combined with function approximation, there are certain challenges that arise. To address these issues, a new reinforcement learning approach has been developed that monitors the learning process and allows the learner to decide when it is best to stop learning. This leads to more stable learning outcomes. 
412485	41248550	Differential variational inequalities	The paper discusses the use of differential variational inequalities (DVIs) in modeling various applied problems that involve dynamics, inequalities, and discontinuities. It unifies different mathematical problem classes such as ordinary differential equations (ODEs), differential algebraic equations (DAEs), and dynamic complementarity systems. Conditions are presented for converting DVIs to equivalent ODEs and for numerically solving them using an Euler time-stepping procedure. The convergence of this procedure is established for initial-value and boundary-value DVIs with a maximum index of two and absolutely continuous solutions. The paper also addresses a class of DVIs for which traditional methods are not applicable. 
412485	41248567	Lyapunov Stability of Complementarity and Extended Systems	The article discusses the properties of a linear complementarity system (LCS) which is a linear dynamical system with an algebraic variable that must be a solution to a linear complementarity problem (LCP). The authors use a quadratic Lyapunov function to analyze the exponential and asymptotic stability of the LCS, taking into account the nonsmoothness of the function. They also introduce copositivity conditions to verify stability and provide examples to illustrate their results. Additionally, they extend the local stability theory to nonlinear complementarity systems and differential variational inequalities, showing that the existence of a "B-differentiable Lyapunov function" is necessary and sufficient for exponential stability.
412486	41248632	A shape-based camera angle-invariant retrieval scheme for 3D objects	 Camera angle-invariant object retrieval is essential in various applications such as security, surveillance, and place recognition. However, to achieve this, we need object images at every angle, leading to a large image database. Existing indexing schemes are not suitable for handling such a huge database, resulting in performance issues. To address this problem, a new indexing and matching scheme is proposed in this paper, focusing on two types of camera movements: horizontal rotation and fixed rotation. This scheme involves two levels of indexing structures to reduce the space requirement, namely static indexing for horizontal angle invariance and dynamic indexing for rotation invariance. Implementation of this scheme in a prototype image retrieval system showed better performance compared to other methods.
412486	41248613	Pruning and Matching Scheme for Rotation Invariant Leaf Image Retrieval	For efficient content-based image retrieval, diverse visual features like color, texture, and shape are used. For leaf images, blade-based matching is more effective than whole shape-based matching due to the unique shape of leaves. Rotational invariance is crucial for matching accuracy, and a new shape representation and matching scheme called RI-PDTW is proposed in this paper. The shape representation is a distance curve, and the matching algorithm includes techniques to speed up the process. A prototype system was implemented and showed better performance than other schemes.
412487	41248763	Clairvoyance: look-ahead compile-time scheduling.	Hardware designs have been developed to improve the performance of memory-bound applications by hiding memory latency. This is achieved through the use of out-of-order (OoO) execution engines, but this comes at the cost of increased energy consumption. Contemporary processor cores offer a range of performance and energy efficiency options, from fast and power-hungry OoO processors to slower, but more efficient in-order processors. This proposal suggests using a simple OoO core, which strikes a balance between performance and energy efficiency, to achieve higher performance for memory-bound applications. This is made possible by using a new compile-time technique called Clairvoyance, which overcomes limitations such as unknown dependencies, insufficient independent instructions, and register pressure. Clairvoyance can improve execution time by up to 13% while maintaining high performance for compute-bound applications.
412487	41248760	The load slice core microarchitecture	The development of microprocessor cores has been driven by the desire to expose instruction-level parallelism (ILP). This has led to the evolution of simple, in-order pipelines into complex, superscalar out-of-order designs. These processors not only extract ILP, but also allow for parallel cache and memory operations. However, the increasing cost of memory access in many-core processors has highlighted the need for extracting memory hierarchy parallelism (MHP) while also focusing on energy efficiency. To address this, a new core microarchitecture called the Load Slice Core has been proposed. It adds a second pipeline to the efficient in-order core, allowing for memory accesses and address-generating instructions to bypass stalled instructions. This results in a 53% performance improvement with only a 15% increase in area and 22% increase in power consumption compared to a baseline in-order processor. In addition, for power- and area-constrained many-core designs, the Load Slice Core outperforms both in-order and out-of-order designs, providing a promising direction for future processors.
412488	41248814	Vehicle routing in a Spanish distribution company: Saving using a savings-based heuristic.	The article discusses a Vehicle Routing Problem (VRP) faced by a large distribution company in Northeast Spain. The company has a diverse fleet of vehicles with different capacities and makes multiple trips per day to distribute products to 400 stores across the country. This variant of the VRP, known as Heterogeneous Fleet and Multi-trip VRP, has not been extensively studied. To solve the problem, the company uses a simple algorithm based on the savings heuristic with a biased-randomization effect and three local search operations. This approach has resulted in savings of around 12% in transportation costs, equivalent to €30 000 per week.
412488	41248833	Development and assessment of the SHARP and RandSHARP algorithms for the arc routing problem	The Capacitated Arc Routing Problem (CARP) is a combinatorial optimization problem that involves finding the most efficient route for a vehicle to service customers' demands located on edges of a graph. This is different from the Capacitated Vehicle Routing Problem (CVRP), where demands are located on nodes of a complete graph. While the CVRP has been extensively studied, there is a lack of research on the CARP. This paper proposes a new heuristic and two algorithms for the CARP, including a savings-based heuristic inspired by the Clarke and Wright algorithm for the CVRP. The results show that the new heuristic is more efficient than the traditional heuristic and can be integrated into meta-heuristics for solving larger CARPs. The paper also presents two randomized algorithms that use biased probability distributions to introduce randomness while still maintaining the heuristic's effectiveness. The results show that the randomized version of the new heuristic is superior to the randomized version of the traditional heuristic. 
412489	41248911	A Novel Mutual Authentication Scheme For Rfid Conforming Epcglobal Class 1 Generation 2 Standards	RFID technology has become increasingly popular in recent years, with many applications being discovered for its use. However, there are still security concerns that need to be addressed. A mutual authentication scheme is proposed in this paper to address issues such as privacy, replay attack, forward security, and user location privacy. The scheme utilizes the tag as a storage media based on EPCglobal Class 1 Generation 2 (CIG2) standards. Through analysis, it is determined that the proposed scheme is able to withstand known attacks and can be implemented in low-cost RFID systems using current technology.
412489	41248935	A Non-repudiated Self-service Store System Based on Portable Trusted Device	The increasing use of Personal Trusted Devices (PTDs) and advancements in wireless network technology have made them an ideal platform for offering personalized services to mobile users. However, current mobile transactions lack non-repudiation, which can be a problem for security. To address this issue, a new protocol is proposed that involves an "observer" to coordinate transactions and overcome the computing limitations of PTDs. This protocol integrates various cryptology techniques to ensure fairness, non-repudiation, anonymity, efficient verification, simplicity, and practicability. As PTDs are more portable and personal than personal computers, this scheme is expected to be widely used in PTD applications, facilitating mobile commerce and providing convenience to users in daily life.
412490	41249060	Software tool combining fault masking with user-defined recovery strategies	.The voting farm is a distributed software tool that allows for parallel message passing systems to implement a fault tolerant voting mechanism. It was developed as part of the EFTOS project and can be used alone or with other EFTOS tools. The tool can be used to implement various recovery strategies, such as restoring organs or N-modular redundancy systems with N-replicated voters. By combining these strategies with the fault masking capabilities of the voting tool, it is possible to create complex fault tolerant systems. The tool offers replication transparency, flexibility, ease-of-use, and good performance, making it a valuable resource for improving reliability. 
412490	41249070	Software Pipelining for Coarse-Grained Reconfigurable Instruction Set Processors	This paper discusses the effectiveness of software pipelining for generating code for coarse-grained reconfigurable instruction set processors. The proposed technique combines operation assignment and software pipelining, which optimizes instruction generation and scheduling for reconfigurable processors. This is in contrast to traditional compilers that perform these steps separately. By utilizing the reconfigurable resources more efficiently, the combined approach leads to a reduced cycle count. Additionally, the paper introduces a method for exploiting spatial computation within the reconfigurable unit, resulting in further cycle count reduction. The key to this technique is the assignment algorithm, which enables successful usage of software pipelining for reconfigurable processors. 
412491	41249119	A Privacy-Preserving Solution for Tracking People in Critical Environments	In highly sensitive environments, strict security measures are necessary to control physical access and monitor the location of individuals. The use of RFID technology allows for tracking and logging of people's movements, providing crucial information in case of a security incident. However, this solution raises privacy concerns. This paper proposes an RFID-based technique that allows for tracking while preserving privacy by introducing uncertainty into the tracking process. This approach implements the k-anonymity property, where the probability of correctly identifying a person's location at a given time is k-1. The proposed technique is cost-effective and has been successfully evaluated in experiments.
412491	41249179	Approximating sliding windows by cyclic tree-like histograms for efficient range queries	The main challenge in dealing with data streams is providing fast and approximate answers to range queries on sliding windows while using minimal storage space. These queries are important as they allow for the extraction of abstract knowledge from the data stream. Techniques such as histograms are effective in handling these queries, but they require multiple scans on data. In this paper, a novel histogram-based technique is proposed that supports hierarchical queries and allows for quick query answering. The method also incorporates a bit-saving approach for compressing the sliding window with minimal loss in accuracy. Experimental analysis shows that this technique outperforms existing methods. 
412492	41249258	Reasoning with minimal models: efficient algorithms and applications	The paper discusses the challenges of reasoning with minimal models in knowledge-representation systems, even for simple theories. The authors introduce the elimination algorithm, which can efficiently find and check minimal models for a subset of positive CNF theories called positive bead-cycle-free (HCF) theories. They also show that this algorithm is more effective for minimal entailment than for all positive CNF theories. The paper concludes by demonstrating how variations of the elimination algorithm can be used for efficient querying in disjunctive deductive databases and default theories. This research was published in 1997 by Elsevier Science B.V. 
412492	412492119	Detecting outlying properties of exceptional objects	This article discusses the problem of identifying the reasons for an individual's abnormality within a given dataset. A criterion is presented for measuring the abnormality of combinations of attribute values in comparison to the rest of the population. Two types of properties, global and local, are distinguished, with global properties explaining the abnormality within the entire population and local properties explaining it within a selected subpopulation. The article presents a formal characterization of this problem and proposes efficient algorithms for identifying both global and local abnormal properties. Experimental evidence shows that these algorithms are effective in mining meaningful information while examining only a small portion of the search space.
412493	41249320	TSEB: More Efficient Thompson Sampling for Policy Learning	In this paper, the authors propose a new algorithm called TSEB, which is based on Thompson Sampling and aims to solve the problem of learning in an unknown environment with tighter PAC guarantees. The algorithm maintains distributions over the model parameters and uses an exploration bonus to generate more informative exploration. This allows for better PAC bounds and helps to update the posterior over the model parameters. The authors provide a detailed analysis of the algorithm's PAC guarantees and show that the adaptive exploration bonus encourages additional exploration for better performance. Empirical analysis on simulated domains supports the effectiveness of the proposed approach.
412493	4124935	Edge Replacement Grammars: A Formal Language Approach for Generating Graphs.	Graphs are commonly used to represent structured data and a generative model that closely resembles the structure of a given set of graphs can be useful in various fields. However, current approaches require a large number of parameters to be estimated from the data, which can be computationally expensive. In this paper, the authors propose a graph generation model based on Probabilistic Edge Replacement Grammars (PERGs) and introduce a variant called Restricted PERG (RPERG). This model is able to learn the parameters of the grammar from graph data and outperforms existing methods. Despite being a context-free grammar, RPERG is able to accurately capture the structural properties of real networks. 
412494	41249416	Intrinsically Motivated Hierarchical Skill Learning in Structured Environments	In this article, the authors propose a framework for reinforcement learning agents to learn abstract skill hierarchies in structured environments. This can greatly improve the efficiency of the agents in solving complex tasks. By understanding the causal relationships between actions and their effects on different features of the environment, agents can learn incrementally and autonomously using Bayesian network structure and structured dynamic programming algorithms. The authors also introduce a novel active learning scheme that uses intrinsic motivation to maximize the efficiency of learning the structure of the environment. This approach allows for a developmental learning process in which agents continuously acquire new knowledge and skills as they explore their environment.
412494	4124946	On Ensuring that Intelligent Machines Are Well-Behaved.	Machine learning algorithms are widely used in various fields and can range from simple data analysis to achieving super-human performance. It is important to ensure that these algorithms behave ethically and do not cause harm to humans or exhibit biased behaviors. To address this issue, a new framework for designing machine learning algorithms is proposed, which simplifies the process of identifying and regulating undesirable behaviors. Experiments have shown that this framework can successfully prevent sexist and harmful behaviors in machine learning algorithms. This framework aims to promote the safe and responsible use of machine learning in various applications.
412495	41249532	Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach	The paper discusses the challenge of learning a regression model using a fixed-rank positive semidefinite matrix and its nonlinear search space. It focuses on scalability for high-dimensional problems and uses the theory of gradient descent algorithms with Riemannian geometry. Unlike previous work, no restrictions are placed on the range space of the matrix, resulting in linear complexity and invariance properties. The proposed algorithms are applied to learning a distance function with successful results on standard benchmarks. 
412495	41249516	R3MC: A Riemannian three-factor algorithm for low-rank matrix completion	R3MC is a new nonlinear conjugate-gradient method for low-rank matrix completion that uses the framework of Riemannian optimization on quotient manifolds. This method is effective at handling problems with sparsely sampled and ill-conditioned data. It introduces a novel Riemannian metric on the search space of fixed-rank matrices, specifically designed for the least-squares cost. Numerical experiments show that R3MC consistently outperforms other state-of-the-art algorithms, making it a robust and versatile approach for low-rank matrix completion problems.
412496	4124966	Planar piecewise algebraic curves	The article introduces a method for connecting sections of planar algebraic curves with continuous derivatives. It explores the use of piecewise algebraic curves in area modelling, which is the two-dimensional equivalent of solid modelling. The approach involves representing a planar rational parametric curve as a segment of an algebraic curve. The article also presents a formula for calculating the maximum distance between two algebraic curves, with one potentially being a parametric curve, within a defined area. This approach has potential applications in computer graphics and geometry.
412496	41249616	Ray tracing of Steiner patches	Steiner patches are triangular surface patches defined by quadratic polynomial functions of two variables. Recent research has discovered that these patches can also be expressed as a degree four polynomial in x,y,z, and the parameters of a point on the surface can be computed as rational polynomial functions of x,y,z. This has led to a more efficient algorithm for ray tracing Steiner patches, as the ray intersection equation is also a degree four polynomial in the parameter of the ray. This new method is a significant improvement over previous techniques for ray tracing free-form surface patches.
412497	41249753	Directed and undirected network evolution from Euler–Lagrange dynamics	This paper investigates the evolution of undirected and directed networks using the Euler-Lagrange equation and the von Neumann entropy. The equation is used to develop a variational principle based on the entropy, and correlations in the degree difference are used to determine changes in entropy over time. The model is applied to three complex network models and is found to effectively capture both undirected and directed structural transitions. It is then applied to real-world networks, including stock price data from the NYSE and gene regulatory networks in Drosophila. The model accurately simulates degree statistics and detects anomalous network evolution. Overall, the model accurately captures topological variations in evolving networks.
412497	41249740	Entropic Graph Embedding via Multivariate Degree Distributions.	The paper discusses the lack of research on using structural characterizations for embedding, clustering, and classification of directed graphs, compared to the existing methods for undirected graphs. The authors propose a new method for characterizing the structure of directed graphs, based on the distribution of the von Neumann entropy and the in and out-degree configurations associated with directed edges. This distribution is encoded as a multi-dimensional histogram and used for principal components analysis to embed directed graphs into a low dimensional space. The effectiveness of this method is demonstrated through experiments on artificial and real-world data. 
412498	41249845	Conceptual clustering in information retrieval	Clustering is a technique used in information retrieval systems to organize documents into groups based on their associations with each other. This is achieved by examining the index terms or user feedback on queries. Characterizing these clusters can further enhance the retrieval process. In this paper, the authors propose a method for developing clusters and characterizations using a user viewpoint elicited through a structured interview based on personal construct theory. This approach results in a more effective way of organizing and assigning documents to clusters, improving both the query process and the assignment of new documents. 
412498	41249831	Query Formulation through Knowledge Acquisition	In information retrieval systems, the interpretation of keywords in a query may not match the user's intended meaning due to language ambiguity. Conceptual queries aim to solve this issue by creating a semantic representation of relevant concepts using a set of keywords. However, it can be challenging for users to formulate these queries due to the complexity of specifying keywords and their relationships for each concept. This paper introduces a technique that uses knowledge acquisition to identify relevant concepts and examples of related documents. A representation of each concept is then created for use in the query. The system was validated and evaluated through experiments.
412499	41249945	Smashing the Gadgets: Hindering Return-Oriented Programming Using In-place Code Randomization	The rise of non-executable page protections in popular operating systems has led to an increase in return-oriented programming (ROP) attacks, which allow for arbitrary code execution without injecting any code. Existing defenses against ROP exploits require source code or have a high runtime overhead, limiting their effectiveness for third-party applications. This paper introduces in-place code randomization, a practical mitigation technique that can be applied directly to third-party software. By using narrow-scope code transformations that do not change the location of basic blocks, in-place code randomization can safely randomize stripped binaries without adding any code. This technique effectively eliminates and breaks a large portion of useful instruction sequences found in PE files. It also does not introduce any measurable runtime overhead, making it compatible with other exploit mitigations such as address space layout randomization. The evaluation showed that in-place code randomization successfully prevented the exploitation of vulnerable Windows 7 applications and thwarted attempts to bypass it through alternative ROP payloads.
412499	41249934	Compiler-Assisted Code Randomization	Despite extensive research, only address space layout randomization has been widely used as a defense against software exploits. Code randomization, another effective technique, has been limited by the lack of a streamlined deployment model and compatibility issues with existing software practices. In response, the authors propose compiler-assisted code randomization (CCR), which uses a combination of compiler and rewriter tools to quickly and seamlessly randomize code on end-user systems. This is achieved by embedding metadata in binaries and using a simple binary rewriter to reorder basic blocks. Their experiments show that CCR has a relatively small file size increase and negligible runtime overhead, making it a practical and compatible solution for code randomization.
